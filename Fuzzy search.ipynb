{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['was known before', 'never', 'always', 'we knew it']\n",
      "[('was known before', 57), ('we knew it', 57), ('always', 38), ('never', 24)]\n",
      "<class 'list'>\n",
      "('was known before', 57)\n",
      "<class 'tuple'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'#loop1 - creation of headers\\nwhile counter_url <= len(df_m)-1:\\n    counter_bias = 0\\n    while counter_bias <= len(df_b)-1:\\n        bias_column_name=df_b.at[counter_bias, \\'bias\\']\\n        df_m[bias_column_name] = \\'\\'\\n        counter_bias += 1\\n    counter_url += 1\\n    print(str(len(df_m)-counter_url)+\\' remained\\')\\n\\ncounter_url = 0\\n#loop2 - cell population\\nwhile counter_url <= len(df_m)-1:\\n    counter_bias = 0\\n    while counter_bias <= len(df_b)-1:\\n        bias_column_name=df_b.at[counter_bias, \\'bias\\']\\n        print(bias_column_name)\\n        df_m.at[counter_url,bias_column_name] = counter_url+counter_bias\\n        counter_bias += 1\\n    #len1=len(str(df3.at[counter,\\'text 1\\']))\\n   # text11=str(df3.at[counter,\\'text 1\\'])\\n   # text22=str(df3.at[counter,\\'text 2\\'])\\n    #df3.at[counter,\\'len1\\'] = len1\\n    counter_url += 1\\n    print(str(len(df_m)-counter_url)+\\' remained\\')\\n\\n#file creation\\nprint(df_m)\\nimport datetime\\nnow = datetime.datetime.now()\\nname = now.strftime(\"%m%d%Y%H%M%S\")\\nfile_name = \"output_fuzzy_search\" + name + \".csv\"\\ndf_m.to_csv(file_name, index = False)\\nprint(file_name)'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "#get the latest file with output_merged* prefix\n",
    "list_of_files = glob.glob('output_merged*') # * means all if need specific format then *.csv\n",
    "latest_file = max(list_of_files, key=os.path.getctime)\n",
    "#print(latest_file)\n",
    "df_m = pd.read_csv(latest_file)\n",
    "df_m = df_m.drop_duplicates(['url'])\n",
    "#print(df3)\n",
    "\n",
    "#read list of biases and keywords\n",
    "df_b = pd.read_csv('Bias indicators.csv')\n",
    "#print(df_b_i)\n",
    "\n",
    "#populate df_m\n",
    "counter_url = 0\n",
    "threshhold=0.9\n",
    "nwordsbefore=10\n",
    "nwordsafter=10\n",
    "\n",
    "#keywords\n",
    "ini_list = df_b.at[0, 'keywords']\n",
    "keywords = ini_list.strip('][').split('; ')\n",
    "print(keywords)\n",
    "\n",
    "#fuzzy search\n",
    "from fuzzywuzzy import process\n",
    "#str2Match = \"apple inc\"\n",
    "#strOptions = [\"Apple Inc.\",\"apple park\",\"apple incorporated\",\"iphone\"]\n",
    "str2Match = df_m.at[5, 'text']\n",
    "strOptions = keywords\n",
    "Ratios = process.extract(str2Match,strOptions)\n",
    "print(Ratios)\n",
    "print (type(Ratios)) \n",
    "# You can also select the string with the highest matching percentage\n",
    "highest = process.extractOne(str2Match,strOptions)\n",
    "print(highest)\n",
    "print (type(highest)) \n",
    "\n",
    "\"\"\"#loop1 - creation of headers\n",
    "while counter_url <= len(df_m)-1:\n",
    "    counter_bias = 0\n",
    "    while counter_bias <= len(df_b)-1:\n",
    "        bias_column_name=df_b.at[counter_bias, 'bias']\n",
    "        df_m[bias_column_name] = ''\n",
    "        counter_bias += 1\n",
    "    counter_url += 1\n",
    "    print(str(len(df_m)-counter_url)+' remained')\n",
    "\n",
    "counter_url = 0\n",
    "#loop2 - cell population\n",
    "while counter_url <= len(df_m)-1:\n",
    "    counter_bias = 0\n",
    "    while counter_bias <= len(df_b)-1:\n",
    "        bias_column_name=df_b.at[counter_bias, 'bias']\n",
    "        print(bias_column_name)\n",
    "        df_m.at[counter_url,bias_column_name] = counter_url+counter_bias\n",
    "        counter_bias += 1\n",
    "    #len1=len(str(df3.at[counter,'text 1']))\n",
    "   # text11=str(df3.at[counter,'text 1'])\n",
    "   # text22=str(df3.at[counter,'text 2'])\n",
    "    #df3.at[counter,'len1'] = len1\n",
    "    counter_url += 1\n",
    "    print(str(len(df_m)-counter_url)+' remained')\n",
    "\n",
    "#file creation\n",
    "print(df_m)\n",
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "name = now.strftime(\"%m%d%Y%H%M%S\")\n",
    "file_name = \"output_fuzzy_search\" + name + \".csv\"\n",
    "df_m.to_csv(file_name, index = False)\n",
    "print(file_name)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

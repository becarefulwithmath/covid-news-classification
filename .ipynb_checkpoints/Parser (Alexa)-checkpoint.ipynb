{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 link to the article\n",
      "0  https://sunnybrook.ca/research/media/item.asp?...\n",
      "1  https://www.cdc.gov/coronavirus/2019-nCoV/summ...\n",
      "2  https://theconversation.com/coronavirus-and-ca...\n",
      "3  https://www.abc.net.au/news/2020-06-28/world-r...\n",
      "4  https://www.medrxiv.org/content/10.1101/2020.0...\n",
      "5  https://www.cdc.gov/coronavirus/2019-ncov/prev...\n",
      "5 remained\n",
      "4 remained\n",
      "3 remained\n",
      "2 remained\n",
      "1 remained\n",
      "0 remained\n",
      "                                                 url  \\\n",
      "0  https://sunnybrook.ca/research/media/item.asp?...   \n",
      "1  https://www.cdc.gov/coronavirus/2019-nCoV/summ...   \n",
      "2  https://theconversation.com/coronavirus-and-ca...   \n",
      "3  https://www.abc.net.au/news/2020-06-28/world-r...   \n",
      "4  https://www.medrxiv.org/content/10.1101/2020.0...   \n",
      "5  https://www.cdc.gov/coronavirus/2019-ncov/prev...   \n",
      "\n",
      "                                              text 1  \\\n",
      "0  A team of researchers from Sunnybrook, McMaste...   \n",
      "1                                                      \n",
      "2  Most antivirals in use today target parts of a...   \n",
      "3  The world has now recorded its 10 millionth pe...   \n",
      "4                                                      \n",
      "5  CDC recommends that people wear cloth face cov...   \n",
      "\n",
      "                                              text 2  \n",
      "0                                                NaN  \n",
      "1                                                     \n",
      "2             Editions    Africa  Australia  Cana...  \n",
      "3       Skip to main content ABC News Homepage Se...  \n",
      "4          Skip to main content                  ...  \n",
      "5                                               S...  \n",
      "output_alexa06292020142748.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'def TextA(text_url):\\n    article = Article(text_url)\\n    article.download()\\n    article.html\\n    article.parse()\\n    return str(article.text)\\n\\nprint(TextA(\\'https://sunnybrook.ca/research/media/item.asp?c=2&i=2069&f=covid-19-isolated-2020#.XmqdRmlrVfY.twitter\\')\\n#print(df.iloc[0][\\'link to the article\\'])\\n#df[\\'text1\\'] = df[[\\'link to the article\\']].apply(lambda x: text1(x), axis = 1)\\n#print(df[[\\'Text1\\']])\\n\\n#for search\\nfrom googlesearch import search\\nimport csv\\nimport random\\nimport datetime\\n\\n#add article text1\\nfrom newspaper import Article\\n\\n#add article text2\\nfrom bs4 import BeautifulSoup\\nfrom bs4.element import Comment\\nimport urllib.request\\ndef tag_visible(element):\\n    if element.parent.name in [\\'style\\', \\'script\\', \\'head\\', \\'title\\', \\'meta\\', \\'[document]\\']:\\n        return False\\n    if isinstance(element, Comment):\\n        return False\\n    return True\\ndef text_from_html(body):\\n    soup = BeautifulSoup(body, \\'html.parser\\')\\n    texts = soup.findAll(text=True)\\n    visible_texts = filter(tag_visible, texts)  \\n    return u\" \".join(t.strip() for t in visible_texts)\\n\\n#file creation\\nnow = datetime.datetime.now()\\nname = now.strftime(\"%m%d%Y%H%M%S\")\\nfile_name = \"output_alexa\" + name + \".csv\"\\nprint(file_name) \\nwith open(file_name, \"a\", newline=\\'\\') as w:\\n            writer = csv.writer(w)\\n            row = [\"query_id\", \"query\", \"result position\", \"url\", \"article_text1\", \"article_text2\"]\\n            print(row)\\n            writer.writerow(row)\\n\\n#array with search queries\\narray_of_q = []\\nwith open(\"Keywords - Sample.csv\", \"r\") as f:\\n    reader = csv.reader(f)\\n    next(reader)\\n    for row in reader:\\n        array_of_q.append(row[1]+query_suffix)\\nprint(array_of_q)\\n\\n#search and results write\\nnum_id = 1\\nfor i in array_of_q:\\n    num = 0\\n    for url in search(i, tld=\\'com\\', lang=\\'en\\', stop=5, pause=random.randint(1,3)):\\n        num += 1\\n        with open(file_name, \"a\", newline=\\'\\', encoding=\\'utf-8\\') as w:\\n            writer = csv.writer(w)\\n                #get article text1\\n            article = Article(url)\\n            article.download()\\n            article.html\\n            article.parse()\\n                #get article text2\\n            html = urllib.request.urlopen(url).read()\\n                #write results\\n            row = [str(num_id), str(i), str(num), str(url), str(article.text),str(text_from_html(html))]\\n            print(row)\\n            writer.writerow(row)\\n    num_id += 1'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data read\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"Alexa data.csv\")\n",
    "array_links=df[['link to the article']]\n",
    "print(array_links)\n",
    "\n",
    "#add article text1\n",
    "from newspaper import Article\n",
    "#text1 function\n",
    "def text1(text_url: str) -> str:\n",
    "    article = Article(text_url)\n",
    "    article.download()\n",
    "    article.html\n",
    "    article.parse()\n",
    "    return str(article.text)\n",
    "\n",
    "#add article text2\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "import urllib.request\n",
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True\n",
    "def text_from_html(body):\n",
    "    soup = BeautifulSoup(body, 'html.parser')\n",
    "    texts = soup.findAll(text=True)\n",
    "    visible_texts = filter(tag_visible, texts)  \n",
    "    return u\" \".join(t.strip() for t in visible_texts)\n",
    "from urllib.error import HTTPError\n",
    "            \n",
    "\n",
    "#parsing\n",
    "df2 = pd.DataFrame(columns = ['url','text 1', 'text 2'])\n",
    "counter = 0\n",
    "while counter <= len(df)-1:\n",
    "    #print(text1(array_links.iloc[counter][0]))\n",
    "    #print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "    #df2.set_value(counter,'text 1',1)\n",
    "    url = array_links.iloc[counter][0]\n",
    "    df2.at[counter,'url'] = url\n",
    "    df2.at[counter,'text 1'] = text1(url)\n",
    "    try:\n",
    "        html = urllib.request.urlopen(url).read() \n",
    "        df2.at[counter,'text 2'] = text_from_html(html)\n",
    "    except HTTPError as e:\n",
    "        content = e.read()\n",
    "    counter += 1\n",
    "    print(str(len(df)-counter)+' remained')\n",
    "print(df2)\n",
    "\n",
    "#file creation\n",
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "name = now.strftime(\"%m%d%Y%H%M%S\")\n",
    "file_name = \"output_alexa\" + name + \".csv\"\n",
    "print(file_name) \n",
    "\n",
    "df2.to_csv(file_name, index = False)\n",
    "\n",
    "#print(text1('https://sunnybrook.ca/research/media/item.asp?c=2&i=2069&f=covid-19-isolated-2020#.XmqdRmlrVfY.twitter'))\n",
    "#df['text1'] = df[['link to the article']].apply(lambda x: text1(x), axis = 1)\n",
    "#print(df[['text1']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

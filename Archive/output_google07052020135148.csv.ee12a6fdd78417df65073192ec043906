query_id,query,result position,url,article_text1,article_text2
1,anchoring(fallacy OR bias OR heuristic) (coronavirus OR covid OR pandemic),1,https://blogs.bmj.com/bmj/2020/06/09/covid-19-and-cognitive-bias/,"Cognitive biases can shape our understanding, often at an unconscious level, of events such as the covid-19 pandemic. An awareness of these biases can lead to more effective messages and measures to mitigate the effects of the pandemic, argues Narinder Kapur

Our success in dealing with the covid-19 pandemic is partly determined by decisions and behaviour, whether that is on the part of politicians, scientists, and clinical leaders, or the general public. Such decisions and behaviour are in part determined by data and evidence, but are also determined by thinking processes. In the absence of key empirical evidence about a novel virus, thinking processes such as conjecture and speculation may play a more prominent part in decision making, and there may also be an over-reliance on models, which come with their own assumptions and implicit biases. Thinking may be biased in certain directions, and these directions may be helpful or unhelpful.1 Awareness of such biases may help to formulate more effective messages and measures to alleviate the effects of the pandemic.

Cognitive bias has frequently been discussed in general healthcare environments where it may affect both patient care and staff wellbeing,2-4 and also in science settings.5-7 Biases in public health medicine have been well recognised.8-12 There are a number of ways in which cognitive bias can be seen to play out in the covid-19 pandemic.

Confirmation bias – seeking information that supports an initial conjecture and ignoring or playing down evidence that would be contradictory. This can be seen if, for example, evidence is selected where countries have introduced strict lockdowns, physical distancing, face mask use, etc, and improvements in the trajectory of the pandemic have occurred, with evidence from countries which have been more lax, but have had similar trajectories, being given less weight. It may also lead to contextual variables in certain countries being ignored, but which may turn out to be as important as measures such as lockdown (e.g. environmental factors, population density, history of vaccination).

Stereotype bias – attaching a blanket categorising label to an individual or event. This can be seen in the use of the terms “Chinese Virus” or “Kung Flu” to describe the pandemic.

Sunk cost bias – once organisations or individuals have committed resources and reputations to a line of action, they may be reluctant to make different choices or admit to error, in spite of those lines of action now being considered as possibly mistaken. This can be seen in the reluctance of some government and expert figures to take courses of action in the pandemic which may be different from those to which they have committed significant resources or adopted major positions, even though the original commitments may now appear to be open to question. One way out of sunk cost bias is to encourage positive mood states and cognitive flexibility on the part of the key organisations and individuals.

Common practice bias – reasoning that because something is common, it therefore has validity. This was evident when the US government trade adviser Peter Navarro stated that because (as he claimed) all New York Hospitals were giving coronavirus patients the anti-malaria drug hydroxychloroquine on admission, therefore the drug was likely to be an effective remedy for the condition.

Optimism bias – the view that adverse events are more likely to happen to others than to oneself. This could be seen in the early stages of the pandemic, both in countries and in people within a country – with some western countries thinking that the pandemic would be confined to Asia, and people within a country underestimating the likelihood that they will catch the virus, and therefore ignoring public health warnings and physical distancing guidance. The key effective measures would have been greater awareness and preventative actions in those early stages, with realism replacing optimism.

Loss aversion bias – the pain of suffering a loss has more impact than the gain from an equivalent benefit. This has been evident when the consequences of the fear of catching a virus appear to outweigh the benefits from taking alternative measures. Thus, suffering chest pains and refusing to attend hospital due to fear of catching a virus at the hospital may override consideration of the benefits of having investigations for a possible cardiac illness.

Anchoring bias – excess focus on information which happens to be prominent. This can be seen when a reduction in cases of covid-19 or deaths is automatically associated with salient measures such as lockdown, when it could also be due to other factors, such as the natural evolution of the virus over time, changes in environmental variables such as temperature, etc. In clinical decision making settings during the current pandemic, such bias may be evident where shortness of breath and a high temperature are immediately diagnosed as likely being due to covid-19, while other possible important diagnoses such as sepsis may be ignored13 (although such a diagnosis of covid-19 may be understandable in the current testing climate).

Zero-sum bias – A loss to another person is erroneously seen as a gain to you, and vice versa. In the case of the coronavirus pandemic, fewer resources for some countries may mean more resources for other countries, but this may result in more infections in the former countries, which may then be more likely to spread to the latter countries.

Status quo bias – people prefer a sense of familiarity and for things to stay as they are, with inertia taking priority over action. This may in part explain why some governments were more willing to accept the status quo and ignore warnings of the devastating consequences of pandemics, even though the need for pandemic preparedness had been clearly and repeatedly stated.14 Related to this is a clinging to old habits, especially those driven by self-interest, where members of the public, and even senior figures in public life, have given in to the temptation to ignore social distancing guidance and have ventured out.

Framing bias – reactions to information may depend on how it is presented. In the case of the covid-19 pandemic, there is an abundance of some data (while certain important data are not available), and the way data are presented may well influence thinking and behaviour. In general, data which are framed so as to highlight benefits as opposed to risks will appeal to people. Thus, stating that X action will save 90/100 lives will make it more likely to be adopted compared to saying that Y action will result in the loss of 10/100 lives.

The biases listed above are of necessity provisional and tentative. As we improve our understanding of the science of covid-19, its prevention, transmission, and treatment, we may then also have a better understanding of the biases that influenced our thinking and actions about the virus, and thus which actions might be more effective in the future. One clear lesson is that government advice in the future should as much as possible be based on relevant empirical evidence,15 rather than assumptions, biases, or fears, which could be regarded as adding “noise” to a system. Where such evidence does not exist, this should be openly acknowledged, so as to avoid over-confidence in decisions which may prove erroneous, or the promulgation of fear in a way that can be harmful and contagious.16

Although my main message is that awareness of cognitive biases can lead to more effective messages and measures to mitigate the effects of the pandemic,17,18 where cognitive bias is regarded as harmful, it may be helpful to take steps to reduce such bias. Education and awareness of cognitive biases are key, so that individuals and organisations question flawed or traditional thinking habits and try to promote evidence based thinking. At an individual level, the additional advice is to slow down in your thinking, pause and reflect, and seek external views.

Narinder Kapur is a consultant neuropsychologist and visiting professor of neuropsychology at University College London, UK.

Acknowledgements: I am grateful to Veronica Bradley, Peter Jones, and Steven Kemp for their comments on this article.

Competing interests: Narinder Kapur is a member of the Confidential Reporting System for Surgery (CORESS) advisory committee.","                                                          Skip to content                  Latest  Authors   Columnists  Guest writers  Editors at large  A to Z    Topics   NHS  US healthcare  South Asia  China  Patient and public perspectives  More …             Access thebmj.com -             Covid-19 and cognitive bias   June 9, 2020        Cognitive biases can shape our understanding, often at an unconscious level, of events such as the covid-19 pandemic. An awareness of these biases can lead to more effective messages and measures to mitigate the effects of the pandemic, argues Narinder Kapur   Our success in dealing with the covid-19 pandemic is partly determined by decisions and behaviour, whether that is on the part of politicians, scientists, and clinical leaders, or the general public. Such decisions and behaviour are in part determined by data and evidence, but are also determined by thinking processes. In the absence of key empirical evidence about a novel virus, thinking processes such as conjecture and speculation may play a more prominent part in decision making, and there may also be an over-reliance on models, which come with their own assumptions and implicit biases. Thinking may be biased in certain directions, and these directions may be helpful or unhelpful. 1 Awareness of such biases may help to formulate more effective messages and measures to alleviate the effects of the pandemic.  Cognitive bias has frequently been discussed in general healthcare environments where it may affect both patient care and staff wellbeing, 2-4 and also in science settings. 5-7 Biases in public health medicine have been well recognised. 8-12 There are a number of ways in which cognitive bias can be seen to play out in the covid-19 pandemic.  Confirmation bias – seeking information that supports an initial conjecture and ignoring or playing down evidence that would be contradictory .  This can be seen if, for example, evidence is selected where countries have introduced strict lockdowns, physical distancing, face mask use, etc, and improvements in the trajectory of the pandemic have occurred, with evidence from countries which have been more lax, but have had similar trajectories, being given less weight. It may also lead to contextual variables in certain countries being ignored, but which may turn out to be as important as measures such as lockdown (e.g. environmental factors, population density, history of vaccination).  Stereotype bias – attaching a blanket categorising label to an individual or event. This can be seen in the use of the terms “Chinese Virus” or “Kung Flu” to describe the pandemic.  Sunk cost bias – once organisations or individuals have committed resources and reputations to a line of action, they may be reluctant to make different choices or admit to error, in spite of those lines of action now being considered as possibly mistaken. This can be seen in the reluctance of some government and expert figures to take courses of action in the pandemic which may be different from those to which they have committed significant resources or adopted major positions, even though the original commitments may now appear to be open to question. One way out of sunk cost bias is to encourage positive mood states and cognitive flexibility on the part of the key organisations and individuals.  Common practice bias – reasoning that because something is common, it therefore has validity. This was evident when the US government trade adviser Peter Navarro stated that because (as he claimed) all New York Hospitals were giving coronavirus patients the anti-malaria drug hydroxychloroquine on admission, therefore the drug was likely to be an effective remedy for the condition.  Optimism bias – the view that adverse events are more likely to happen to others than to oneself. This could be seen in the early stages of the pandemic, both in countries and in people within a country – with some western countries thinking that the pandemic would be confined to Asia, and people within a country underestimating the likelihood that they will catch the virus, and therefore ignoring public health warnings and physical distancing guidance. The key effective measures would have been greater awareness and preventative actions in those early stages, with realism replacing optimism.  Loss aversion bias – the pain of suffering a loss has more impact than the gain from an equivalent benefit. This has been evident when the consequences of the fear of catching a virus appear to outweigh the benefits from taking alternative measures. Thus, suffering chest pains and refusing to attend hospital due to fear of catching a virus at the hospital may override consideration of the benefits of having investigations for a possible cardiac illness.  Anchoring bias – excess focus on information which happens to be prominent. This can be seen when a reduction in cases of covid-19 or deaths is automatically associated with salient measures such as lockdown, when it could also be due to other factors, such as the natural evolution of the virus over time, changes in environmental variables such as temperature, etc. In clinical decision making settings during the current pandemic, such bias may be evident where shortness of breath and a high temperature are immediately diagnosed as likely being due to covid-19, while other possible important diagnoses such as sepsis may be ignored 13 (although such a diagnosis of covid-19 may be understandable in the current testing climate).  Zero-sum bias – A loss to another person is erroneously seen as a gain to you, and vice versa. In the case of the coronavirus pandemic, fewer resources for some countries may mean more resources for other countries, but this may result in more infections in the former countries, which may then be more likely to spread to the latter countries.  Status quo bias – people prefer a sense of familiarity and for things to stay as they are, with inertia taking priority over action. This may in part explain why some governments were more willing to accept the status quo and ignore warnings of the devastating consequences of pandemics, even though the need for pandemic preparedness had been clearly and repeatedly stated. 14 Related to this is a clinging to old habits, especially those driven by self-interest, where  members of the public, and even senior figures in public life, have given in to the temptation to ignore social distancing guidance and have ventured out.  Framing bias – reactions to information may depend on how it is presented. In the case of the covid-19 pandemic, there is an abundance of some data (while certain important data are not available), and the way data are presented may well influence thinking and behaviour. In general, data which are framed so as to highlight benefits as opposed to risks will appeal to people. Thus, stating that X action will save 90/100 lives will make it more likely to be adopted compared to saying that Y action will result in the loss of 10/100 lives.  The biases listed above are of necessity provisional and tentative. As we improve our understanding of the science of covid-19, its prevention, transmission, and treatment, we may then also have a better understanding of the biases that influenced our thinking and actions about the virus, and thus which actions might be more effective in the future. One clear lesson is that government advice in the future should as much as possible be based on relevant empirical evidence, 15 rather than assumptions, biases, or fears, which could be regarded as adding “noise” to a system. Where such evidence does not exist, this should be openly acknowledged, so as to avoid over-confidence in decisions which may prove erroneous, or the promulgation of fear in a way that can be harmful and contagious. 16   Although my main message is that awareness of cognitive biases can lead to more effective messages and measures to mitigate the effects of the pandemic, 17,18 where cognitive bias is regarded as harmful, it may be helpful to take steps to reduce such bias. Education and awareness of cognitive biases are key, so that individuals and organisations question flawed or traditional thinking habits and try to promote evidence based thinking. At an individual level, the additional advice is to slow down in your thinking, pause and reflect, and seek external views.   Narinder Kapur is a consultant neuropsychologist and visiting professor of neuropsychology at University College London, UK.  Acknowledgements: I am grateful to Veronica Bradley, Peter Jones, and Steven Kemp for their comments on this article.  Competing interests: Narinder Kapur is a member of the Confidential Reporting System for Surgery (CORESS) advisory committee.   Van Bavel J, Baicker K, Boggio P, et al. Using social and behavioural science to support COVID-19 pandemic response. Nature Human Behaviour, doi – 10.31234/osf.io/y38m9  Kapur, N. Unconscious bias harms patients and staff. BMJ 2015;351:h6347.  Croskerry P. Cognitive Autopsy: A root cause analysis of medical decision-making . Oxford University Press, 2020.  O’Sullivan E, Schofield S. Cognitive bias in clinical medicine. J R Coll Physicians Edinb 2018;48:225-232.  Jager K, Tripepi G, Chesnaye N, et al. Where to look for the most frequent biases? Nephrology 2020;1-7.  Matute H, Blanco F, Yarritu I, et al. Illusions of causality: how they bias our everyday thinking and how they could be reduced. Frontiers in Psychology 2015;6:1-14.  Blanco F, Matute H. The illusion of causality: A cognitive bias underlying pseudoscience. In: Kaufman A, Kaufman J (Eds), Pseudoscience. The Conspiracy against Science . MIT Press, Cambridge: Mass, 2018:45-75.  Kelly M. Cognitive biases in public health and how economics and sociology can help overcome them. Public Health 2019;169:163-172.  Hansen C, North A, Niccolal L. Cognitive bias in clinicians’ communication about human papillomavirus vaccination. Health Communication 2020;35:430-437.  Olsen J, Jensen U. Causal criteria: time has come for a revision. European Journal of Epidemiology 2019;34:537-41.  Grimes D, Schulz K. Bias and causal associations in observational research. The Lancet 2002;359:248-252.  Pomares T, Buttenheim A, Amin A, et al. Association of cognitive biases with human papillomavirus vaccine hesitancy. Human Vaccines & Immunotherapies , 2019, 1-6.  Brown L. COVID blindness. Diagnosis 2020;7:83-84.  Gates B. Innovation for pandemics. N Engl J Med 2018;378:2057-60.   Haushofer J, Metcalf J. Which interventions work best in a pandemic? Science 2020; 10.1126/science.abb6144  Debiec J. Fear can spread from person to person faster than the coronavirus – but there are ways to slow it down. Discover Magazine , March 25, 2020.  Soofi M, Najafi F, Karami-Matin B. Using insights from behavioural economics to mitigate the spread of COVID-19. Appl Health Econ Health  Policy 2020;18:345-50.  Jetten J, Reicher S, Haslam A, et al. Together Apart. The Psychology of COVID-19 . London: Sage Publishing, 2020.      Global health       Post navigation   Michele Acuto: Covid-19 should spur us to think like cities Non communicable diseases and covid-19: a perfect storm    Comment and opinion from The BMJ's international community of readers, authors, and editors       Access bmj.com      Search         Most Read Devi Sridhar and Adriel Chen: Why Scotland’s… US purchases world stocks of Remdesivir—why the rest… Paul Garner: Covid-19 at 14 weeks—phantom speed… Categories   Author's perspective   BMJ Clinical Evidence   Brexit   China   Christmas appeal   Climate change   Columnists   Abraar Karan   Andy Cowper   Billy Boland   Daniel Sokol   David Kerr   David Lock   David Oliver   Desmond O'Neill   Douglas Noble   Edzard Ernst   From the other side   Gerd Gigerenzer   Giles Maskell   Harlan Krumholz   Hilda Bastian   Iain Chalmers   James Raftery's NICE blogs   Jeff Aronson's Words   Jim Murray   Julian Sheather   Julie K Silver   Kieran Walsh   Liz Wager   Marge Berer   Martin McKee   Martin McShane   Mary E Black   Mary Higgins   Matt Morgan   Metaphor watch   Muir Gray   Neal Maskrey   Neena Modi   Nick Hopkinson   Paul Glasziou   Penny Campling   Peter Brindley   Pritpal S Tamber   Rachel Clarke   Richard Lehman   Richard Smith   Sandra Lako   Sharon Roman   Sian Griffiths   Siddhartha Yadav   Simon Chapman   Tara Lamont   Tiago Villanueva   Tom Jefferson   Tracey Koehlmoos   William Cayley     Editors at large   Anita Jain   Anya de Iongh   Birte Twisselmann   Carl Heneghan   David Payne   Domhnall MacAuley   Elizabeth Loder   Fiona Godlee   Georg Röggla   Juliet Dobson   Paul Simpson   Readers' editor   Robin Baddeley   Sally Carter   Tessa Richards   The BMJ today     Featured   From the archive   Global health   Global health disruptors     Guest writers   The King's fund     Junior doctors   Literature and medicine   Medical ethics   MSF   NHS   Open data   Partnership in practice   Patient and public perspectives   Richard Lehman's weekly review of medical journals   South Asia   Students   Too much medicine   Uncategorized   Unreported trial of the week   US healthcare   Weekly review of medical journals   Wellbeing    BMJ CAREERS            Information for Authors  BMJ Opinion provides comment and opinion written by The BMJ's international community of readers, authors, and editors.  We welcome submissions for consideration. Your article should be clear, compelling, and appeal to our international readership of doctors and other health professionals. The best pieces make a single topical point. They are well argued with new insights.  For more information on how to submit, please see our instructions for authors.   Cookie Settings                 Contact us   Website terms & conditions   Privacy policy   Revenue sources   Home   Top   © BMJ Publishing Group Limited 2020. All rights reserved.        "
1,anchoring(fallacy OR bias OR heuristic) (coronavirus OR covid OR pandemic),2,https://www.sciencedirect.com/science/article/abs/pii/S1053535710001411,"The anchoring effect is one of the most robust cognitive heuristics. This paper reviews the literature in this area including various different models, explanations and underlying mechanisms used to explain anchoring effects. The anchoring effect is both robust and has many implications in all decision making processes. This review paper documents the many different domains and tasks in which the effect has been shown. It also considers mood and individual difference (ability, personality, information styles) correlates of anchoring as well as the effect of motivation and knowledge on decisions affected by anchoring. Finally the review looks at the applicants of the anchoring effects in everyday life.","                                                          Skip to content                  Latest  Authors   Columnists  Guest writers  Editors at large  A to Z    Topics   NHS  US healthcare  South Asia  China  Patient and public perspectives  More …             Access thebmj.com -             Covid-19 and cognitive bias   June 9, 2020        Cognitive biases can shape our understanding, often at an unconscious level, of events such as the covid-19 pandemic. An awareness of these biases can lead to more effective messages and measures to mitigate the effects of the pandemic, argues Narinder Kapur   Our success in dealing with the covid-19 pandemic is partly determined by decisions and behaviour, whether that is on the part of politicians, scientists, and clinical leaders, or the general public. Such decisions and behaviour are in part determined by data and evidence, but are also determined by thinking processes. In the absence of key empirical evidence about a novel virus, thinking processes such as conjecture and speculation may play a more prominent part in decision making, and there may also be an over-reliance on models, which come with their own assumptions and implicit biases. Thinking may be biased in certain directions, and these directions may be helpful or unhelpful. 1 Awareness of such biases may help to formulate more effective messages and measures to alleviate the effects of the pandemic.  Cognitive bias has frequently been discussed in general healthcare environments where it may affect both patient care and staff wellbeing, 2-4 and also in science settings. 5-7 Biases in public health medicine have been well recognised. 8-12 There are a number of ways in which cognitive bias can be seen to play out in the covid-19 pandemic.  Confirmation bias – seeking information that supports an initial conjecture and ignoring or playing down evidence that would be contradictory .  This can be seen if, for example, evidence is selected where countries have introduced strict lockdowns, physical distancing, face mask use, etc, and improvements in the trajectory of the pandemic have occurred, with evidence from countries which have been more lax, but have had similar trajectories, being given less weight. It may also lead to contextual variables in certain countries being ignored, but which may turn out to be as important as measures such as lockdown (e.g. environmental factors, population density, history of vaccination).  Stereotype bias – attaching a blanket categorising label to an individual or event. This can be seen in the use of the terms “Chinese Virus” or “Kung Flu” to describe the pandemic.  Sunk cost bias – once organisations or individuals have committed resources and reputations to a line of action, they may be reluctant to make different choices or admit to error, in spite of those lines of action now being considered as possibly mistaken. This can be seen in the reluctance of some government and expert figures to take courses of action in the pandemic which may be different from those to which they have committed significant resources or adopted major positions, even though the original commitments may now appear to be open to question. One way out of sunk cost bias is to encourage positive mood states and cognitive flexibility on the part of the key organisations and individuals.  Common practice bias – reasoning that because something is common, it therefore has validity. This was evident when the US government trade adviser Peter Navarro stated that because (as he claimed) all New York Hospitals were giving coronavirus patients the anti-malaria drug hydroxychloroquine on admission, therefore the drug was likely to be an effective remedy for the condition.  Optimism bias – the view that adverse events are more likely to happen to others than to oneself. This could be seen in the early stages of the pandemic, both in countries and in people within a country – with some western countries thinking that the pandemic would be confined to Asia, and people within a country underestimating the likelihood that they will catch the virus, and therefore ignoring public health warnings and physical distancing guidance. The key effective measures would have been greater awareness and preventative actions in those early stages, with realism replacing optimism.  Loss aversion bias – the pain of suffering a loss has more impact than the gain from an equivalent benefit. This has been evident when the consequences of the fear of catching a virus appear to outweigh the benefits from taking alternative measures. Thus, suffering chest pains and refusing to attend hospital due to fear of catching a virus at the hospital may override consideration of the benefits of having investigations for a possible cardiac illness.  Anchoring bias – excess focus on information which happens to be prominent. This can be seen when a reduction in cases of covid-19 or deaths is automatically associated with salient measures such as lockdown, when it could also be due to other factors, such as the natural evolution of the virus over time, changes in environmental variables such as temperature, etc. In clinical decision making settings during the current pandemic, such bias may be evident where shortness of breath and a high temperature are immediately diagnosed as likely being due to covid-19, while other possible important diagnoses such as sepsis may be ignored 13 (although such a diagnosis of covid-19 may be understandable in the current testing climate).  Zero-sum bias – A loss to another person is erroneously seen as a gain to you, and vice versa. In the case of the coronavirus pandemic, fewer resources for some countries may mean more resources for other countries, but this may result in more infections in the former countries, which may then be more likely to spread to the latter countries.  Status quo bias – people prefer a sense of familiarity and for things to stay as they are, with inertia taking priority over action. This may in part explain why some governments were more willing to accept the status quo and ignore warnings of the devastating consequences of pandemics, even though the need for pandemic preparedness had been clearly and repeatedly stated. 14 Related to this is a clinging to old habits, especially those driven by self-interest, where  members of the public, and even senior figures in public life, have given in to the temptation to ignore social distancing guidance and have ventured out.  Framing bias – reactions to information may depend on how it is presented. In the case of the covid-19 pandemic, there is an abundance of some data (while certain important data are not available), and the way data are presented may well influence thinking and behaviour. In general, data which are framed so as to highlight benefits as opposed to risks will appeal to people. Thus, stating that X action will save 90/100 lives will make it more likely to be adopted compared to saying that Y action will result in the loss of 10/100 lives.  The biases listed above are of necessity provisional and tentative. As we improve our understanding of the science of covid-19, its prevention, transmission, and treatment, we may then also have a better understanding of the biases that influenced our thinking and actions about the virus, and thus which actions might be more effective in the future. One clear lesson is that government advice in the future should as much as possible be based on relevant empirical evidence, 15 rather than assumptions, biases, or fears, which could be regarded as adding “noise” to a system. Where such evidence does not exist, this should be openly acknowledged, so as to avoid over-confidence in decisions which may prove erroneous, or the promulgation of fear in a way that can be harmful and contagious. 16   Although my main message is that awareness of cognitive biases can lead to more effective messages and measures to mitigate the effects of the pandemic, 17,18 where cognitive bias is regarded as harmful, it may be helpful to take steps to reduce such bias. Education and awareness of cognitive biases are key, so that individuals and organisations question flawed or traditional thinking habits and try to promote evidence based thinking. At an individual level, the additional advice is to slow down in your thinking, pause and reflect, and seek external views.   Narinder Kapur is a consultant neuropsychologist and visiting professor of neuropsychology at University College London, UK.  Acknowledgements: I am grateful to Veronica Bradley, Peter Jones, and Steven Kemp for their comments on this article.  Competing interests: Narinder Kapur is a member of the Confidential Reporting System for Surgery (CORESS) advisory committee.   Van Bavel J, Baicker K, Boggio P, et al. Using social and behavioural science to support COVID-19 pandemic response. Nature Human Behaviour, doi – 10.31234/osf.io/y38m9  Kapur, N. Unconscious bias harms patients and staff. BMJ 2015;351:h6347.  Croskerry P. Cognitive Autopsy: A root cause analysis of medical decision-making . Oxford University Press, 2020.  O’Sullivan E, Schofield S. Cognitive bias in clinical medicine. J R Coll Physicians Edinb 2018;48:225-232.  Jager K, Tripepi G, Chesnaye N, et al. Where to look for the most frequent biases? Nephrology 2020;1-7.  Matute H, Blanco F, Yarritu I, et al. Illusions of causality: how they bias our everyday thinking and how they could be reduced. Frontiers in Psychology 2015;6:1-14.  Blanco F, Matute H. The illusion of causality: A cognitive bias underlying pseudoscience. In: Kaufman A, Kaufman J (Eds), Pseudoscience. The Conspiracy against Science . MIT Press, Cambridge: Mass, 2018:45-75.  Kelly M. Cognitive biases in public health and how economics and sociology can help overcome them. Public Health 2019;169:163-172.  Hansen C, North A, Niccolal L. Cognitive bias in clinicians’ communication about human papillomavirus vaccination. Health Communication 2020;35:430-437.  Olsen J, Jensen U. Causal criteria: time has come for a revision. European Journal of Epidemiology 2019;34:537-41.  Grimes D, Schulz K. Bias and causal associations in observational research. The Lancet 2002;359:248-252.  Pomares T, Buttenheim A, Amin A, et al. Association of cognitive biases with human papillomavirus vaccine hesitancy. Human Vaccines & Immunotherapies , 2019, 1-6.  Brown L. COVID blindness. Diagnosis 2020;7:83-84.  Gates B. Innovation for pandemics. N Engl J Med 2018;378:2057-60.   Haushofer J, Metcalf J. Which interventions work best in a pandemic? Science 2020; 10.1126/science.abb6144  Debiec J. Fear can spread from person to person faster than the coronavirus – but there are ways to slow it down. Discover Magazine , March 25, 2020.  Soofi M, Najafi F, Karami-Matin B. Using insights from behavioural economics to mitigate the spread of COVID-19. Appl Health Econ Health  Policy 2020;18:345-50.  Jetten J, Reicher S, Haslam A, et al. Together Apart. The Psychology of COVID-19 . London: Sage Publishing, 2020.      Global health       Post navigation   Michele Acuto: Covid-19 should spur us to think like cities Non communicable diseases and covid-19: a perfect storm    Comment and opinion from The BMJ's international community of readers, authors, and editors       Access bmj.com      Search         Most Read Devi Sridhar and Adriel Chen: Why Scotland’s… US purchases world stocks of Remdesivir—why the rest… Paul Garner: Covid-19 at 14 weeks—phantom speed… Categories   Author's perspective   BMJ Clinical Evidence   Brexit   China   Christmas appeal   Climate change   Columnists   Abraar Karan   Andy Cowper   Billy Boland   Daniel Sokol   David Kerr   David Lock   David Oliver   Desmond O'Neill   Douglas Noble   Edzard Ernst   From the other side   Gerd Gigerenzer   Giles Maskell   Harlan Krumholz   Hilda Bastian   Iain Chalmers   James Raftery's NICE blogs   Jeff Aronson's Words   Jim Murray   Julian Sheather   Julie K Silver   Kieran Walsh   Liz Wager   Marge Berer   Martin McKee   Martin McShane   Mary E Black   Mary Higgins   Matt Morgan   Metaphor watch   Muir Gray   Neal Maskrey   Neena Modi   Nick Hopkinson   Paul Glasziou   Penny Campling   Peter Brindley   Pritpal S Tamber   Rachel Clarke   Richard Lehman   Richard Smith   Sandra Lako   Sharon Roman   Sian Griffiths   Siddhartha Yadav   Simon Chapman   Tara Lamont   Tiago Villanueva   Tom Jefferson   Tracey Koehlmoos   William Cayley     Editors at large   Anita Jain   Anya de Iongh   Birte Twisselmann   Carl Heneghan   David Payne   Domhnall MacAuley   Elizabeth Loder   Fiona Godlee   Georg Röggla   Juliet Dobson   Paul Simpson   Readers' editor   Robin Baddeley   Sally Carter   Tessa Richards   The BMJ today     Featured   From the archive   Global health   Global health disruptors     Guest writers   The King's fund     Junior doctors   Literature and medicine   Medical ethics   MSF   NHS   Open data   Partnership in practice   Patient and public perspectives   Richard Lehman's weekly review of medical journals   South Asia   Students   Too much medicine   Uncategorized   Unreported trial of the week   US healthcare   Weekly review of medical journals   Wellbeing    BMJ CAREERS            Information for Authors  BMJ Opinion provides comment and opinion written by The BMJ's international community of readers, authors, and editors.  We welcome submissions for consideration. Your article should be clear, compelling, and appeal to our international readership of doctors and other health professionals. The best pieces make a single topical point. They are well argued with new insights.  For more information on how to submit, please see our instructions for authors.   Cookie Settings                 Contact us   Website terms & conditions   Privacy policy   Revenue sources   Home   Top   © BMJ Publishing Group Limited 2020. All rights reserved.        "
2,(availability or ease of recall)(fallacy OR bias OR heuristic) (coronavirus OR covid OR pandemic),1,https://en.wikipedia.org/wiki/Availability_heuristic,"The availability heuristic, also known as availability bias, is a mental shortcut that relies on immediate examples that come to a given person's mind when evaluating a specific topic, concept, method or decision. The availability heuristic operates on the notion that if something can be recalled, it must be important, or at least more important than alternative solutions which are not as readily recalled.[1] Subsequently, under the availability heuristic, people tend to heavily weigh their judgments toward more recent information, making new opinions biased toward that latest news.[2][3]

The availability of consequences associated with an action is positively related to perceptions of the magnitude of the consequences of that action. In other words, the easier it is to recall the consequences of something the greater those consequences are often perceived to be. Most notably, people often rely on the content of their recall if its implications are not called into question by the difficulty that they experience in bringing the relevant material to mind.[4]

Overview and history [ edit ]

In the late 1960s and early 1970s, Amos Tversky and Daniel Kahneman began work on a series of papers examining ""heuristic and biases"" used in the judgment under uncertainty. Prior to that, the predominant view in the field of human judgment was that humans are rational actors. Kahneman and Tversky explained that judgment under uncertainty often relies on a limited number of simplifying heuristics rather than extensive algorithmic processing. Soon, this idea spread beyond academic psychology, into law, medicine, and political science. This research questioned the descriptive adequacy of idealized models of judgment, and offered insights into the cognitive processes that explained human error without invoking motivated irrationality.[5] One simplifying strategy people may rely on is the tendency to make a judgment about the frequency of an event based on how many similar instances are brought to mind. In 1973, Amos Tversky and Daniel Kahneman first studied this phenomenon and labeled it the ""availability heuristic"". An availability heuristic is a mental shortcut that relies on immediate examples that come to a given person's mind when evaluating a specific topic, concept, method or decision. As follows, people tend to use a readily available fact to base their beliefs about a comparably distant concept. There has been much research done with this heuristic, but studies on the issue are still questionable with regard to the underlying process. Studies illustrate that manipulations intended to increase the subjective experience of ease of recall are also likely to affect the amount of recall. Furthermore, this makes it difficult to determine if the obtained estimates of frequency, likelihood, or typicality are based on participants' phenomenal experiences or on a biased sample of recalled information.[5]

However, some textbooks have chosen the latter interpretation introducing the availability heuristic as ""one's judgments are always based on what comes to mind"". For example, if a person is asked whether there are more words in the English language that begin with a t or k, the person will probably be able to think of more words that begin with the letter t, concluding that t is more frequent than k.[6]

Research [ edit ]

Chapman (1967) described a bias in the judgment of the frequency with which two events co-occur. This demonstration showed that the co-occurrence of paired stimuli resulted in participants overestimating the frequency of the pairings.[7] To test this idea, participants were given information about several hypothetical mental patients. The data for each patient consisted of a clinical diagnosis and a drawing made by the patient. Later, participants estimated the frequency with which each diagnosis had been accompanied by various features of the drawing. The subjects vastly overestimated the frequency of this co-occurrence (such as suspiciousness and peculiar eyes). This effect was labeled the illusory correlation. Tversky and Kahneman suggested that availability provides a natural account for the illusory-correlation effect. The strength of the association between two events could provide the basis for the judgment of how frequently the two events co-occur. When the association is strong, it becomes more likely to conclude that the events have been paired frequently. Strong associations will be thought of as having occurred together frequently.[8]

In Tversky & Kahneman's first examination of availability heuristics, subjects were asked, ""If a random word is taken from an English text, is it more likely that the word starts with a K, or that K is the third letter?"" They argue that English-speaking people would immediately think of many words that begin with the letter ""K"" (kangaroo, kitchen, kale), but that it would take a more concentrated effort to think of any words in which ""K"" is the third letter (acknowledge, ask). Results indicated that participants overestimated the number of words that began with the letter ""K"" and underestimated the number of words that had ""K"" as the third letter. Tversky and Kahneman concluded that people answer questions like these by comparing the availability of the two categories and assessing how easily they can recall these instances. In other words, it is easier to think of words that begin with ""K"", more than words with ""K"" as the third letter. Thus, people judge words beginning with a ""K"" to be a more common occurrence. In reality, however, a typical text contains twice as many words that have ""K"" as the third letter than ""K"" as the first letter. There are three times more words with ""K"" in the third position than words that begin with ""K"".[8]

In Tversky and Kahneman's seminal paper, they include findings from several other studies, which also show support for the availability heuristic. Apart from their findings in the ""K"" study, they also found:

When participants were shown two visual structures and asked to pick the structure that had more paths, participants saw more paths in the structure that had more obvious available paths. In the structure that participants chose, there were more columns and shorter obvious paths, making it more available to them. When participants were asked to complete tasks involving estimation, they would often underestimate the end result. Participants were basing their final estimation off of a quick first impression of the problem. Participants particularly struggled when the problems consisted of multiple steps. This occurred because participants were basing their estimation on an initial impression. Participants failed to account for the high rate of growth in the later steps due to the impression they formed in the initial steps. This was shown again in a task that asked participants to estimate the answer to a multiplication task, in which the numbers were presented as either 1x2x3x4x5x6x7x8 or 8x7x6x5x4x3x2x1. Participants who were presented the equation with the larger numbers first (8x7x6...), estimated a significantly higher result than participants with the lower numbers first (1x2x3...). Participants were given a short amount of time to make the estimation, thus participants based their estimates off of what was easily available, which in this case was the first few numbers in the sequence.[8]

Explanations [ edit ]

Many researchers have attempted to identify the psychological process which create the availability heuristic.

Tversky and Kahneman argue that the number of examples recalled from memory is used to infer the frequency with which such instances occur. In an experiment to test this explanation, participants listened to lists of names containing either 19 famous women and 20 less famous men or 19 famous men and 20 less famous women. Subsequently, some participants were asked to recall as many names as possible whereas others were asked to estimate whether male or female names were more frequent on the list. The names of the famous celebrities were recalled more frequently compared to those of the less famous celebrities. The majority of the participants incorrectly judged that the gender associated with more famous names had been presented more often than the gender associated with less famous names. Tversky and Kahneman argue that although the availability heuristic is an effective strategy in many situations, when judging probability use of this heuristic can lead to predictable patterns of errors.[8]

Schwarz and his colleagues, on the other hand, proposed the ease of retrieval explanation, in which is the ease with which examples come to mind, not the number of examples, is used to infer the frequency of a given class. In a study by Schwarz and colleagues to test their explanation, participants were asked to recall either six or twelve examples of their assertive or very unassertive behavior. Participants were later asked to rate their own assertiveness. Pretesting had indicated that although most participants were capable of generating twelve examples, this was a difficult task. The results indicated that participants rated themselves as more assertive after describing six examples of assertive compared with unassertive behavior condition, but rated themselves as less assertive after describing twelve examples of assertive compared with unassertive behavior condition. The study reflected that the extent to which recalled content impacted judgment was determined by the ease with which the content could be brought to mind (it was easier to recall 6 examples than 12), rather than the amount of content brought to mind.[4]

Research by Vaugh (1999) looked at the effects of uncertainty on the use of the availability heuristic. College students were asked to list either three or eight different study methods they could use in order to get an A on their final exams. The researchers also manipulated the time during the semester they would ask the students to complete the questionnaire. Approximately half of the participants were asked for their study methods during the third week of classes, and the other half were asked on last day of classes. Next, participants were asked to rate how likely they would be to get an A in their easiest and hardest classes. Participants were then asked to rank the difficulty they experienced in recalling the examples they had previously listed. The researchers hypothesized that students would use the availability heuristic, based on the number of study methods they listed, to predict their grade only when asked at the beginning of the semester and about their hardest final. Students were not expected to use the availability heuristic to predict their grade at the end of the semester or about their easiest final. The researchers predicted this use of availability heuristic because participants would be uncertain about their performance throughout the semester. The results indicated that students used the availability heuristic, based on the ease of recall of the study methods they listed, to predict their performance when asked at the beginning of the semester and about their hardest final. If the student listed only three study methods, they predicted a higher grade at the end of the semester only on their hardest final. If students listed eight study methods, they had a harder time recalling the methods and thus predicted a lower final grade on their hardest final. The results were not seen in the easy final condition because the students were certain they would get an A, regardless of study method. The results supported this hypothesis and gave evidence to the fact that levels of uncertainty affect the use of the availability heuristic.[9]

Applications [ edit ]

Media [ edit ]

After seeing news stories about child abductions, people may judge that the likelihood of this event is greater. Media coverage can help fuel a person's example bias with widespread and extensive coverage of unusual events, such as homicide or airline accidents, and less coverage of more routine, less sensational events, such as common diseases or car accidents. For example, when asked to rate the probability of a variety of causes of death, people tend to rate ""newsworthy"" events as more likely[citation needed] because they can more readily recall an example from memory[citation needed]. Moreover, unusual and vivid events like homicides, shark attacks, or lightning are more often reported in mass media than common and un-sensational causes of death like common diseases.[citation needed]

For example, many people think that the likelihood of dying from shark attacks is greater than that of dying from being hit by falling airplane parts, when more people actually die from falling airplane parts.[10] When a shark attack occurs, the deaths are widely reported in the media whereas deaths as a result of being hit by falling airplane parts are rarely reported in the media.[11]

In a 2010 study exploring how vivid television portrayals are used when forming social reality judgments, people watching vivid violent media gave higher estimates of the prevalence of crime and police immorality in the real world than those not exposed to vivid television. These results suggest that television violence does in fact have a direct causal impact on participants' social reality beliefs. Repeated exposure to vivid violence leads to an increase in people's risk estimates about the prevalence of crime and violence in the real world.[12] Counter to these findings, researchers from a similar study argued that these effects may be due to effects of new information. Researchers tested the new information effect by showing movies depicting dramatic risk events and measuring their risk assessment after the film. Contrary to previous research, there were no long-term effects on risk perception due to exposure to dramatic movies. However, the study did find evidence of idiosyncratic effects of the movies - that is, people reacted immediately after the movies with enhanced or diminished risk beliefs, which faded after a period of 10 days.[13]

Health [ edit ]

Researchers examined the role of cognitive heuristics in the AIDS risk-assessment process. 331 physicians reported worry about on-the-job HIV exposure, and experience with patients who have HIV. By analyzing answers to questionnaires handed out, researchers concluded that availability of AIDS information did not relate strongly to perceived risk.[14]

Participants in a 1992 study read case descriptions of hypothetical patients who varied on their sex and sexual preference. These hypothetical patients showed symptoms of two different diseases. Participants were instructed to indicate which disease they thought the patient had and then they rated patient responsibility and interaction desirability. Consistent with the availability heuristic, either the more common (influenza) or the more publicized (AIDS) disease was chosen.[15]

Business and economy [ edit ]

One study sought to analyze the role of the availability heuristic in financial markets. Researchers defined and tested two aspects of the availability heuristic:[16]

Outcome Availability – availability of positive and negative investment outcomes, and

Risk Availability – availability of financial risk.[16]

On days of substantial stock market moves, abnormal stock price reactions to upgrades are weaker, than those to downgrades. These availability effects are still significant even after controlling for event-specific and company-specific factors.[16]

Similarly, research has pointed out that under the availability heuristic, humans are not reliable because they assess probabilities by giving more weight to current or easily recalled information instead of processing all relevant information. Since information regarding the current state of the economy is readily available, researchers attempted to expose the properties of business cycles to predict the availability bias in analysts' growth forecasts. They showed the availability heuristic to play a role in analysis of forecasts and influence investments because of this.[17]

In effect, investors are using availability heuristic to make decisions and subsequently, may be obstructing their own investment success. An investor's lingering perceptions of a dire market environment may be causing them to view investment opportunities through an overly negative lens, making it less appealing to consider taking on investment risk, no matter how small the returns on perceived ""safe"" investments. To illustrate, Franklin Templeton's annual Global Investor Sentiment Survey 1 asked individuals how they believed the S&P 500 Index performed in 2009, 2010 and 2011. 66 percent of respondents stated that they believed the market was either flat or down in 2009, 48 percent said the same about 2010 and 53 percent also said the same about 2011. In reality, the S&P 500 saw 26.5 percent annual returns in 2009, 15.1 percent annual returns in 2010 and 2.1 percent annual returns in 2011, meaning lingering perceptions based on dramatic, painful events are impacting decision-making even when those events are over.[18]

Additionally, a study by Hayibor and Wasieleski found that the availability of others who believe that a particular act is morally acceptable is positively related to others' perceptions of the morality of that act. This suggests that availability heuristic also has an effect on ethical decision making and ethical behavior in organizations.[19]

Education [ edit ]

A study done by Craig R. Fox provides an example of how availability heuristics can work in the classroom. In this study, Fox tests whether difficulty of recall influences judgment, specifically with course evaluations among college students. In his study he had two groups complete a course evaluation form. He asked the first group to write two recommended improvements for the course (a relatively easy task) and then write two positives about the class. The second group was asked to write ten suggestions where the professor could improve (a relatively difficult task) and then write two positive comments about the course. At the end of the evaluation both groups were asked to rate the course on a scale from one to seven. The results showed that students asked to write ten suggestions (difficult task) rated the course less harshly because it was more difficult for them to recall the information. Most of the students in the group that were asked to fill in 10 suggestions didn't fill in more than two being unable to recall more instances where they were unsatisfied with the class. Students asked to do the easier evaluation with only two complaints had less difficulty in terms of availability of information, so they rated the course more harshly.[20]

A study done was testing the memory of children and the ease of recall. They were asked to learn a list of names and then to recall different amounts. Researchers found that when asked to recall lower amounts compared to larger amounts and then asked what was easier to remember. They responded the shorter list going along with the theory of availability heuristic.[21]

Criminal justice [ edit ]

The media usually focuses on violent or extreme cases, which are more readily available in the public's mind. This may come into play when it is time for the judicial system to evaluate and determine the proper punishment for a crime. In one study, respondents rated how much they agreed with hypothetical laws and policies such as ""Would you support a law that required all offenders convicted of unarmed muggings to serve a minimum prison term of two years?"" Participants then read cases and rated each case on several questions about punishment. As hypothesized, respondents recalled more easily from long-term memory stories that contain severe harm, which seemed to influence their sentencing choices to make them push for harsher punishments. This can be eliminated by adding high concrete or high contextually distinct details into the crime stories about less severe injuries.[22]

A similar study asked jurors and college students to choose sentences on four severe criminal cases in which prison was a possible but not an inevitable sentencing outcome. Respondents answering questions about court performance on a public opinion formulated a picture of what the courts do and then evaluated the appropriateness of that behavior. Respondents recalled from public information about crime and sentencing. This type of information is incomplete because the news media present a highly selective and non-representative selection of crime, focusing on the violent and extreme, rather than the ordinary. This makes most people think that judges are too lenient. But, when asked to choose the punishments, the sentences given by students were equal to or less severe than those given by judges. In other words, the availability heuristic made people believe that judges and jurors were too lenient in the courtroom, but the participants gave similar sentences when placed in the position of the judge, suggesting that the information they recalled was not correct.[23]

Researchers in 1989 predicted that mock jurors would rate a witness to be more deceptive if the witness testified truthfully before lying than when the witness was caught lying first before telling the truth. If the availability heuristic played a role in this, lying second would remain in jurors' minds (since it was more recent) and they would most likely remember the witness lying over the truthfulness. To test the hypothesis, 312 university students played the roles of mock jurors and watched a videotape of a witness presenting testimony during a trial. Results confirmed the hypothesis, as mock jurors were most influenced by the most recent act.[24]

Perceived risk [ edit ]

Previous studies have indicated that explaining a hypothetical event makes the event seem more likely through the creation of causal connections. However, such effects could arise through the use of the availability heuristic; that is, subjective likelihood is increased by an event becoming easier to imagine.[25]

A study done asked those participating to pick between two illnesses. Those doing the study wanted to know which disease they thought was more likely to cause death. In the study they asked participants to choose between a stroke and asthma as to which one someone was more likely to die from. The researchers concluded that it depended on what experiences were available to them. If they knew someone or heard of someone that died from one of the diseases that is the one they perceived to be a higher risk to pass away from.[26]

Vividness effects [ edit ]

Two studies with 108 undergraduates investigated vivid information and its impact on social judgment and the availability heuristic and its role in mediating vividness effects.

In study 1, Subjects listened to a tape recording that described a woman who lived with her 7-year-old son. Subjects then heard arguments about the woman's fitness as a parent and were asked to draw their own conclusions regarding her fitness or unfitness. Concrete and colorful language was found to influence judgments about the woman's fitness as a mother.

In study 2, a series of male and female names was presented to subjects; for each name, subjects were told the university affiliation of the individual (Yale or Stanford). When some names were presented, subjects were simultaneously shown a photograph that purportedly portrayed the named individual. Subsequently, to assess what subjects could remember (as a measure of availability), each name was re-presented, as well as the appropriate photograph if one had been originally presented. The study considered whether the display or non-display of photographs biased subjects' estimates as to the percentage of Yale (vs Stanford) students in the sample of men and women whose names appeared on the original list, and whether these estimated percentages were causally related to the respondents' memory for the college affiliations of the individual students on the list. The presence of photographs affected judgments about the proportion of male and female students at the two universities. Such effects have typically been attributed to the ready accessibility of vividly presented information in memory—that is, to the availability heuristic.

In both studies, vividness affected both availability (ability to recall) and judgments. However, causal modeling results indicated that the availability heuristic did not play a role in the judgment process.[27]

Judging frequency and probability [ edit ]

In general, availability is correlated with ecological frequency, but it is also affected by other factors. Consequently, the reliance on the availability heuristic leads to systematic biases. Such biases are demonstrated in the judged frequency of classes of words, of combinatoric outcomes, and of repeated events. The phenomenon of illusory correlation is explained as an availability bias.[8]

In the original Tversky and Kahneman (1973) research, three major factors that are discussed are the frequency of repetition, frequency of co-occurrence, and illusory correlation. The use of frequency of repetition aids in the retrieval of relevant instances. The idea behind this phenomenon, is that the more an instance is repeated within a category or list, the stronger the link between the two instances becomes. Individuals then use the strong association between the instances to determine the frequency of an instance. Consequently, the association between the category or list and the specific instance, often influences frequency judgement. Frequency of co-occurrence strongly relates to Frequency of repetition, such that the more an item-pair is repeated, the stronger the association between the two items becomes, leading to a bias when estimating frequency of co-occurrence. Due to the phenomena of frequency of co-occurrence, illusory correlations also often play a big role.[8]

Another factor that affects the availability heuristic in frequency and probability is exemplars. Exemplars are the typical examples that stand out during the process of recall. If asked what participants thought different set sizes were (how many men and how many women are in the class), participants would use exemplars to determine the size of each set. Participants would derive their answer on ease of recall of the names that stood out. Participants read a list of names of members of a class for 30 seconds, and then participants were asked the male to female ratio of the class. The participant's answer would depend on the recall of exemplars. If the participant reading the list recalled seeing more common male names, such as Jack, but the only female names in the class were uncommon names, such as Deepika, then the participant will recall that there were more men than women. The opposite would be true if there were more common female names on the list and uncommon male names. Due to the availability heuristic, names that are more easily available are more likely to be recalled, and can thus alter judgments of probability.[28]

Another example of the availability heuristic and exemplars would be seeing a shark in the ocean. Seeing a shark has a greater impact on an individual's memory than seeing a dolphin. If someone sees both sharks and dolphins in the ocean, they will be less aware of seeing the dolphins, because the dolphins had less of an impact on their memory. Due to the greater impact of seeing a shark, the availability heuristic can influence the probability judgement of the ratio of sharks and dolphins in the water. Thus, an individual who saw both a shark and a dolphin would assume a higher ratio of sharks in the water, even if there are more dolphins in reality.[28]

Critiques [ edit ]

Ease of recall as a critique [ edit ]

One of the earliest and most powerful critiques of the original Tversky and Kahneman[29] study on the availability heuristic was the Schwarz et al.[4] study which found that the ease of recall was a key component in determining whether a concept became available. Many studies since this criticism of the original availability heuristic model have repeated this initial criticism, that the ease of recall factor became an integral facet of the availability heuristic itself (see Research section).

Alternative explanations [ edit ]

Much of the criticism against the availability heuristic has claimed that making use of the content that becomes available in our mind is not based on the ease of recall as suggested by Schwarz et al.[4] For example, it could be argued that recalling more words that begin with K than words with the third letter being K could arise from how we categorize and process words into our memory. If we categorize words by first letter, and recall them through the same process, this would show more support for the representatives heuristic than the availability heuristic. Based on the possibility of explanations such as these, some researchers have claimed that the classic studies on the availability heuristic are too vague in that they fail to account for people's underlying mental processes. Indeed, a study conducted by Wanke et al. demonstrated this scenario can occur in situations used to test the availability heuristic.[30] Future studies should be conducted to determine if and when this alternative explanation will occur.

A second line of study has shown that frequency estimation may not be the only strategy we use when making frequency judgments. A recent line of research has shown that our situational working memory can access long term memories, and this memory retrieval process includes the ability to determine more accurate probabilities.[31] This finding suggests that more research should be conducted to determine how much memory activation affects the availability heuristic.

See also [ edit ]

References [ edit ]","          Availability heuristic   From Wikipedia, the free encyclopedia    Jump to navigation  Jump to search  The availability heuristic , also known as availability bias, is a mental shortcut that relies on immediate examples that come to a given person's mind when evaluating a specific topic, concept, method or decision. The availability heuristic operates on the notion that if something can be recalled , it must be important, or at least more important than alternative solutions which are not as readily recalled. [1] Subsequently, under the availability heuristic, people tend to heavily weigh their judgments toward more recent information, making new opinions biased toward that latest news. [2] [3]  The availability of consequences associated with an action is positively related to perceptions of the magnitude of the consequences of that action. In other words, the easier it is to recall the consequences of something the greater those consequences are often perceived to be. Most notably, people often rely on the content of their recall if its implications are not called into question by the difficulty that they experience in bringing the relevant material to mind. [4]   Contents   1  Overview and history  2  Research  3  Explanations  4  Applications   4.1  Media  4.2  Health  4.3  Business and economy  4.4  Education  4.5  Criminal justice  4.6  Perceived risk  4.7  Vividness effects  4.8  Judging frequency and probability    5  Critiques   5.1  Ease of recall as a critique  5.2  Alternative explanations    6  See also  7  References  8  External links    Overview and history [ edit ]  In the late 1960s and early 1970s, Amos Tversky and Daniel Kahneman began work on a series of papers examining ""heuristic and biases "" used in the judgment under uncertainty . Prior to that, the predominant view in the field of human judgment was that humans are rational actors . Kahneman and Tversky explained that judgment under uncertainty often relies on a limited number of simplifying heuristics rather than extensive algorithmic processing. Soon, this idea spread beyond academic psychology, into law, medicine, and political science. This research questioned the descriptive adequacy of idealized models of judgment, and offered insights into the cognitive processes that explained human error without invoking motivated irrationality . [5] One simplifying strategy people may rely on is the tendency to make a judgment about the frequency of an event based on how many similar instances are brought to mind. In 1973, Amos Tversky and Daniel Kahneman first studied this phenomenon and labeled it the ""availability heuristic"". An availability heuristic is a mental shortcut that relies on immediate examples that come to a given person's mind when evaluating a specific topic, concept, method or decision. As follows, people tend to use a readily available fact to base their beliefs about a comparably distant concept. There has been much research done with this heuristic, but studies on the issue are still questionable with regard to the underlying process. Studies illustrate that manipulations intended to increase the subjective experience of ease of recall are also likely to affect the amount of recall. Furthermore, this makes it difficult to determine if the obtained estimates of frequency, likelihood, or typicality are based on participants' phenomenal experiences or on a biased sample of recalled information. [5]  However, some textbooks have chosen the latter interpretation introducing the availability heuristic as ""one's judgments are always based on what comes to mind"". For example, if a person is asked whether there are more words in the English language that begin with a t or k, the person will probably be able to think of more words that begin with the letter t, concluding that t is more frequent than k. [6]   Research [ edit ]  Chapman (1967) described a bias in the judgment of the frequency with which two events co-occur. This demonstration showed that the co-occurrence of paired stimuli resulted in participants overestimating the frequency of the pairings. [7] To test this idea, participants were given information about several hypothetical mental patients. The data for each patient consisted of a clinical diagnosis and a drawing made by the patient. Later, participants estimated the frequency with which each diagnosis had been accompanied by various features of the drawing. The subjects vastly overestimated the frequency of this co-occurrence (such as suspiciousness and peculiar eyes). This effect was labeled the illusory correlation . Tversky and Kahneman suggested that availability provides a natural account for the illusory-correlation effect. The strength of the association between two events could provide the basis for the judgment of how frequently the two events co-occur. When the association is strong, it becomes more likely to conclude that the events have been paired frequently. Strong associations will be thought of as having occurred together frequently. [8]  In Tversky & Kahneman's first examination of availability heuristics, subjects were asked, ""If a random word is taken from an English text, is it more likely that the word starts with a K, or that K is the third letter?"" They argue that English-speaking people would immediately think of many words that begin with the letter ""K"" (kangaroo, kitchen, kale), but that it would take a more concentrated effort to think of any words in which ""K"" is the third letter (acknowledge, ask). Results indicated that participants overestimated the number of words that began with the letter ""K"" and underestimated the number of words that had ""K"" as the third letter. Tversky and Kahneman concluded that people answer questions like these by comparing the availability of the two categories and assessing how easily they can recall these instances. In other words, it is easier to think of words that begin with ""K"", more than words with ""K"" as the third letter. Thus, people judge words beginning with a ""K"" to be a more common occurrence. In reality, however, a typical text contains twice as many words that have ""K"" as the third letter than ""K"" as the first letter. There are three times more words with ""K"" in the third position than words that begin with ""K"". [8]  In Tversky and Kahneman's seminal paper, they include findings from several other studies, which also show support for the availability heuristic. Apart from their findings in the ""K"" study, they also found:  When participants were shown two visual structures and asked to pick the structure that had more paths, participants saw more paths in the structure that had more obvious available paths. In the structure that participants chose, there were more columns and shorter obvious paths, making it more available to them. When participants were asked to complete tasks involving estimation, they would often underestimate the end result. Participants were basing their final estimation off of a quick first impression of the problem. Participants particularly struggled when the problems consisted of multiple steps. This occurred because participants were basing their estimation on an initial impression. Participants failed to account for the high rate of growth in the later steps due to the impression they formed in the initial steps. This was shown again in a task that asked participants to estimate the answer to a multiplication task, in which the numbers were presented as either 1x2x3x4x5x6x7x8 or 8x7x6x5x4x3x2x1. Participants who were presented the equation with the larger numbers first (8x7x6...), estimated a significantly higher result than participants with the lower numbers first (1x2x3...). Participants were given a short amount of time to make the estimation, thus participants based their estimates off of what was easily available, which in this case was the first few numbers in the sequence. [8]  Explanations [ edit ]  Many researchers have attempted to identify the psychological process which create the availability heuristic. Tversky and Kahneman argue that the number of examples recalled from memory is used to infer the frequency with which such instances occur. In an experiment to test this explanation, participants listened to lists of names containing either 19 famous women and 20 less famous men or 19 famous men and 20 less famous women. Subsequently, some participants were asked to recall as many names as possible whereas others were asked to estimate whether male or female names were more frequent on the list. The names of the famous celebrities were recalled more frequently compared to those of the less famous celebrities. The majority of the participants incorrectly judged that the gender associated with more famous names had been presented more often than the gender associated with less famous names. Tversky and Kahneman argue that although the availability heuristic is an effective strategy in many situations, when judging probability use of this heuristic can lead to predictable patterns of errors. [8]  Schwarz and his colleagues, on the other hand, proposed the ease of retrieval explanation, in which is the ease with which examples come to mind, not the number of examples, is used to infer the frequency of a given class. In a study by Schwarz and colleagues to test their explanation, participants were asked to recall either six or twelve examples of their assertive or very unassertive behavior. Participants were later asked to rate their own assertiveness. Pretesting had indicated that although most participants were capable of generating twelve examples, this was a difficult task. The results indicated that participants rated themselves as more assertive after describing six examples of assertive compared with unassertive behavior condition, but rated themselves as less assertive after describing twelve examples of assertive compared with unassertive behavior condition. The study reflected that the extent to which recalled content impacted judgment was determined by the ease with which the content could be brought to mind (it was easier to recall 6 examples than 12), rather than the amount of content brought to mind. [4]  Research by Vaugh (1999) looked at the effects of uncertainty on the use of the availability heuristic. College students were asked to list either three or eight different study methods they could use in order to get an A on their final exams. The researchers also manipulated the time during the semester they would ask the students to complete the questionnaire.  Approximately half of the participants were asked for their study methods during the third week of classes, and the other half were asked on last day of classes. Next, participants were asked to rate how likely they would be to get an A in their easiest and hardest classes.  Participants were then asked to rank the difficulty they experienced in recalling the examples they had previously listed. The researchers hypothesized that students would use the availability heuristic, based on the number of study methods they listed, to predict their grade only when asked at the beginning of the semester and about their hardest final. Students were not expected to use the availability heuristic to predict their grade at the end of the semester or about their easiest final. The researchers predicted this use of availability heuristic because participants would be uncertain about their performance throughout the semester. The results indicated that students used the availability heuristic, based on the ease of recall of the study methods they listed, to predict their performance when asked at the beginning of the semester and about their hardest final. If the student listed only three study methods, they predicted a higher grade at the end of the semester only on their hardest final. If students listed eight study methods, they had a harder time recalling the methods and thus predicted a lower final grade on their hardest final. The results were not seen in the easy final condition because the students were certain they would get an A, regardless of study method. The results supported this hypothesis and gave evidence to the fact that levels of uncertainty affect the use of the availability heuristic. [9]   Applications [ edit ]  Media [ edit ]  After seeing news stories about child abductions, people may judge that the likelihood of this event is greater. Media coverage can help fuel a person's example bias with widespread and extensive coverage of unusual events, such as homicide or airline accidents , and less coverage of more routine, less sensational events, such as common diseases or car accidents . For example, when asked to rate the probability of a variety of causes of death, people tend to rate ""newsworthy"" events as more likely [ citation needed ] because they can more readily recall an example from memory [ citation needed ] . Moreover, unusual and vivid events like homicides, shark attacks , or lightning are more often reported in mass media than common and un-sensational causes of death like common diseases. [ citation needed ]  For example, many people think that the likelihood of dying from shark attacks is greater than that of dying from being hit by falling airplane parts, when more people actually die from falling airplane parts. [10] When a shark attack occurs, the deaths are widely reported in the media whereas deaths as a result of being hit by falling airplane parts are rarely reported in the media. [11]  In a 2010 study exploring how vivid television portrayals are used when forming social reality judgments, people watching vivid violent media gave higher estimates of the prevalence of crime and police immorality in the real world than those not exposed to vivid television. These results suggest that television violence does in fact have a direct causal impact on participants' social reality beliefs. Repeated exposure to vivid violence leads to an increase in people's risk estimates about the prevalence of crime and violence in the real world. [12] Counter to these findings, researchers from a similar study argued that these effects may be due to effects of new information. Researchers tested the new information effect by showing movies depicting dramatic risk events and measuring their risk assessment after the film. Contrary to previous research, there were no long-term effects on risk perception due to exposure to dramatic movies. However, the study did find evidence of idiosyncratic effects of the movies - that is, people reacted immediately after the movies with enhanced or diminished risk beliefs, which faded after a period of 10 days. [13]   Health [ edit ]  Researchers examined the role of cognitive heuristics in the AIDS risk-assessment process. 331 physicians reported worry about on-the-job HIV exposure, and experience with patients who have HIV. By analyzing answers to questionnaires handed out, researchers concluded that availability of AIDS information did not relate strongly to perceived risk . [14]  Participants in a 1992 study read case descriptions of hypothetical patients who varied on their sex and sexual preference. These hypothetical patients showed symptoms of two different diseases. Participants were instructed to indicate which disease they thought the patient had and then they rated patient responsibility and interaction desirability. Consistent with the availability heuristic, either the more common ( influenza ) or the more publicized (AIDS) disease was chosen. [15]   Business and economy [ edit ]  One study sought to analyze the role of the availability heuristic in financial markets. Researchers defined and tested two aspects of the availability heuristic: [16]   Outcome Availability – availability of positive and negative investment outcomes, and  Risk Availability – availability of financial risk. [16]  On days of substantial stock market moves, abnormal stock price reactions to upgrades are weaker, than those to downgrades. These availability effects are still significant even after controlling for event-specific and company-specific factors. [16]  Similarly, research has pointed out that under the availability heuristic, humans are not reliable because they assess probabilities by giving more weight to current or easily recalled information instead of  processing all relevant information. Since information regarding the current state of the economy is readily available, researchers attempted to expose the properties of business cycles to predict the availability bias in analysts' growth forecasts. They showed the availability heuristic to play a role in analysis of forecasts and influence investments because of this. [17]  In effect, investors are using availability heuristic to make decisions and subsequently, may be obstructing their own investment success. An investor's lingering perceptions of a dire market environment may be causing them to view investment opportunities through an overly negative lens, making it less appealing to consider taking on investment risk, no matter how small the returns on perceived ""safe"" investments.  To illustrate, Franklin Templeton 's annual Global Investor Sentiment Survey 1 asked individuals how they believed the S&P 500 Index performed in 2009, 2010 and 2011. 66 percent of respondents stated that they believed the market was either flat or down in 2009, 48 percent said the same about 2010 and 53 percent also said the same about 2011.  In reality, the S&P 500 saw 26.5 percent annual returns in 2009, 15.1 percent annual returns in 2010 and 2.1 percent annual returns in 2011, meaning lingering perceptions based on dramatic, painful events are impacting decision-making even when those events are over. [18]  Additionally, a study by Hayibor and Wasieleski found that the availability of others who believe that a particular act is morally acceptable is positively related to others' perceptions of the morality of that act. This suggests that availability heuristic also has an effect on ethical  decision making and ethical behavior in organizations. [19]   Education [ edit ]  A study done by Craig R. Fox provides an example of how availability heuristics can work in the classroom. In this study, Fox tests whether difficulty of recall influences judgment, specifically with course evaluations among college students.  In his study he had two groups complete a course evaluation form.  He asked the first group to write two recommended improvements for the course (a relatively easy task) and then write two positives about the class. The second group was asked to write ten suggestions where the professor could improve (a relatively difficult task) and then write two positive comments about the course.  At the end of the evaluation both groups were asked to rate the course on a scale from one to seven.  The results showed that students asked to write ten suggestions (difficult task) rated the course less harshly because it was more difficult for them to recall the information. Most of the students in the group that were asked to fill in 10 suggestions didn't fill in more than two being unable to recall more instances where they were unsatisfied with the class. Students asked to do the easier evaluation with only two complaints had less difficulty in terms of availability of information, so they rated the course more harshly. [20]  A study done was testing the memory of children and the ease of recall. They were asked to learn a list of names and then to recall different amounts. Researchers found that when asked to recall lower amounts compared to larger amounts and then asked what was easier to remember. They responded the shorter list going along with the theory of availability heuristic. [21]   Criminal justice [ edit ]  The media usually focuses on violent or extreme cases, which are more readily available in the public's mind. This may come into play when it is time for the judicial system to evaluate and determine the proper punishment for a crime. In one study, respondents rated how much they agreed with hypothetical laws and policies such as ""Would you support a law that required all offenders convicted of unarmed muggings to serve a minimum prison term of two years?"" Participants then read cases and rated each case on several questions about punishment. As hypothesized, respondents recalled more easily from long-term memory stories that contain severe harm, which seemed to influence their sentencing choices to make them push for harsher punishments. This can be eliminated by adding high concrete or high contextually distinct details into the crime stories about less severe injuries. [22]  A similar study asked jurors and college students to choose sentences on four severe criminal cases in which prison was a possible but not an inevitable sentencing outcome. Respondents answering questions about court performance on a public opinion formulated a picture of what the courts do and then evaluated the appropriateness of that behavior. Respondents recalled from public information about crime and sentencing. This type of information is  incomplete because the news media present a highly selective and non- representative selection of crime, focusing on the violent and extreme, rather than the ordinary. This makes most people think that judges are too lenient. But, when asked to choose the punishments, the sentences given by students were equal to or less severe than those given by judges. In other words, the availability heuristic made people believe that judges and jurors were too lenient in the courtroom, but the participants gave similar sentences when placed in the position of the judge, suggesting that the information they recalled was not correct. [23]  Researchers in 1989 predicted that mock jurors would rate a witness to be more deceptive if the witness testified truthfully before lying than when the witness was caught lying first before telling the truth. If the availability heuristic played a role in this, lying second would remain in jurors' minds (since it was more recent) and they would most likely remember the witness lying over the truthfulness. To test the hypothesis, 312 university students played the roles of mock jurors and watched a videotape of a witness presenting testimony during a trial. Results confirmed the hypothesis, as mock jurors were most influenced by the most recent act. [24]   Perceived risk [ edit ]  Further information: Perceived risk  Previous studies have indicated that explaining a hypothetical event makes the event seem more likely through the creation of causal connections. However, such effects could arise through the use of the availability heuristic; that is, subjective likelihood is increased by an event becoming easier to imagine. [25]  A study done asked those participating to pick between two illnesses. Those doing the study wanted to know which disease they thought was more likely to cause death. In the study they asked participants to choose between a stroke and asthma as to which one someone was more likely to die from. The researchers concluded that it depended on what experiences were available to them. If they knew someone or heard of someone that died from one of the diseases that is the one they perceived to be a higher risk to pass away from. [26]   Vividness effects [ edit ]  Two studies with 108 undergraduates investigated vivid information and its impact on social judgment and the availability heuristic and its role in mediating vividness effects. In study 1, Subjects listened to a tape recording that described a woman who lived with her 7-year-old son. Subjects then heard arguments about the woman's fitness as a parent and were asked to draw their own conclusions regarding her fitness or unfitness. Concrete and colorful language was found to influence judgments about the woman's fitness as a mother. In study 2, a series of male and female names was presented to subjects; for each name, subjects were told the university affiliation of the individual (Yale or Stanford). When some names were presented, subjects were simultaneously shown a photograph that purportedly portrayed the named individual. Subsequently, to assess what subjects could remember (as a measure of availability), each name was re-presented, as well as the appropriate photograph if one had been originally presented. The study considered whether the display or non-display of photographs biased subjects' estimates as to the percentage of Yale (vs Stanford) students in the sample of men and women whose names appeared on the original list, and whether these estimated percentages were causally related to the respondents' memory for the college affiliations of the individual students on the list. The presence of photographs affected judgments about the proportion of male and female students at the two universities. Such effects have typically been attributed to the ready accessibility of vividly presented information in memory—that is, to the availability heuristic. In both studies, vividness affected both availability (ability to recall) and judgments. However, causal modeling results indicated that the availability heuristic did not play a role in the judgment process. [27]   Judging frequency and probability [ edit ]  In general, availability is correlated with ecological frequency, but it is also affected by other factors. Consequently, the reliance on the availability heuristic leads to systematic biases. Such biases are demonstrated in the judged frequency of classes of words, of combinatoric outcomes, and of repeated events. The phenomenon of illusory correlation is explained as an availability bias. [8]  In the original Tversky and Kahneman (1973) research, three major factors that are discussed are the frequency of repetition, frequency of co-occurrence, and illusory correlation. The use of frequency of repetition aids in the retrieval of relevant instances. The idea behind this phenomenon, is that the more an instance is repeated within a category or list, the stronger the link between the two instances becomes. Individuals then use the strong association between the instances to determine the frequency of an instance. Consequently, the association between the category or list and the specific instance, often influences frequency judgement. Frequency of co-occurrence strongly relates to Frequency of repetition, such that the more an item-pair is repeated, the stronger the association between the two items becomes, leading to a bias when estimating frequency of co-occurrence. Due to the phenomena of frequency of co-occurrence, illusory correlations also often play a big role. [8]  Another factor that affects the availability heuristic in frequency and probability is exemplars. Exemplars are the typical examples that stand out during the process of recall. If asked what participants thought different set sizes were (how many men and how many women are in the class), participants would use exemplars to determine the size of each set.  Participants would derive their answer on ease of recall of the names that stood out. Participants read a list of names of members of a class for 30 seconds, and then participants were asked the male to female ratio of the class. The participant's answer would depend on the recall of exemplars. If the participant reading the list recalled seeing more common male names, such as Jack, but the only female names in the class were uncommon names, such as Deepika, then the participant will recall that there were more men than women. The opposite would be true if there were more common female names on the list and uncommon male names. Due to the availability heuristic, names that are more easily available are more likely to be recalled, and can thus alter judgments of probability. [28]  Another example of the availability heuristic and exemplars would be seeing a shark in the ocean. Seeing a shark has a greater impact on an individual's memory than seeing a dolphin.  If someone sees both sharks and dolphins in the ocean, they will be less aware of seeing the dolphins, because the dolphins had less of an impact on their memory. Due to the greater impact of seeing a shark, the availability heuristic can influence the probability judgement of the ratio of sharks and dolphins in the water. Thus, an individual who saw both a shark and a dolphin would assume a higher ratio of sharks in the water, even if there are more dolphins in reality. [28]   Critiques [ edit ]  Ease of recall as a critique [ edit ]  One of the earliest and most powerful critiques of the original Tversky and Kahneman [29] study on the availability heuristic was the Schwarz et al. [4] study which found that the ease of recall was a key component in determining whether a concept became available.  Many studies since this criticism of the original availability heuristic model have repeated this initial criticism, that the ease of recall factor became an integral facet of the availability heuristic itself (see Research section).  Alternative explanations [ edit ]  Much of the criticism against the availability heuristic has claimed that making use of the content that becomes available in our mind is not based on the ease of recall as suggested by Schwarz et al. [4] For example, it could be argued that recalling more words that begin with K than words with the third letter being K could arise from how we categorize and process words into our memory.  If we categorize words by first letter, and recall them through the same process, this would show more support for the representatives heuristic than the availability heuristic.  Based on the possibility of explanations such as these, some researchers have claimed that the classic studies on the availability heuristic are too vague in that they fail to account for people's underlying mental processes.  Indeed, a study conducted by Wanke et al. demonstrated this scenario can occur in situations used to test the availability heuristic. [30] Future studies should be conducted to determine if and when this alternative explanation will occur. A second line of study has shown that frequency estimation may not be the only strategy we use when making frequency judgments.  A recent line of research has shown that our situational working memory can access long term memories, and this memory retrieval process includes the ability to determine more accurate probabilities. [31] This finding suggests that more research should be conducted to determine how much memory activation affects the availability heuristic.  See also [ edit ]    Psychology portal  Affect heuristic  Agenda-setting theory  Anecdotal evidence  Anecdotal value  Attribute substitution  Confirmation bias  Cultivation theory  Gambler's fallacy  List of biases in judgment and decision making  Mean world syndrome  Misleading vividness  Omission bias  Processing fluency  Representativeness heuristic  Salience bias  Texas sharpshooter fallacy  References [ edit ]    ^  Esgate, Anthony; Groome, David (2005). An Introduction to Applied Cognitive Psychology . Psychology Press. p. 201. ISBN  978-1-84169-318-7 .   ^  ""Availability heuristic - Oxford Reference"" .  Cite journal requires |journal= ( help )   ^  Phung, Albert. ""Behavioral Finance: Key Concept- Overreaction and Availability Bias"" . Investopedia. February 25, 2009. p.10. December 1, 2013.   ^ a  b  c  d  Schwarz, Norbert; Bless, Herbert; Strack, Fritz; Klumpp, Gisela; Rittenauer-Schatka, Helga; Simons, Annette (1991). ""Ease of retrieval as information: Another look at the availability heuristic"" . Journal of Personality and Social Psychology . 61 (2): 195–202. doi : 10.1037/0022-3514.61.2.195 .   ^ a  b  Gilovich, Thomas; Griffin, Dale; Kahneman, Daniel (2002-07-08). Heuristics and Biases: The Psychology of Intuitive Judgment . ISBN  9780521796798 .   ^  Gilovich, T. D.; Griffin, D.; Kahneman, D. (2002). ""Heuristics and Biases: The Psychology of Intuitive Judgment"". New York, NY: Cambridge University Press.   ^  Chapman, L.J (1967). ""Illusory correlation in observational report"". Journal of Verbal Learning . 6 : 151–155. doi : 10.1016/s0022-5371(67)80066-5 .   ^ a  b  c  d  e  f  Tversky, Amos; Kahneman, Daniel (1973). ""Availability: A heuristic for judging frequency and probability"". Cognitive Psychology . 5 (2): 207–232. doi : 10.1016/0010-0285(73)90033-9 . ISSN  0010-0285 .   ^  Vaugh, Leigh Ann (1999). ""Effects of uncertainty on use of the availability of heuristic for self-efficacy judgments""  (PDF) . European Journal of Social Psychology . 29 (2/3): 407–410. doi : 10.1002/(sici)1099-0992(199903/05)29:2/3<407::aid-ejsp943>3.0.co;2-3 . hdl : 2027.42/34564 .   ^  ""Odds and ends - The San Diego Union-Tribune"" . legacy.sandiegouniontribune.com . Archived from the original on 2019-05-25.   ^  Read, J.D. (1995). ""The availability heuristic in person identification: The sometimes misleading consequences of enhanced contextual information"". Applied Cognitive Psychology . 9 (2): 91–121. doi : 10.1002/acp.2350090202 .   ^  Riddle, Karen (2010). ""Always on My Mind: Exploring How Frequent, Recent, and Vivid Television Portrayals Are Used in the Formation of Social Reality Judgments"". Media Psychology . 13 (2): 155–179. doi : 10.1080/15213261003800140 .   ^  Sjöberg, Lennart; Engelberg, Elisabeth (2010). ""Risk Perception and Movies: A Study of Availability as a Factor in Risk Perception"". Risk Analysis . 30 (1): 95–106. doi : 10.1111/j.1539-6924.2009.01335.x . ISSN  0272-4332 . PMID  20055978 .   ^  Heath, Linda; Acklin, Marvin; Wiley, Katherine (1991). ""Cognitive Heuristics and AIDS Risk Assessment Among Physicians"". Journal of Applied Social Psychology . 21 (22): 1859–1867. doi : 10.1111/j.1559-1816.1991.tb00509.x . ISSN  0021-9029 .   ^  Triplet, R.G (1992). ""Discriminatory biases in the perception of illness: The application of availability and representativeness heuristics to the AIDS crisis"". Basic and Applied Social Psychology . 13 (3): 303–322. doi : 10.1207/s15324834basp1303_3 .   ^ a  b  c  Kliger, Doron; Kudryavtsev, Andrey (2010). ""The Availability Heuristic and Investors' Reaction to Company-Specific Events"". Journal of Behavioral Finance . 11 (1): 50–65. doi : 10.1080/15427561003591116 . ISSN  1542-7560 .   ^  Lee, Byunghwan; O'Brien, John; Sivaramakrishnan, K. (2008). ""An Analysis of Financial Analysts' Optimism in Long-term Growth Forecasts"". Journal of Behavioral Finance . 9 (3): 171–184. doi : 10.1080/15427560802341889 . ISSN  1542-7560 .   ^  http://www.businessinsider.com/the-availability-bias-is-driving-investor-decisions-2012-10 . Franklin Templeton Investments. ""Investors Should Beware The Role of 'Availability Bias'"". Business Insider. Oct. 6, 2012. Dec. 1, 2013.   ^  Hayibor, Sefa; Wasieleski, David M. (2008). ""Effects of the Use of the Availability Heuristic on Ethical Decision-Making in Organizations"". Journal of Business Ethics . 84 (S1): 151–165. doi : 10.1007/s10551-008-9690-7 . ISSN  0167-4544 .   ^  Fox, Craig R. (2006). ""The availability heuristic in the classroom: How soliciting more criticism can boost your course ratings""  (PDF) . Judgment and Decision Making . 1 (1): 86–90. ISSN  1930-2975 .   ^  Geurten, Marie; Willems, Sylvie; Germain, Sophie; Meulemans, Thierry (November 2015). ""Less is more: The availability heuristic in early childhood"". British Journal of Developmental Psychology . 33 (4): 405–410. doi : 10.1111/bjdp.12114 . PMID  26332945 .   ^  Stalans, Loretta J. (1993). ""Citizens' crime stereotypes, biased recall, and punishment preferences in abstract cases: The educative role of interpersonal sources"". Law and Human Behavior . 17 (4): 451–470. doi : 10.1007/BF01044378 . ISSN  1573-661X .   ^  Diamond, Shari Seidman; Stalans, Loretta J. (1989). ""The myth of judicial leniency in sentencing"". Behavioral Sciences & the Law . 7 (1): 73–89. doi : 10.1002/bsl.2370070106 . ISSN  0735-3936 .   ^  deTurck, M. A.; Texter, L. A.; Harszlak, J. J. (1989). ""Effects of Information Processing Objectives on Judgments of Deception Following Perjury"". Communication Research . 16 (3): 434–452. doi : 10.1177/009365089016003006 . ISSN  0093-6502 .   ^  Carroll, John S. (1978). ""The effect of imagining an event on expectations for the event: An interpretation in terms of the availability heuristic"". Journal of Experimental Social Psychology . 14 (1): 88–96. doi : 10.1016/0022-1031(78)90062-8 . ISSN  0022-1031 .   ^  Pachur, Thorsten; Hertwig, Ralph; Steinmann, Florian (2012). ""How do people judge risks: Availability heuristic, affect heuristic, or both?"". Journal of Experimental Psychology: Applied . 18 (3): 314–330. doi : 10.1037/a0028279 . hdl : 11858/00-001M-0000-0024-F052-7 . ISSN  1939-2192 . PMID  22564084 .   ^  Shedler, Jonathan; Manis, Melvin (1986). ""Can the availability heuristic explain vividness effects?"". Journal of Personality and Social Psychology . 51 (1): 26–36. doi : 10.1037/0022-3514.51.1.26 . ISSN  1939-1315 .   ^ a  b  Manis, Melvin; Jonides, Jonathan; Shedler, John; Nelson, Thomas (1993). ""Availability Heuristic in Judgments of Set Size and Frequency of Occurrence"". Journal of Personality & Social Psychology . 65 (3): 448–457. doi : 10.1037/0022-3514.65.3.448 .   ^  Tversky, A.; Kahneman, D. (1974). ""Judgment under Uncertainty: Heuristics and Biases"". Science . 185 (4157): 1124–1131. Bibcode : 1974Sci...185.1124T . doi : 10.1126/science.185.4157.1124 . ISSN  0036-8075 . PMID  17835457 .   ^  Wänke, Michaela; Schwarz, Norbert; Bless, Herbert (1995). ""The availability heuristic revisited: Experienced ease of retrieval in mundane frequency estimates"". Acta Psychologica . 89 (1): 83–90. doi : 10.1016/0001-6918(93)E0072-A . ISSN  0001-6918 .   ^  Hulme, Charles; Roodenrys, Steven; Brown, Gordon; Mercer, Robin (1995). ""The role of long-term memory mechanisms in memory span"". British Journal of Psychology . 86 (4): 527–536. doi : 10.1111/j.2044-8295.1995.tb02570.x . ISSN  0007-1269 .    External links [ edit ]  How Belief Works – an article on the origins of the availability bias.  v t e Biases Cognitive biases  Actor–observer  Acquiescence  Ambiguity  Anchoring  Attentional  Attribution  Authority  Automation  Belief  Blind spot  Choice-supportive  Confirmation  Congruence  Cultural  Distinction  Dunning–Kruger  Egocentric  Emotional  Extrinsic incentives  Fading affect  Framing  Correspondence  Halo effect  Hindsight  Horn effect  Hostile attribution  Impact  Implicit  In-group  Mere-exposure effect  Negativity  Normalcy  Omission  Optimism  Out-group homogeneity  Outcome  Overton window  Precision  Present  Pro-innovation  Response  Restraint  Self-serving  Social comparison  Status quo  Time-saving  Trait ascription  von Restorff effect  Zero-risk  In animals  Statistical biases  Estimator  Forecast  Healthy user  Information  Psychological  Lead time  Length time  Non-response  Observer  Omitted-variable  Participation  Recall  Sampling  Selection  Self-selection  Social desirability  Spectrum  Survivorship  Systematic error  Systemic  Verification  Wet  Other biases  Academic  Funding  FUTON  Inductive  Infrastructure  Inherent  In education  Media  False balance  Vietnam War  Norway  South Asia  Sweden  United States  Arab–Israeli conflict  Ukraine  Net  Political bias  Publication  Reporting  White hat  Bias reduction  Cognitive bias mitigation  Debiasing  Heuristics in judgment and decision-making  Lists: General  ·  Memory  v t e Fallacies ( list ) Formal In propositional logic  Affirming a disjunct  Affirming the consequent  Denying the antecedent  Argument from fallacy  In quantificational logic  Existential  Illicit conversion  Proof by example  Quantifier shift  Syllogistic fallacy  Affirmative conclusion from a negative premise  Exclusive premises  Existential  Necessity  Four terms  Illicit major  Illicit minor  Negative conclusion from affirmative premises  Undistributed middle   Masked man  Mathematical fallacy  Informal Equivocation  Equivocation  False equivalence  False attribution  Quoting out of context  Loki's Wager  No true Scotsman  Reification  Question-begging fallacies  Circular reasoning / Begging the question  Loaded language  Leading question  Compound question / Loaded question / Complex question  No true Scotsman  Correlative-based fallacies  False dilemma  Perfect solution  Denying the correlative  Suppressed correlative  Illicit transference  Composition  Division  Ecological  Secundum quid  Accident  Converse accident  Faulty generalization  Anecdotal evidence  Sampling bias  Cherry picking  McNamara  Base rate / Conjunction  Double counting  False analogy  Slothful induction  Overwhelming exception  Vagueness / Ambiguity  Accent  False precision  Moving the goalposts  Quoting out of context  Slippery slope  Sorites paradox  Syntactic ambiguity  Questionable cause  Animistic  Furtive  Correlation implies causation Cum hoc  Post hoc  Gambler's  Inverse  Regression  Single cause  Slippery slope  Texas sharpshooter  Fallacies of relevance Appeals to emotion  Fear  Flattery  Novelty  Pity  Ridicule  Think of the children  In-group favoritism  Invented here / Not invented here  Island mentality  Loyalty  Parade of horribles  Spite  Stirring symbols  Wisdom of repugnance  Genetic fallacies Ad hominem  Appeal to motive  Association  Reductio ad Hitlerum  Godwin's law  Reductio ad Stalinum  Bulverism  Poisoning the well  Tone  Tu quoque  Whataboutism   Authority  Accomplishment  Ipse dixit  Poverty / Wealth  Etymology  Nature  Tradition / Novelty  Chronological snobbery  Appeals to consequences  Argumentum ad baculum  Wishful thinking   Ad nauseam  Argument to moderation  Argumentum ad populum  Appeal to the stone / Proof by assertion  Ignoratio elenchi  Argument from silence  Invincible ignorance  Moralistic / Naturalistic  Motte-and-bailey fallacy  Rationalization  Red herring  Two wrongs make a right  Special pleading  Straw man  Cliché  I'm entitled to my opinion    Category       Retrieved from "" https://en.wikipedia.org/w/index.php?title=Availability_heuristic&oldid=964955134 ""  Categories : Prospect theory Cognitive biases Causal fallacies Inductive fallacies Heuristics Hidden categories: CS1 errors: missing periodical All articles with unsourced statements Articles with unsourced statements from November 2018         Navigation menu      Personal tools      Not logged in Talk Contributions Create account Log in        Namespaces      Article Talk        Variants              Views      Read Edit View history        More           Search                     Navigation      Main page Contents Current events Random article About Wikipedia Contact us Donate Wikipedia store       Contribute      Help Community portal Recent changes Upload file       Tools      What links here Related changes Upload file Special pages Permanent link Page information Wikidata item Cite this page       Print/export      Download as PDF Printable version       Languages      Afrikaans العربية Čeština Deutsch Español فارسی Français Italiano עברית Magyar Nederlands Polski Português Русский Sicilianu Svenska ไทย Українська  Edit links        This page was last edited on 28 June 2020, at 15:59 (UTC) .  Text is available under the Creative Commons Attribution-ShareAlike License ;
additional terms may apply.  By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc. , a non-profit organization.    Privacy policy  About Wikipedia  Disclaimers  Contact Wikipedia  Developers  Statistics  Cookie statement  Mobile view          "
2,(availability or ease of recall)(fallacy OR bias OR heuristic) (coronavirus OR covid OR pandemic),2,https://fs.blog/2011/08/mental-model-availability-bias/,"3 Things Everyone Should Know About the Availability Heuristic

There are 3 things you should know about the availability heuristic:

We often misjudge the frequency and magnitude of events that have happened recently. This happens, in part, because of the limitations on memory. We remember things better when they come in a vivid narrative.

***

There are two biases emanating from the availability heuristic (a.k.a. the availability bias): Ease of recall and retrievability.

Because of the availability bias, our perceptions of risk may be in error and we might worry about the wrong risks. This can have disastrous impacts.

Ease of recall suggests that if something is more easily recalled in memory it must occur with a higher probability.

The availability heuristic distorts our understanding of real risks.

“The attention which we lend to an experience is proportional to its vivid or interesting character; and it is a notorious fact that what interests us most vividly at the time is, other things equal, what we remember best.” — William James

When we make decisions we tend to be swayed by what we remember. What we remember is influenced by many things including beliefs, expectations, emotions, and feelings as well as things like frequency of exposure. Media coverage (e.g., Internet, radio, television) makes a big difference. When rare events occur they become very visible to us as they receive heavy coverage by the media. This means we are more likely to recall it, especially in the immediate aftermath of the event. However, recalling an event and estimating its real probability are two different things. If you’re in a car accident, for example, you are likely to rate the odds of getting into another car accident much higher than base rates would indicate.

Retrievability suggests that we are biased in assessments of frequency in part because of our memory structure limitations and our search mechanisms. It’s the way we remember that matters.

The retrievability and ease of recall biases indicate that the availability bias can substantially and unconsciously influence our judgment. We too easily assume that our recollections are representative and true and discount events that are outside of our immediate memory.

***

In Thinking Fast and Slow, Kahneman writes:

People tend to assess the relative importance of issues by the ease with which they are retrieved from memory—and this is largely determined by the extent of coverage in the media.

***

Nobel Prize winning Social Scientist and Father of Artificial Intelligence, Herbert Simon, wrote in Models of My life:

I soon learned that one wins awards mainly for winning awards: an example of what Bob Merton calls the Matthew Effect. It is akin also to the phenomenon known in politics as “availability,” or name recognition. Once one becomes sufficiently well known, one’s name surfaces automatically as soon as an award committee assembles.

* * *

According to Harvard professor Max Bazerman

Many life decisions are affected by the vividness of information. Although most people recognize that AIDS is a devastating disease, many individuals ignore clear data about how to avoid contracting AIDS. In the fall of 1991, however, sexual behavior in Dallas was dramatically affected by one vivid piece of data that may or may not have been true. In a chilling interview, a Dallas woman calling herself C.J. claimed she had AIDS and was trying to spread the disease out of revenge against the man who had infected her. After this vivid interview made the local news, attendance at Dallas AIDS seminary increased dramatically. Although C.J.’s possible actions were a legitimate cause for concern, it is clear that most of the health risks related to AIDS are not a result of one woman’s actions. There are many more important reasons to be concerned about AIDS. However, C.J.’s vivid report had a more substantial effect on many people’s behavior than the mountains of data available. The Availability Heuristic describes the inferences we make about even commonness based on the ease with which we can remember instances of that event… While this example of vividness may seem fairly benign, it is not difficult to see how the availability bias could lead managers to make potentially destructive workplace decisions. The following came from the experience of one of our MBA students: As a purchasing agent, he had to select one of several possible suppliers. He chose the firm with whose name was the most familiar to him. He later found out that the salience of the name resulted from recent adverse publicity concerning the firm’s extortion of funds from client companies! Managers conducting performance appraisals often fall victim to the availability heuristic. Working from memory, vivid instances of an employee’s behavior (either positive or negative) will be most easily recalled from memory, will appear more numerous than commonplace incidents, and will therefore be weighted more heavily in the performance appraisals. The recency of events is also a factor: Managers give more weight to performance during the three months prior to the evaluation than to the previous nine months of the evaluation period because it is more available in memory.

* * *

There are numerous implications for availability bias for investors.

A study by Karlsson, Loewenstein, and Ariely (2008) showed that people are more likely to purchase insurance to protect themselves after a natural disaster they have just experienced than they are to purchase insurance on this type of disaster before it happens.

Bazerman adds:

This pattern may be sensible for some types of risks. After all, the experience of surviving a hurricane may offer solid evidence that your property is more vulnerable to hurricanes than you had thought or that climate change is increasing your vulnerability to hurricanes.

Robyn M. Dawes, in his book Everyday Irrationality, says:

What is a little less obvious is that people can make judgments of the ease with which instances can come to mind without actually recalling specific instances. We know, for example, whether we can recall the presidents of the United States–or rather how well we can recall their names; moreover, we know at which periods of history we are better at recalling them than at which other periods. We can make judgments without actually listing in our minds the names of the specific presidents. This recall of ease of creating instances is not limited to actual experience, but extends to hypothetical experience as well. For example, subjects are asked to consider how many subcommittees of two people can be formed from a committee of eight, and either the same or other subjects are asked to estimate how many subcommittees of six can be formed from a committee of eight people. It is much easier to think about pairs of people than to think about sets of six people, with the result that the estimate of pairs tends to be much higher than the estimate of subsets of six. In point of logic, however, the number of subsets of two is identical that of six; the formation of a particular subset of two people automatically involves the formation of a particular subset consisting of the remaining six. Because these unique subsets are paired together, there are the same number of each. This availability to the imagination also creates a particularly striking irrationality, which can be termed with the conjunction fallacy or compound probability fallacy. Often combinations of events or entities are easier to think about than their components, because the combination might make sense whereas the individual component does not. A classic example is that of a hypothetical woman names Linda who is said to have been a social activist majoring in philosophy as a college undergraduate. What is the probability that at age thirty she is a bank teller? Subjects judge the probability as very unlikely. But when asked whether she might be a bank teller active in a feminist movement, subjects judge this combination to be more likely than for her to be a bank teller.

* * *

Retrievability (based on memory structures)

We are better at retrieving words from memory using the word’s initial letter than a random position like 3 (Tversky & Kahneman, 1973).

In 1984 Tverksy and Kahneman demonstrated the retrievability bias again when they asked participants in their study to estimate the frequency of seven-letter words that had the letter “n” in the sixth position. Their participants estimated such words to be less common than seven letter words ending in the more memorable “ing”. This response is incorrect. All seven letter words ending with “ing” also have an “n” in the sixth position. However, it’s easy to recall seven letter words ending with ing. As we demonstrated with Dawes above, this is another example of the conjunction fallacy.

Retail locations are chosen based on search as well, which explains why gas stations and retail stores are often “clumped” together. Consumers learn the location of a product and organize their mind accordingly. While you may not remember the name of all three gas stations on the same corner, your mind tells you that is where to go to find gas. Each station, assuming all else equal, then has a 1/3 shot at your business which is much better than gas stations you don’t visit because their location doesn’t resonate with your minds search. In order to maximize traffic stores must find locations that consumers associate with a product.

* * *

Exposure Effect

People tend to develop a preference for things because they are familiar with them. This is called the exposure effect. According to Titchener (1910) the exposure effect leads people to experience a “glow or warmth, a sense of ownership, a feeling of intimacy.”

The exposure effect applies only to things that are perceived as neutral to positive. If you are repeatedly exposed to something perceived as a negative stimuli it may in fact amplify negative feelings. For example, when someone is playing loud music you tend to have a lot of patience at first. However, as time goes on you get increasingly aggravated as your exposure to the stimuli increases.

The more we are exposed to something the easier it is to recall in our minds. The exposure effect influences us in many ways. Think about brands, stocks, songs, companies, and even the old saying “the devil you know.”

* * *

The Von Restorff Effect

“One of these things doesn’t belong,” can accurately summarize the Von Restorff Effect (also known as the isolation effect and novelty effect). In our minds, things that stand out are more likely to be remembered and recalled because we give increased attention to distinctive items in a set.

For example, if I asked you to remember the following sequence of characters “RTASDT9RTGS” I suspect the most common character remembered would be the “9” because it stands out and thus your mind gives it more attention.

The Von Restorff Effect leads us to Vivid evidence.

* * *

Vivid Evidence

According to William James in the Principles of Psychology:

An impression may be so exciting emotionally as to almost leave a scar upon the cerebral tissues; and thus originates a pathological delusion. For example “A woman attacked by robbers takes all the men whom she sees, even her own son, for brigands bent on killing her. Another woman sees her child run over by a horse; no amount of reasoning, not even the sight of the living child, will persuade her that he is not killed.

M. Taine wrote:

If we compare different sensations, images, or ideas, we find that their aptitudes for revival are not equal. A large number of them are obliterated, and never reappear throughout life; for instance, I drove through Paris a day or two ago, and though I saw plainly some sixty or eighty new faces, I cannot now recall any one of them; some extraordinary circumstance, a fit of delirium, or the excitement of hashish would be necessary to give me a chance at revival. On the other hand, there are sensations with a force of revival which nothing destroys or decreases. Though, as a rule, time weakens and impairs our strongest sensations, these reappear entire and intense, without having lost a particle of their detail, or any degree of their force. M. Breirre de Boismont, having suffered when a child from a disease of the scalp, asserts that ‘after fifty-five years have elapsed he can still feel his hair pulled out under the treatment of the ‘skull-cap.’–For my own part, after thirty years, I remember feature for feature the appearance of the theater to which I was taken for the first time. From the third row of boxes, the body of the theater appeared to me an immense well, red and flaming, swarming with heads; below, on the right, on a narrow floor, two men and a woman entered, went out, and re-entered, made gestures, and seemed to me like lively dwarfs: to my great surprise one of these dwarfs fell on his knees, kissed the lady’s hand, then hid behind a screen: the other, who was coming in, seemed angry, and raised his arm. I was then seven, I could understand nothing of what was going on; but the well of crimson velvet was so crowded, and bright, that after a quarter of an hour i was, as it were, intoxicated, and fell asleep. Every one of us may find similar recollections in his memory, and may distinguish them in a common character. The primitive impression has been accompanied by an extraordinary degree of attention, either as being horrible or delightful, or as being new, surprising, and out of proportion to the ordinary run of life; this it is we express by saying that we have been strongly impressed; that we were absorbed, that we could not think of anything else; that our other sensations were effaced; that we were pursued all the next day by the resulting image; that it beset us, that we could not drive it away; that all distractions were feeble beside it. It is by force of this disproportion that impressions of childhood are so persistent; the mind being quite fresh, ordinary objects and events are surprising… Whatever may be the kind of attention, voluntary or involuntary, it always acts alike; the image of an object or event is capable of revival, and of complete revival, in proportion to the degree of attention with which we have considered the object or event. We put this rule into practice at every moment in ordinary life.

An example from Freeman Dyson:

A striking example of availability bias is the fact that sharks save the lives of swimmers. Careful analysis of deaths in the ocean near San Diego shows that on average, the death of each swimmer killed by a shark saves the lives of ten others. Every time a swimmer is killed, the number of deaths by drowning goes down for a few years and then returns to the normal level. The effect occurs because reports of death by shark attack are remembered more vividly than reports of drownings.

Availability Bias is a Mental Model in the Farnam Street Mental Model Index","          Availability heuristic   From Wikipedia, the free encyclopedia    Jump to navigation  Jump to search  The availability heuristic , also known as availability bias, is a mental shortcut that relies on immediate examples that come to a given person's mind when evaluating a specific topic, concept, method or decision. The availability heuristic operates on the notion that if something can be recalled , it must be important, or at least more important than alternative solutions which are not as readily recalled. [1] Subsequently, under the availability heuristic, people tend to heavily weigh their judgments toward more recent information, making new opinions biased toward that latest news. [2] [3]  The availability of consequences associated with an action is positively related to perceptions of the magnitude of the consequences of that action. In other words, the easier it is to recall the consequences of something the greater those consequences are often perceived to be. Most notably, people often rely on the content of their recall if its implications are not called into question by the difficulty that they experience in bringing the relevant material to mind. [4]   Contents   1  Overview and history  2  Research  3  Explanations  4  Applications   4.1  Media  4.2  Health  4.3  Business and economy  4.4  Education  4.5  Criminal justice  4.6  Perceived risk  4.7  Vividness effects  4.8  Judging frequency and probability    5  Critiques   5.1  Ease of recall as a critique  5.2  Alternative explanations    6  See also  7  References  8  External links    Overview and history [ edit ]  In the late 1960s and early 1970s, Amos Tversky and Daniel Kahneman began work on a series of papers examining ""heuristic and biases "" used in the judgment under uncertainty . Prior to that, the predominant view in the field of human judgment was that humans are rational actors . Kahneman and Tversky explained that judgment under uncertainty often relies on a limited number of simplifying heuristics rather than extensive algorithmic processing. Soon, this idea spread beyond academic psychology, into law, medicine, and political science. This research questioned the descriptive adequacy of idealized models of judgment, and offered insights into the cognitive processes that explained human error without invoking motivated irrationality . [5] One simplifying strategy people may rely on is the tendency to make a judgment about the frequency of an event based on how many similar instances are brought to mind. In 1973, Amos Tversky and Daniel Kahneman first studied this phenomenon and labeled it the ""availability heuristic"". An availability heuristic is a mental shortcut that relies on immediate examples that come to a given person's mind when evaluating a specific topic, concept, method or decision. As follows, people tend to use a readily available fact to base their beliefs about a comparably distant concept. There has been much research done with this heuristic, but studies on the issue are still questionable with regard to the underlying process. Studies illustrate that manipulations intended to increase the subjective experience of ease of recall are also likely to affect the amount of recall. Furthermore, this makes it difficult to determine if the obtained estimates of frequency, likelihood, or typicality are based on participants' phenomenal experiences or on a biased sample of recalled information. [5]  However, some textbooks have chosen the latter interpretation introducing the availability heuristic as ""one's judgments are always based on what comes to mind"". For example, if a person is asked whether there are more words in the English language that begin with a t or k, the person will probably be able to think of more words that begin with the letter t, concluding that t is more frequent than k. [6]   Research [ edit ]  Chapman (1967) described a bias in the judgment of the frequency with which two events co-occur. This demonstration showed that the co-occurrence of paired stimuli resulted in participants overestimating the frequency of the pairings. [7] To test this idea, participants were given information about several hypothetical mental patients. The data for each patient consisted of a clinical diagnosis and a drawing made by the patient. Later, participants estimated the frequency with which each diagnosis had been accompanied by various features of the drawing. The subjects vastly overestimated the frequency of this co-occurrence (such as suspiciousness and peculiar eyes). This effect was labeled the illusory correlation . Tversky and Kahneman suggested that availability provides a natural account for the illusory-correlation effect. The strength of the association between two events could provide the basis for the judgment of how frequently the two events co-occur. When the association is strong, it becomes more likely to conclude that the events have been paired frequently. Strong associations will be thought of as having occurred together frequently. [8]  In Tversky & Kahneman's first examination of availability heuristics, subjects were asked, ""If a random word is taken from an English text, is it more likely that the word starts with a K, or that K is the third letter?"" They argue that English-speaking people would immediately think of many words that begin with the letter ""K"" (kangaroo, kitchen, kale), but that it would take a more concentrated effort to think of any words in which ""K"" is the third letter (acknowledge, ask). Results indicated that participants overestimated the number of words that began with the letter ""K"" and underestimated the number of words that had ""K"" as the third letter. Tversky and Kahneman concluded that people answer questions like these by comparing the availability of the two categories and assessing how easily they can recall these instances. In other words, it is easier to think of words that begin with ""K"", more than words with ""K"" as the third letter. Thus, people judge words beginning with a ""K"" to be a more common occurrence. In reality, however, a typical text contains twice as many words that have ""K"" as the third letter than ""K"" as the first letter. There are three times more words with ""K"" in the third position than words that begin with ""K"". [8]  In Tversky and Kahneman's seminal paper, they include findings from several other studies, which also show support for the availability heuristic. Apart from their findings in the ""K"" study, they also found:  When participants were shown two visual structures and asked to pick the structure that had more paths, participants saw more paths in the structure that had more obvious available paths. In the structure that participants chose, there were more columns and shorter obvious paths, making it more available to them. When participants were asked to complete tasks involving estimation, they would often underestimate the end result. Participants were basing their final estimation off of a quick first impression of the problem. Participants particularly struggled when the problems consisted of multiple steps. This occurred because participants were basing their estimation on an initial impression. Participants failed to account for the high rate of growth in the later steps due to the impression they formed in the initial steps. This was shown again in a task that asked participants to estimate the answer to a multiplication task, in which the numbers were presented as either 1x2x3x4x5x6x7x8 or 8x7x6x5x4x3x2x1. Participants who were presented the equation with the larger numbers first (8x7x6...), estimated a significantly higher result than participants with the lower numbers first (1x2x3...). Participants were given a short amount of time to make the estimation, thus participants based their estimates off of what was easily available, which in this case was the first few numbers in the sequence. [8]  Explanations [ edit ]  Many researchers have attempted to identify the psychological process which create the availability heuristic. Tversky and Kahneman argue that the number of examples recalled from memory is used to infer the frequency with which such instances occur. In an experiment to test this explanation, participants listened to lists of names containing either 19 famous women and 20 less famous men or 19 famous men and 20 less famous women. Subsequently, some participants were asked to recall as many names as possible whereas others were asked to estimate whether male or female names were more frequent on the list. The names of the famous celebrities were recalled more frequently compared to those of the less famous celebrities. The majority of the participants incorrectly judged that the gender associated with more famous names had been presented more often than the gender associated with less famous names. Tversky and Kahneman argue that although the availability heuristic is an effective strategy in many situations, when judging probability use of this heuristic can lead to predictable patterns of errors. [8]  Schwarz and his colleagues, on the other hand, proposed the ease of retrieval explanation, in which is the ease with which examples come to mind, not the number of examples, is used to infer the frequency of a given class. In a study by Schwarz and colleagues to test their explanation, participants were asked to recall either six or twelve examples of their assertive or very unassertive behavior. Participants were later asked to rate their own assertiveness. Pretesting had indicated that although most participants were capable of generating twelve examples, this was a difficult task. The results indicated that participants rated themselves as more assertive after describing six examples of assertive compared with unassertive behavior condition, but rated themselves as less assertive after describing twelve examples of assertive compared with unassertive behavior condition. The study reflected that the extent to which recalled content impacted judgment was determined by the ease with which the content could be brought to mind (it was easier to recall 6 examples than 12), rather than the amount of content brought to mind. [4]  Research by Vaugh (1999) looked at the effects of uncertainty on the use of the availability heuristic. College students were asked to list either three or eight different study methods they could use in order to get an A on their final exams. The researchers also manipulated the time during the semester they would ask the students to complete the questionnaire.  Approximately half of the participants were asked for their study methods during the third week of classes, and the other half were asked on last day of classes. Next, participants were asked to rate how likely they would be to get an A in their easiest and hardest classes.  Participants were then asked to rank the difficulty they experienced in recalling the examples they had previously listed. The researchers hypothesized that students would use the availability heuristic, based on the number of study methods they listed, to predict their grade only when asked at the beginning of the semester and about their hardest final. Students were not expected to use the availability heuristic to predict their grade at the end of the semester or about their easiest final. The researchers predicted this use of availability heuristic because participants would be uncertain about their performance throughout the semester. The results indicated that students used the availability heuristic, based on the ease of recall of the study methods they listed, to predict their performance when asked at the beginning of the semester and about their hardest final. If the student listed only three study methods, they predicted a higher grade at the end of the semester only on their hardest final. If students listed eight study methods, they had a harder time recalling the methods and thus predicted a lower final grade on their hardest final. The results were not seen in the easy final condition because the students were certain they would get an A, regardless of study method. The results supported this hypothesis and gave evidence to the fact that levels of uncertainty affect the use of the availability heuristic. [9]   Applications [ edit ]  Media [ edit ]  After seeing news stories about child abductions, people may judge that the likelihood of this event is greater. Media coverage can help fuel a person's example bias with widespread and extensive coverage of unusual events, such as homicide or airline accidents , and less coverage of more routine, less sensational events, such as common diseases or car accidents . For example, when asked to rate the probability of a variety of causes of death, people tend to rate ""newsworthy"" events as more likely [ citation needed ] because they can more readily recall an example from memory [ citation needed ] . Moreover, unusual and vivid events like homicides, shark attacks , or lightning are more often reported in mass media than common and un-sensational causes of death like common diseases. [ citation needed ]  For example, many people think that the likelihood of dying from shark attacks is greater than that of dying from being hit by falling airplane parts, when more people actually die from falling airplane parts. [10] When a shark attack occurs, the deaths are widely reported in the media whereas deaths as a result of being hit by falling airplane parts are rarely reported in the media. [11]  In a 2010 study exploring how vivid television portrayals are used when forming social reality judgments, people watching vivid violent media gave higher estimates of the prevalence of crime and police immorality in the real world than those not exposed to vivid television. These results suggest that television violence does in fact have a direct causal impact on participants' social reality beliefs. Repeated exposure to vivid violence leads to an increase in people's risk estimates about the prevalence of crime and violence in the real world. [12] Counter to these findings, researchers from a similar study argued that these effects may be due to effects of new information. Researchers tested the new information effect by showing movies depicting dramatic risk events and measuring their risk assessment after the film. Contrary to previous research, there were no long-term effects on risk perception due to exposure to dramatic movies. However, the study did find evidence of idiosyncratic effects of the movies - that is, people reacted immediately after the movies with enhanced or diminished risk beliefs, which faded after a period of 10 days. [13]   Health [ edit ]  Researchers examined the role of cognitive heuristics in the AIDS risk-assessment process. 331 physicians reported worry about on-the-job HIV exposure, and experience with patients who have HIV. By analyzing answers to questionnaires handed out, researchers concluded that availability of AIDS information did not relate strongly to perceived risk . [14]  Participants in a 1992 study read case descriptions of hypothetical patients who varied on their sex and sexual preference. These hypothetical patients showed symptoms of two different diseases. Participants were instructed to indicate which disease they thought the patient had and then they rated patient responsibility and interaction desirability. Consistent with the availability heuristic, either the more common ( influenza ) or the more publicized (AIDS) disease was chosen. [15]   Business and economy [ edit ]  One study sought to analyze the role of the availability heuristic in financial markets. Researchers defined and tested two aspects of the availability heuristic: [16]   Outcome Availability – availability of positive and negative investment outcomes, and  Risk Availability – availability of financial risk. [16]  On days of substantial stock market moves, abnormal stock price reactions to upgrades are weaker, than those to downgrades. These availability effects are still significant even after controlling for event-specific and company-specific factors. [16]  Similarly, research has pointed out that under the availability heuristic, humans are not reliable because they assess probabilities by giving more weight to current or easily recalled information instead of  processing all relevant information. Since information regarding the current state of the economy is readily available, researchers attempted to expose the properties of business cycles to predict the availability bias in analysts' growth forecasts. They showed the availability heuristic to play a role in analysis of forecasts and influence investments because of this. [17]  In effect, investors are using availability heuristic to make decisions and subsequently, may be obstructing their own investment success. An investor's lingering perceptions of a dire market environment may be causing them to view investment opportunities through an overly negative lens, making it less appealing to consider taking on investment risk, no matter how small the returns on perceived ""safe"" investments.  To illustrate, Franklin Templeton 's annual Global Investor Sentiment Survey 1 asked individuals how they believed the S&P 500 Index performed in 2009, 2010 and 2011. 66 percent of respondents stated that they believed the market was either flat or down in 2009, 48 percent said the same about 2010 and 53 percent also said the same about 2011.  In reality, the S&P 500 saw 26.5 percent annual returns in 2009, 15.1 percent annual returns in 2010 and 2.1 percent annual returns in 2011, meaning lingering perceptions based on dramatic, painful events are impacting decision-making even when those events are over. [18]  Additionally, a study by Hayibor and Wasieleski found that the availability of others who believe that a particular act is morally acceptable is positively related to others' perceptions of the morality of that act. This suggests that availability heuristic also has an effect on ethical  decision making and ethical behavior in organizations. [19]   Education [ edit ]  A study done by Craig R. Fox provides an example of how availability heuristics can work in the classroom. In this study, Fox tests whether difficulty of recall influences judgment, specifically with course evaluations among college students.  In his study he had two groups complete a course evaluation form.  He asked the first group to write two recommended improvements for the course (a relatively easy task) and then write two positives about the class. The second group was asked to write ten suggestions where the professor could improve (a relatively difficult task) and then write two positive comments about the course.  At the end of the evaluation both groups were asked to rate the course on a scale from one to seven.  The results showed that students asked to write ten suggestions (difficult task) rated the course less harshly because it was more difficult for them to recall the information. Most of the students in the group that were asked to fill in 10 suggestions didn't fill in more than two being unable to recall more instances where they were unsatisfied with the class. Students asked to do the easier evaluation with only two complaints had less difficulty in terms of availability of information, so they rated the course more harshly. [20]  A study done was testing the memory of children and the ease of recall. They were asked to learn a list of names and then to recall different amounts. Researchers found that when asked to recall lower amounts compared to larger amounts and then asked what was easier to remember. They responded the shorter list going along with the theory of availability heuristic. [21]   Criminal justice [ edit ]  The media usually focuses on violent or extreme cases, which are more readily available in the public's mind. This may come into play when it is time for the judicial system to evaluate and determine the proper punishment for a crime. In one study, respondents rated how much they agreed with hypothetical laws and policies such as ""Would you support a law that required all offenders convicted of unarmed muggings to serve a minimum prison term of two years?"" Participants then read cases and rated each case on several questions about punishment. As hypothesized, respondents recalled more easily from long-term memory stories that contain severe harm, which seemed to influence their sentencing choices to make them push for harsher punishments. This can be eliminated by adding high concrete or high contextually distinct details into the crime stories about less severe injuries. [22]  A similar study asked jurors and college students to choose sentences on four severe criminal cases in which prison was a possible but not an inevitable sentencing outcome. Respondents answering questions about court performance on a public opinion formulated a picture of what the courts do and then evaluated the appropriateness of that behavior. Respondents recalled from public information about crime and sentencing. This type of information is  incomplete because the news media present a highly selective and non- representative selection of crime, focusing on the violent and extreme, rather than the ordinary. This makes most people think that judges are too lenient. But, when asked to choose the punishments, the sentences given by students were equal to or less severe than those given by judges. In other words, the availability heuristic made people believe that judges and jurors were too lenient in the courtroom, but the participants gave similar sentences when placed in the position of the judge, suggesting that the information they recalled was not correct. [23]  Researchers in 1989 predicted that mock jurors would rate a witness to be more deceptive if the witness testified truthfully before lying than when the witness was caught lying first before telling the truth. If the availability heuristic played a role in this, lying second would remain in jurors' minds (since it was more recent) and they would most likely remember the witness lying over the truthfulness. To test the hypothesis, 312 university students played the roles of mock jurors and watched a videotape of a witness presenting testimony during a trial. Results confirmed the hypothesis, as mock jurors were most influenced by the most recent act. [24]   Perceived risk [ edit ]  Further information: Perceived risk  Previous studies have indicated that explaining a hypothetical event makes the event seem more likely through the creation of causal connections. However, such effects could arise through the use of the availability heuristic; that is, subjective likelihood is increased by an event becoming easier to imagine. [25]  A study done asked those participating to pick between two illnesses. Those doing the study wanted to know which disease they thought was more likely to cause death. In the study they asked participants to choose between a stroke and asthma as to which one someone was more likely to die from. The researchers concluded that it depended on what experiences were available to them. If they knew someone or heard of someone that died from one of the diseases that is the one they perceived to be a higher risk to pass away from. [26]   Vividness effects [ edit ]  Two studies with 108 undergraduates investigated vivid information and its impact on social judgment and the availability heuristic and its role in mediating vividness effects. In study 1, Subjects listened to a tape recording that described a woman who lived with her 7-year-old son. Subjects then heard arguments about the woman's fitness as a parent and were asked to draw their own conclusions regarding her fitness or unfitness. Concrete and colorful language was found to influence judgments about the woman's fitness as a mother. In study 2, a series of male and female names was presented to subjects; for each name, subjects were told the university affiliation of the individual (Yale or Stanford). When some names were presented, subjects were simultaneously shown a photograph that purportedly portrayed the named individual. Subsequently, to assess what subjects could remember (as a measure of availability), each name was re-presented, as well as the appropriate photograph if one had been originally presented. The study considered whether the display or non-display of photographs biased subjects' estimates as to the percentage of Yale (vs Stanford) students in the sample of men and women whose names appeared on the original list, and whether these estimated percentages were causally related to the respondents' memory for the college affiliations of the individual students on the list. The presence of photographs affected judgments about the proportion of male and female students at the two universities. Such effects have typically been attributed to the ready accessibility of vividly presented information in memory—that is, to the availability heuristic. In both studies, vividness affected both availability (ability to recall) and judgments. However, causal modeling results indicated that the availability heuristic did not play a role in the judgment process. [27]   Judging frequency and probability [ edit ]  In general, availability is correlated with ecological frequency, but it is also affected by other factors. Consequently, the reliance on the availability heuristic leads to systematic biases. Such biases are demonstrated in the judged frequency of classes of words, of combinatoric outcomes, and of repeated events. The phenomenon of illusory correlation is explained as an availability bias. [8]  In the original Tversky and Kahneman (1973) research, three major factors that are discussed are the frequency of repetition, frequency of co-occurrence, and illusory correlation. The use of frequency of repetition aids in the retrieval of relevant instances. The idea behind this phenomenon, is that the more an instance is repeated within a category or list, the stronger the link between the two instances becomes. Individuals then use the strong association between the instances to determine the frequency of an instance. Consequently, the association between the category or list and the specific instance, often influences frequency judgement. Frequency of co-occurrence strongly relates to Frequency of repetition, such that the more an item-pair is repeated, the stronger the association between the two items becomes, leading to a bias when estimating frequency of co-occurrence. Due to the phenomena of frequency of co-occurrence, illusory correlations also often play a big role. [8]  Another factor that affects the availability heuristic in frequency and probability is exemplars. Exemplars are the typical examples that stand out during the process of recall. If asked what participants thought different set sizes were (how many men and how many women are in the class), participants would use exemplars to determine the size of each set.  Participants would derive their answer on ease of recall of the names that stood out. Participants read a list of names of members of a class for 30 seconds, and then participants were asked the male to female ratio of the class. The participant's answer would depend on the recall of exemplars. If the participant reading the list recalled seeing more common male names, such as Jack, but the only female names in the class were uncommon names, such as Deepika, then the participant will recall that there were more men than women. The opposite would be true if there were more common female names on the list and uncommon male names. Due to the availability heuristic, names that are more easily available are more likely to be recalled, and can thus alter judgments of probability. [28]  Another example of the availability heuristic and exemplars would be seeing a shark in the ocean. Seeing a shark has a greater impact on an individual's memory than seeing a dolphin.  If someone sees both sharks and dolphins in the ocean, they will be less aware of seeing the dolphins, because the dolphins had less of an impact on their memory. Due to the greater impact of seeing a shark, the availability heuristic can influence the probability judgement of the ratio of sharks and dolphins in the water. Thus, an individual who saw both a shark and a dolphin would assume a higher ratio of sharks in the water, even if there are more dolphins in reality. [28]   Critiques [ edit ]  Ease of recall as a critique [ edit ]  One of the earliest and most powerful critiques of the original Tversky and Kahneman [29] study on the availability heuristic was the Schwarz et al. [4] study which found that the ease of recall was a key component in determining whether a concept became available.  Many studies since this criticism of the original availability heuristic model have repeated this initial criticism, that the ease of recall factor became an integral facet of the availability heuristic itself (see Research section).  Alternative explanations [ edit ]  Much of the criticism against the availability heuristic has claimed that making use of the content that becomes available in our mind is not based on the ease of recall as suggested by Schwarz et al. [4] For example, it could be argued that recalling more words that begin with K than words with the third letter being K could arise from how we categorize and process words into our memory.  If we categorize words by first letter, and recall them through the same process, this would show more support for the representatives heuristic than the availability heuristic.  Based on the possibility of explanations such as these, some researchers have claimed that the classic studies on the availability heuristic are too vague in that they fail to account for people's underlying mental processes.  Indeed, a study conducted by Wanke et al. demonstrated this scenario can occur in situations used to test the availability heuristic. [30] Future studies should be conducted to determine if and when this alternative explanation will occur. A second line of study has shown that frequency estimation may not be the only strategy we use when making frequency judgments.  A recent line of research has shown that our situational working memory can access long term memories, and this memory retrieval process includes the ability to determine more accurate probabilities. [31] This finding suggests that more research should be conducted to determine how much memory activation affects the availability heuristic.  See also [ edit ]    Psychology portal  Affect heuristic  Agenda-setting theory  Anecdotal evidence  Anecdotal value  Attribute substitution  Confirmation bias  Cultivation theory  Gambler's fallacy  List of biases in judgment and decision making  Mean world syndrome  Misleading vividness  Omission bias  Processing fluency  Representativeness heuristic  Salience bias  Texas sharpshooter fallacy  References [ edit ]    ^  Esgate, Anthony; Groome, David (2005). An Introduction to Applied Cognitive Psychology . Psychology Press. p. 201. ISBN  978-1-84169-318-7 .   ^  ""Availability heuristic - Oxford Reference"" .  Cite journal requires |journal= ( help )   ^  Phung, Albert. ""Behavioral Finance: Key Concept- Overreaction and Availability Bias"" . Investopedia. February 25, 2009. p.10. December 1, 2013.   ^ a  b  c  d  Schwarz, Norbert; Bless, Herbert; Strack, Fritz; Klumpp, Gisela; Rittenauer-Schatka, Helga; Simons, Annette (1991). ""Ease of retrieval as information: Another look at the availability heuristic"" . Journal of Personality and Social Psychology . 61 (2): 195–202. doi : 10.1037/0022-3514.61.2.195 .   ^ a  b  Gilovich, Thomas; Griffin, Dale; Kahneman, Daniel (2002-07-08). Heuristics and Biases: The Psychology of Intuitive Judgment . ISBN  9780521796798 .   ^  Gilovich, T. D.; Griffin, D.; Kahneman, D. (2002). ""Heuristics and Biases: The Psychology of Intuitive Judgment"". New York, NY: Cambridge University Press.   ^  Chapman, L.J (1967). ""Illusory correlation in observational report"". Journal of Verbal Learning . 6 : 151–155. doi : 10.1016/s0022-5371(67)80066-5 .   ^ a  b  c  d  e  f  Tversky, Amos; Kahneman, Daniel (1973). ""Availability: A heuristic for judging frequency and probability"". Cognitive Psychology . 5 (2): 207–232. doi : 10.1016/0010-0285(73)90033-9 . ISSN  0010-0285 .   ^  Vaugh, Leigh Ann (1999). ""Effects of uncertainty on use of the availability of heuristic for self-efficacy judgments""  (PDF) . European Journal of Social Psychology . 29 (2/3): 407–410. doi : 10.1002/(sici)1099-0992(199903/05)29:2/3<407::aid-ejsp943>3.0.co;2-3 . hdl : 2027.42/34564 .   ^  ""Odds and ends - The San Diego Union-Tribune"" . legacy.sandiegouniontribune.com . Archived from the original on 2019-05-25.   ^  Read, J.D. (1995). ""The availability heuristic in person identification: The sometimes misleading consequences of enhanced contextual information"". Applied Cognitive Psychology . 9 (2): 91–121. doi : 10.1002/acp.2350090202 .   ^  Riddle, Karen (2010). ""Always on My Mind: Exploring How Frequent, Recent, and Vivid Television Portrayals Are Used in the Formation of Social Reality Judgments"". Media Psychology . 13 (2): 155–179. doi : 10.1080/15213261003800140 .   ^  Sjöberg, Lennart; Engelberg, Elisabeth (2010). ""Risk Perception and Movies: A Study of Availability as a Factor in Risk Perception"". Risk Analysis . 30 (1): 95–106. doi : 10.1111/j.1539-6924.2009.01335.x . ISSN  0272-4332 . PMID  20055978 .   ^  Heath, Linda; Acklin, Marvin; Wiley, Katherine (1991). ""Cognitive Heuristics and AIDS Risk Assessment Among Physicians"". Journal of Applied Social Psychology . 21 (22): 1859–1867. doi : 10.1111/j.1559-1816.1991.tb00509.x . ISSN  0021-9029 .   ^  Triplet, R.G (1992). ""Discriminatory biases in the perception of illness: The application of availability and representativeness heuristics to the AIDS crisis"". Basic and Applied Social Psychology . 13 (3): 303–322. doi : 10.1207/s15324834basp1303_3 .   ^ a  b  c  Kliger, Doron; Kudryavtsev, Andrey (2010). ""The Availability Heuristic and Investors' Reaction to Company-Specific Events"". Journal of Behavioral Finance . 11 (1): 50–65. doi : 10.1080/15427561003591116 . ISSN  1542-7560 .   ^  Lee, Byunghwan; O'Brien, John; Sivaramakrishnan, K. (2008). ""An Analysis of Financial Analysts' Optimism in Long-term Growth Forecasts"". Journal of Behavioral Finance . 9 (3): 171–184. doi : 10.1080/15427560802341889 . ISSN  1542-7560 .   ^  http://www.businessinsider.com/the-availability-bias-is-driving-investor-decisions-2012-10 . Franklin Templeton Investments. ""Investors Should Beware The Role of 'Availability Bias'"". Business Insider. Oct. 6, 2012. Dec. 1, 2013.   ^  Hayibor, Sefa; Wasieleski, David M. (2008). ""Effects of the Use of the Availability Heuristic on Ethical Decision-Making in Organizations"". Journal of Business Ethics . 84 (S1): 151–165. doi : 10.1007/s10551-008-9690-7 . ISSN  0167-4544 .   ^  Fox, Craig R. (2006). ""The availability heuristic in the classroom: How soliciting more criticism can boost your course ratings""  (PDF) . Judgment and Decision Making . 1 (1): 86–90. ISSN  1930-2975 .   ^  Geurten, Marie; Willems, Sylvie; Germain, Sophie; Meulemans, Thierry (November 2015). ""Less is more: The availability heuristic in early childhood"". British Journal of Developmental Psychology . 33 (4): 405–410. doi : 10.1111/bjdp.12114 . PMID  26332945 .   ^  Stalans, Loretta J. (1993). ""Citizens' crime stereotypes, biased recall, and punishment preferences in abstract cases: The educative role of interpersonal sources"". Law and Human Behavior . 17 (4): 451–470. doi : 10.1007/BF01044378 . ISSN  1573-661X .   ^  Diamond, Shari Seidman; Stalans, Loretta J. (1989). ""The myth of judicial leniency in sentencing"". Behavioral Sciences & the Law . 7 (1): 73–89. doi : 10.1002/bsl.2370070106 . ISSN  0735-3936 .   ^  deTurck, M. A.; Texter, L. A.; Harszlak, J. J. (1989). ""Effects of Information Processing Objectives on Judgments of Deception Following Perjury"". Communication Research . 16 (3): 434–452. doi : 10.1177/009365089016003006 . ISSN  0093-6502 .   ^  Carroll, John S. (1978). ""The effect of imagining an event on expectations for the event: An interpretation in terms of the availability heuristic"". Journal of Experimental Social Psychology . 14 (1): 88–96. doi : 10.1016/0022-1031(78)90062-8 . ISSN  0022-1031 .   ^  Pachur, Thorsten; Hertwig, Ralph; Steinmann, Florian (2012). ""How do people judge risks: Availability heuristic, affect heuristic, or both?"". Journal of Experimental Psychology: Applied . 18 (3): 314–330. doi : 10.1037/a0028279 . hdl : 11858/00-001M-0000-0024-F052-7 . ISSN  1939-2192 . PMID  22564084 .   ^  Shedler, Jonathan; Manis, Melvin (1986). ""Can the availability heuristic explain vividness effects?"". Journal of Personality and Social Psychology . 51 (1): 26–36. doi : 10.1037/0022-3514.51.1.26 . ISSN  1939-1315 .   ^ a  b  Manis, Melvin; Jonides, Jonathan; Shedler, John; Nelson, Thomas (1993). ""Availability Heuristic in Judgments of Set Size and Frequency of Occurrence"". Journal of Personality & Social Psychology . 65 (3): 448–457. doi : 10.1037/0022-3514.65.3.448 .   ^  Tversky, A.; Kahneman, D. (1974). ""Judgment under Uncertainty: Heuristics and Biases"". Science . 185 (4157): 1124–1131. Bibcode : 1974Sci...185.1124T . doi : 10.1126/science.185.4157.1124 . ISSN  0036-8075 . PMID  17835457 .   ^  Wänke, Michaela; Schwarz, Norbert; Bless, Herbert (1995). ""The availability heuristic revisited: Experienced ease of retrieval in mundane frequency estimates"". Acta Psychologica . 89 (1): 83–90. doi : 10.1016/0001-6918(93)E0072-A . ISSN  0001-6918 .   ^  Hulme, Charles; Roodenrys, Steven; Brown, Gordon; Mercer, Robin (1995). ""The role of long-term memory mechanisms in memory span"". British Journal of Psychology . 86 (4): 527–536. doi : 10.1111/j.2044-8295.1995.tb02570.x . ISSN  0007-1269 .    External links [ edit ]  How Belief Works – an article on the origins of the availability bias.  v t e Biases Cognitive biases  Actor–observer  Acquiescence  Ambiguity  Anchoring  Attentional  Attribution  Authority  Automation  Belief  Blind spot  Choice-supportive  Confirmation  Congruence  Cultural  Distinction  Dunning–Kruger  Egocentric  Emotional  Extrinsic incentives  Fading affect  Framing  Correspondence  Halo effect  Hindsight  Horn effect  Hostile attribution  Impact  Implicit  In-group  Mere-exposure effect  Negativity  Normalcy  Omission  Optimism  Out-group homogeneity  Outcome  Overton window  Precision  Present  Pro-innovation  Response  Restraint  Self-serving  Social comparison  Status quo  Time-saving  Trait ascription  von Restorff effect  Zero-risk  In animals  Statistical biases  Estimator  Forecast  Healthy user  Information  Psychological  Lead time  Length time  Non-response  Observer  Omitted-variable  Participation  Recall  Sampling  Selection  Self-selection  Social desirability  Spectrum  Survivorship  Systematic error  Systemic  Verification  Wet  Other biases  Academic  Funding  FUTON  Inductive  Infrastructure  Inherent  In education  Media  False balance  Vietnam War  Norway  South Asia  Sweden  United States  Arab–Israeli conflict  Ukraine  Net  Political bias  Publication  Reporting  White hat  Bias reduction  Cognitive bias mitigation  Debiasing  Heuristics in judgment and decision-making  Lists: General  ·  Memory  v t e Fallacies ( list ) Formal In propositional logic  Affirming a disjunct  Affirming the consequent  Denying the antecedent  Argument from fallacy  In quantificational logic  Existential  Illicit conversion  Proof by example  Quantifier shift  Syllogistic fallacy  Affirmative conclusion from a negative premise  Exclusive premises  Existential  Necessity  Four terms  Illicit major  Illicit minor  Negative conclusion from affirmative premises  Undistributed middle   Masked man  Mathematical fallacy  Informal Equivocation  Equivocation  False equivalence  False attribution  Quoting out of context  Loki's Wager  No true Scotsman  Reification  Question-begging fallacies  Circular reasoning / Begging the question  Loaded language  Leading question  Compound question / Loaded question / Complex question  No true Scotsman  Correlative-based fallacies  False dilemma  Perfect solution  Denying the correlative  Suppressed correlative  Illicit transference  Composition  Division  Ecological  Secundum quid  Accident  Converse accident  Faulty generalization  Anecdotal evidence  Sampling bias  Cherry picking  McNamara  Base rate / Conjunction  Double counting  False analogy  Slothful induction  Overwhelming exception  Vagueness / Ambiguity  Accent  False precision  Moving the goalposts  Quoting out of context  Slippery slope  Sorites paradox  Syntactic ambiguity  Questionable cause  Animistic  Furtive  Correlation implies causation Cum hoc  Post hoc  Gambler's  Inverse  Regression  Single cause  Slippery slope  Texas sharpshooter  Fallacies of relevance Appeals to emotion  Fear  Flattery  Novelty  Pity  Ridicule  Think of the children  In-group favoritism  Invented here / Not invented here  Island mentality  Loyalty  Parade of horribles  Spite  Stirring symbols  Wisdom of repugnance  Genetic fallacies Ad hominem  Appeal to motive  Association  Reductio ad Hitlerum  Godwin's law  Reductio ad Stalinum  Bulverism  Poisoning the well  Tone  Tu quoque  Whataboutism   Authority  Accomplishment  Ipse dixit  Poverty / Wealth  Etymology  Nature  Tradition / Novelty  Chronological snobbery  Appeals to consequences  Argumentum ad baculum  Wishful thinking   Ad nauseam  Argument to moderation  Argumentum ad populum  Appeal to the stone / Proof by assertion  Ignoratio elenchi  Argument from silence  Invincible ignorance  Moralistic / Naturalistic  Motte-and-bailey fallacy  Rationalization  Red herring  Two wrongs make a right  Special pleading  Straw man  Cliché  I'm entitled to my opinion    Category       Retrieved from "" https://en.wikipedia.org/w/index.php?title=Availability_heuristic&oldid=964955134 ""  Categories : Prospect theory Cognitive biases Causal fallacies Inductive fallacies Heuristics Hidden categories: CS1 errors: missing periodical All articles with unsourced statements Articles with unsourced statements from November 2018         Navigation menu      Personal tools      Not logged in Talk Contributions Create account Log in        Namespaces      Article Talk        Variants              Views      Read Edit View history        More           Search                     Navigation      Main page Contents Current events Random article About Wikipedia Contact us Donate Wikipedia store       Contribute      Help Community portal Recent changes Upload file       Tools      What links here Related changes Upload file Special pages Permanent link Page information Wikidata item Cite this page       Print/export      Download as PDF Printable version       Languages      Afrikaans العربية Čeština Deutsch Español فارسی Français Italiano עברית Magyar Nederlands Polski Português Русский Sicilianu Svenska ไทย Українська  Edit links        This page was last edited on 28 June 2020, at 15:59 (UTC) .  Text is available under the Creative Commons Attribution-ShareAlike License ;
additional terms may apply.  By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc. , a non-profit organization.    Privacy policy  About Wikipedia  Disclaimers  Contact Wikipedia  Developers  Statistics  Cookie statement  Mobile view          "
8,overconfidence(fallacy OR bias OR heuristic) (coronavirus OR covid OR pandemic),1,https://towardsdatascience.com/overcoming-confirmation-bias-during-covid-19-51a64205eceb,"Overcoming confirmation bias during COVID-19

Why confirmation bias is the archnemesis of data science and how you can fight it

Two weeks ago, I published a decision-making guide to help you navigate the choices you’re grappling with during the COVID-19 pandemic.

This week, let’s talk about a nasty psychological effect which might be ruining your ability to cope effectively with the information you’re being flooded with daily: confirmation bias.

What is it?

“Confirmation bias is the tendency to search for, interpret, favor, and recall information in a way that confirms or strengthens one’s prior personal beliefs.” — Wikipedia

Yeah, but what is it?

This:

Illustration by Paul J.

If you’re keen to nerd out on psychology, let’s separate confirmation bias from some other phenomena:

Confirmation bias : your existing opinion changes how you perceive information.

your changes how you perceive information. Wishful blindness : your self-interest changes how you perceive information, especially in the context of ethical decision-making. (Not to be confused with willful blindness, which is a term from law.)

your changes how you perceive information, especially in the context of ethical decision-making. (Not to be confused with willful blindness, which is a term from law.) “Filter bubble” refers to intellectual isolation in online echo chambers. If an algorithm is designed to prioritize content you’ll like, and you tend to like content from people who already agree with you, then you’re less likely to be exposed to content that might change your mind.

I won’t bore you with advice on how to crawl out of an echo chamber — you’ve heard it all already. “Something something …intellectual discipline to seek… something something… people who disagree with you.” Right? Right.

Wishful blindness is a tricky one, since it’s a relatively new theory without much research on how to overcome it. Confirmation bias, though, has been on the dissection table since the 1960s, so let’s focus on that.

Filter bubbles are about skewed information, while confirmation bias is about skewed perception.

Confirmation bias is not the situation where your social media feed agrees with itself too much. It’s far more subtle: even if you’re exposed to information that disagrees with your opinion, you might not take it in. You might misremember it. You might find a reason to ignore it. You’ll keep digging until the numbers you see are the ones you wanted to see. The mind is funny like that.

Confirmation bias means we can all look at the same number and perceive it differently. A fact is no longer just a fact.

In other words, confirmation bias is the archnemesis of data science since it means that a fact is no longer just a fact, no matter how much math and science you throw into getting it. You and I can look at the same number and perceive it differently. Diligently exposing yourself to better information sources is not enough to overcome a problem that starts behind your eyes.

Confirmation bias is the archnemesis of data science.

You should be careful when it comes to trusting your own brain. (That traitor!) If you start with a strong uninformed opinion and go foraging for information, you’re likely to end up with the same opinion after you’re finished. Why bother? Unless you fight confirmation bias up front and do things in the right order, your foray into data is destined to be a gooey waste of time.

Interested in learning more about skewed perception of data? See my article on apophenia.

Confirmation bias and COVID-19

In a nutshell: If you have strong opinions about COVID-19 and then you go looking for evidence that supports them, you’ll think you see it… no matter how outlandish those opinions are. You’ll also have a harder time absorbing evidence that points in the opposite direction.

If you’re feeling scared and you go looking for information to make you feel better, confirmation bias can make you end up feeling worse instead.

To make matters worse, whenever you’re panicking and you try to soothe yourself with a nice relaxing internet session, you’re likely to find more reasons to panic.

Being quick to panic is not a virtue — it’s generally counterproductive.

While panic can give you an adrenaline surge — “confirmation bias: one weird trick to boost your at-home fitness routine” — it doesn’t help you make better decisions. It’s generally counterproductive, impairs your ability to think clearly, and does nothing good for your mood or your ability to cope with stress. No matter how bad your situation, level-headed realism will serve you better than panic.

How to fight confirmation bias

The upshot is that a strong starting opinion can mess up both your decision-making and your mood. Luckily, there are four easy ways you can fight confirmation bias:

Hold your opinions loosely.

Emphasize decisions, not opinions.

Focus on the things you have control over.

Change the order in which you approach information.

Hold your opinions loosely

This one is perhaps easier said than done, especially if you’ve been stewing in headlines for a while, but it’s good intellectual practice to keep an open mind and not to take your opinions too seriously, even if they’re based on plenty of data (hi, Bayesians).

Here’s another bias worth knowing about: overconfidence bias. (“A well-established bias in which a person’s subjective confidence in his or her judgements is reliably greater than the objective accuracy of those judgements, especially when confidence is relatively high.” — Wikipedia) Yeah, but what is it? Something you can reduce by developing the habit of holding your opinions loosely.

Holding your opinions loosely reduces their ability to skew your perception.

The best decision-makers I know are adaptable. They have the skills to take in new information and admit they were wrong. Before you run off to admire leaders who are unwavering in their judgments (steadfast and loyal, right?), take a moment to see them the way a psychologist seem them: they’re so hampered by confirmation bias that they’re unable to process new information appropriately (or they’re superb actors).

If you’re keen to curb your own tendencies towards hubris, take a page out of the scientist’s book. Ever noticed how research papers in applied science are filled with caveats, humbly reminding the reader about unknowns that could invalidate their conclusions? That’s not a bad standard to aim for.

Emphasize decisions, not opinions

In my, er, opinion, the best way to loosen the grip your opinions have on both your emotions and your decisions is to get into the habit of taking a decision-first perspective. If an opinion falls in the forest and no action is affected by it, did the opinion matter?

It’s through our decisions — our actions — that we affect the world around us.

Imagine that I believe that the Loch Ness monster is real. If my belief does not influence my decisions — interactions with the world — in any way, what harm is there in it? On the other hand, if my belief does influence my actions (consciously or subconsciously), then perhaps I should evaluate my opinion in context of the actions that it stands to influence. That involves thinking about the actions (in order of importance) before the opinion and then forming hypotheses for testing as a second step. My article on that is here.

“Will this information make me act differently?”

Whenever you find yourself receiving new information, remember to ask yourself: “is this actionable?” If yes, which of your decisions does it affect?

Focus on the things you have control over

When it comes to COVID-19, many of us already find our decision-making constrained by new rules. If we don’t take a moment to recognize that some decisions we’ve been worrying about are no longer on the table, we’ll waste unnecessary energy on opinions without a function.

Wouldn’t you prefer to focus on making decisions and plans about things that are under your control? (Or to use that time on activities that bring you joy?)

For example, here in NYC (yup, I know) I no longer have the option to go to the theatre this month. Whatever my opinion about how being in a crowded space might affect my health — I have such an opinion, loosely held, from reading WHO publications — there’s no decision that I can make which takes that information as input.

I no longer have the option to be in a crowd this month even if I wanted to. That decision is not on the table.

Perhaps I’d be better served by shifting my cognitive efforts elsewhere, towards decisions that are on the table for me, such as whether to help out remotely (yes), whether to order toilet paper online (no, that’s what all my textbooks are for), and whether to invest mountains of effort into filming an at-home video series about data science (perhaps not, unless the videos I already put on YouTube get more traffic).

Broadway is dark and Times Square is eerily empty these days. Source: Spencer Platt/Getty Images/AFP.

Stepping away from agonizing over things you can’t influence isn’t a call for ignorance. Compare the following ways to use your mental energy:

Worrying about a decision your mayor is grappling with while trying to think about how you would make it in their place. Examining your mayor’s decision skills from the perspective of a decision you’re making about them (e.g. to vote for/against the mayor’s reelection). Considering which actions you should take— if any — in response to (or in preparation for) each of the possible options your mayor is choosing between. Deciding whether you should try to exert your own influence/effort to affect the mayor’s decision or its potential consequences.

Of these, (2)-(4) are the more useful perspectives (unless your interest is academic), while (1) is good practice for students of decision-making (“How would I make this decision if I were in charge and what skills can I learn here?”) but not an efficient use of headspace for the emotionally overwhelmed.

The difference is not what information you seek. It’s how you seek it. As a bonus, you may find that you have control over more than you realize.

Notice that the difference isn’t a matter of being more or less informed. The difference is whether you’re explicitly focusing your energy on decisions that are yours to make (or influence). Obsessing over someone else’s decisions is likely to leave you feeling powerless and inundated with ambiguity (especially if the person responsible for the decision has more information than you). If you find yourself taking perspectives like (1) during a high-stress time, shifting to (2), (3), or (4) might bring you some relief. As a bonus, you may find that you have control over more than you realize.

Change the order in which you approach information

Now that you’re focused on actions and decisions, it’s time to reveal the big punchline: the strongest antidote to confirmation bias is planning your decision-making before you seek information.

Frame your decision-making in a way that prevents you from moving the goalposts after you’ve seen where the ball landed.

In other words, it’s important to frame your decision-making in a way that prevents you from moving the goalposts after you’ve seen where the ball landed. Curious to learn more? I have a general article plus a step-by-step COVID-19 decision-making guide to help you out.","        Skip to main content                   Home  About  Submit  ALERTS / RSS             Search for this keyword           Advanced Search                               New Results  Omitted variable bias in GLMs of neural spiking activity   View ORCID Profile Ian H.  Stevenson  doi: https://doi.org/10.1101/317511   Ian H. Stevenson 1 University of Connecticut, Department of Psychological Sciences 2 University of Connecticut, Department of Biomedical Engineering 3 CT Institute for Brain and Cognitive Sciences Find this author on Google Scholar Find this author on PubMed Search for this author on this site ORCID record for Ian H. Stevenson      Abstract Full Text Info/History Metrics Preview PDF          Abstract Generalized linear models (GLMs) have a wide range of applications in systems neuroscience describing the encoding of stimulus and behavioral variables as well as the dynamics of single neurons. However, in any given experiment, many variables that impact neural activity are not observed or not modeled. Here we demonstrate, in both theory and practice, how these omitted variables can result in biased parameter estimates for the effects that are included. In three case studies, we estimate tuning functions for common experiments in motor cortex, hippocampus, and visual cortex. We find that including traditionally omitted variables changes estimates of the original parameters and that modulation originally attributed to one variable is reduced after new variables are included. In GLMs describing single-neuron dynamics, we then demonstrate how post-spike history effects can also be biased by omitted variables. Here we find that omitted variable bias can lead to mistaken conclusions about the stability of single neuron firing. Omitted variable bias can appear in any model with confounders – where omitted variables modulate neural activity and the effects of the omitted variables covary with the included effects. Understanding how and to what extent omitted variable bias affects parameter estimates is likely to be important for interpreting the parameters and predictions of many neural encoding models. Introduction Regression models have been widely used in systems neuroscience to explain how external stimulus and task variables as well as internal state variables may relate to observed neural activity ( Brown et al., 2003 ; Kass et al., 2005 ). However, in many cases, the full set of variables that explain the activity of the observed neurons is not observed or is not even known. It is important to recognize that, in these cases, omitted variables can cause the parameter estimates for the effects that are included in a regression model to be biased. That is, parameter estimates for the modeled effects would be different if other, omitted variables were to be included in the model ( Box, 1966 ). In experiments from behaving animals ( Niell and Stryker, 2010 ; Reimer et al., 2014 ), but also in more controlled sensory tasks ( Kelly et al., 2010 ; Arandia-Romero et al., 2016 ), there is growing evidence that neural activity is affected by many more variables than are typically considered relevant ( Kandler et al., 2017 ; Stringer et al., 2018 ). At the same time, although it has long been a concern in statistics ( Pearl, 2009 ) and has received some attention in other fields ( Greenland, 1989 ; Clarke, 2005 ), omitted variable bias, as a general problem, appears underappreciated in systems neuroscience. Here we demonstrate why systematically considering omitted variable bias may be important in neural data analysis and examine how omitted variable bias can affect one popular framework for describing neural spiking activity – the generalized linear model (GLM) with Poisson observations. In general, regression methods aim to estimate variations in a response variable as a function of other variables or covariates. When the goal of modeling is to maximize prediction accuracy, such as with brain machine interfaces, interpreting the model parameters may not be a high priority. However, in many other cases, parameter estimates are, at least to some extent, interpreted and analyzed. For instance, tuning curves or receptive fields may be measured and compared under different stimulus or task conditions or before and after a manipulation. In fully controlled experiments where the covariates are assigned at random, estimated coefficients can often be interpreted as estimates of causal effects ( Gelman and Hill, 2007 ). However, for many cases in neuroscience, it may be difficult or impossible to completely control or randomize all the relevant variables. In modeling neural activity, omitted variable bias can appear in any situation where neurons are modulated by omitted variables and the omitted variables (often called confounders) are not independent from the variables included in the model – the ones whose effects we are trying to estimate. Minimizing the influence of confounding variables is a major part of most experimental design ( Rust and Movshon, 2005 ), and the statistical effects of confounding variables are well understood (Wasserman, 2004). However, when the goal of modeling is description or explanation ( Shmueli, 2010 ), the effects of these omitted variables are frequently neglected. To give a concrete example, imagine an idealized neuron in primary motor cortex (M1) whose firing, unlike typical M1 neurons ( Georgopoulos et al., 1982 ), is not at all modulated by reach direction but, instead, is modulated by reach speed ( Fig 1 ). In a typical experimental setting, an animal’s reach directions are randomized, but reach speed cannot be randomized or tightly controlled. If the average speed differs across reach directions, such a hypothetical neuron will appear to be tuned to reach direction, despite not being directly affected by direction. First, fitting a typical tuning curve for reach direction, we would infer that such a neuron has a clear preferred direction and non-zero modulation depth. On the other hand, if we then fit a second model that included both reach direction and speed, we would infer that the neuron is modulated by speed alone, and it would be apparent that the original preferred direction and modulation depth estimates were biased due to the omitted variable. Download figure Open in new tab Figure 1:  When relevant variables are omitted from the model, estimates of the included effects can be biased. Consider two hypothetical neurons tuned to an observed variable x and an omitted variable x h . Neuron 1 is not tuned to the observed variable x , but its rate is modulated by the omitted variable with true tuning curves denoted by the gray curves (top left). If x and x h covary, the apparent tuning of this neuron to x when the tuning curve is estimated using x alone will then be biased (red and blue curves, top middle). This neuron will appear to be tuned to x despite not actually being tuned to this variable. In addition, to this type of illusory tuning, there can also be more subtle biases. Here, neuron 2 is tuned to x and x h (gray curves, bottom left). However, depending on how x and x h covary, the preferred stimulus or the modulation can be misestimated. Here the true tuning to x is shown at three different fixed values of x h (three gray curves, left panels). The estimated tuning when x h is omitted are shown at center with red and blue curves corresponding to the estimates under two different joint distributions (matching borders, right). Dashed line denotes the effect of x when x h is fixed. In adding additional variables, previous studies have largely focused on the fact that including previously omitted variables improves model accuracy or the fact that neural activity is often influenced by a host of task variables. In M1, for instance, including speed improves model accuracy ( Moran and Schwartz, 1999 ), but the presence of many correlated motor variables (e.g. kinematics, end-point forces, muscle activity) makes it difficult to interpret how neurons represent movement overall ( Humphrey et al., 1970 ; Omrani et al., 2017 ). Here, instead of focusing on the advantages or complexities of models with many variables, we focus on the well-known, but under-discussed, fact that the parameters describing the original effects change as additional variables are included. The hypothetical M1 neuron above points to a more general question about regression models of neural activity. What happens when we cannot or do not include variables that are relevant to the process that we are modeling? Here we first evaluate the statistical problem of omitted variable bias in the canonical generalized linear model with Poisson observations. Then, as a case study, we examine how speed affects estimates of direction tuning of neurons in primary motor cortex, as well as, two other case studies where the spike counts are modeled as a function of external variables: orientation tuning in primary visual cortex (V1) and place tuning in the hippocampus (HC). In each of these case studies we find that commonly omitted variables (speed in M1, population activity in V1, and speed and heading in HC) can bias the estimated effects of commonly included variables (reach direction in M1, stimulus orientation/direction in V1, and place in HC). Across all three case studies, including the omitted variables reduces the estimated modulation due to typical tuning effects. We also illustrate how omitted variable bias can affect generalized linear models of spike dynamics where a post-spike history filter aims to describe refractoriness and bursting ( Truccolo et al., 2005 ). The goal of these models is typically to differentiate aspects of spike dynamics that are due to the neurons own properties (e.g. membrane time constant, resting potential, after-hyper-polarization currents) from those due to input to the neuron from other sources ( Brillinger and Segundo, 1979 ; Paninski, 2004 ). In this setting, the input to the neuron is typically not directly observed, but is approximated by stimulus or behavioral covariates, local field potential, or the activity of other neurons. Here we show that omitting the input can lead to large biases in post-spike history filters, and that including omitted variables describing the input can change the interpretation and stability of the estimated history effects. GLMs have been used in many settings to disentangle the effects of multiple, possibly correlated, stimulus or task variables ( Fernandes et al., 2014 ; Park et al., 2014 ; Runyan et al., 2017 ) and also to model neural mechanisms such as post-spike dynamics, interactions between neurons, and coupling to local fields ( Harris et al., 2003 ; Truccolo et al., 2005 ; Pillow et al., 2008 ). It is often argued that GLMs are advantageous because they have unique maximum likelihood estimates and can be more robust to non-spherical covariate distributions than other methods, such as spike-triggered averaging ( Paninski, 2004 ; Pillow, 2007 ). Although these advantages are important, GLMs are not immune to bias. Here we show how the possibility of omitted variable bias, in particular, should encourage researchers to be cautious in their interpretation of model parameters, even in cases where a GLM achieves high predictive accuracy ( Shmueli, 2010 ). Results Here we introduce the problem of omitted variable bias and examine differences between omitted variable bias in linear models and the canonical Poisson GLM. We then consider three tuning curve estimation problems: estimating direction tuning in primary motor cortex, place tuning in hippocampus, and orientation tuning in primary visual cortex and show how omitted variables in each of these three cases can alter parameter estimates. Finally, we consider a GLM that aims to describe the dynamics of post-spike history and show how omitted inputs can bias the estimated history effects and qualitatively change model stability. Omitted Variable Bias in Linear Regression and canonical Poisson GLMs When relevant variables are not included in a regression model, the estimated effects for the variables that are included can be biased ( Box, 1966 ). Omitted variable biases can cause the parameters describing the effects of the original variables to be over- or under-estimated, and model fits can change qualitatively when omitted variables are included ( Fig 1 ). To understand the problem of omitted variable bias it will be helpful to briefly review the well-known case of multiple linear regression, where the bias can be described analytically ( Box, 1966 ). In the linear setting, consider the generative model  where observations y are a linear combination of observed X and omitted X h variables plus normally-distributed i.i.d. noise ϵ∼N (0, σ ). For simplicity, we ignore the intercept term, but in the analysis that follows it may also be considered as part of X . If we then fit the (mis-specified) model without X h using maximum likelihood estimation (equivalent to the ordinary least squares solution, in this case) the estimated parameters will be  where ξ denotes the effect of the noise, and the bias ( X T X ) −1 X T X h β h will, generally, be non-zero. There will be no bias only in the cases where the omitted variables do not affect the observations ( β h = 0) or when the omitted variables and observed variables are not collinear ( X T X h = 0). Note that ( X T X ) −1 X T X h is the matrix of regression coefficients for the omitted variables using the observed variables as predictors. For linear regression, the omitted variable bias thus depends on both the extent to which the omitted variables affect the observations β h and the extent to which the omitted variables can be (linearly) predicted from the observed variables. Although there is a closed-form solution for the omitted variable bias for linear regression, the generalized linear setting is not as tractable ( Gail et al., 1984 ; Clogg et al., 1992 ; Drake and McQuarrie, 1995 ). We will consider the case of a canonical Poisson GLM, in particular, where  In the more general case, GLMs have  where g −1 (·) is the inverse link function, and y is distributed following an exponential family distribution ( McCullagh and Nelder, 1989 ). For a canonical GLM the log-likelihood takes the form  where the nonlinear function G (·) depends on both the link function and the noise model. For canonical GLMs, this log-likelihood is concave and the maximum likelihood estimate satisfies . The exact form of G (·) will depend on the model, but for linear regression G ( x ) is proportional to , and for canonical (log-link) Poisson regression G (·) = exp(·). Now, with omitted variables, instead of maximizing the correct log-likelihood, we maximize instead  For the omitted variable bias in to be 0, we need both and at the same value of β . Although, neither MLE has a closed form solution, this condition implies that, if there is no bias due to the omitted variables,  where G ′ (·) is the derivative of G (·). For linear regression this equality reduces to the OLS form derived above, and for canonical Poisson regression we have  This equality is satisfied when observations are not modulated by the omitted variables β h = 0 or, more generally, when the effect of the omitted variables δλ = exp( Xβ + X h β h ) – exp( Xβ ) is orthogonal to the included variables X . Note that with linear regression, X T X h = 0 implies that the estimates will not be biased, but here this is not the case unless X T δλ = 0 as well. Due to the structure of the canonical Poisson GLM, omitted variable bias can thus occur even in a properly randomized, controlled experiment ( Gail et al., 1984 ), It is important to note that the maximum likelihood estimates themselves are consistent. That is, the estimators converge (in probability) to their true values when the generative model is correct. The bias here is a result of the model being mis-specified. This mis-specification affects the location of the maximum and, also, the shape of the likelihood. Optimization methods, such as Newton’s method, will typically contain omitted variable bias in each parameter update. For canonical Poisson regression, for instance, the updates take the form  at iteration k where the weight matrix W is diagonal with entries W ii = λ i and ( X T WX ) −1 is the Fisher scoring matrix (inverse Hessian of the log-likelihood) at the current estimate . Since the mis-specified model will use λ = exp( Xβ ) instead of the exp( Xβ + X h β h ), both the weight matrix and the gradient X T ( y – λ ) will be biased at each step of the optimization (except when ). Traditional standard errors for the MLE will also typically be influenced by omitted variables, since . Moreover, as previous studies have shown, omitted variables can lead to misestimation of the variability in E [ y ] and dispersion var ( y ) ( Czanner et al., 2008 ; Goris et al., 2014 ). If the omitted variables affect the observations, then they will generally increase the variability of E [ y ]. Then, unless the omitted variables are perfectly predicted by the included variables, the explained variance var ( E [ y ]) of the mis-specified model will be lower than that of the full model. This may, in turn, lead to overestimates of dispersion, since var ( y ) = E [ var ( y )] + var ( E [ y ]). Omitted Variable Bias in Tuning Curve Estimation When fitting tuning curve models to spike count data, omitted variable bias can cause preferred stimuli and modulation depths to be misestimated and can even lead to completely illusory tuning ( Fig 1 ). To illustrate how omitted variable bias affects GLMs of neural spiking, not just in theory, but in practice, we consider three case studies where we fit typical tuning curve models that omit potentially relevant variables along with augmented models that include these additional variables. We first consider modeling spike counts across trials and on slow (>100ms) timescales. Here we assess 1) the tuning of neurons in motor cortex to reach direction, with speed as a potential omitted variable, 2) the tuning of neurons in hippocampus to position, with both speed and head-direction as potential omitted variables, and 3) the tuning of neurons in visual cortex to the direction of motion of a sine-wave grating, with population activity as a potential omitted variable. In each of these cases studies, we show how the omitted variables are not independent from the commonly included variables and how neural responses are modulated by the omitted variables. These two properties, together, can lead to omitted variable biases. In our first case study, we model data recorded from primary motor cortex (M1) of a macaque monkey performing a center-out, planar reaching task. In this task, speed differs systematically across reach directions ( Fig 2A ), with average speed differing by as much as 35±3% (for the targets at 45 and 225 deg relative to right, Fig 2B ). To model neural responses, we first fit a traditional tuning curve model ( Georgopoulos et al., 1982 ; Amirikian and Georgopulos, 2000 ), where the predicted responses depend only on target direction. Here we use a circular, cubic B-spline basis (5 equally spaced knots) to allow for deviations from sinusoidal firing, but, in most cases, the responses of the n=81 neurons in this experiment are well described by cosine-like tuning curves with clear modulation for reach direction. We then fit a second model that includes effects from movement speed. Here we use covariates based on ( Moran and Schwartz, 1999 ), including a linear speed effect, as well as, cosine-tuned direction-speed interactions (see Methods). This model captures the responses of individual neurons, where spike counts can increase ( Fig 2C , top) or decrease ( Fig 2C , middle) as a function of speed, and, in some cases, speed and direction appear to interact ( Fig 2C , bottom). Together, the fact that direction and speed are not independent along with the fact that neural responses appear to be modulated by speed could lead to biased parameters estimates for the model where speed is omitted. Comparing the models with and without omitted variables we find that, averaged across the population, there are only minimal shifts in the preferred direction (3±2 deg) when speed is included in the traditional tuning curve model, and there do not appear to large, systematic shifts in the population distribution of PDs (Kuiper’s test, p>0.1). At the same time, there is substantial variability between neurons in the size of the PD-shift (circular SD 32±5 deg). Across the population, modulation depth (measured using the standard deviation of the tuning curve) decreases slightly on average (3±2%), and the size of the modulation change also varies substantially between individual neurons (SD of changes 18±3%). An example neuron in Fig 1C (bottom), for instance, has a modulation decrease of 9±5% and the preferred direction changes 4±9 deg when speed is included in the model (standard error from bootstrapping). Overall, ∼10% of neurons have statistically significant changes in PD, and ∼14% have significant changes in modulation (bootstrap tests α =0.05, not corrected for multiple comparisons). For some individual neurons, at least, the parameters of the model without speed, thus, have clear omitted variable bias. However, since individual neurons have diverse speed dependencies, in this case, the average biases across the population are minimal. When speed is included in the model, model accuracy (pseudo-R 2 ) does increase slightly (p=0.01, one-sided paired t-test across neurons). The average cross-validated (jack-knife) pseudo-R 2 for the original model is 0.23±0.01 and for the model with speed 0.24±.01 ( Fig 5 ). However, it seems likely that in other experimental contexts the effects of omitting speed could be more pronounced. By requiring the animal to make reaches to the same targets at different speeds, previous studies have more clearly demonstrated that responses in M1 are modulated by speed ( Churchland et al., 2006 ). Here we demonstrate how this type of modulation can lead to omitted variable biases in the estimated parameters of typical tuning curve models without speed. Download figure Open in new tab Figure 2:  Speed as an omitted variable in M1 tuning for reach direction. A) The distribution of reach speeds differs by target direction in a center-out task. Circles denote median, boxes denote IQR. B) Speed profiles for the two targets showing the largest speed differences. Individual traces denote individual trials aligned to the half-max (black arrow). Inset shows the position of each trial with colors denoting reach direction. C) The responses of 3 M1 neurons show typical tuning for reach direction. The tuning curve estimated using direction covariates alone (black) changes when speed covariates are included (red). Red curves denote the direction effect within the full model and are generated by assuming speed is constant (equal to the mean speed across all trials). Right panels illustrate the speed dependence for the preferred direction and its opposite. Dark lines denote the estimated effect of speed under the full model. Data points show single trial data, along with the mean speed and rate for each direction (big data point). Light lines show linear trends (OLS) using only the trials from each specific target. In our second case study we examine the activity of neurons in the dorsal hippocampus of a rat foraging in an open field. Here we consider to what extent the practice of omitting speed and head direction from a place field model biases estimates of a neuron’s position tuning. As in the first case study, omitted variable bias can occur if neural activity is modulated by omitted variables and the omitted variables covary with the included variables. In the case of the hippocampus, neural activity is known to be modulated by both movement speed and head direction ( McNaughton et al., 1983 ), in addition to an animal’s position ( O’Keefe and Dostrovsky, 1971 ). Additionally, behavioral variables can be highly nonuniform across the open field ( Walsh and Cummins, 1976 ), for instance, near and far from the walls. Together the fact that the omitted variables may covary with position and the fact that neurons appears to be modulated by the omitted variables, suggest that there may be omitted variable bias. Here, in one recording during an open field foraging task we find that the average speed ( Fig 2A ) and heading ( Fig 2B ) differ extensively as a function of position. Within a given neuron’s place field, the distributions of speed and heading may be very different from their marginal distributions. Across the population of n=68 place cells (selected from 117 simultaneously recorded neurons, see Methods), average in-field speed was between 80-135% of the average overall speed (5.5cm/s), and the animal’s heading can be either more or less variable in-field (circular SD 57-80 deg) compared to overall (75 deg). As previous studies have shown, we also find that neural responses are modulated by speed and head direction. Responses due to place, speed, and heading are shown for one example neuron in Fig 3 . This neuron shows a stereotypical place-dependent response ( Fig 2B ), but splitting the observations by speed ( Fig 3C , top) or heading ( Fig 3B , bottom) by quartiles/quadrants reveals that there is also tuning to these variables. The neuron appears to increase its firing with increasing speed and responds most strongly when the rat is facing the left. These dependencies are well fit by the full model where the firing rate depends, not just on position, but also on the (log-transformed) speed and the heading ( Fig 3D , bottom). For the example place cell shown here, the location of the place-field does not change substantially when the omitted variables are included ( Fig 3E ). However, the modulation (SD of the rate map) decreases by 27%. That is, 27% of the apparent modulation due to position when it is modeled alone, can be explained by speed and heading effects. Across the population of place cells, there were no clear, systematic difference in the place field locations, but the modulation (SD of the rate map λ ( x )) decreases by 9±1% on average when speed and heading are included. Individual neurons showed substantial variability in their modulation differences (population SD 10±1%). As in M1, including the omitted variables increased spike prediction accuracy – the average cross-validated (10-fold) pseudo-R 2 was .29±.02 for the original model and .31±.02 for the model including speed and heading activity. This difference seems small, since there is large variability in pseudo-R 2 values across the population, but the average increase in pseudo-R 2 was 11±3% ( Fig 5 ). Given that neurons appear to be modulated by speed and heading, it is unsurprising that including these variables improves model fit. However, as before, it is important to note that this modulation can lead to biases in the place field estimates for the model with only position. Download figure Open in new tab Figure 3:  Speed and heading as omitted variables in hippocampal place cells. A) Average speed and heading as a function of position for a rat foraging in an open field. B) An example place cell tends to spike (red dots) when the animal is at a specific position in space. C) The activity of this neuron is modulated by the animal’s speed (top row) and heading (bottom row). Speed is split into quartiles, subplots include all headings. Heading is split into quadrants, subplots include all speeds. D) The distributions of speed and heading within the place field differ from the overall distributions, and the neuron is tuned to these variables. Blue curve shows model fit. E) After modeling the effect of speed and heading within the place field, the location of the place field does not change but the apparent modulation due to position is reduced. In our third case study, we examine the activity of neurons in a more controlled sensory experiment. Here we use data recorded from primary visual cortex (V1) of an anesthetized monkey viewing oriented sine-wave gratings moving in on of 12 different directions (see Methods). In this experiment, variability in the animal’s behavior is purposefully minimized, and, instead of considering the effect of omitting a behavioral variable, here we consider the effect of omitting a variable relating to the animal’s internal state – the total population activity. Several studies have previously shown that population activity alters neural responses in V1 ( Arieli et al., 1996 ; Kelly et al., 2010 ; Okun et al., 2015 ; Arandia-Romero et al., 2016 ). If the distribution of population activity also varies with stimulus direction, then there is the potential for omitted variable bias. Here we assess neural activity from n=90 simultaneously recorded neurons across many (2400) repeated trials with 12 different movement directions. We find that there is high trial-to-trial variability in the population rate ( Fig 4A ), and the average firing across all neurons does differs across stimulus directions, up to ∼50%. For this recording, the most extreme differences were between the 180 deg stimulus where the average rate across the population was 3.4±0.1Hz and the 60 deg stimulus where the average rate was 6.3±0.1Hz ( Fig 4B ). By adding the (log-transformed) population rate as a covariate to a more typical model of direction tuning, we find that population activity may lead to omitted variable bias in models of direction tuning alone. As in the case studies above, there do not appear to be any consistent or systematic effects on the preferred stimulus direction at the population level (Kuiper’s test, p=0.1). However, the modulation depth (measured using SD of the tuning curve) decreases substantially 15±2% when population rate is included in the model, and there is again high variability across neurons (SD 20±2%). In this case, model accuracy increases substantially when the omitted variable is included. The cross-validated (10-fold) pseudo-R 2 is .26±.02 for the original model and .43±.02 for the model including population activity, with an average increase of 164±31% ( Fig 5 ). Download figure Open in new tab Figure 4:  Population rate as an omitted variable in primary visual cortex. A) Correlated trial to trial variability. Population rasters for three trials of the same drifting grating stimulus (0 deg, red and 30 deg, orange). Neurons are sorted by overall firing rate. B) Histograms of the population rate across trials. As a population, the neurons respond at higher rates to 30 deg stimuli, but there is high trial-to-trial variability. C) The responses of 2 V1 neurons show typical tuning for direction of motion. The tuning curve estimated using direction covariates alone (black) changes when the population rate covariate are included (red). Right panels illustrate the dependence for the preferred direction and an orthogonal direction. Dark lines denote the estimated effect of speed under the full model. Data points show single trial data, along with the mean count and rate (big data point). Light lines show linear trends (OLS) using only the trials from each specific stimulus. Unlike in M1 where the effect of speed was highly diverse for different neurons, in this case study the effect of the population rate is largely consistent. Higher population rates are associated with higher firing rates, and, for most neurons, the effect of the population rate is stronger in the preferred direction(s), consistent with a multiplicative effect. Note that here, we do not include the neuron whose rate we are modeling in the calculation of the population rate. However, using the population rate as an omitted variable requires some interpretation. The population rate will certainly be affected by the tuning of the, relatively small, sample of neurons that we observe. If we have a disproportionate number of neurons tuned to a specific preferred direction, the population rate in those directions will be higher. This suggests that in a different recording, the covariation between the stimulus and the population rate could very likely be different. However, it appears that the omitted variable biases in this case are mostly driven by noise correlations, where neural activity is correlated on single trials even within the same stimulus condition, rather than stimulus correlations, where neural activity is correlated due to similar tuning. When we shuffle the data within each stimulus condition (removing noise correlations) the average change in the modulation depth is -1±2% (SD 18±3%), and the effect of the omitted variable becomes negligible. Download figure Open in new tab Figure 5:  For each of the case studies, on average, the model accuracy increases when omitted variables are included (top) and the modulation due to the original variables decreases (bottom). Scatter plots indicate cross-validated pseudo-R2 values for each neuron under the two models. Modulation denotes the standard deviation of the tuning to the original variable(s) under each model. Here, modulation values are normalized by the average rate of each neuron. Black lines denote equality. Red dashed lines denote linear fit with 0 intercept. Download figure Open in new tab Figure 6:  Estimated post-spike history filters can be heavily biased when the input is not included in the model. A) Here we simulate from an inhomogeneous Poisson model with sinusoidal input (no post-spike history effects). The input and spike responses from 20 trials are shown. Although there are no history effects in the generative model, a GLM with history effects that is missing the correct input covariate will use the history terms to capture the structure in the autocorrelation (C). Traces denote the estimated rate for the 20 trials shown above. When the history term is included in the model, but the input is not, the GLM can still reconstruct PSTH responses using the post-spike history alone. B) Post-spike filters for the models in (A) with 95% confidence bands. Note that when input is included in the model the filters correctly reconstruct the true (lack of) filter, and that there is higher uncertainty around the regions where the ISI distribution does not constrain the model. Omitted Variable Bias in the Estimation of Post-Spike History Effects In addition to modeling spike counts over trials or on relatively slow (>100ms) behavioral timescales, GLMs are also often used to describe detailed, single-trial spike dynamics on fast (<10ms) timescales. One common covariate used in these types of models is a post-spike history effect where the probably of spiking at a given time depends on the recent history of spiking. Modeling these effects allows us to describe refractoriness, bursting ( Paninski, 2004 ; Truccolo et al., 2005 ), and a whole host of other dynamics ( Weber and Pillow, 2017 ). Conceptually, the goal of these models is to disentangle the sources of rate variation based only on observations of a neuron’s spiking, with history effects, ideally, reflecting intrinsic biophysics. However, since the full synaptic input is typically not known with extracellular spike recordings there is potential for omitted variable biases. To illustrate the potential pitfalls of omitting the input to a neuron, consider using the GLM to capture single neuron dynamics in the complete absence of external covariates  where the rate λ is determined by a baseline parameter μ along with a filtered version of the neuron’s past spiking with h i ( t ) = ∑ τ>0  f i (τ) n ( t – τ). This is a perfectly acceptable model of intrinsic dynamics, but for most spike data that we observe this isolated neuron model may not provide a realistic description of a neuron receiving thousands of time-varying synaptic inputs. If we fit this model to data where the input to the neuron did vary over time,  then the history filter in the first model will attempt to capture variation in spiking due to the time-varying input, in addition to any intrinsic dynamics. For example, when x h is periodic, the estimated history filters of the original model will attempt to capture this periodic structure ( Fig 6A-B ). Just as in the tuning curve examples above, the fact that history effects covary with the input and the fact that the input modulates the neuron’s firing leads to omitted variable bias. When the input is omitted from the model, the biased history effects simply provide the best (maximum likelihood) explanation of the observed spiking ( Fig 6C ). These examples with strong, periodic input are not necessarily biologically realistic, but they make it apparent how the post-spike history can be biased by omitted input variables. In vivo, neurons instead appear to be in a high-conductance state, where membrane potential fluctuations have approximately 1/ f power spectra ( Destexhe et al., 2001 , 2003 ). When these naturalistic input statistics are used to drive the GLM, omitted variable bias can occur, as well. Here we simulate a GLM receiving 1/ f α noise input with α = 0 (white noise) 1 and 2 ( Fig 7 ). For white noise input, the MLE accurately recovers the simulated post-spike history filter when the input is omitted from the model, but when α = 1 or 2 the estimates become increasingly biased ( Fig 7A,C ). With the full model, where the input is included as a covariate, the history is recovered accurately no matter what the input statistics are. Just as in the periodic case, however, these different input statistics alter the auto-correlation, and, when the input is omitted from the model, the maximum likelihood history filter simply aims to capture these patterns. Download figure Open in new tab Figure 7:  Post-spike filters can show omitted variable bias even in a more realistic scenario. Here we simulate from a GLM with a refractory post-spike filter and drive the neurons with 1/ f α noise. Excepting the case of white noise ( α = 0), the post-spike filters estimated for the GLM without input are heavily biased (A). C) Even when the effect of the true post-spike filter is to strictly decrease the firing rate, the estimated filters can increase the firing rate. B,D) Approximate transfer functions from a quasi-renewal approximation. When the true filter is stable, the estimated filters can result in fragile dynamics. In GLMs for single-neuron dynamics, one effect of omitted variable bias is that it may lead us to misinterpret how stable a neuron’s dynamics are. Even if the true history filter only reduces the neuron’s firing rate following a spike (as in Fig 7C ), the estimated filter can be biased upwards when the input is omitted. If we were to simulate the activity of this neuron based on the biased filter, the bias could cause the neuron’s rate to diverge if the rate becomes high enough. To assess the stability of the estimated post-spike history effects quantitatively, here we make use of an quasi-renewal approximation analysis introduced in ( Gerhard et al., 2017 ). Given a history filter, this approach finds an approximate transfer function describing the neuron’s future firing rate (output) given its recent (input) firing rate (see Methods). For all estimated models, the transfer function has a stable fixed point near the neuron’s baseline firing rate. When the true input is omitted and α > 0, the estimated history filters also have an unstable fixed point where the neuron’s firing rate will diverge if the rate exceeds this point ( Gerhard et al., 2017 ). Here we find that omitted variable bias leads to apparent fragility ( Fig 7B,D ). The stable region shrinks as α increases, and even when the true dynamics are strictly stable (as in Fig 7C,D ), omitted variable bias can lead us to mistakenly conclude that the neuron has fragile dynamics. With most extracellular spike recordings, the synaptic input that the neuron receives is unknown. However, there may also be omitted variable bias when history effects are estimated from real data. In this case, the input to a neuron can be approximated by stimulus or behavioral variables, local field potentials, or the activity of simultaneously recorded neurons ( Harris et al., 2003 ; Truccolo et al., 2005 , 2010 ; Pillow et al., 2008 ; Kelly et al., 2010 ; Gerhard et al., 2013 ; Volgushev et al., 2015 ). Just as in the simulations above, including or omitting these variables can then alter the estimated history effects, even though they are not as directly related to spiking as the synaptic input itself. Here we consider total population spiking activity as a proxy for synaptic input and consider how including population activity alters the history filters when compared to a model of history alone. We examine two datasets: spontaneous activity from primary visual cortex of an anesthetized monkey with n=62 simultaneously recorded neurons and activity from dorsal hippocampus of a sleeping rate with n=39 simultaneously recorded neurons. To model population covariates we sum the spiking of all neurons, excepting the one whose spiking we aim to predict, and low-pass filter the signal (see Methods). Similar to previous results ( Okun et al., 2015 ), we find that, since neurons often have correlated fluctuations in their spiking ( Fig 8A, D ), the population rate is a good predictor for single neuron activity. Moreover, when we add population covariates to a GLM with post-spike history effects the history filter changes. In the V1 dataset, the post-spike gain decreases by 7.8±0.5% on average when population covariates are included, and 14.9±0.8% when considering only the first 50ms after a spike ( Fig 8B ). The effects of adding population covariates are less pronounced in the hippocampal dataset. The post-spike gain decreases by 2.5±0.3% on average, and 9.5±1.2% when considering only the first 50ms after the spike ( Fig 8E ). Based on the quasi-renewal approximation, all neurons in both the V1 and hippocampal datasets have fragile transfer functions where there is a stable fixed point (near the neuron’s average firing rate) and an unstable fixed point where the neuron’s rate diverges if the input becomes too strong. For V1, the average upper-limit of the stable region is 80±3Hz for the models with history only and 143±7Hz for the models with population covariates ( Fig 8C ). In the hippocampal data, the average upper-limit of the stable region is 38±6Hz for the models with history only and 75±13Hz for the models with population covariates ( Fig 8F ). Each neuron is, thus, apparently, more stable after the population covariates are included. As in the case studies using tuning curves, adding covariates also improves spike prediction accuracy. In the V1 dataset, the average log likelihood ratio relative to a homogeneous Poisson model is 2.2±0.3 bits/s for the history model and 3.3±0.3 bits/s for the model with population covariates. In hippocampus, the log likelihood ratio is 0.9±0.3 bits/s for the history model and 2.0±0.5 bits/s for the model with population covariates. The larger effects in V1 are likely explained by the fact that the population rate is predictive for many more neurons here than for the hippocampal data. In the hippocampus, only 26% of the neurons have an increase of over 0.5 bits/s when the population covariates are included, compared to 85% of neurons in V1. Altogether these results demonstrate how omitted variable bias could affect estimates of post-spike history filters in vivo. In both datasets we find that when population covariates are included in the GLM spike prediction accuracy increases, post-spike gain decreases, and apparent stability increases. Download figure Open in new tab Figure 8:  Post-spike filters estimated from real data decrease when population activity is included as a covariate. Segments of spontaneous activity are shown for V1 (A) and during sleep for hippocampus (D). Neurons are sorted by firing rate. B and E show estimated post-spike filters. Black lines denote the average filter (thick) and standard deviation (thin). For clarity, only filters for neurons with firing rates >1Hz are shown. C and F show average quasi-renewal transfer functions for the same set of neurons. All neurons appear to have fragile dynamics with one stable fixed point near the neuron’s average firing rate and an unstable fixed point, beyond which the neuron’s firing rate diverges. Including population covariates increases the region of stability. Discussion When the goal of modeling is causal inference or understanding of biological mechanisms, the potential for biases due to omitted variables is often clear. The statistical effects of confounders (Wasserman, 2004), as well, as the limits that they place on neuroscientific understanding are widely appreciated ( Jonas and Kording, 2017 ; Krakauer et al., 2017 ; Yoshihara and Yoshihara, 2018 ). However, when the goal of modeling is to create an abstract, explanation or summary of observed neural activity, the fact that omitted variables can bias these explanations is not always widely acknowledged. Here we have illustrated the potential for omitted variable bias in two types of commonly used GLMs for neural spiking activity: tuning curve models using spike counts across trials and models that capture single-neuron dynamics with a post-spike history filter. In each model, adding a previously omitted variable, as expected, improved spike prediction accuracy. However, what we emphasize here is that, when omitted variables were included, the estimates of the original parameters changed. For three case studies using tuning curves we found that by adding a traditionally omitted variable tuning curves showed less modulation due to the originally included variables. In models of single neuron dynamics, adding omitted variables led to decreased post-spike gain and greater apparent stability. Importantly, omitted variables can arise in GLMs in any situation where an omitted variable affects neural activity and the effect of the omitted variable is not independent of the included variables. The case studies here are not unique, and many studies have described how adding additional variables to a tuning curve or single neuron model can improve prediction accuracy. In M1, in addition to movement speed, joint angles, muscle activity, end-point force, and many other variables also appear to modulate neural responses ( Fetz, 1992 ; Kalaska, 2009). In addition to speed and head direction in the hippocampus, theta-band LFP, sharp-wave ripples, and environmental features, such as borders, appear to modulate neural activity ( Hartley et al., 2014 ). And in V1, there is growing evidence that population activity ( Lin et al., 2015 ) and non-visual information ( Ghazanfar and Schroeder, 2006 ) modulates neural responses. In each of these systems, neural responses are affected by many, many factors. Responding to many task variables may even be functional, allowing downstream neurons to more effectively discriminate inputs ( Fusi et al., 2016 ). In any case, it seems clear that our models do not yet capture the full complexity of neural responses ( Carandini et al., 2005 ). By omitting relevant variables, current models are likely to be not just less accurate but also biased. Parameter bias may be problematic in and of itself. However, omitted variable bias may also have an important effect on generalization performance. As noted in ( Box, 1966 ), in a new context, the effect of the omitted variables and the relationship between the omitted and included variables may be different. Since the parameters of the included variables are biased, this change can reduce generalization accuracy. This phenomena may explain, to some extent, why tuning models fit in one condition often do not generalize to others ( Graf et al., 2011 ; Oby et al., 2013 ). For models of single-neuron dynamics, omitted variable bias can also have a negative effect on the accuracy of simulations. Previous work has shown that simulating a GLM with post-spike filters estimated from data often results in unstable, diverging simulations. Although several methods for stabilizing these simulations have recently been developed ( Gerhard et al., 2017 ; Hocker and Park, 2017), one, perhaps primary, reason for this instability may be that the post-spike filters are biased due to omitted synaptic input. Since estimated post-spike filters may reflect not just intrinsic neuron properties but also the statistics of the input, interpreting and comparing post-spike filters may be difficult. Different history parameters may be different due to intrinsic biophysics ( Tripathy et al., 2013 ) or due to differing input, and resolving this ambiguity will likely involve more accurately accounting for the input itself ( Kim and Shinomoto, 2012 ). The possibility of omitted variable bias does not mean that estimated parameters, predictions, and simulations from simplified model are useless, but it may mean that we need to be cautious in interpreting these models and their outputs. When reporting the results of regression, in addition to avoiding describing associations with causal language, it may be generally useful to discuss known and potential confounds. Previous studies have already identified several specific cases of omitted variable bias where careful interpretation is necessary. For instance, omitted common input can bias estimates of interactions between neurons ( Brody, 1999 ), and omitted history effects can bias receptive field estimates ( Pillow and Simoncelli, 2003 ). In estimating peri-stimulus time histograms, omitting variables that account for trial-to-trial variation may cause biases ( Czanner et al., 2008 ) or issues with identifiability ( Amarasingham et al., 2015 ). Similarly, biases due to spike sorting errors ( Ventura, 2009 ) could be framed as a result of omitting variables related to missing/excess spikes. Since we typically do not model or observe all the variables that affect neural activity, omitted variable problems are likely to be pervasive in systems neuroscience far beyond these specific cases. Although we have focused on GLMs here, omitted variable bias can affect any model and other types of model misspecification can also result in biased parameter estimates. Adding input nonlinearities ( Ahrens et al., 2008 ; David et al., 2009 ), interaction effects ( McFarland et al., 2013 ), or higher-order terms to the GLM ( Berger et al., 2010 ; Park et al., 2013 ) may fix certain types of model misspecification, but any model that omits relevant variables is still likely to suffer from the same problems. This includes both machine learning methods that may provide better prediction accuracy than GLMs ( Benjamin et al., 2017 ) and single neuron models aiming to describe greater biophysical detail ( Herz et al., 2006 ). Unlike over-fitting or non-convergence ( Zhao and Iyengar, 2010 ), omitted variable bias will generally not be resolved by including additional data or by adding regularization. Moreover, adding one omitted variable, as we have done with the case studies here, is no guarantee that there are not other relevant variables being omitted. One approach that could potentially reduce omitted variable bias is latent variable modeling, where the effects of unknown covariates are explicitly included (constrained by simplifying assumptions). Recent work has introduced latent variables for neural activity with linear dynamics ( Smith and Brown, 2003 ; Kulkarni and Paninski, 2007 ; Paninski et al., 2010 ), switching dynamics ( Putzky et al., 2014 ), rectification ( Whiteway and Butts, 2017 ), and oscillations ( Arai and Kass, 2017 ). And these models appear to out-perform GLMs on population data in retina ( Vidne et al., 2012 ), visual ( Archer et al., 2014 ), and motor cortices ( Chase et al., 2010 ; Macke et al., 2011 ). Inferring latent variables requires making (sometimes strong) assumptions about the nature of the variables and may require observations from multiple neurons or across multiple trials, but, by approximating some of the effects of relevant omitted variables, latent variables may reduce omitted variable bias. However, generally determining when relevant variables are omitted from a model and what those variables are is not a trivial problem. There is a well-known aphorism from George EP Box that, “All models are wrong, but some are useful.” The lengthier version of this quip is, “All models are approximations. Essentially, all models are wrong, but some are useful. However, the approximate nature of the model must always be borne in mind.” GLMs are certainly useful descriptions of neural activity. They are computationally tractable, can disentangle the relative influence of multiple covariates, and often provide the core components for Bayesian decoders. Here we emphasize, however, one ubiquitous circumstance in systems neuroscience where the “approximate nature” of the models should be “borne in mind.” Namely, omitted variables can bias estimates of the included effects. Methods Neural Data All data analyzed here was previously recorded and shared by other researchers through the Collaborative Research in Computation Neuroscience (CRCNS) Data Sharing Initiative (crcns.org). Data from primary motor cortex is from CRCNS dataset DREAM-Stevenson_2011 ( Walker and Kording, 2013 ). These data were recorded using a 100-electrode Utah array (Blackrock Microsystems, 400 mm spacing, 1.5 mm length) chronically implanted in the arm area of primary motor cortex of an adult macaque monkey. The monkey made center-out reaches in a 20x20cm workspace while seated in a primate chair, grasping a two-link manipulandum in the horizontal plane (arm roughly in a sagittal plane). Each trial for the center-out task began with a hold period at a center target (0.3–0.5 s). After a go cue, subjects had 1.25 s to reach one of eight peripheral targets and then held this outer target for at least 0.2–0.5 s. Each success was rewarded with juice, and feedback (1-2cm diam) about arm position was displayed onscreen as a circular cursor. Spike sorting was performed by manual cluster cutting using an offline sorter (Plexon, Inc) with waveforms classified as single- or multi-unit based on action potential shape and minimum ISIs greater than 1.6 ms (yielding n=81 single units). Here we take model tuning curves using spike counts between 150ms before to 350ms after the speed reached its half-max. Average movement speed for each trial was calculated from 0-250ms after the speed reached its half-max (290 trials in total). For the details of the surgery, recording, and spike sorting see ( Stevenson et al., 2011 ). Hippocampal data is from CRCNS HC-3 ( Mizuseki et al., 2013 ). Here we use recording sessions ec16.19.272 and ec014.215, where a Long-Evans rat was sleeping and foraging in a 180x180cm maze, respectively. For both recordings, 12-shank silicon probes (with 8 recording sites each, 20μm separation) were implanted in CA1 (8 shanks) and EC3-5 (4 shanks) (based on histology). Spikes were sorted automatically using KlustaKwick and then manually adjusted (Klusters) yielding 85 units for the sleep data and 117 for the open field data. For the sleep data where we model post-spike history effects, the spike trains were binned at 1ms and the recording length was 27min. Here we model all neurons with firing rates >0.5Hz (n=39). For the open field data where we model place tuning, spike trains were binned at 250ms and the recording length was 93min. Place cells (n=68) were selected based on having an overall firing rate <5Hz (to rule out interneurons), a peak firing rate >2Hz, and a contiguous set of pixels (after smoothing with an isotropic Gaussian σ =8cm) of at least 200 cm 2 where the firing rate was above 10% of the peak rate. For details of the surgery, recording, and spike sorting see ( Diba and Buzsaki, 2008 ; Mizuseki et al., 2009 ). Data from primary visual cortex is from CRCNS dataset PVC-11 ( Kohn and Smith, 2016 ). Here we use spontaneous activity, during a gray screen, (Monkey 1) and responses to drifting sine-wave gratings (Monkey 1) both from an anesthetized (Sufentanil - 4-18 microg/kg/hr) adult monkey (Macaca fascicularis). Recordings were made in parafoveal V1 (RFs within 5 degrees of the fovea) using a 96-channel multi-electrode array (Blackrock Microsystems), 400 mm electrode spacing, 1mm depth. After automatic spike sorting and manual cluster adjustment, 87 and 106 units were recorded during spontaneous activity and grating presentation, respectively. Only neurons with waveform SNR>2 and firing rates >1Hz were analyzed, n=62 for spontaneous and n=90 for grating data. For the spontaneous activity we bin spike counts at 1ms and the recording length was 20min. For the drifting grating data, we analyzed spike counts from 200ms to 1.2s after stimulus onset on each trial – 12 directions, 2400 trials total. Gratings had a spatial frequency of 1.3 cyc/deg, temporal frequency of 6.25Hz, size of 8-10 deg (to cover receptive fields of all recorded neurons) and were presented for 1.25s with a 1.5s inter-trial interval between stimuli. For surgical, stimulus, and preprocessing details see ( Smith and Kohn, 2008 ; Kelly et al., 2010 ). Tuning Curve Models For the M1 data we use a circular, cubic B-spline basis with 5 equally spaced knots  where g (·) are the splines that depend on the reach direction θ , weighted by parameters β and the parameter μ defines a baseline firing rate. To include the effect of speed, we then add three covariates  where s indicates the speed, and the parameters α allow for a multiplicative speed effect as well as possible cosine-tuned speed x direction interactions as in ( Moran and Schwartz, 1999 ). For place fields in hippocampus we use isotropic Gaussian radial basis functions f (·) equally spaced (30cm) on an 6x6 square lattice with a standard deviation of 30cm  We find that the effect of speed is well modeled using the log-transformed speed s , and to model head direction-dependence we use circular, cubic B-splines g (·) with 6 equally spaced knots  For the V1 data we again use a circular, cubic B-spline basis for the direction of the sine-wave grating (7 equally spaced knots).  We find that the effect of population activity is well modeled using the total log-transformed firing rate of all neuron’s excepting the one being modeled  where z = ∑ i ≠ j log( n i + 1). In all models, to avoid overfitting, especially for low firing rate neurons, we add a small L2 penalty to the log-likelihood with a fixed hyperparameter of 10 -4 . Post-spike History Simulations and Population Rate Models In addition to capturing tuning curves, many studies have used GLMs to describe the dynamics of single spike trains ( Brillinger, 1988 ; Harris et al., 2003 ; Paninski, 2004 ; Okatan et al., 2005 ; Truccolo et al., 2005 ; Weber and Pillow, 2017 ). Here, to account for post-spike history effects, we use a GLM taking the form  where h ( t ) denotes the vector of spike history covariates representing the recent history of spiking and μ determines a baseline firing rate. Here we assume h i ( t ) = ∑ τ>0  f i (τ) n ( t – τ), and we use neuron-specific, cubic B-spline bases f (·) whose knots are determined by the quantiles of each neuron’s ISI distribution. Specifically, we choose knots spaced between 10 and 400ms (HC) or 2 and 200ms (V1), where the spacing follows equal percentile regions of the ISI distribution in that same range. This gives 6 basis functions, and coefficients α to capture the spike-history. To enforce refractoriness, we fix the coefficient of the fastest basis (which peaks at 0 and ends at 10ms) to be -5, leaving 5 coefficients to be estimated. The population rate model simply adds covariates where, for each neuron i   Here we use a set of acausal Gaussian filters for g (·) with standard deviations 20, 50, and 100ms. Note that spikes from the neuron being modeled are excluded from the population covariates. Stability analysis Here we make use of a stability analysis proposed in ( Gerhard et al., 2017 ). Briefly, we use a quasi-renewal approximation of the conditional intensity by considering the effect of the most recent spike, at time t ′, and averaging over possible spike histories preceding this spike  where H ( t – t ′) = αf ( t – t ′) and S represents the history of spiking. By assuming that S is generated from a homogeneous Poisson process with firing rate A 0 , the second term can be approximated by  Given this approximation, we can then estimate the inter-spike interval distribution as we would for a true renewal process and the steady-state distribution of inter-spike intervals is given by  and the predicted steady-state firing rate is f ( A 0 ) = 1 /E P ( τ ) [τ]. To assess stability, we can then examine how the predicted steady-state firing rate depends on the assumed rate of the homogeneous Poisson process A 0 . In particular, when f ( A 0 ) = A 0 the quasi-renewal model has a fixed-point. To allow for external input, we incorporate the average effect of the covariates X into the conditional intensity approximation  Note that, in general, adding inputs X will only change the stability of the model to the extent that these covariates change the estimate of h . References ↵ Ahrens  MB , Paninski  L , Sahani  M.  Inferring input nonlinearities in neural encoding models . Netw. Comput. Neural Syst . 19 : 35 – 67 , 2008 . OpenUrl ↵ Amarasingham  A , Geman  S , Harrison  MT . Ambiguity and nonidentifiability in the statistical analysis of neural codes . Proc. Natl. Acad. Sci. U. S. A . 112 : 6455 – 60 , 2015 . OpenUrl Abstract / FREE Full Text ↵ Amirikian  B , Georgopulos  AP . Directional tuning profiles of motor cortical cells . 36 : 73 – 79 , 2000 . ↵ Arai  K , Kass  RE . Inferring oscillatory modulation in neural spike trains . PLOS Comput. Biol . 13 : e1005596 , 2017 . OpenUrl ↵ Arandia-Romero  I , Tanabe  S , Drugowitsch  J , Kohn  A , Moreno-Bote  R.  Multiplicative and Additive Modulation of Neuronal Tuning with Population Activity Affects Encoded Information . Neuron  89 : 1305 – 1316 , 2016 . OpenUrl CrossRef PubMed ↵ Archer  EW , Koster  U , Pillow  JW , Macke  JH . Low-dimensional models of neural population activity in sensory cortical circuits . In: Advances in Neural Information Processing Systems  27 . 2014 , p. 343 – 351 . OpenUrl ↵ Arieli  A , Sterkin  A , Grinvald  A , Aertsen  A.  Dynamics of Ongoing Activity: Explanation of the Large Variability in Evoked Cortical Responses . Science  273 : 1868 – 1871 , 1996 . OpenUrl Abstract / FREE Full Text ↵ Benjamin  AS , Fernandes  HL , Tomlinson  T , Ramkumar  P , VerSteeg  C , Miller  L , Kording  KP . Modern machine learning far outperforms GLMs at predicting spikes . bioRxiv ( February 24, 2017 ).  doi: 10.1101/111450. OpenUrl CrossRef ↵ Berger  TW , Song  D , Chan  RHM , Marmarelis  VZ . The Neurobiological Basis of Cognition: Identification by Multi-Input, Multioutput Nonlinear Dynamic Modeling: A method is proposed for measuring and modeling human long-term memory formation by mathematical analysis and computer simulation of nerve-cell . Proc. IEEE  98 : 356 – 374 , 2010 . OpenUrl ↵ Box  GEP . Use and Abuse of Regression . Technometrics  8 : 625 , 1966 . OpenUrl CrossRef Web of Science ↵ Brillinger  DR . Maximum likelihood analysis of spike trains of interacting nerve cells . Biol. Cybern . 59 : 189 – 200 , 1988 . OpenUrl CrossRef PubMed Web of Science ↵ Brillinger  DR , Segundo  JP . Empirical examination of the threshold model of neuron firing . Biol. Cybern . 35 : 213 – 220 , 1979 . OpenUrl CrossRef PubMed Web of Science ↵ Brody  CD . Disambiguating Different Covariation Types . Neural Comput . 11 : 1527 – 1535 , 1999 . OpenUrl CrossRef PubMed Web of Science ↵ Brown  E , Barbieri  R , Eden  U , Frank  L.  Likelihood methods for neural data analysis . In: Computational Neuroscience: a comprehensive approach, edited by  Feng J. London: Chapman and Hall , 2003 , p. 253 – 286 . ↵ Carandini  M , Demb  JB , Mante  V , Tolhurst  DJ , Dan  Y , Olshausen  BA , Gallant  JL , Rust  NC . Do we know what the early visual system does?  J. Neurosci . 25 : 10577 , 2005 . OpenUrl Abstract / FREE Full Text ↵ Chase  SM , Schwartz  AB , Kass  RE . Latent Inputs Improve Estimates of Neural Encoding in Motor Cortex . J. Neurosci . 30 : 13873 – 13882 , 2010 . OpenUrl Abstract / FREE Full Text ↵ Churchland  MM , Santhanam  G , Shenoy  K V.  Preparatory Activity in Premotor and Motor Cortex Reflects the Speed of the Upcoming Reach . J. Neurophysiol . 96 : 3130 , 2006 . OpenUrl CrossRef PubMed Web of Science ↵ Clarke  KA . The Phantom Menace: Omitted Variable Bias in Econometric Research . Confl. Manag. Peace Sci . 22 : 341 – 352 , 2005 . OpenUrl ↵ Clogg  CC , Petkova  E , Shihadeh  ES . Statistical Methods for Analyzing Collapsibility in Regression Models . J. Educ. Stat . 17 : 51 , 1992 . OpenUrl CrossRef ↵ Czanner  G , Eden  UT , Wirth  S , Yanike  M , Suzuki  WA , Brown  EN . Analysis of between-trial and within-trial neural spiking dynamics . J. Neurophysiol . 99 : 2672 – 2693 , 2008 . OpenUrl CrossRef PubMed Web of Science ↵ David  S V , Mesgarani  N , Fritz  JB , Shamma  SA . Rapid synaptic depression explains nonlinear modulation of spectro-temporal tuning in primary auditory cortex by natural stimuli . J. Neurosci . 29 : 3374 , 2009 . OpenUrl Abstract / FREE Full Text ↵ Destexhe  A , Rudolph  M , Fellous  JM , Sejnowski  TJ . Fluctuating synaptic conductances recreate in vivo-like activity in neocortical neurons . Neuroscience  107 : 13 – 24 , 2001 . OpenUrl CrossRef PubMed Web of Science ↵ Destexhe  A , Rudolph  M , Paré  D.  The high-conductance state of neocortical neurons in vivo . Nat. Rev. Neurosci . 4 : 739 – 751 , 2003 . OpenUrl CrossRef PubMed Web of Science ↵ Diba  K , Buzsaki  G.  Hippocampal Network Dynamics Constrain the Time Lag between Pyramidal Cells across Modified Environments . J. Neurosci . 28 : 13448 – 13456 , 2008 . OpenUrl Abstract / FREE Full Text ↵ Drake  C , McQuarrie  A.  A note on the bias due to omitted confounders . Biometrika  82 : 633 – 638 , 1995 . OpenUrl CrossRef Web of Science ↵ Fernandes  HL , Stevenson  IH , Phillips  AN , Segraves  MA , Kording  KP . Saliency and saccade encoding in the frontal eye field during natural scene search . Cereb. Cortex  24 , 2014 . ↵ Fetz  EE . Are movement parameters recognizably coded in the activity of single neurons?  Behav. Brain Sci . 15 : 679 – 690 , 1992 . OpenUrl CrossRef Web of Science ↵ Fusi  S , Miller  EK , Rigotti  M.  Why neurons mix: high dimensionality for higher cognition . Curr. Opin. Neurobiol . 37 : 66 – 74 , 2016 . OpenUrl CrossRef PubMed ↵ Gail  MH , Wieand  S , Piantadosi  S.  Biased Estimates of Treatment Effect in Randomized Experiments with Nonlinear Regressions and Omitted Covariates . Biometrika  71 : 431 , 1984 . OpenUrl CrossRef Web of Science ↵ Gelman  A , Hill  J.  Data Analysis Using Regression and Multilevel/Hierarchical Models . Cambridge University Press , 2007 . ↵ Georgopoulos  AP , Kalaska  JF , Caminiti  R , Massey  JT . On the relations between the direction of two-dimensional arm movements and cell discharge in primate motor cortex . J. Neurosci . 2 : 1527 – 1537 , 1982 . OpenUrl Abstract / FREE Full Text ↵ Gerhard  F , Deger  M , Truccolo  W.  On the stability and dynamics of stochastic spiking neuron models: Nonlinear Hawkes process and point process GLMs . PLOS Comput. Biol . 13 : e1005390, 2017 . ↵ Gerhard  F , Kispersky  T , Gutierrez  GJ , Marder  E , Kramer  M , Eden  U.  Successful Reconstruction of a Physiological Circuit with Known Connectivity from Spiking Activity Alone . PLoS Comput. Biol . 9 : e1003138 , 2013 . OpenUrl CrossRef PubMed ↵ Ghazanfar  AA , Schroeder  CE . Is neocortex essentially multisensory?  Trends Cogn. Sci . 10 : 278 – 285 , 2006 . OpenUrl CrossRef PubMed Web of Science ↵ Goris  RLT , Movshon  JA , Simoncelli  EP . Partitioning neuronal variability . Nat. Neurosci . 17 : 858 – 65 , 2014 . OpenUrl CrossRef PubMed ↵ Graf  ABA , Kohn  A , Jazayeri  M , Movshon  JA . Decoding the activity of neuronal populations in macaque primary visual cortex . Nat. Neurosci . 14 : 239 – 245 , 2011 . OpenUrl CrossRef PubMed Web of Science ↵ Greenland  S.  Modeling and variable selection in epidemiologic analysis . Am. J. Public Health  79 : 340 – 9 , 1989 . OpenUrl CrossRef PubMed Web of Science ↵ Harris  KD , Csicsvari  J , Hirase  H , Dragoi  G , Buzsáki  G.  Organization of cell assemblies in the hippocampus . Nature  424 : 552 – 556 , 2003 . OpenUrl CrossRef PubMed Web of Science ↵ Hartley  T , Lever  C , Burgess  N , O’Keefe  J.  Space in the brain: how the hippocampal formation supports spatial cognition . Philos. Trans. R. Soc. Lond. B. Biol. Sci . 369 : 20120510 , 2014 . OpenUrl CrossRef PubMed ↵ Herz  AVM , Gollisch  T , Machens  CK , Jaeger  D.  Modeling single-neuron dynamics and computations: a balance of detail and abstraction . Science  314 : 80 – 5 , 2006 . OpenUrl Abstract / FREE Full Text Hocker  D , Park  IM . Multistep inference for generalized linear spiking models curbs runaway excitation . In: 2017 8th International IEEE/EMBS Conference on Neural Engineering (NER). IEEE , p. 613 – 616 . ↵ Humphrey  DR , Schmidt  EM , Thompson  WD . Predicting measures of motor performance from multiple cortical spike trains . Science  170 : 758 – 62 , 1970 . OpenUrl Abstract / FREE Full Text ↵ Jonas  E , Kording  KP . Could a Neuroscientist Understand a Microprocessor? PLOS Comput. Biol.  13 : e1005268 , 2017 . Kalaska  JF . From Intention to Action: Motor Cortex and the Control of Reaching Movements. Springer , Boston, MA , p. 139 – 178 . ↵ Kandler  S , Mao  D , McNaughton  BL , Bonin  V.  Encoding of Tactile Context in the Mouse Visual Cortex . bioRxiv ( October  6 , 2017 ).  doi: 10.1101/199364. OpenUrl CrossRef ↵ Kass  RE , Ventura  V , Brown  EN . Statistical Issues in the Analysis of Neuronal Data . J. Neurophysiol . 94 : 8 – 25 , 2005 . OpenUrl CrossRef PubMed Web of Science ↵ Kelly  RC , Smith  MA , Kass  RE , Lee  TS . Local field potentials indicate network state and account for neuronal response variability . J. Comput. Neurosci . 29 : 567 – 579 , 2010 . OpenUrl CrossRef PubMed Web of Science ↵ Kim  H , Shinomoto  S.  Estimating nonstationary input signals from a single neuronal spike train . Phys. Rev. E  86 : 051903 , 2012 . OpenUrl ↵ Kohn  A , Smith  MA . Utah array extracellular recordings of spontaneous and visually evoked activity from anesthetized macaque primary visual cortex (V1 ). CRCNS.org . 2016 . ↵ Krakauer  JW , Ghazanfar  AA , Gomez-Marin  A , MacIver  MA , Poeppel  D.  Neuroscience Needs Behavior: Correcting a Reductionist Bias . Neuron  93 : 480 – 490 , 2017 . OpenUrl CrossRef PubMed ↵ Kulkarni  JE , Paninski  L.  Common-input models for multiple neural spike-train data . Netw. Comput. Neural Syst . 18 : 375 – 407 , 2007 . OpenUrl ↵ Lin  I-C , Okun  M , Carandini  M , Harris  KD . The Nature of Shared Cortical Variability . Neuron  87 : 644 – 656 , 2015 . OpenUrl CrossRef PubMed ↵ Macke  J , Büsing  L , Cunningham  J , Yu  B , Shenoy  K , Sahani  M.  Empirical models of spiking in neural populations . In: Advances in Neural Information Processing Systems . 2011 , p. 1350 – 1358 . ↵ McCullagh  P , Nelder  JA . Generalized Linear Models . 2nd ed. CRC Press , 1989 . ↵ McFarland  JM , Cui  Y , Butts  DA . Inferring Nonlinear Neuronal Computation Based on Physiologically Plausible Inputs . PLoS Comput. Biol . 9 : e1003143 , 2013 . OpenUrl CrossRef PubMed ↵ McNaughton  BL , Barnes  CA , O’Keefe  J.  The contributions of position, direction, and velocity to single unit activity in the hippocampus of freely-moving rats . Exp. Brain Res . 52 : 41 – 49 , 1983 . OpenUrl CrossRef PubMed Web of Science ↵ Mizuseki  K , Sirota  A , Pastalkova  E , Buzsáki  G.  Theta oscillations provide temporal windows for local circuit computation in the entorhinal-hippocampal loop . Neuron  64 : 267 – 280 , 2009 . OpenUrl CrossRef PubMed Web of Science ↵ Mizuseki  K , Sirota  A , Pastalkova  E , Diba  K , Buzsáki  G.  Multiple single unit recordings from different rat hippocampal and entorhinal regions while the animals were performing multiple behavioral tasks . CRCNS.org.  2013 . ↵ Moran  DW , Schwartz  a B.  Motor cortical representation of speed and direction during reaching . J. Neurophysiol . 82 : 2676 – 2692 , 1999 . OpenUrl CrossRef PubMed Web of Science ↵ Niell  CM , Stryker  MP . Modulation of Visual Responses by Behavioral State in Mouse Visual Cortex . Neuron  65 : 472 – 479 , 2010 . OpenUrl CrossRef PubMed Web of Science ↵ O’Keefe  J , Dostrovsky  J.  The hippocampus as a spatial map: Preliminary evidence from unit activity in the freely-moving rat . Brain Res . 34 : 171 – 175 , 1971 . OpenUrl CrossRef PubMed Web of Science ↵ Oby  ER , Ethier  C , Miller  LE . Movement representation in the primary motor cortex and its contribution to generalizable EMG predictions . J. Neurophysiol . 109 : 666 – 678 , 2013 . OpenUrl CrossRef PubMed Web of Science ↵ Okatan  M , Wilson  MA , Brown  EN . Analyzing Functional Connectivity Using a Network Likelihood Model of Ensemble Neural Spiking Activity . Neural Comput . 17 : 1927 – 1961 , 2005 . OpenUrl CrossRef PubMed Web of Science ↵ Okun  M , Steinmetz  NA , Cossell  L , Iacaruso  MF , Ko  H , Barthó  P , Moore  T , Hofer  SB , Mrsic-Flogel  TD , Carandini  M , Harris  KD . Diverse coupling of neurons to populations in sensory cortex . Nature  521 : 511 – 515 , 2015 . OpenUrl CrossRef PubMed ↵ Omrani  M , Kaufman  MT , Hatsopoulos  NG , Cheney  PD . Perspectives on classical controversies about the motor cortex . J. Neurophysiol . 118 : 1828 – 1848 , 2017 . OpenUrl CrossRef PubMed ↵ Paninski  L.  Maximum likelihood estimation of cascade point-process neural encoding models . Netw. Comput. Neural Syst . 15 : 243 – 262 , 2004 . OpenUrl CrossRef ↵ Paninski  L , Ahmadian  Y , Ferreira  DG , Koyama  S , Rahnama Rad  K , Vidne  M , Vogelstein  J , Wu  W.  A new look at state-space models for neural data . J. Comput. Neurosci . 29 : 107 – 126 , 2010 . OpenUrl CrossRef PubMed Web of Science ↵ Park  IM , Archer  EW , Priebe  N , Pillow  JW . Spectral methods for neural characterization using generalized quadratic models [Online] . : 2454 – 2462 , 2013 . http://papers.nips.cc/paper/4993-spectral-methods-for-neural-characterization-using-generalized-quadratic-models [4 May. 2018]. ↵ Park  IM , Meister  MLR , Huk  AC , Pillow  JW . Encoding and decoding in parietal cortex during sensorimotor decision-making . Nat. Neurosci . 17 : 1395 – 1403 , 2014 . OpenUrl CrossRef PubMed ↵ Pearl  J.  Causal inference in statistics: An overview . Stat. Surv . 3 : 96 – 146 , 2009 . OpenUrl CrossRef ↵  Kenji Doya Alexandre  Pouget , and Rajesh  P.N.  Rao  SI. Pillow  J.  Likelihood-Based Approaches to Modeling the Neural Code. In: Bayesian brain: Probabilistic approaches to neural coding , edited by  Kenji Doya Alexandre  Pouget , and Rajesh  P.N.  Rao  SI.  MIT Press , 2007 , p. 53 – 70 . ↵ Pillow  JW , Shlens  J , Paninski  L , Sher  A , Litke  AM , Chichilnisky  EJ , Simoncelli  EP . Spatio-temporal correlations and visual signalling in a complete neuronal population . Nature  454 : 995 – 999 , 2008 . OpenUrl CrossRef PubMed Web of Science ↵ Pillow  JW , Simoncelli  EP . Biases in white noise analysis due to non-Poisson spike generation . Neurocomputing  52 – 54 : 109–115, 2003 . ↵ Putzky  P , Franzen  F , Bassetto  G , Macke  JH . A Bayesian model for identifying hierarchically organised states in neural population activity [Online]. : 3095–3103 , 2014 . http://papers.nips.cc/paper/5338-a-bayesian-model-for-identifying-hierarchically-organised-states-in-neural-population-activity [4 May. 2018]. ↵ Reimer  J , Froudarakis  E , Cadwell  CR , Yatsenko  D , Denfield  GH , Tolias  AS . Pupil Fluctuations Track Fast Switching of Cortical States during Quiet Wakefulness . Neuron  84 : 355 – 362 , 2014 . OpenUrl CrossRef PubMed ↵ Runyan  CA , Piasini  E , Panzeri  S , Harvey  CD . Distinct timescales of population coding across cortex . Nature  548 : 92 – 96 , 2017 . OpenUrl CrossRef PubMed ↵ Rust  NC , Movshon  JA . In praise of artifice . Nat. Neurosci . 8 : 1647 – 1650 , 2005 . OpenUrl CrossRef PubMed Web of Science ↵ Shmueli  G.  To Explain or to Predict?  Stat. Sci . 25 : 289 – 310 , 2010 . OpenUrl CrossRef Web of Science ↵ Smith  AC , Brown  EN . Estimating a State-Space Model from Point Process Observations . Neural Comput . 15 : 965 – 991 , 2003 . OpenUrl CrossRef PubMed Web of Science ↵ Smith  MA , Kohn  A.  Spatial and temporal scales of neuronal correlation in primary visual cortex . J. Neurosci . 28 : 12591 – 12603 , 2008 . OpenUrl Abstract / FREE Full Text ↵ Stevenson  IH , Cherian  A , London  BM , Sachs  NA , Lindberg  E , Reimer  J , Slutzky  MW , Hatsopoulos  NG , Miller  LE , Kording  KP . Statistical assessment of the stability of neural movement representations . J. Neurophysiol . 106 , 2011 . ↵ Stringer  C , Pachitariu  M , Steinmetz  N , Reddy  CB , Carandini  M , Harris  KD . Spontaneous behaviors drive multidimensional, brain-wide population activity . bioRxiv ( April 22, 2018 ).  doi: 10.1101/306019. OpenUrl CrossRef ↵ Tripathy  SJ , Padmanabhan  K , Gerkin  RC , Urban  NN . Intermediate intrinsic diversity enhances neural population coding . Proc. Natl. Acad. Sci. U. S. A . 110 : 8248 – 53 , 2013 . OpenUrl Abstract / FREE Full Text ↵ Truccolo  W , Eden  UT , Fellows  MR , Donoghue  JP , Brown  EN . A Point Process Framework for Relating Neural Spiking Activity to Spiking History, Neural Ensemble, and Extrinsic Covariate Effects . J. Neurophysiol . 93 : 1074 – 1089 , 2005 . OpenUrl CrossRef PubMed Web of Science ↵ Truccolo  W , Hochberg  LR , Donoghue  JP . Collective dynamics in human and monkey sensorimotor cortex: predicting single neuron spikes . Nat. Neurosci . 13 : 105 – 111 , 2010 . OpenUrl CrossRef PubMed Web of Science ↵ Ventura  V.  Traditional waveform based spike sorting yields biased rate code estimates . Proc. Natl. Acad. Sci . 106 : 6921 , 2009 . OpenUrl Abstract / FREE Full Text ↵ Vidne  M , Ahmadian  Y , Shlens  J , Pillow  JW , Kulkarni  J , Litke  AM , Chichilnisky  EJ , Simoncelli  E , Paninski  L.  Modeling the impact of common noise inputs on the network activity of retinal ganglion cells . J. Comput. Neurosci . 33 : 97 – 121 , 2012 . OpenUrl CrossRef PubMed ↵ Volgushev  M , Ilin  V , Stevenson  IH . Identifying and Tracking Simulated Synaptic Inputs from Neuronal Firing: Insights from In Vitro Experiments . PLOS Comput. Biol . 11 : e1004167 , 2015 . OpenUrl ↵ Walker  B , Kording  K.  The Database for Reaching Experiments and Models . PLoS One  8 : e78747 , 2013 . OpenUrl ↵ Walsh  RN , Cummins  RA . The open-field test: A critical review . Psychol. Bull . 83 : 482 – 504 , 1976 . OpenUrl CrossRef PubMed Web of Science Wasserman  L.  All of Statistics . Springer New York . ↵ Weber  AI , Pillow  JW . Capturing the Dynamical Repertoire of Single Neurons with Generalized Linear Models . Neural Comput . 29 : 3260 – 3289 , 2017 . OpenUrl ↵ Whiteway  MR , Butts  DA . Revealing unobserved factors underlying cortical activity with a rectified latent variable model applied to neural population recordings . J. Neurophysiol . 117 : 919 – 936 , 2017 . OpenUrl CrossRef PubMed ↵ Yoshihara  M , Yoshihara  M.  ‘Necessary and sufficient’ in biology is not necessarily necessary–confusions and erroneous conclusions resulting from misapplied logic in the field of biology, especially neuroscience . J. Neurogenet . 32 : 53 – 64 , 2018 . OpenUrl ↵ Zhao  M , Iyengar  S.  Nonconvergence in logistic and poisson models for neural spiking . Neural Comput . 22 : 1231 – 1244 , 2010 . OpenUrl CrossRef PubMed Web of Science View Abstract            View the discussion thread.      Back to top            Previous Next      Posted August 21, 2018.            Download PDF           Email      Thank you for your interest in spreading the word about bioRxiv. NOTE: Your email address is requested solely to identify you as the sender of this article.    Your Email *     Your Name *     Send To *   Enter multiple addresses on separate lines or separate them with commas.    You are going to email the following  Omitted variable bias in GLMs of neural spiking activity    Message Subject (Your Name) has forwarded a page to you from bioRxiv   Message Body (Your Name) thought you would like to see this page from the bioRxiv website.   Your Personal Message         CAPTCHA This question is for testing whether or not you are a human visitor and to prevent automated spam submissions.                 Share            Omitted variable bias in GLMs of neural spiking activity   Ian H.  Stevenson  bioRxiv 317511; doi: https://doi.org/10.1101/317511             Share This Article:       Copy                            Citation Tools         Omitted variable bias in GLMs of neural spiking activity   Ian H.  Stevenson  bioRxiv 317511; doi: https://doi.org/10.1101/317511      Citation Manager Formats   BibTeX Bookends EasyBib EndNote (tagged) EndNote 8 (xml) Medlars Mendeley Papers RefWorks Tagged Ref Manager RIS Zotero                       Tweet Widget Facebook Like Google Plus One     Subject Area   Neuroscience               Subject Areas          All Articles        Animal Behavior and Cognition  (1992)  Biochemistry  (3745)  Bioengineering  (2526)  Bioinformatics  (12280)  Biophysics  (5250)  Cancer Biology  (4087)  Cell Biology  (5858)  Clinical Trials  (138)  Developmental Biology  (3503)  Ecology  (5543)  Epidemiology  (2052)  Evolutionary Biology  (8377)  Genetics  (6345)  Genomics  (8079)  Immunology  (3269)  Microbiology  (9800)  Molecular Biology  (3870)  Neuroscience  (22816)  Paleontology  (166)  Pathology  (631)  Pharmacology and Toxicology  (1014)  Physiology  (1543)  Plant Biology  (3471)  Scientific Communication and Education  (811)  Synthetic Biology  (1085)  Systems Biology  (3309)  Zoology  (569)                                   "
8,overconfidence(fallacy OR bias OR heuristic) (coronavirus OR covid OR pandemic),2,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7144592/,"Access Denied

Your access to the NCBI website at www.ncbi.nlm.nih.gov has been temporarily blocked due to a possible misuse/abuse situation involving your site. This is not an indication of a security issue such as a virus or attack. It could be something as simple as a run away script or learning how to better use E-utilities, http://www.ncbi.nlm.nih.gov/books/NBK25497/, for more efficient work such that your work does not impact the ability of other researchers to also use our site. To restore access and understand how to better interact with our site to avoid this in the future, please have your system administrator contact info@ncbi.nlm.nih.gov.","        Skip to main content                   Home  About  Submit  ALERTS / RSS             Search for this keyword           Advanced Search                               New Results  Omitted variable bias in GLMs of neural spiking activity   View ORCID Profile Ian H.  Stevenson  doi: https://doi.org/10.1101/317511   Ian H. Stevenson 1 University of Connecticut, Department of Psychological Sciences 2 University of Connecticut, Department of Biomedical Engineering 3 CT Institute for Brain and Cognitive Sciences Find this author on Google Scholar Find this author on PubMed Search for this author on this site ORCID record for Ian H. Stevenson      Abstract Full Text Info/History Metrics Preview PDF          Abstract Generalized linear models (GLMs) have a wide range of applications in systems neuroscience describing the encoding of stimulus and behavioral variables as well as the dynamics of single neurons. However, in any given experiment, many variables that impact neural activity are not observed or not modeled. Here we demonstrate, in both theory and practice, how these omitted variables can result in biased parameter estimates for the effects that are included. In three case studies, we estimate tuning functions for common experiments in motor cortex, hippocampus, and visual cortex. We find that including traditionally omitted variables changes estimates of the original parameters and that modulation originally attributed to one variable is reduced after new variables are included. In GLMs describing single-neuron dynamics, we then demonstrate how post-spike history effects can also be biased by omitted variables. Here we find that omitted variable bias can lead to mistaken conclusions about the stability of single neuron firing. Omitted variable bias can appear in any model with confounders – where omitted variables modulate neural activity and the effects of the omitted variables covary with the included effects. Understanding how and to what extent omitted variable bias affects parameter estimates is likely to be important for interpreting the parameters and predictions of many neural encoding models. Introduction Regression models have been widely used in systems neuroscience to explain how external stimulus and task variables as well as internal state variables may relate to observed neural activity ( Brown et al., 2003 ; Kass et al., 2005 ). However, in many cases, the full set of variables that explain the activity of the observed neurons is not observed or is not even known. It is important to recognize that, in these cases, omitted variables can cause the parameter estimates for the effects that are included in a regression model to be biased. That is, parameter estimates for the modeled effects would be different if other, omitted variables were to be included in the model ( Box, 1966 ). In experiments from behaving animals ( Niell and Stryker, 2010 ; Reimer et al., 2014 ), but also in more controlled sensory tasks ( Kelly et al., 2010 ; Arandia-Romero et al., 2016 ), there is growing evidence that neural activity is affected by many more variables than are typically considered relevant ( Kandler et al., 2017 ; Stringer et al., 2018 ). At the same time, although it has long been a concern in statistics ( Pearl, 2009 ) and has received some attention in other fields ( Greenland, 1989 ; Clarke, 2005 ), omitted variable bias, as a general problem, appears underappreciated in systems neuroscience. Here we demonstrate why systematically considering omitted variable bias may be important in neural data analysis and examine how omitted variable bias can affect one popular framework for describing neural spiking activity – the generalized linear model (GLM) with Poisson observations. In general, regression methods aim to estimate variations in a response variable as a function of other variables or covariates. When the goal of modeling is to maximize prediction accuracy, such as with brain machine interfaces, interpreting the model parameters may not be a high priority. However, in many other cases, parameter estimates are, at least to some extent, interpreted and analyzed. For instance, tuning curves or receptive fields may be measured and compared under different stimulus or task conditions or before and after a manipulation. In fully controlled experiments where the covariates are assigned at random, estimated coefficients can often be interpreted as estimates of causal effects ( Gelman and Hill, 2007 ). However, for many cases in neuroscience, it may be difficult or impossible to completely control or randomize all the relevant variables. In modeling neural activity, omitted variable bias can appear in any situation where neurons are modulated by omitted variables and the omitted variables (often called confounders) are not independent from the variables included in the model – the ones whose effects we are trying to estimate. Minimizing the influence of confounding variables is a major part of most experimental design ( Rust and Movshon, 2005 ), and the statistical effects of confounding variables are well understood (Wasserman, 2004). However, when the goal of modeling is description or explanation ( Shmueli, 2010 ), the effects of these omitted variables are frequently neglected. To give a concrete example, imagine an idealized neuron in primary motor cortex (M1) whose firing, unlike typical M1 neurons ( Georgopoulos et al., 1982 ), is not at all modulated by reach direction but, instead, is modulated by reach speed ( Fig 1 ). In a typical experimental setting, an animal’s reach directions are randomized, but reach speed cannot be randomized or tightly controlled. If the average speed differs across reach directions, such a hypothetical neuron will appear to be tuned to reach direction, despite not being directly affected by direction. First, fitting a typical tuning curve for reach direction, we would infer that such a neuron has a clear preferred direction and non-zero modulation depth. On the other hand, if we then fit a second model that included both reach direction and speed, we would infer that the neuron is modulated by speed alone, and it would be apparent that the original preferred direction and modulation depth estimates were biased due to the omitted variable. Download figure Open in new tab Figure 1:  When relevant variables are omitted from the model, estimates of the included effects can be biased. Consider two hypothetical neurons tuned to an observed variable x and an omitted variable x h . Neuron 1 is not tuned to the observed variable x , but its rate is modulated by the omitted variable with true tuning curves denoted by the gray curves (top left). If x and x h covary, the apparent tuning of this neuron to x when the tuning curve is estimated using x alone will then be biased (red and blue curves, top middle). This neuron will appear to be tuned to x despite not actually being tuned to this variable. In addition, to this type of illusory tuning, there can also be more subtle biases. Here, neuron 2 is tuned to x and x h (gray curves, bottom left). However, depending on how x and x h covary, the preferred stimulus or the modulation can be misestimated. Here the true tuning to x is shown at three different fixed values of x h (three gray curves, left panels). The estimated tuning when x h is omitted are shown at center with red and blue curves corresponding to the estimates under two different joint distributions (matching borders, right). Dashed line denotes the effect of x when x h is fixed. In adding additional variables, previous studies have largely focused on the fact that including previously omitted variables improves model accuracy or the fact that neural activity is often influenced by a host of task variables. In M1, for instance, including speed improves model accuracy ( Moran and Schwartz, 1999 ), but the presence of many correlated motor variables (e.g. kinematics, end-point forces, muscle activity) makes it difficult to interpret how neurons represent movement overall ( Humphrey et al., 1970 ; Omrani et al., 2017 ). Here, instead of focusing on the advantages or complexities of models with many variables, we focus on the well-known, but under-discussed, fact that the parameters describing the original effects change as additional variables are included. The hypothetical M1 neuron above points to a more general question about regression models of neural activity. What happens when we cannot or do not include variables that are relevant to the process that we are modeling? Here we first evaluate the statistical problem of omitted variable bias in the canonical generalized linear model with Poisson observations. Then, as a case study, we examine how speed affects estimates of direction tuning of neurons in primary motor cortex, as well as, two other case studies where the spike counts are modeled as a function of external variables: orientation tuning in primary visual cortex (V1) and place tuning in the hippocampus (HC). In each of these case studies we find that commonly omitted variables (speed in M1, population activity in V1, and speed and heading in HC) can bias the estimated effects of commonly included variables (reach direction in M1, stimulus orientation/direction in V1, and place in HC). Across all three case studies, including the omitted variables reduces the estimated modulation due to typical tuning effects. We also illustrate how omitted variable bias can affect generalized linear models of spike dynamics where a post-spike history filter aims to describe refractoriness and bursting ( Truccolo et al., 2005 ). The goal of these models is typically to differentiate aspects of spike dynamics that are due to the neurons own properties (e.g. membrane time constant, resting potential, after-hyper-polarization currents) from those due to input to the neuron from other sources ( Brillinger and Segundo, 1979 ; Paninski, 2004 ). In this setting, the input to the neuron is typically not directly observed, but is approximated by stimulus or behavioral covariates, local field potential, or the activity of other neurons. Here we show that omitting the input can lead to large biases in post-spike history filters, and that including omitted variables describing the input can change the interpretation and stability of the estimated history effects. GLMs have been used in many settings to disentangle the effects of multiple, possibly correlated, stimulus or task variables ( Fernandes et al., 2014 ; Park et al., 2014 ; Runyan et al., 2017 ) and also to model neural mechanisms such as post-spike dynamics, interactions between neurons, and coupling to local fields ( Harris et al., 2003 ; Truccolo et al., 2005 ; Pillow et al., 2008 ). It is often argued that GLMs are advantageous because they have unique maximum likelihood estimates and can be more robust to non-spherical covariate distributions than other methods, such as spike-triggered averaging ( Paninski, 2004 ; Pillow, 2007 ). Although these advantages are important, GLMs are not immune to bias. Here we show how the possibility of omitted variable bias, in particular, should encourage researchers to be cautious in their interpretation of model parameters, even in cases where a GLM achieves high predictive accuracy ( Shmueli, 2010 ). Results Here we introduce the problem of omitted variable bias and examine differences between omitted variable bias in linear models and the canonical Poisson GLM. We then consider three tuning curve estimation problems: estimating direction tuning in primary motor cortex, place tuning in hippocampus, and orientation tuning in primary visual cortex and show how omitted variables in each of these three cases can alter parameter estimates. Finally, we consider a GLM that aims to describe the dynamics of post-spike history and show how omitted inputs can bias the estimated history effects and qualitatively change model stability. Omitted Variable Bias in Linear Regression and canonical Poisson GLMs When relevant variables are not included in a regression model, the estimated effects for the variables that are included can be biased ( Box, 1966 ). Omitted variable biases can cause the parameters describing the effects of the original variables to be over- or under-estimated, and model fits can change qualitatively when omitted variables are included ( Fig 1 ). To understand the problem of omitted variable bias it will be helpful to briefly review the well-known case of multiple linear regression, where the bias can be described analytically ( Box, 1966 ). In the linear setting, consider the generative model  where observations y are a linear combination of observed X and omitted X h variables plus normally-distributed i.i.d. noise ϵ∼N (0, σ ). For simplicity, we ignore the intercept term, but in the analysis that follows it may also be considered as part of X . If we then fit the (mis-specified) model without X h using maximum likelihood estimation (equivalent to the ordinary least squares solution, in this case) the estimated parameters will be  where ξ denotes the effect of the noise, and the bias ( X T X ) −1 X T X h β h will, generally, be non-zero. There will be no bias only in the cases where the omitted variables do not affect the observations ( β h = 0) or when the omitted variables and observed variables are not collinear ( X T X h = 0). Note that ( X T X ) −1 X T X h is the matrix of regression coefficients for the omitted variables using the observed variables as predictors. For linear regression, the omitted variable bias thus depends on both the extent to which the omitted variables affect the observations β h and the extent to which the omitted variables can be (linearly) predicted from the observed variables. Although there is a closed-form solution for the omitted variable bias for linear regression, the generalized linear setting is not as tractable ( Gail et al., 1984 ; Clogg et al., 1992 ; Drake and McQuarrie, 1995 ). We will consider the case of a canonical Poisson GLM, in particular, where  In the more general case, GLMs have  where g −1 (·) is the inverse link function, and y is distributed following an exponential family distribution ( McCullagh and Nelder, 1989 ). For a canonical GLM the log-likelihood takes the form  where the nonlinear function G (·) depends on both the link function and the noise model. For canonical GLMs, this log-likelihood is concave and the maximum likelihood estimate satisfies . The exact form of G (·) will depend on the model, but for linear regression G ( x ) is proportional to , and for canonical (log-link) Poisson regression G (·) = exp(·). Now, with omitted variables, instead of maximizing the correct log-likelihood, we maximize instead  For the omitted variable bias in to be 0, we need both and at the same value of β . Although, neither MLE has a closed form solution, this condition implies that, if there is no bias due to the omitted variables,  where G ′ (·) is the derivative of G (·). For linear regression this equality reduces to the OLS form derived above, and for canonical Poisson regression we have  This equality is satisfied when observations are not modulated by the omitted variables β h = 0 or, more generally, when the effect of the omitted variables δλ = exp( Xβ + X h β h ) – exp( Xβ ) is orthogonal to the included variables X . Note that with linear regression, X T X h = 0 implies that the estimates will not be biased, but here this is not the case unless X T δλ = 0 as well. Due to the structure of the canonical Poisson GLM, omitted variable bias can thus occur even in a properly randomized, controlled experiment ( Gail et al., 1984 ), It is important to note that the maximum likelihood estimates themselves are consistent. That is, the estimators converge (in probability) to their true values when the generative model is correct. The bias here is a result of the model being mis-specified. This mis-specification affects the location of the maximum and, also, the shape of the likelihood. Optimization methods, such as Newton’s method, will typically contain omitted variable bias in each parameter update. For canonical Poisson regression, for instance, the updates take the form  at iteration k where the weight matrix W is diagonal with entries W ii = λ i and ( X T WX ) −1 is the Fisher scoring matrix (inverse Hessian of the log-likelihood) at the current estimate . Since the mis-specified model will use λ = exp( Xβ ) instead of the exp( Xβ + X h β h ), both the weight matrix and the gradient X T ( y – λ ) will be biased at each step of the optimization (except when ). Traditional standard errors for the MLE will also typically be influenced by omitted variables, since . Moreover, as previous studies have shown, omitted variables can lead to misestimation of the variability in E [ y ] and dispersion var ( y ) ( Czanner et al., 2008 ; Goris et al., 2014 ). If the omitted variables affect the observations, then they will generally increase the variability of E [ y ]. Then, unless the omitted variables are perfectly predicted by the included variables, the explained variance var ( E [ y ]) of the mis-specified model will be lower than that of the full model. This may, in turn, lead to overestimates of dispersion, since var ( y ) = E [ var ( y )] + var ( E [ y ]). Omitted Variable Bias in Tuning Curve Estimation When fitting tuning curve models to spike count data, omitted variable bias can cause preferred stimuli and modulation depths to be misestimated and can even lead to completely illusory tuning ( Fig 1 ). To illustrate how omitted variable bias affects GLMs of neural spiking, not just in theory, but in practice, we consider three case studies where we fit typical tuning curve models that omit potentially relevant variables along with augmented models that include these additional variables. We first consider modeling spike counts across trials and on slow (>100ms) timescales. Here we assess 1) the tuning of neurons in motor cortex to reach direction, with speed as a potential omitted variable, 2) the tuning of neurons in hippocampus to position, with both speed and head-direction as potential omitted variables, and 3) the tuning of neurons in visual cortex to the direction of motion of a sine-wave grating, with population activity as a potential omitted variable. In each of these cases studies, we show how the omitted variables are not independent from the commonly included variables and how neural responses are modulated by the omitted variables. These two properties, together, can lead to omitted variable biases. In our first case study, we model data recorded from primary motor cortex (M1) of a macaque monkey performing a center-out, planar reaching task. In this task, speed differs systematically across reach directions ( Fig 2A ), with average speed differing by as much as 35±3% (for the targets at 45 and 225 deg relative to right, Fig 2B ). To model neural responses, we first fit a traditional tuning curve model ( Georgopoulos et al., 1982 ; Amirikian and Georgopulos, 2000 ), where the predicted responses depend only on target direction. Here we use a circular, cubic B-spline basis (5 equally spaced knots) to allow for deviations from sinusoidal firing, but, in most cases, the responses of the n=81 neurons in this experiment are well described by cosine-like tuning curves with clear modulation for reach direction. We then fit a second model that includes effects from movement speed. Here we use covariates based on ( Moran and Schwartz, 1999 ), including a linear speed effect, as well as, cosine-tuned direction-speed interactions (see Methods). This model captures the responses of individual neurons, where spike counts can increase ( Fig 2C , top) or decrease ( Fig 2C , middle) as a function of speed, and, in some cases, speed and direction appear to interact ( Fig 2C , bottom). Together, the fact that direction and speed are not independent along with the fact that neural responses appear to be modulated by speed could lead to biased parameters estimates for the model where speed is omitted. Comparing the models with and without omitted variables we find that, averaged across the population, there are only minimal shifts in the preferred direction (3±2 deg) when speed is included in the traditional tuning curve model, and there do not appear to large, systematic shifts in the population distribution of PDs (Kuiper’s test, p>0.1). At the same time, there is substantial variability between neurons in the size of the PD-shift (circular SD 32±5 deg). Across the population, modulation depth (measured using the standard deviation of the tuning curve) decreases slightly on average (3±2%), and the size of the modulation change also varies substantially between individual neurons (SD of changes 18±3%). An example neuron in Fig 1C (bottom), for instance, has a modulation decrease of 9±5% and the preferred direction changes 4±9 deg when speed is included in the model (standard error from bootstrapping). Overall, ∼10% of neurons have statistically significant changes in PD, and ∼14% have significant changes in modulation (bootstrap tests α =0.05, not corrected for multiple comparisons). For some individual neurons, at least, the parameters of the model without speed, thus, have clear omitted variable bias. However, since individual neurons have diverse speed dependencies, in this case, the average biases across the population are minimal. When speed is included in the model, model accuracy (pseudo-R 2 ) does increase slightly (p=0.01, one-sided paired t-test across neurons). The average cross-validated (jack-knife) pseudo-R 2 for the original model is 0.23±0.01 and for the model with speed 0.24±.01 ( Fig 5 ). However, it seems likely that in other experimental contexts the effects of omitting speed could be more pronounced. By requiring the animal to make reaches to the same targets at different speeds, previous studies have more clearly demonstrated that responses in M1 are modulated by speed ( Churchland et al., 2006 ). Here we demonstrate how this type of modulation can lead to omitted variable biases in the estimated parameters of typical tuning curve models without speed. Download figure Open in new tab Figure 2:  Speed as an omitted variable in M1 tuning for reach direction. A) The distribution of reach speeds differs by target direction in a center-out task. Circles denote median, boxes denote IQR. B) Speed profiles for the two targets showing the largest speed differences. Individual traces denote individual trials aligned to the half-max (black arrow). Inset shows the position of each trial with colors denoting reach direction. C) The responses of 3 M1 neurons show typical tuning for reach direction. The tuning curve estimated using direction covariates alone (black) changes when speed covariates are included (red). Red curves denote the direction effect within the full model and are generated by assuming speed is constant (equal to the mean speed across all trials). Right panels illustrate the speed dependence for the preferred direction and its opposite. Dark lines denote the estimated effect of speed under the full model. Data points show single trial data, along with the mean speed and rate for each direction (big data point). Light lines show linear trends (OLS) using only the trials from each specific target. In our second case study we examine the activity of neurons in the dorsal hippocampus of a rat foraging in an open field. Here we consider to what extent the practice of omitting speed and head direction from a place field model biases estimates of a neuron’s position tuning. As in the first case study, omitted variable bias can occur if neural activity is modulated by omitted variables and the omitted variables covary with the included variables. In the case of the hippocampus, neural activity is known to be modulated by both movement speed and head direction ( McNaughton et al., 1983 ), in addition to an animal’s position ( O’Keefe and Dostrovsky, 1971 ). Additionally, behavioral variables can be highly nonuniform across the open field ( Walsh and Cummins, 1976 ), for instance, near and far from the walls. Together the fact that the omitted variables may covary with position and the fact that neurons appears to be modulated by the omitted variables, suggest that there may be omitted variable bias. Here, in one recording during an open field foraging task we find that the average speed ( Fig 2A ) and heading ( Fig 2B ) differ extensively as a function of position. Within a given neuron’s place field, the distributions of speed and heading may be very different from their marginal distributions. Across the population of n=68 place cells (selected from 117 simultaneously recorded neurons, see Methods), average in-field speed was between 80-135% of the average overall speed (5.5cm/s), and the animal’s heading can be either more or less variable in-field (circular SD 57-80 deg) compared to overall (75 deg). As previous studies have shown, we also find that neural responses are modulated by speed and head direction. Responses due to place, speed, and heading are shown for one example neuron in Fig 3 . This neuron shows a stereotypical place-dependent response ( Fig 2B ), but splitting the observations by speed ( Fig 3C , top) or heading ( Fig 3B , bottom) by quartiles/quadrants reveals that there is also tuning to these variables. The neuron appears to increase its firing with increasing speed and responds most strongly when the rat is facing the left. These dependencies are well fit by the full model where the firing rate depends, not just on position, but also on the (log-transformed) speed and the heading ( Fig 3D , bottom). For the example place cell shown here, the location of the place-field does not change substantially when the omitted variables are included ( Fig 3E ). However, the modulation (SD of the rate map) decreases by 27%. That is, 27% of the apparent modulation due to position when it is modeled alone, can be explained by speed and heading effects. Across the population of place cells, there were no clear, systematic difference in the place field locations, but the modulation (SD of the rate map λ ( x )) decreases by 9±1% on average when speed and heading are included. Individual neurons showed substantial variability in their modulation differences (population SD 10±1%). As in M1, including the omitted variables increased spike prediction accuracy – the average cross-validated (10-fold) pseudo-R 2 was .29±.02 for the original model and .31±.02 for the model including speed and heading activity. This difference seems small, since there is large variability in pseudo-R 2 values across the population, but the average increase in pseudo-R 2 was 11±3% ( Fig 5 ). Given that neurons appear to be modulated by speed and heading, it is unsurprising that including these variables improves model fit. However, as before, it is important to note that this modulation can lead to biases in the place field estimates for the model with only position. Download figure Open in new tab Figure 3:  Speed and heading as omitted variables in hippocampal place cells. A) Average speed and heading as a function of position for a rat foraging in an open field. B) An example place cell tends to spike (red dots) when the animal is at a specific position in space. C) The activity of this neuron is modulated by the animal’s speed (top row) and heading (bottom row). Speed is split into quartiles, subplots include all headings. Heading is split into quadrants, subplots include all speeds. D) The distributions of speed and heading within the place field differ from the overall distributions, and the neuron is tuned to these variables. Blue curve shows model fit. E) After modeling the effect of speed and heading within the place field, the location of the place field does not change but the apparent modulation due to position is reduced. In our third case study, we examine the activity of neurons in a more controlled sensory experiment. Here we use data recorded from primary visual cortex (V1) of an anesthetized monkey viewing oriented sine-wave gratings moving in on of 12 different directions (see Methods). In this experiment, variability in the animal’s behavior is purposefully minimized, and, instead of considering the effect of omitting a behavioral variable, here we consider the effect of omitting a variable relating to the animal’s internal state – the total population activity. Several studies have previously shown that population activity alters neural responses in V1 ( Arieli et al., 1996 ; Kelly et al., 2010 ; Okun et al., 2015 ; Arandia-Romero et al., 2016 ). If the distribution of population activity also varies with stimulus direction, then there is the potential for omitted variable bias. Here we assess neural activity from n=90 simultaneously recorded neurons across many (2400) repeated trials with 12 different movement directions. We find that there is high trial-to-trial variability in the population rate ( Fig 4A ), and the average firing across all neurons does differs across stimulus directions, up to ∼50%. For this recording, the most extreme differences were between the 180 deg stimulus where the average rate across the population was 3.4±0.1Hz and the 60 deg stimulus where the average rate was 6.3±0.1Hz ( Fig 4B ). By adding the (log-transformed) population rate as a covariate to a more typical model of direction tuning, we find that population activity may lead to omitted variable bias in models of direction tuning alone. As in the case studies above, there do not appear to be any consistent or systematic effects on the preferred stimulus direction at the population level (Kuiper’s test, p=0.1). However, the modulation depth (measured using SD of the tuning curve) decreases substantially 15±2% when population rate is included in the model, and there is again high variability across neurons (SD 20±2%). In this case, model accuracy increases substantially when the omitted variable is included. The cross-validated (10-fold) pseudo-R 2 is .26±.02 for the original model and .43±.02 for the model including population activity, with an average increase of 164±31% ( Fig 5 ). Download figure Open in new tab Figure 4:  Population rate as an omitted variable in primary visual cortex. A) Correlated trial to trial variability. Population rasters for three trials of the same drifting grating stimulus (0 deg, red and 30 deg, orange). Neurons are sorted by overall firing rate. B) Histograms of the population rate across trials. As a population, the neurons respond at higher rates to 30 deg stimuli, but there is high trial-to-trial variability. C) The responses of 2 V1 neurons show typical tuning for direction of motion. The tuning curve estimated using direction covariates alone (black) changes when the population rate covariate are included (red). Right panels illustrate the dependence for the preferred direction and an orthogonal direction. Dark lines denote the estimated effect of speed under the full model. Data points show single trial data, along with the mean count and rate (big data point). Light lines show linear trends (OLS) using only the trials from each specific stimulus. Unlike in M1 where the effect of speed was highly diverse for different neurons, in this case study the effect of the population rate is largely consistent. Higher population rates are associated with higher firing rates, and, for most neurons, the effect of the population rate is stronger in the preferred direction(s), consistent with a multiplicative effect. Note that here, we do not include the neuron whose rate we are modeling in the calculation of the population rate. However, using the population rate as an omitted variable requires some interpretation. The population rate will certainly be affected by the tuning of the, relatively small, sample of neurons that we observe. If we have a disproportionate number of neurons tuned to a specific preferred direction, the population rate in those directions will be higher. This suggests that in a different recording, the covariation between the stimulus and the population rate could very likely be different. However, it appears that the omitted variable biases in this case are mostly driven by noise correlations, where neural activity is correlated on single trials even within the same stimulus condition, rather than stimulus correlations, where neural activity is correlated due to similar tuning. When we shuffle the data within each stimulus condition (removing noise correlations) the average change in the modulation depth is -1±2% (SD 18±3%), and the effect of the omitted variable becomes negligible. Download figure Open in new tab Figure 5:  For each of the case studies, on average, the model accuracy increases when omitted variables are included (top) and the modulation due to the original variables decreases (bottom). Scatter plots indicate cross-validated pseudo-R2 values for each neuron under the two models. Modulation denotes the standard deviation of the tuning to the original variable(s) under each model. Here, modulation values are normalized by the average rate of each neuron. Black lines denote equality. Red dashed lines denote linear fit with 0 intercept. Download figure Open in new tab Figure 6:  Estimated post-spike history filters can be heavily biased when the input is not included in the model. A) Here we simulate from an inhomogeneous Poisson model with sinusoidal input (no post-spike history effects). The input and spike responses from 20 trials are shown. Although there are no history effects in the generative model, a GLM with history effects that is missing the correct input covariate will use the history terms to capture the structure in the autocorrelation (C). Traces denote the estimated rate for the 20 trials shown above. When the history term is included in the model, but the input is not, the GLM can still reconstruct PSTH responses using the post-spike history alone. B) Post-spike filters for the models in (A) with 95% confidence bands. Note that when input is included in the model the filters correctly reconstruct the true (lack of) filter, and that there is higher uncertainty around the regions where the ISI distribution does not constrain the model. Omitted Variable Bias in the Estimation of Post-Spike History Effects In addition to modeling spike counts over trials or on relatively slow (>100ms) behavioral timescales, GLMs are also often used to describe detailed, single-trial spike dynamics on fast (<10ms) timescales. One common covariate used in these types of models is a post-spike history effect where the probably of spiking at a given time depends on the recent history of spiking. Modeling these effects allows us to describe refractoriness, bursting ( Paninski, 2004 ; Truccolo et al., 2005 ), and a whole host of other dynamics ( Weber and Pillow, 2017 ). Conceptually, the goal of these models is to disentangle the sources of rate variation based only on observations of a neuron’s spiking, with history effects, ideally, reflecting intrinsic biophysics. However, since the full synaptic input is typically not known with extracellular spike recordings there is potential for omitted variable biases. To illustrate the potential pitfalls of omitting the input to a neuron, consider using the GLM to capture single neuron dynamics in the complete absence of external covariates  where the rate λ is determined by a baseline parameter μ along with a filtered version of the neuron’s past spiking with h i ( t ) = ∑ τ>0  f i (τ) n ( t – τ). This is a perfectly acceptable model of intrinsic dynamics, but for most spike data that we observe this isolated neuron model may not provide a realistic description of a neuron receiving thousands of time-varying synaptic inputs. If we fit this model to data where the input to the neuron did vary over time,  then the history filter in the first model will attempt to capture variation in spiking due to the time-varying input, in addition to any intrinsic dynamics. For example, when x h is periodic, the estimated history filters of the original model will attempt to capture this periodic structure ( Fig 6A-B ). Just as in the tuning curve examples above, the fact that history effects covary with the input and the fact that the input modulates the neuron’s firing leads to omitted variable bias. When the input is omitted from the model, the biased history effects simply provide the best (maximum likelihood) explanation of the observed spiking ( Fig 6C ). These examples with strong, periodic input are not necessarily biologically realistic, but they make it apparent how the post-spike history can be biased by omitted input variables. In vivo, neurons instead appear to be in a high-conductance state, where membrane potential fluctuations have approximately 1/ f power spectra ( Destexhe et al., 2001 , 2003 ). When these naturalistic input statistics are used to drive the GLM, omitted variable bias can occur, as well. Here we simulate a GLM receiving 1/ f α noise input with α = 0 (white noise) 1 and 2 ( Fig 7 ). For white noise input, the MLE accurately recovers the simulated post-spike history filter when the input is omitted from the model, but when α = 1 or 2 the estimates become increasingly biased ( Fig 7A,C ). With the full model, where the input is included as a covariate, the history is recovered accurately no matter what the input statistics are. Just as in the periodic case, however, these different input statistics alter the auto-correlation, and, when the input is omitted from the model, the maximum likelihood history filter simply aims to capture these patterns. Download figure Open in new tab Figure 7:  Post-spike filters can show omitted variable bias even in a more realistic scenario. Here we simulate from a GLM with a refractory post-spike filter and drive the neurons with 1/ f α noise. Excepting the case of white noise ( α = 0), the post-spike filters estimated for the GLM without input are heavily biased (A). C) Even when the effect of the true post-spike filter is to strictly decrease the firing rate, the estimated filters can increase the firing rate. B,D) Approximate transfer functions from a quasi-renewal approximation. When the true filter is stable, the estimated filters can result in fragile dynamics. In GLMs for single-neuron dynamics, one effect of omitted variable bias is that it may lead us to misinterpret how stable a neuron’s dynamics are. Even if the true history filter only reduces the neuron’s firing rate following a spike (as in Fig 7C ), the estimated filter can be biased upwards when the input is omitted. If we were to simulate the activity of this neuron based on the biased filter, the bias could cause the neuron’s rate to diverge if the rate becomes high enough. To assess the stability of the estimated post-spike history effects quantitatively, here we make use of an quasi-renewal approximation analysis introduced in ( Gerhard et al., 2017 ). Given a history filter, this approach finds an approximate transfer function describing the neuron’s future firing rate (output) given its recent (input) firing rate (see Methods). For all estimated models, the transfer function has a stable fixed point near the neuron’s baseline firing rate. When the true input is omitted and α > 0, the estimated history filters also have an unstable fixed point where the neuron’s firing rate will diverge if the rate exceeds this point ( Gerhard et al., 2017 ). Here we find that omitted variable bias leads to apparent fragility ( Fig 7B,D ). The stable region shrinks as α increases, and even when the true dynamics are strictly stable (as in Fig 7C,D ), omitted variable bias can lead us to mistakenly conclude that the neuron has fragile dynamics. With most extracellular spike recordings, the synaptic input that the neuron receives is unknown. However, there may also be omitted variable bias when history effects are estimated from real data. In this case, the input to a neuron can be approximated by stimulus or behavioral variables, local field potentials, or the activity of simultaneously recorded neurons ( Harris et al., 2003 ; Truccolo et al., 2005 , 2010 ; Pillow et al., 2008 ; Kelly et al., 2010 ; Gerhard et al., 2013 ; Volgushev et al., 2015 ). Just as in the simulations above, including or omitting these variables can then alter the estimated history effects, even though they are not as directly related to spiking as the synaptic input itself. Here we consider total population spiking activity as a proxy for synaptic input and consider how including population activity alters the history filters when compared to a model of history alone. We examine two datasets: spontaneous activity from primary visual cortex of an anesthetized monkey with n=62 simultaneously recorded neurons and activity from dorsal hippocampus of a sleeping rate with n=39 simultaneously recorded neurons. To model population covariates we sum the spiking of all neurons, excepting the one whose spiking we aim to predict, and low-pass filter the signal (see Methods). Similar to previous results ( Okun et al., 2015 ), we find that, since neurons often have correlated fluctuations in their spiking ( Fig 8A, D ), the population rate is a good predictor for single neuron activity. Moreover, when we add population covariates to a GLM with post-spike history effects the history filter changes. In the V1 dataset, the post-spike gain decreases by 7.8±0.5% on average when population covariates are included, and 14.9±0.8% when considering only the first 50ms after a spike ( Fig 8B ). The effects of adding population covariates are less pronounced in the hippocampal dataset. The post-spike gain decreases by 2.5±0.3% on average, and 9.5±1.2% when considering only the first 50ms after the spike ( Fig 8E ). Based on the quasi-renewal approximation, all neurons in both the V1 and hippocampal datasets have fragile transfer functions where there is a stable fixed point (near the neuron’s average firing rate) and an unstable fixed point where the neuron’s rate diverges if the input becomes too strong. For V1, the average upper-limit of the stable region is 80±3Hz for the models with history only and 143±7Hz for the models with population covariates ( Fig 8C ). In the hippocampal data, the average upper-limit of the stable region is 38±6Hz for the models with history only and 75±13Hz for the models with population covariates ( Fig 8F ). Each neuron is, thus, apparently, more stable after the population covariates are included. As in the case studies using tuning curves, adding covariates also improves spike prediction accuracy. In the V1 dataset, the average log likelihood ratio relative to a homogeneous Poisson model is 2.2±0.3 bits/s for the history model and 3.3±0.3 bits/s for the model with population covariates. In hippocampus, the log likelihood ratio is 0.9±0.3 bits/s for the history model and 2.0±0.5 bits/s for the model with population covariates. The larger effects in V1 are likely explained by the fact that the population rate is predictive for many more neurons here than for the hippocampal data. In the hippocampus, only 26% of the neurons have an increase of over 0.5 bits/s when the population covariates are included, compared to 85% of neurons in V1. Altogether these results demonstrate how omitted variable bias could affect estimates of post-spike history filters in vivo. In both datasets we find that when population covariates are included in the GLM spike prediction accuracy increases, post-spike gain decreases, and apparent stability increases. Download figure Open in new tab Figure 8:  Post-spike filters estimated from real data decrease when population activity is included as a covariate. Segments of spontaneous activity are shown for V1 (A) and during sleep for hippocampus (D). Neurons are sorted by firing rate. B and E show estimated post-spike filters. Black lines denote the average filter (thick) and standard deviation (thin). For clarity, only filters for neurons with firing rates >1Hz are shown. C and F show average quasi-renewal transfer functions for the same set of neurons. All neurons appear to have fragile dynamics with one stable fixed point near the neuron’s average firing rate and an unstable fixed point, beyond which the neuron’s firing rate diverges. Including population covariates increases the region of stability. Discussion When the goal of modeling is causal inference or understanding of biological mechanisms, the potential for biases due to omitted variables is often clear. The statistical effects of confounders (Wasserman, 2004), as well, as the limits that they place on neuroscientific understanding are widely appreciated ( Jonas and Kording, 2017 ; Krakauer et al., 2017 ; Yoshihara and Yoshihara, 2018 ). However, when the goal of modeling is to create an abstract, explanation or summary of observed neural activity, the fact that omitted variables can bias these explanations is not always widely acknowledged. Here we have illustrated the potential for omitted variable bias in two types of commonly used GLMs for neural spiking activity: tuning curve models using spike counts across trials and models that capture single-neuron dynamics with a post-spike history filter. In each model, adding a previously omitted variable, as expected, improved spike prediction accuracy. However, what we emphasize here is that, when omitted variables were included, the estimates of the original parameters changed. For three case studies using tuning curves we found that by adding a traditionally omitted variable tuning curves showed less modulation due to the originally included variables. In models of single neuron dynamics, adding omitted variables led to decreased post-spike gain and greater apparent stability. Importantly, omitted variables can arise in GLMs in any situation where an omitted variable affects neural activity and the effect of the omitted variable is not independent of the included variables. The case studies here are not unique, and many studies have described how adding additional variables to a tuning curve or single neuron model can improve prediction accuracy. In M1, in addition to movement speed, joint angles, muscle activity, end-point force, and many other variables also appear to modulate neural responses ( Fetz, 1992 ; Kalaska, 2009). In addition to speed and head direction in the hippocampus, theta-band LFP, sharp-wave ripples, and environmental features, such as borders, appear to modulate neural activity ( Hartley et al., 2014 ). And in V1, there is growing evidence that population activity ( Lin et al., 2015 ) and non-visual information ( Ghazanfar and Schroeder, 2006 ) modulates neural responses. In each of these systems, neural responses are affected by many, many factors. Responding to many task variables may even be functional, allowing downstream neurons to more effectively discriminate inputs ( Fusi et al., 2016 ). In any case, it seems clear that our models do not yet capture the full complexity of neural responses ( Carandini et al., 2005 ). By omitting relevant variables, current models are likely to be not just less accurate but also biased. Parameter bias may be problematic in and of itself. However, omitted variable bias may also have an important effect on generalization performance. As noted in ( Box, 1966 ), in a new context, the effect of the omitted variables and the relationship between the omitted and included variables may be different. Since the parameters of the included variables are biased, this change can reduce generalization accuracy. This phenomena may explain, to some extent, why tuning models fit in one condition often do not generalize to others ( Graf et al., 2011 ; Oby et al., 2013 ). For models of single-neuron dynamics, omitted variable bias can also have a negative effect on the accuracy of simulations. Previous work has shown that simulating a GLM with post-spike filters estimated from data often results in unstable, diverging simulations. Although several methods for stabilizing these simulations have recently been developed ( Gerhard et al., 2017 ; Hocker and Park, 2017), one, perhaps primary, reason for this instability may be that the post-spike filters are biased due to omitted synaptic input. Since estimated post-spike filters may reflect not just intrinsic neuron properties but also the statistics of the input, interpreting and comparing post-spike filters may be difficult. Different history parameters may be different due to intrinsic biophysics ( Tripathy et al., 2013 ) or due to differing input, and resolving this ambiguity will likely involve more accurately accounting for the input itself ( Kim and Shinomoto, 2012 ). The possibility of omitted variable bias does not mean that estimated parameters, predictions, and simulations from simplified model are useless, but it may mean that we need to be cautious in interpreting these models and their outputs. When reporting the results of regression, in addition to avoiding describing associations with causal language, it may be generally useful to discuss known and potential confounds. Previous studies have already identified several specific cases of omitted variable bias where careful interpretation is necessary. For instance, omitted common input can bias estimates of interactions between neurons ( Brody, 1999 ), and omitted history effects can bias receptive field estimates ( Pillow and Simoncelli, 2003 ). In estimating peri-stimulus time histograms, omitting variables that account for trial-to-trial variation may cause biases ( Czanner et al., 2008 ) or issues with identifiability ( Amarasingham et al., 2015 ). Similarly, biases due to spike sorting errors ( Ventura, 2009 ) could be framed as a result of omitting variables related to missing/excess spikes. Since we typically do not model or observe all the variables that affect neural activity, omitted variable problems are likely to be pervasive in systems neuroscience far beyond these specific cases. Although we have focused on GLMs here, omitted variable bias can affect any model and other types of model misspecification can also result in biased parameter estimates. Adding input nonlinearities ( Ahrens et al., 2008 ; David et al., 2009 ), interaction effects ( McFarland et al., 2013 ), or higher-order terms to the GLM ( Berger et al., 2010 ; Park et al., 2013 ) may fix certain types of model misspecification, but any model that omits relevant variables is still likely to suffer from the same problems. This includes both machine learning methods that may provide better prediction accuracy than GLMs ( Benjamin et al., 2017 ) and single neuron models aiming to describe greater biophysical detail ( Herz et al., 2006 ). Unlike over-fitting or non-convergence ( Zhao and Iyengar, 2010 ), omitted variable bias will generally not be resolved by including additional data or by adding regularization. Moreover, adding one omitted variable, as we have done with the case studies here, is no guarantee that there are not other relevant variables being omitted. One approach that could potentially reduce omitted variable bias is latent variable modeling, where the effects of unknown covariates are explicitly included (constrained by simplifying assumptions). Recent work has introduced latent variables for neural activity with linear dynamics ( Smith and Brown, 2003 ; Kulkarni and Paninski, 2007 ; Paninski et al., 2010 ), switching dynamics ( Putzky et al., 2014 ), rectification ( Whiteway and Butts, 2017 ), and oscillations ( Arai and Kass, 2017 ). And these models appear to out-perform GLMs on population data in retina ( Vidne et al., 2012 ), visual ( Archer et al., 2014 ), and motor cortices ( Chase et al., 2010 ; Macke et al., 2011 ). Inferring latent variables requires making (sometimes strong) assumptions about the nature of the variables and may require observations from multiple neurons or across multiple trials, but, by approximating some of the effects of relevant omitted variables, latent variables may reduce omitted variable bias. However, generally determining when relevant variables are omitted from a model and what those variables are is not a trivial problem. There is a well-known aphorism from George EP Box that, “All models are wrong, but some are useful.” The lengthier version of this quip is, “All models are approximations. Essentially, all models are wrong, but some are useful. However, the approximate nature of the model must always be borne in mind.” GLMs are certainly useful descriptions of neural activity. They are computationally tractable, can disentangle the relative influence of multiple covariates, and often provide the core components for Bayesian decoders. Here we emphasize, however, one ubiquitous circumstance in systems neuroscience where the “approximate nature” of the models should be “borne in mind.” Namely, omitted variables can bias estimates of the included effects. Methods Neural Data All data analyzed here was previously recorded and shared by other researchers through the Collaborative Research in Computation Neuroscience (CRCNS) Data Sharing Initiative (crcns.org). Data from primary motor cortex is from CRCNS dataset DREAM-Stevenson_2011 ( Walker and Kording, 2013 ). These data were recorded using a 100-electrode Utah array (Blackrock Microsystems, 400 mm spacing, 1.5 mm length) chronically implanted in the arm area of primary motor cortex of an adult macaque monkey. The monkey made center-out reaches in a 20x20cm workspace while seated in a primate chair, grasping a two-link manipulandum in the horizontal plane (arm roughly in a sagittal plane). Each trial for the center-out task began with a hold period at a center target (0.3–0.5 s). After a go cue, subjects had 1.25 s to reach one of eight peripheral targets and then held this outer target for at least 0.2–0.5 s. Each success was rewarded with juice, and feedback (1-2cm diam) about arm position was displayed onscreen as a circular cursor. Spike sorting was performed by manual cluster cutting using an offline sorter (Plexon, Inc) with waveforms classified as single- or multi-unit based on action potential shape and minimum ISIs greater than 1.6 ms (yielding n=81 single units). Here we take model tuning curves using spike counts between 150ms before to 350ms after the speed reached its half-max. Average movement speed for each trial was calculated from 0-250ms after the speed reached its half-max (290 trials in total). For the details of the surgery, recording, and spike sorting see ( Stevenson et al., 2011 ). Hippocampal data is from CRCNS HC-3 ( Mizuseki et al., 2013 ). Here we use recording sessions ec16.19.272 and ec014.215, where a Long-Evans rat was sleeping and foraging in a 180x180cm maze, respectively. For both recordings, 12-shank silicon probes (with 8 recording sites each, 20μm separation) were implanted in CA1 (8 shanks) and EC3-5 (4 shanks) (based on histology). Spikes were sorted automatically using KlustaKwick and then manually adjusted (Klusters) yielding 85 units for the sleep data and 117 for the open field data. For the sleep data where we model post-spike history effects, the spike trains were binned at 1ms and the recording length was 27min. Here we model all neurons with firing rates >0.5Hz (n=39). For the open field data where we model place tuning, spike trains were binned at 250ms and the recording length was 93min. Place cells (n=68) were selected based on having an overall firing rate <5Hz (to rule out interneurons), a peak firing rate >2Hz, and a contiguous set of pixels (after smoothing with an isotropic Gaussian σ =8cm) of at least 200 cm 2 where the firing rate was above 10% of the peak rate. For details of the surgery, recording, and spike sorting see ( Diba and Buzsaki, 2008 ; Mizuseki et al., 2009 ). Data from primary visual cortex is from CRCNS dataset PVC-11 ( Kohn and Smith, 2016 ). Here we use spontaneous activity, during a gray screen, (Monkey 1) and responses to drifting sine-wave gratings (Monkey 1) both from an anesthetized (Sufentanil - 4-18 microg/kg/hr) adult monkey (Macaca fascicularis). Recordings were made in parafoveal V1 (RFs within 5 degrees of the fovea) using a 96-channel multi-electrode array (Blackrock Microsystems), 400 mm electrode spacing, 1mm depth. After automatic spike sorting and manual cluster adjustment, 87 and 106 units were recorded during spontaneous activity and grating presentation, respectively. Only neurons with waveform SNR>2 and firing rates >1Hz were analyzed, n=62 for spontaneous and n=90 for grating data. For the spontaneous activity we bin spike counts at 1ms and the recording length was 20min. For the drifting grating data, we analyzed spike counts from 200ms to 1.2s after stimulus onset on each trial – 12 directions, 2400 trials total. Gratings had a spatial frequency of 1.3 cyc/deg, temporal frequency of 6.25Hz, size of 8-10 deg (to cover receptive fields of all recorded neurons) and were presented for 1.25s with a 1.5s inter-trial interval between stimuli. For surgical, stimulus, and preprocessing details see ( Smith and Kohn, 2008 ; Kelly et al., 2010 ). Tuning Curve Models For the M1 data we use a circular, cubic B-spline basis with 5 equally spaced knots  where g (·) are the splines that depend on the reach direction θ , weighted by parameters β and the parameter μ defines a baseline firing rate. To include the effect of speed, we then add three covariates  where s indicates the speed, and the parameters α allow for a multiplicative speed effect as well as possible cosine-tuned speed x direction interactions as in ( Moran and Schwartz, 1999 ). For place fields in hippocampus we use isotropic Gaussian radial basis functions f (·) equally spaced (30cm) on an 6x6 square lattice with a standard deviation of 30cm  We find that the effect of speed is well modeled using the log-transformed speed s , and to model head direction-dependence we use circular, cubic B-splines g (·) with 6 equally spaced knots  For the V1 data we again use a circular, cubic B-spline basis for the direction of the sine-wave grating (7 equally spaced knots).  We find that the effect of population activity is well modeled using the total log-transformed firing rate of all neuron’s excepting the one being modeled  where z = ∑ i ≠ j log( n i + 1). In all models, to avoid overfitting, especially for low firing rate neurons, we add a small L2 penalty to the log-likelihood with a fixed hyperparameter of 10 -4 . Post-spike History Simulations and Population Rate Models In addition to capturing tuning curves, many studies have used GLMs to describe the dynamics of single spike trains ( Brillinger, 1988 ; Harris et al., 2003 ; Paninski, 2004 ; Okatan et al., 2005 ; Truccolo et al., 2005 ; Weber and Pillow, 2017 ). Here, to account for post-spike history effects, we use a GLM taking the form  where h ( t ) denotes the vector of spike history covariates representing the recent history of spiking and μ determines a baseline firing rate. Here we assume h i ( t ) = ∑ τ>0  f i (τ) n ( t – τ), and we use neuron-specific, cubic B-spline bases f (·) whose knots are determined by the quantiles of each neuron’s ISI distribution. Specifically, we choose knots spaced between 10 and 400ms (HC) or 2 and 200ms (V1), where the spacing follows equal percentile regions of the ISI distribution in that same range. This gives 6 basis functions, and coefficients α to capture the spike-history. To enforce refractoriness, we fix the coefficient of the fastest basis (which peaks at 0 and ends at 10ms) to be -5, leaving 5 coefficients to be estimated. The population rate model simply adds covariates where, for each neuron i   Here we use a set of acausal Gaussian filters for g (·) with standard deviations 20, 50, and 100ms. Note that spikes from the neuron being modeled are excluded from the population covariates. Stability analysis Here we make use of a stability analysis proposed in ( Gerhard et al., 2017 ). Briefly, we use a quasi-renewal approximation of the conditional intensity by considering the effect of the most recent spike, at time t ′, and averaging over possible spike histories preceding this spike  where H ( t – t ′) = αf ( t – t ′) and S represents the history of spiking. By assuming that S is generated from a homogeneous Poisson process with firing rate A 0 , the second term can be approximated by  Given this approximation, we can then estimate the inter-spike interval distribution as we would for a true renewal process and the steady-state distribution of inter-spike intervals is given by  and the predicted steady-state firing rate is f ( A 0 ) = 1 /E P ( τ ) [τ]. To assess stability, we can then examine how the predicted steady-state firing rate depends on the assumed rate of the homogeneous Poisson process A 0 . In particular, when f ( A 0 ) = A 0 the quasi-renewal model has a fixed-point. To allow for external input, we incorporate the average effect of the covariates X into the conditional intensity approximation  Note that, in general, adding inputs X will only change the stability of the model to the extent that these covariates change the estimate of h . References ↵ Ahrens  MB , Paninski  L , Sahani  M.  Inferring input nonlinearities in neural encoding models . Netw. Comput. Neural Syst . 19 : 35 – 67 , 2008 . OpenUrl ↵ Amarasingham  A , Geman  S , Harrison  MT . Ambiguity and nonidentifiability in the statistical analysis of neural codes . Proc. Natl. Acad. Sci. U. S. A . 112 : 6455 – 60 , 2015 . OpenUrl Abstract / FREE Full Text ↵ Amirikian  B , Georgopulos  AP . Directional tuning profiles of motor cortical cells . 36 : 73 – 79 , 2000 . ↵ Arai  K , Kass  RE . Inferring oscillatory modulation in neural spike trains . PLOS Comput. Biol . 13 : e1005596 , 2017 . OpenUrl ↵ Arandia-Romero  I , Tanabe  S , Drugowitsch  J , Kohn  A , Moreno-Bote  R.  Multiplicative and Additive Modulation of Neuronal Tuning with Population Activity Affects Encoded Information . Neuron  89 : 1305 – 1316 , 2016 . OpenUrl CrossRef PubMed ↵ Archer  EW , Koster  U , Pillow  JW , Macke  JH . Low-dimensional models of neural population activity in sensory cortical circuits . In: Advances in Neural Information Processing Systems  27 . 2014 , p. 343 – 351 . OpenUrl ↵ Arieli  A , Sterkin  A , Grinvald  A , Aertsen  A.  Dynamics of Ongoing Activity: Explanation of the Large Variability in Evoked Cortical Responses . Science  273 : 1868 – 1871 , 1996 . OpenUrl Abstract / FREE Full Text ↵ Benjamin  AS , Fernandes  HL , Tomlinson  T , Ramkumar  P , VerSteeg  C , Miller  L , Kording  KP . Modern machine learning far outperforms GLMs at predicting spikes . bioRxiv ( February 24, 2017 ).  doi: 10.1101/111450. OpenUrl CrossRef ↵ Berger  TW , Song  D , Chan  RHM , Marmarelis  VZ . The Neurobiological Basis of Cognition: Identification by Multi-Input, Multioutput Nonlinear Dynamic Modeling: A method is proposed for measuring and modeling human long-term memory formation by mathematical analysis and computer simulation of nerve-cell . Proc. IEEE  98 : 356 – 374 , 2010 . OpenUrl ↵ Box  GEP . Use and Abuse of Regression . Technometrics  8 : 625 , 1966 . OpenUrl CrossRef Web of Science ↵ Brillinger  DR . Maximum likelihood analysis of spike trains of interacting nerve cells . Biol. Cybern . 59 : 189 – 200 , 1988 . OpenUrl CrossRef PubMed Web of Science ↵ Brillinger  DR , Segundo  JP . Empirical examination of the threshold model of neuron firing . Biol. Cybern . 35 : 213 – 220 , 1979 . OpenUrl CrossRef PubMed Web of Science ↵ Brody  CD . Disambiguating Different Covariation Types . Neural Comput . 11 : 1527 – 1535 , 1999 . OpenUrl CrossRef PubMed Web of Science ↵ Brown  E , Barbieri  R , Eden  U , Frank  L.  Likelihood methods for neural data analysis . In: Computational Neuroscience: a comprehensive approach, edited by  Feng J. London: Chapman and Hall , 2003 , p. 253 – 286 . ↵ Carandini  M , Demb  JB , Mante  V , Tolhurst  DJ , Dan  Y , Olshausen  BA , Gallant  JL , Rust  NC . Do we know what the early visual system does?  J. Neurosci . 25 : 10577 , 2005 . OpenUrl Abstract / FREE Full Text ↵ Chase  SM , Schwartz  AB , Kass  RE . Latent Inputs Improve Estimates of Neural Encoding in Motor Cortex . J. Neurosci . 30 : 13873 – 13882 , 2010 . OpenUrl Abstract / FREE Full Text ↵ Churchland  MM , Santhanam  G , Shenoy  K V.  Preparatory Activity in Premotor and Motor Cortex Reflects the Speed of the Upcoming Reach . J. Neurophysiol . 96 : 3130 , 2006 . OpenUrl CrossRef PubMed Web of Science ↵ Clarke  KA . The Phantom Menace: Omitted Variable Bias in Econometric Research . Confl. Manag. Peace Sci . 22 : 341 – 352 , 2005 . OpenUrl ↵ Clogg  CC , Petkova  E , Shihadeh  ES . Statistical Methods for Analyzing Collapsibility in Regression Models . J. Educ. Stat . 17 : 51 , 1992 . OpenUrl CrossRef ↵ Czanner  G , Eden  UT , Wirth  S , Yanike  M , Suzuki  WA , Brown  EN . Analysis of between-trial and within-trial neural spiking dynamics . J. Neurophysiol . 99 : 2672 – 2693 , 2008 . OpenUrl CrossRef PubMed Web of Science ↵ David  S V , Mesgarani  N , Fritz  JB , Shamma  SA . Rapid synaptic depression explains nonlinear modulation of spectro-temporal tuning in primary auditory cortex by natural stimuli . J. Neurosci . 29 : 3374 , 2009 . OpenUrl Abstract / FREE Full Text ↵ Destexhe  A , Rudolph  M , Fellous  JM , Sejnowski  TJ . Fluctuating synaptic conductances recreate in vivo-like activity in neocortical neurons . Neuroscience  107 : 13 – 24 , 2001 . OpenUrl CrossRef PubMed Web of Science ↵ Destexhe  A , Rudolph  M , Paré  D.  The high-conductance state of neocortical neurons in vivo . Nat. Rev. Neurosci . 4 : 739 – 751 , 2003 . OpenUrl CrossRef PubMed Web of Science ↵ Diba  K , Buzsaki  G.  Hippocampal Network Dynamics Constrain the Time Lag between Pyramidal Cells across Modified Environments . J. Neurosci . 28 : 13448 – 13456 , 2008 . OpenUrl Abstract / FREE Full Text ↵ Drake  C , McQuarrie  A.  A note on the bias due to omitted confounders . Biometrika  82 : 633 – 638 , 1995 . OpenUrl CrossRef Web of Science ↵ Fernandes  HL , Stevenson  IH , Phillips  AN , Segraves  MA , Kording  KP . Saliency and saccade encoding in the frontal eye field during natural scene search . Cereb. Cortex  24 , 2014 . ↵ Fetz  EE . Are movement parameters recognizably coded in the activity of single neurons?  Behav. Brain Sci . 15 : 679 – 690 , 1992 . OpenUrl CrossRef Web of Science ↵ Fusi  S , Miller  EK , Rigotti  M.  Why neurons mix: high dimensionality for higher cognition . Curr. Opin. Neurobiol . 37 : 66 – 74 , 2016 . OpenUrl CrossRef PubMed ↵ Gail  MH , Wieand  S , Piantadosi  S.  Biased Estimates of Treatment Effect in Randomized Experiments with Nonlinear Regressions and Omitted Covariates . Biometrika  71 : 431 , 1984 . OpenUrl CrossRef Web of Science ↵ Gelman  A , Hill  J.  Data Analysis Using Regression and Multilevel/Hierarchical Models . Cambridge University Press , 2007 . ↵ Georgopoulos  AP , Kalaska  JF , Caminiti  R , Massey  JT . On the relations between the direction of two-dimensional arm movements and cell discharge in primate motor cortex . J. Neurosci . 2 : 1527 – 1537 , 1982 . OpenUrl Abstract / FREE Full Text ↵ Gerhard  F , Deger  M , Truccolo  W.  On the stability and dynamics of stochastic spiking neuron models: Nonlinear Hawkes process and point process GLMs . PLOS Comput. Biol . 13 : e1005390, 2017 . ↵ Gerhard  F , Kispersky  T , Gutierrez  GJ , Marder  E , Kramer  M , Eden  U.  Successful Reconstruction of a Physiological Circuit with Known Connectivity from Spiking Activity Alone . PLoS Comput. Biol . 9 : e1003138 , 2013 . OpenUrl CrossRef PubMed ↵ Ghazanfar  AA , Schroeder  CE . Is neocortex essentially multisensory?  Trends Cogn. Sci . 10 : 278 – 285 , 2006 . OpenUrl CrossRef PubMed Web of Science ↵ Goris  RLT , Movshon  JA , Simoncelli  EP . Partitioning neuronal variability . Nat. Neurosci . 17 : 858 – 65 , 2014 . OpenUrl CrossRef PubMed ↵ Graf  ABA , Kohn  A , Jazayeri  M , Movshon  JA . Decoding the activity of neuronal populations in macaque primary visual cortex . Nat. Neurosci . 14 : 239 – 245 , 2011 . OpenUrl CrossRef PubMed Web of Science ↵ Greenland  S.  Modeling and variable selection in epidemiologic analysis . Am. J. Public Health  79 : 340 – 9 , 1989 . OpenUrl CrossRef PubMed Web of Science ↵ Harris  KD , Csicsvari  J , Hirase  H , Dragoi  G , Buzsáki  G.  Organization of cell assemblies in the hippocampus . Nature  424 : 552 – 556 , 2003 . OpenUrl CrossRef PubMed Web of Science ↵ Hartley  T , Lever  C , Burgess  N , O’Keefe  J.  Space in the brain: how the hippocampal formation supports spatial cognition . Philos. Trans. R. Soc. Lond. B. Biol. Sci . 369 : 20120510 , 2014 . OpenUrl CrossRef PubMed ↵ Herz  AVM , Gollisch  T , Machens  CK , Jaeger  D.  Modeling single-neuron dynamics and computations: a balance of detail and abstraction . Science  314 : 80 – 5 , 2006 . OpenUrl Abstract / FREE Full Text Hocker  D , Park  IM . Multistep inference for generalized linear spiking models curbs runaway excitation . In: 2017 8th International IEEE/EMBS Conference on Neural Engineering (NER). IEEE , p. 613 – 616 . ↵ Humphrey  DR , Schmidt  EM , Thompson  WD . Predicting measures of motor performance from multiple cortical spike trains . Science  170 : 758 – 62 , 1970 . OpenUrl Abstract / FREE Full Text ↵ Jonas  E , Kording  KP . Could a Neuroscientist Understand a Microprocessor? PLOS Comput. Biol.  13 : e1005268 , 2017 . Kalaska  JF . From Intention to Action: Motor Cortex and the Control of Reaching Movements. Springer , Boston, MA , p. 139 – 178 . ↵ Kandler  S , Mao  D , McNaughton  BL , Bonin  V.  Encoding of Tactile Context in the Mouse Visual Cortex . bioRxiv ( October  6 , 2017 ).  doi: 10.1101/199364. OpenUrl CrossRef ↵ Kass  RE , Ventura  V , Brown  EN . Statistical Issues in the Analysis of Neuronal Data . J. Neurophysiol . 94 : 8 – 25 , 2005 . OpenUrl CrossRef PubMed Web of Science ↵ Kelly  RC , Smith  MA , Kass  RE , Lee  TS . Local field potentials indicate network state and account for neuronal response variability . J. Comput. Neurosci . 29 : 567 – 579 , 2010 . OpenUrl CrossRef PubMed Web of Science ↵ Kim  H , Shinomoto  S.  Estimating nonstationary input signals from a single neuronal spike train . Phys. Rev. E  86 : 051903 , 2012 . OpenUrl ↵ Kohn  A , Smith  MA . Utah array extracellular recordings of spontaneous and visually evoked activity from anesthetized macaque primary visual cortex (V1 ). CRCNS.org . 2016 . ↵ Krakauer  JW , Ghazanfar  AA , Gomez-Marin  A , MacIver  MA , Poeppel  D.  Neuroscience Needs Behavior: Correcting a Reductionist Bias . Neuron  93 : 480 – 490 , 2017 . OpenUrl CrossRef PubMed ↵ Kulkarni  JE , Paninski  L.  Common-input models for multiple neural spike-train data . Netw. Comput. Neural Syst . 18 : 375 – 407 , 2007 . OpenUrl ↵ Lin  I-C , Okun  M , Carandini  M , Harris  KD . The Nature of Shared Cortical Variability . Neuron  87 : 644 – 656 , 2015 . OpenUrl CrossRef PubMed ↵ Macke  J , Büsing  L , Cunningham  J , Yu  B , Shenoy  K , Sahani  M.  Empirical models of spiking in neural populations . In: Advances in Neural Information Processing Systems . 2011 , p. 1350 – 1358 . ↵ McCullagh  P , Nelder  JA . Generalized Linear Models . 2nd ed. CRC Press , 1989 . ↵ McFarland  JM , Cui  Y , Butts  DA . Inferring Nonlinear Neuronal Computation Based on Physiologically Plausible Inputs . PLoS Comput. Biol . 9 : e1003143 , 2013 . OpenUrl CrossRef PubMed ↵ McNaughton  BL , Barnes  CA , O’Keefe  J.  The contributions of position, direction, and velocity to single unit activity in the hippocampus of freely-moving rats . Exp. Brain Res . 52 : 41 – 49 , 1983 . OpenUrl CrossRef PubMed Web of Science ↵ Mizuseki  K , Sirota  A , Pastalkova  E , Buzsáki  G.  Theta oscillations provide temporal windows for local circuit computation in the entorhinal-hippocampal loop . Neuron  64 : 267 – 280 , 2009 . OpenUrl CrossRef PubMed Web of Science ↵ Mizuseki  K , Sirota  A , Pastalkova  E , Diba  K , Buzsáki  G.  Multiple single unit recordings from different rat hippocampal and entorhinal regions while the animals were performing multiple behavioral tasks . CRCNS.org.  2013 . ↵ Moran  DW , Schwartz  a B.  Motor cortical representation of speed and direction during reaching . J. Neurophysiol . 82 : 2676 – 2692 , 1999 . OpenUrl CrossRef PubMed Web of Science ↵ Niell  CM , Stryker  MP . Modulation of Visual Responses by Behavioral State in Mouse Visual Cortex . Neuron  65 : 472 – 479 , 2010 . OpenUrl CrossRef PubMed Web of Science ↵ O’Keefe  J , Dostrovsky  J.  The hippocampus as a spatial map: Preliminary evidence from unit activity in the freely-moving rat . Brain Res . 34 : 171 – 175 , 1971 . OpenUrl CrossRef PubMed Web of Science ↵ Oby  ER , Ethier  C , Miller  LE . Movement representation in the primary motor cortex and its contribution to generalizable EMG predictions . J. Neurophysiol . 109 : 666 – 678 , 2013 . OpenUrl CrossRef PubMed Web of Science ↵ Okatan  M , Wilson  MA , Brown  EN . Analyzing Functional Connectivity Using a Network Likelihood Model of Ensemble Neural Spiking Activity . Neural Comput . 17 : 1927 – 1961 , 2005 . OpenUrl CrossRef PubMed Web of Science ↵ Okun  M , Steinmetz  NA , Cossell  L , Iacaruso  MF , Ko  H , Barthó  P , Moore  T , Hofer  SB , Mrsic-Flogel  TD , Carandini  M , Harris  KD . Diverse coupling of neurons to populations in sensory cortex . Nature  521 : 511 – 515 , 2015 . OpenUrl CrossRef PubMed ↵ Omrani  M , Kaufman  MT , Hatsopoulos  NG , Cheney  PD . Perspectives on classical controversies about the motor cortex . J. Neurophysiol . 118 : 1828 – 1848 , 2017 . OpenUrl CrossRef PubMed ↵ Paninski  L.  Maximum likelihood estimation of cascade point-process neural encoding models . Netw. Comput. Neural Syst . 15 : 243 – 262 , 2004 . OpenUrl CrossRef ↵ Paninski  L , Ahmadian  Y , Ferreira  DG , Koyama  S , Rahnama Rad  K , Vidne  M , Vogelstein  J , Wu  W.  A new look at state-space models for neural data . J. Comput. Neurosci . 29 : 107 – 126 , 2010 . OpenUrl CrossRef PubMed Web of Science ↵ Park  IM , Archer  EW , Priebe  N , Pillow  JW . Spectral methods for neural characterization using generalized quadratic models [Online] . : 2454 – 2462 , 2013 . http://papers.nips.cc/paper/4993-spectral-methods-for-neural-characterization-using-generalized-quadratic-models [4 May. 2018]. ↵ Park  IM , Meister  MLR , Huk  AC , Pillow  JW . Encoding and decoding in parietal cortex during sensorimotor decision-making . Nat. Neurosci . 17 : 1395 – 1403 , 2014 . OpenUrl CrossRef PubMed ↵ Pearl  J.  Causal inference in statistics: An overview . Stat. Surv . 3 : 96 – 146 , 2009 . OpenUrl CrossRef ↵  Kenji Doya Alexandre  Pouget , and Rajesh  P.N.  Rao  SI. Pillow  J.  Likelihood-Based Approaches to Modeling the Neural Code. In: Bayesian brain: Probabilistic approaches to neural coding , edited by  Kenji Doya Alexandre  Pouget , and Rajesh  P.N.  Rao  SI.  MIT Press , 2007 , p. 53 – 70 . ↵ Pillow  JW , Shlens  J , Paninski  L , Sher  A , Litke  AM , Chichilnisky  EJ , Simoncelli  EP . Spatio-temporal correlations and visual signalling in a complete neuronal population . Nature  454 : 995 – 999 , 2008 . OpenUrl CrossRef PubMed Web of Science ↵ Pillow  JW , Simoncelli  EP . Biases in white noise analysis due to non-Poisson spike generation . Neurocomputing  52 – 54 : 109–115, 2003 . ↵ Putzky  P , Franzen  F , Bassetto  G , Macke  JH . A Bayesian model for identifying hierarchically organised states in neural population activity [Online]. : 3095–3103 , 2014 . http://papers.nips.cc/paper/5338-a-bayesian-model-for-identifying-hierarchically-organised-states-in-neural-population-activity [4 May. 2018]. ↵ Reimer  J , Froudarakis  E , Cadwell  CR , Yatsenko  D , Denfield  GH , Tolias  AS . Pupil Fluctuations Track Fast Switching of Cortical States during Quiet Wakefulness . Neuron  84 : 355 – 362 , 2014 . OpenUrl CrossRef PubMed ↵ Runyan  CA , Piasini  E , Panzeri  S , Harvey  CD . Distinct timescales of population coding across cortex . Nature  548 : 92 – 96 , 2017 . OpenUrl CrossRef PubMed ↵ Rust  NC , Movshon  JA . In praise of artifice . Nat. Neurosci . 8 : 1647 – 1650 , 2005 . OpenUrl CrossRef PubMed Web of Science ↵ Shmueli  G.  To Explain or to Predict?  Stat. Sci . 25 : 289 – 310 , 2010 . OpenUrl CrossRef Web of Science ↵ Smith  AC , Brown  EN . Estimating a State-Space Model from Point Process Observations . Neural Comput . 15 : 965 – 991 , 2003 . OpenUrl CrossRef PubMed Web of Science ↵ Smith  MA , Kohn  A.  Spatial and temporal scales of neuronal correlation in primary visual cortex . J. Neurosci . 28 : 12591 – 12603 , 2008 . OpenUrl Abstract / FREE Full Text ↵ Stevenson  IH , Cherian  A , London  BM , Sachs  NA , Lindberg  E , Reimer  J , Slutzky  MW , Hatsopoulos  NG , Miller  LE , Kording  KP . Statistical assessment of the stability of neural movement representations . J. Neurophysiol . 106 , 2011 . ↵ Stringer  C , Pachitariu  M , Steinmetz  N , Reddy  CB , Carandini  M , Harris  KD . Spontaneous behaviors drive multidimensional, brain-wide population activity . bioRxiv ( April 22, 2018 ).  doi: 10.1101/306019. OpenUrl CrossRef ↵ Tripathy  SJ , Padmanabhan  K , Gerkin  RC , Urban  NN . Intermediate intrinsic diversity enhances neural population coding . Proc. Natl. Acad. Sci. U. S. A . 110 : 8248 – 53 , 2013 . OpenUrl Abstract / FREE Full Text ↵ Truccolo  W , Eden  UT , Fellows  MR , Donoghue  JP , Brown  EN . A Point Process Framework for Relating Neural Spiking Activity to Spiking History, Neural Ensemble, and Extrinsic Covariate Effects . J. Neurophysiol . 93 : 1074 – 1089 , 2005 . OpenUrl CrossRef PubMed Web of Science ↵ Truccolo  W , Hochberg  LR , Donoghue  JP . Collective dynamics in human and monkey sensorimotor cortex: predicting single neuron spikes . Nat. Neurosci . 13 : 105 – 111 , 2010 . OpenUrl CrossRef PubMed Web of Science ↵ Ventura  V.  Traditional waveform based spike sorting yields biased rate code estimates . Proc. Natl. Acad. Sci . 106 : 6921 , 2009 . OpenUrl Abstract / FREE Full Text ↵ Vidne  M , Ahmadian  Y , Shlens  J , Pillow  JW , Kulkarni  J , Litke  AM , Chichilnisky  EJ , Simoncelli  E , Paninski  L.  Modeling the impact of common noise inputs on the network activity of retinal ganglion cells . J. Comput. Neurosci . 33 : 97 – 121 , 2012 . OpenUrl CrossRef PubMed ↵ Volgushev  M , Ilin  V , Stevenson  IH . Identifying and Tracking Simulated Synaptic Inputs from Neuronal Firing: Insights from In Vitro Experiments . PLOS Comput. Biol . 11 : e1004167 , 2015 . OpenUrl ↵ Walker  B , Kording  K.  The Database for Reaching Experiments and Models . PLoS One  8 : e78747 , 2013 . OpenUrl ↵ Walsh  RN , Cummins  RA . The open-field test: A critical review . Psychol. Bull . 83 : 482 – 504 , 1976 . OpenUrl CrossRef PubMed Web of Science Wasserman  L.  All of Statistics . Springer New York . ↵ Weber  AI , Pillow  JW . Capturing the Dynamical Repertoire of Single Neurons with Generalized Linear Models . Neural Comput . 29 : 3260 – 3289 , 2017 . OpenUrl ↵ Whiteway  MR , Butts  DA . Revealing unobserved factors underlying cortical activity with a rectified latent variable model applied to neural population recordings . J. Neurophysiol . 117 : 919 – 936 , 2017 . OpenUrl CrossRef PubMed ↵ Yoshihara  M , Yoshihara  M.  ‘Necessary and sufficient’ in biology is not necessarily necessary–confusions and erroneous conclusions resulting from misapplied logic in the field of biology, especially neuroscience . J. Neurogenet . 32 : 53 – 64 , 2018 . OpenUrl ↵ Zhao  M , Iyengar  S.  Nonconvergence in logistic and poisson models for neural spiking . Neural Comput . 22 : 1231 – 1244 , 2010 . OpenUrl CrossRef PubMed Web of Science View Abstract            View the discussion thread.      Back to top            Previous Next      Posted August 21, 2018.            Download PDF           Email      Thank you for your interest in spreading the word about bioRxiv. NOTE: Your email address is requested solely to identify you as the sender of this article.    Your Email *     Your Name *     Send To *   Enter multiple addresses on separate lines or separate them with commas.    You are going to email the following  Omitted variable bias in GLMs of neural spiking activity    Message Subject (Your Name) has forwarded a page to you from bioRxiv   Message Body (Your Name) thought you would like to see this page from the bioRxiv website.   Your Personal Message         CAPTCHA This question is for testing whether or not you are a human visitor and to prevent automated spam submissions.                 Share            Omitted variable bias in GLMs of neural spiking activity   Ian H.  Stevenson  bioRxiv 317511; doi: https://doi.org/10.1101/317511             Share This Article:       Copy                            Citation Tools         Omitted variable bias in GLMs of neural spiking activity   Ian H.  Stevenson  bioRxiv 317511; doi: https://doi.org/10.1101/317511      Citation Manager Formats   BibTeX Bookends EasyBib EndNote (tagged) EndNote 8 (xml) Medlars Mendeley Papers RefWorks Tagged Ref Manager RIS Zotero                       Tweet Widget Facebook Like Google Plus One     Subject Area   Neuroscience               Subject Areas          All Articles        Animal Behavior and Cognition  (1992)  Biochemistry  (3745)  Bioengineering  (2526)  Bioinformatics  (12280)  Biophysics  (5250)  Cancer Biology  (4087)  Cell Biology  (5858)  Clinical Trials  (138)  Developmental Biology  (3503)  Ecology  (5543)  Epidemiology  (2052)  Evolutionary Biology  (8377)  Genetics  (6345)  Genomics  (8079)  Immunology  (3269)  Microbiology  (9800)  Molecular Biology  (3870)  Neuroscience  (22816)  Paleontology  (166)  Pathology  (631)  Pharmacology and Toxicology  (1014)  Physiology  (1543)  Plant Biology  (3471)  Scientific Communication and Education  (811)  Synthetic Biology  (1085)  Systems Biology  (3309)  Zoology  (569)                                   "
9,proxy bias(fallacy OR bias OR heuristic) (coronavirus OR covid OR pandemic),1,https://www.thelancet.com/journals/laninf/article/PIIS1473-3099(20)30243-7/fulltext,"These early estimates give an indication of the fatality ratio across the spectrum of COVID-19 disease and show a strong age gradient in risk of death.

Using data on 24 deaths that occurred in mainland China and 165 recoveries outside of China, we estimated the mean duration from onset of symptoms to death to be 17·8 days (95% credible interval [CrI] 16·9–19·2) and to hospital discharge to be 24·7 days (22·9–28·1). In all laboratory confirmed and clinically diagnosed cases from mainland China (n=70 117), we estimated a crude case fatality ratio (adjusted for censoring) of 3·67% (95% CrI 3·56–3·80). However, after further adjusting for demography and under-ascertainment, we obtained a best estimate of the case fatality ratio in China of 1·38% (1·23–1·53), with substantially higher ratios in older age groups (0·32% [0·27–0·38] in those aged <60 years vs 6·4% [5·7–7·2] in those aged ≥60 years), up to 13·4% (11·2–15·9) in those aged 80 years or older. Estimates of case fatality ratio from international cases stratified by age were consistent with those from China (parametric estimate 1·4% [0·4–3·5] in those aged <60 years [n=360] and 4·5% [1·8–11·1] in those aged ≥60 years [n=151]). Our estimated overall infection fatality ratio for China was 0·66% (0·39–1·33), with an increasing profile with age. Similarly, estimates of the proportion of infected individuals likely to be hospitalised increased with age up to a maximum of 18·4% (11·0–37·6) in those aged 80 years or older.

We collected individual-case data for patients who died from COVID-19 in Hubei, mainland China (reported by national and provincial health commissions to Feb 8, 2020), and for cases outside of mainland China (from government or ministry of health websites and media reports for 37 countries, as well as Hong Kong and Macau, until Feb 25, 2020). These individual-case data were used to estimate the time between onset of symptoms and outcome (death or discharge from hospital). We next obtained age-stratified estimates of the case fatality ratio by relating the aggregate distribution of cases to the observed cumulative deaths in China, assuming a constant attack rate by age and adjusting for demography and age-based and location-based under-ascertainment. We also estimated the case fatality ratio from individual line-list data on 1334 cases identified outside of mainland China. Using data on the prevalence of PCR-confirmed cases in international residents repatriated from China, we obtained age-stratified estimates of the infection fatality ratio. Furthermore, data on age-stratified severity in a subset of 3665 cases from China were used to estimate the proportion of infected individuals who are likely to require hospitalisation.

In the face of rapidly changing data, a range of case fatality ratio estimates for coronavirus disease 2019 (COVID-19) have been produced that differ substantially in magnitude. We aimed to provide robust estimates, accounting for censoring and ascertainment biases.

Here we attempt to adjust for these biases in data sources to obtain estimates of the case fatality ratio (proportion of all cases that will eventually lead to death) and infection fatality ratio (the proportion of all infections that will eventually lead to death) using both individual-level case report data and aggregate case and death counts from mainland China, from Hong Kong and Macau, and international case reports. By adjusting for both underlying demography and potential under-ascertainment at different levels of the severity pyramid ( figure 1 ), these estimates should be broadly applicable across a range of settings to inform health planning while more detailed case data accrue.

Second, surveillance of a newly emerged pathogen is typically biased towards detecting clinically severe cases, especially at the start of an epidemic when diagnostic capacity is low ( figure 1 ). Estimates of the case fatality ratio can thus be biased upwards until the extent of clinically milder disease is determined.Data from the epicentre of the outbreak in Wuhan have primarily been obtained through hospital surveillance and, thus, are likely to represent patients with moderate or severe illness, with atypical pneumonia or acute respiratory distress being used to define suspected cases eligible for testing.In these individuals, clinical outcomes are likely to be more severe, so any estimates of the case fatality ratio will be higher. Elsewhere in mainland China and the rest of the world, countries and administrative regions alert to the risk of infection being imported via travel initially instituted surveillance for COVID-19 with a broader set of clinical criteria for defining a suspected case. These criteria typically included a combination of symptoms (eg, cough and fever) combined with recent travel history to the affected region (Wuhan, or Hubei province). Such surveillance is likely to detect clinically mild cases but, by initially restricting testing to those with a travel history or link, might have missed other symptomatic cases.

At the top of the pyramid, those meeting the WHO case criteria for severe or critical cases are likely to be identified in the hospital setting, presenting with atypical viral pneumonia. These cases will have been identified in mainland China and among those categorised internationally as local transmission. Many more cases are likely to be symptomatic (ie, with fever, cough, or myalgia), but might not require hospitalisation. These cases will have been identified through links to international travel to high-risk areas and through contact-tracing of contacts of confirmed cases. They might also be identified through population surveillance of, for example, influenza-like illness. The bottom part of the pyramid represents mild (and possibly asymptomatic) cases. These cases might be identified through contact tracing and subsequently via serological testing.

Assessing the severity of COVID-19 is crucial to determine the appropriateness of mitigation strategies and to enable planning for health-care needs as epidemics unfold. However, crude case fatality ratios obtained by dividing the number of deaths by the number of cases can be misleading.First, there can be a period of 2–3 weeks between a person developing symptoms, the case subsequently being detected and reported, and observation of the final clinical outcome. During a growing epidemic, the final clinical outcome of most of the reported cases is typically unknown. Simply dividing the cumulative reported number of deaths by the cumulative number of reported cases will therefore underestimate the true case fatality ratio early in an epidemic.This effect was observed in past epidemics of respiratory pathogens, including severe acute respiratory syndrome (SARS)and H1N1influenza, and as such is widely recognised. Thus, many of the estimates of the case fatality ratio that have been obtained to date for COVID-19 correct for this effect.Additionally, however, during the exponential growth phase of an epidemic, the observed time lags between the onset of symptoms and outcome (recovery or death) are censored, and naive estimates of the observed times from symptom onset to outcome provide biased estimates of the actual distributions. Ignoring this effect tends to bias the estimated case fatality ratio downwards during the early growth phase of an epidemic.

Our estimates of the case fatality ratio for COVID-19, although lower than some of the crude estimates made to date, are substantially higher than for recent influenza pandemics (eg, H1N1 influenza in 2009). With the rapid geographical spread observed to date, COVID-19 therefore represents a major global health threat in the coming weeks and months. Our estimate of the proportion of infected individuals requiring hospitalisation, when combined with likely infection attack rates (around 50–80%), show that even the most advanced health-care systems are likely to be overwhelmed. These estimates are therefore crucial to enable countries around the world to best prepare as the global pandemic continues to unfold.

By synthesising data from across a range of surveillance settings, we obtained estimates of the age-stratified case fatality ratio and infection fatality ratio that take into account the different denominator populations in the datasets. Our underlying assumption, that attack rates (ie, the probability of becoming infected) do not vary substantially by age, is consistent with previous studies for respiratory infections. Under this assumption, differences in age patterns among cases in Wuhan versus those elsewhere in China would probably due to under-ascertainment of cases, given the different surveillance systems in place. Our results are consistent with this hypothesis, with cases in Wuhan seen in older individuals, who would have been identified through attendance at hospital, whereas cases elsewhere in China being younger overall, which would be explained by the policy of testing those with a travel history to Wuhan. After correcting for these biases, we found that estimates of the case fatality ratio from China are consistent with those obtained from early international cases. Our age-stratified estimates of the infection fatality ratio can be applied to any demography to give an estimate of the infection fatality ratio in older and younger populations. These estimates can be combined with estimates of the infection attack rate (approximately 80% for an unmitigated epidemic) to give rough projections of scale. Similarly, our estimates of the proportion of infections requiring hospitalisation can be combined with the infection attack rate to forecast health-care requirements.

We searched PubMed, medRxiv, bioRxiv, arXiv, SSRN, Research Square, Virological, and Wellcome Open Research for peer-reviewed articles, preprints, and research reports on the severity of coronavirus disease 2019 (COVID-19), using the search terms “coronavirus”, “2019-nCoV”, and similar terms, and “fatality”, up to March 6, 2020. Several studies have estimated the case fatality ratio (the percentage of individuals with symptomatic or confirmed disease who die from the disease) and infection fatality ratio (the percentage of all infected individuals who die from the disease, including those with mild disease) of COVID-19 using a range of different statistical and modelling methods. Studies done solely in hospitalised patients report the highest fatality ratios (8–28%), representing the outcome for the most severely ill patients. Estimates of the population-level case fatality ratio from all case reports are in the range of 2–8%. Estimates of the infection fatality ratio averaged across all age-groups range from 0·2% to 1·6%, while estimates of the infection fatality ratio in the oldest age group (≥80 years) range from 8% to 36%. None of the identified studies had adjusted for differences in the denominator populations to obtain estimates that could be applied across populations. No other studies have estimated the proportion of infected individuals who will require hospitalisation.

Clinical studies of hospitalised patients have shown that, at onset of COVID-19, patients frequently show symptoms associated with viral pneumonia, most commonly fever, cough, sore throat, myalgia, and fatigue.The case definition adopted in China and elsewhere includes further stratification of cases as severe (defined as tachypnoea [≥30 breaths per min], oxygen saturation ≤93% at rest, or PaO/FiOratio <300 mm Hg) and critical (respiratory failure requiring mechanical ventilation, septic shock, or other organ dysfunction or failure that requires intensive care).According to the report from the WHO–China Joint Mission on COVID-19, 80% of the 55 924 patients with laboratory-confirmed COVID-19 in China to Feb 20, 2020, had mild-to-moderate disease, including both non-pneumonia and pneumonia cases, while 13·8% developed severe disease and 6·1% developed to a critical stage requiring intensive care.In a study of clinical progression in 1099 patients,those at highest risk for severe disease and death included people over the age of 60 years and those with underlying conditions, including hypertension, diabetes, cardiovascular disease, chronic respiratory disease, and cancer.

As of March 25, 2020, 414 179 cases and 18 440 deaths due to coronavirus disease 2019 (COVID-19), caused by the novel severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), had been reported worldwide.The epidemic began in mainland China, with a geographical focus in the city of Wuhan, Hubei. However, on Feb 26, 2020, the rate of increase in cases became greater in the rest of the world than inside China. Substantial outbreaks are occurring in Italy (69 176 cases), the USA (51 914 cases), and Iran (24 811 cases), and geographical expansion of the epidemic continues.

The funder of the study had no role in study design, data collection, data analysis, data interpretation, or writing of the report. The corresponding author had full access to all the data in the study and had final responsibility for the decision to submit for publication.

We used parametric and non-parametric methodsto estimate the case fatality ratio in cases reported outside of mainland China using individual-level data. Cases in which the outcome was unknown were treated as censored observations. For parametric and non-parametric analyses, missing onset dates were multiply imputed using information on the onset-to-report distribution, and unreported recoveries were imputed using onset-to-outcome distributions and country summary data. The parametric models were fitted to the data using Bayesian methods ( appendix p 12 ).

To independently validate our infection fatality ratio estimate, we analysed data from the outbreak on the Diamond Princess cruise liner taking the dates of reported positive tests as a proxy for onset date. We calculated the expected proportion of deaths observed until March 25, 2020, given the onset times and estimated onset-to-death distribution ( appendix p 13 ).

Assuming a uniform attack rate by age groups, we used the demography-adjusted under-ascertainment rates calculated above to obtain an estimate of the proportion of infected individuals who would require hospitalisation.

To estimate the infection fatality ratio we fitted to data on infection prevalence from international Wuhan residents who were repatriated to their home countries. Our age-stratified case fatality ratio and infection fatality ratio model was jointly fitted to the case data and infection prevalence data with use of Bayesian methods, using our previous estimate of the onset-to-death distribution as a prior. Full mathematical details are provided in the appendix (p 8)

For a given onset-to-death distribution, we obtained a modelled estimate of the cumulative number of deaths by age under an age-dependent case fatality ratio (fitted relative to the case fatality ratio in the oldest age group, which represented the highest crude case fatality ratio). This estimate was compared with the observed deaths by age using a Poisson likelihood. These data were then jointly fitted alongside the most recent age-aggregated cumulative deaths and cases in mainland China. Given that the numbers of observed cases and deaths have dropped substantially following a peak in late January, the ratio of current cumulative cases to current number of deaths, once corrected for under-ascertainment, should provide a good estimate of the final case fatality ratio.

Estimates of the distribution of times from onset-to-death were used to project the expected cumulative number of deaths given the onsets observed in Wuhan and outside Wuhan, assuming a uniform attack rate across age groups. Using the age-distribution of the population, we obtained an estimate of the expected number of infections in each age group. Under-ascertainment was estimated in and outside of Wuhan by comparing the number of observed cases by age to this expected distribution, assuming perfect ascertainment in the 50–59 age group as this group had the highest number of detected cases relative to population size. We also did a sensitivity analysis assuming a differential attack rate by age ( appendix p 9 ). For Wuhan, we added scaling to account for further under-ascertainment compared with outside of Wuhan. These steps gave us the expected age-distribution of cases.

In estimating time intervals between symptom onset and outcome, it was necessary to account for the fact that, during a growing epidemic, a higher proportion of the cases will have been infected recently ( appendix p 7 ). Therefore, we re-parameterised a gamma model to account for exponential growth using a growth rate of 0·14 per day, obtained from the early case onset data ( appendix p 6 ). Using Bayesian methods, we fitted gamma distributions to the data on time from onset to death and onset to recovery, conditional on having observed the final outcome. Missing onset dates were imputed on the basis of dates of report, where available.

Age-stratified population data for 2018 were obtained from the National Bureau of Statistics of China.According to these data, the population of Wuhan in 2018 was approximately 11 million people.

In early February 2020 a cruise liner named the Diamond Princess was quarantined after a disembarked passenger tested positive for the virus. Subsequently all 3711 passengers on board were tested over the next month. We extracted data on the ages of passengers onboard on Feb 5, 2020, the dates of positive test reports, which were available for 657 out of 712 PCR-confirmed cases, and the dates of ten deaths among these cases from the reports of the Japan Ministry of Health, Labour and Welfareand international media.

Data on infection prevalence in repatriated expatriates returning to their home countries were obtained from government or ministry of health websites and media reports. To match to the incidence reported in Wuhan on Jan 30, 2020, we used data from six flights that departed between Jan 30 and Feb 1, 2020, inclusive.

An earlier (now withdrawn) preprint of a subset of these cases up to Jan 26, 2020 reported the age distribution of cases categorised by severity for 3665 cases.Under the China case definition, a severe case is defined as tachypnoea (≥30 breaths per min) or oxygen saturation 93% or higher at rest, or PaO/FiOratio less than 300 mm Hg.Assuming severe cases to require hospitalisation (as opposed to all of the patients who were hospitalised in China, some of whom will have been hospitalised to reduce onward transmission), we used the proportion of severe cases by age in these patients to estimate the proportion of cases and infections requiring hospitalisation.

Data on 70 117 PCR-confirmed and clinically diagnosed cases by date of onset in Wuhan and elsewhere in China from Jan 1 to Feb 11, 2020, were extracted from the WHO–China Joint Mission report.Over this period a total of 1023 deaths were reported across China, with these data available disaggregated into 10-year age bands between 0–9 years and 70–79 years old, and a further age band for those aged 80 years or older.Using collated data on daily reported deaths obtained each day from the National Health Commission regional websites, we estimated that 74% of deaths occurred in Wuhan and the remainder outside Wuhan. Additionally, the most recent available cumulative estimates (March 3, 2020) of 80 304 confirmed cases and 2946 deaths within China were extracted from the WHO COVID-19 Situation Report (number 43).

We collated data on 2010 cases reported in 37 countries and two special administrative regions of China (Hong Kong and Macau), from government or ministry of health websites and media reports, until Feb 25, 2020. We recorded the following information where available: country or administrative region in which the case was detected, whether the infection was acquired in China or abroad, date of travel, date of symptom onset, date of hospitalisation, date of confirmation, date of recovery, and date of death. We used data from 165 recovered individuals with reported recovery dates and reported or imputed onset dates to estimate the onset-to-recovery distribution, after excluding 26 recoveries without appropriate information on dates of recovery, report, or locality. We used data on 1334 international cases to obtain estimates of the case fatality ratio, not including cases without dates of report.

We identified information on the characteristics of 48 patients who died from COVID-19 in Hubei, reported by the National Health Commission and the Hubei Province Health Commission website up to Feb 8, 2020. We recorded the following data elements, where available: sex, age, date of symptom onset, date of hospitalisation, and date of death. Of the 48 cases, neither the date of symptom onset nor the date of report was available for 13 cases. We also removed eight cases with onset before Jan 1, 2020, or death before Jan 21, 2020, and three deaths after Jan 28, 2020, which were the dates consistent with reliable reporting of onset and death in this setting, respectively, considering the onset-to-death times (including early onsets creates a bias towards long onset-to-death times, reflecting under-ascertainment of deaths early on). This left 24 deaths, which we used to estimate the onset-to-death distribution.

In international Wuhan residents repatriated on six flights, we estimated a prevalence of infection of 0·87% (95% CI 0·32–1·9; six of 689). Adjusting for demography and under-ascertainment, we estimate an infection fatality ratio of 0·66% (95% CrI 0·39–1·33). As for the case fatality ratio, this is strongly age-dependent, with estimates rising steeply from age 50 years upwards ( table 1 ). The demography-adjusted and under-ascertainment-adjusted proportion of infected individuals requiring hospitalisation ranges from 1·1% in the 20–29 years age group up to 18·4% in those 80 years and older ( table 3 ). Using these age-stratified infection fatality ratio estimates, we estimate the infection fatality ratio in the Diamond Princess population to be 2·9%. Given the delay from onset of symptoms to death, we would expect 97% of these deaths to have occurred by March 25, 2020, giving an estimate of the current infection fatality ratio of 2·8%, compared with the empirical estimate of 1·4% (95% CI 0·7–2·6; ten of 712).

Proportions of infected individuals hospitalised are presented as posterior mode (95% credible interval) and are adjusted for under-ascertainment and corrected for demography. Estimates are shown to three signficant figures. We assumed, based on severity classification from a UK context, that cases defined as severe would be hospitalised.

Estimates of the proportion of all infections that would lead to hospitalisation, obtained from a subset of cases reported in mainland China

In cases reported outside of mainland China, we estimated an overall modal case fatality ratio of 2·7% (95% CrI 1·4–4·7) using the parametric model ( table 2 ). In those who reported travel to mainland China (and would therefore have been detected in the surveillance system), we estimated an overall modal case fatality ratio of 1·1% (0·4–4·1), and in those without any reported travel to China (therefore detected either through contact tracing or through hospital surveillance), we estimated a case fatality ratio of 3·6% (1·9–7·2) using the parametric model. The estimated case fatality ratio was lower in those aged under 60 years of age (1·4% [0·4–3·5]) compared with those aged 60 years and over (4·5% [1·8–11·1]). Similar estimates were obtained using non-parametric methods ( table 2 ).

Parametric estimates are presented as posterior mode (95% credible interval), and were obtained using the gamma-distributed estimates of onset-to-death and onset-to-recovery. Non-parametric estimates are presented as maximum likelihood estimate (95% confidence interval) and were obtained using a modified Kaplan-Meier method.Note that due to missing data on age and travel status, numbers in the stratified analysis are lower than for the overall analysis. In addition, the parametric method requires a correction for the epidemic growth rate, and these estimates were therefore obtained from the subset of data for which the travel or local transmission and age was known.

Case fatality ratios were estimated from aggregate data on cases and deaths in mainland China ( table 1 ). A large proportion of the cases, including all of those early in the epidemic, were reported in Wuhan, where the local health system was quickly overwhelmed. As a result, the age distribution of cases reported in Wuhan differed to that in the rest of China ( figure 3A ). Reported cases in Wuhan were more frequent in older age groups, perhaps reflecting higher severity (and therefore prioritisation for hospitalisation in Wuhan), while cases outside of Wuhan might also show a bias in terms of the relationship between age and travel. Adjusting for differences in underlying demography and assuming no overall difference in the attack rate by age, we estimated high under-ascertainment of cases in younger age groups both inside and outside of Wuhan ( figure 3C, D ). Furthermore, we estimated a higher level of under-ascertainment overall in Wuhan compared with outside of Wuhan ( figure 3C ). Accounting for this under-ascertainment, we estimated the highest case fatality ratio (13·4% [11·2–15·9%]) in the 80 years and older age group ( figure 3B table 1 ), with lower case fatality ratios associated with lower age groups, and the lowest in the 0–9 years age group (0·00260% [0·000312–0·0382]).

(A) Age-distribution of cases in Wuhan and elsewhere in China. (B) Estimates of the case fatality ratio by age group, adjusted for demography and under-ascertainment. Boxes represent median (central horizontal line) and IQR, vertical lines represent 1·5 × IQR, and individual points represent any estimates outside of this range. (C) Estimated proportions of cases ascertained in the rest of China and in Wuhan relative to the 50–59 years age group elsewhere in China. Error bars represent 95% CrIs.

Crude case fatality ratios are presented as mean (95% confidence interval). All other fatality ratios are presented as posterior mode (95% credible interval). Estimates are shown to three significant figures. Cases and deaths are aggregate numbers reported from Jan 1 to Feb 11, 2020.Crude case fatality ratios are calculated as the number of deaths divided by the number of laboratory-confirmed cases. Our estimates also include clinically diagnosed cases (a scaling of 1·31 applied across all age-groups, as the breakdown by age was not reported for clinically diagnosed cases), which gives larger denominators and thus lower case fatality ratios than if only laboratory-confirmed cases were included.

Using data on the outcomes of 169 cases reported outside of mainland China, we estimated a mean onset-to-recovery time of 24·7 days (95% CrI 22·9–28·1) and coefficient of variation of 0·35 (0·31–0·39; figure 2 ). Both these onset-to-outcome estimates are consistent with a separate study in China.

In the subset of 24 deaths from COVID-19 that occurred in mainland China early in the epidemic, with correction for bias introduced by the growth of the epidemic, we estimated the mean time from onset to death to be 18·8 days (95% credible interval [CrI] 15·7–49·7; figure 2 ) with a coefficient of variation of 0·45 (95% CrI 0·29–0·54). With the small number of observations in these data and given that they were from early in the epidemic, we could not rule out many deaths occurring with longer times from onset to death, hence the high upper limit of the credible interval. However, given that the epidemic in China has since declined, our posterior estimate of the mean time from onset to death, informed by the analysis of aggregated data from China, is more precise (mean 17·8 days [16·9–19·2]; figure 2 ).

(A) Onset-to-death data from 24 cases in mainland China early in the epidemic. (B) Onset-to-recovery data from 169 cases outside of mainland China. Red lines show the best fit (posterior mode) gamma distributions, uncorrected for epidemic growth, which are biased towards shorter durations. Blue lines show the same distributions corrected for epidemic growth. The black line (panel A) shows the posterior estimate of the onset-to-death distribution following fitting to the aggregate case data.

Discussion

24 Lau EHY

Hsiung CA

Cowling BJ

et al. A comparative epidemiologic analysis of SARS in Hong Kong, Beijing and Taiwan. 25 Lessler J

Salje H

Van Kerkhove MD

et al. Estimating the severity and subclinical burden of middle east respiratory syndrome coronavirus infection in the Kingdom of Saudi Arabia. 26 Riley S

Kwok KO

Wu KM

et al. Epidemiological characteristics of 2009 (H1N1) pandemic influenza based on paired sera from a longitudinal community cohort study. , 27 Kwok KO

Riley S

Perera RAPM

et al. Relative incidence and individual-level severity of seasonal influenza A H3N2 compared with 2009 pandemic H1N1. From an extensive analysis of data from different regions of the world, our best estimate at the current time for the case fatality ratio of COVID-19 in China is 1·38% (95% CrI 1·23–1·53). Although this value remains lower than estimates for other coronaviruses, including SARSand Middle East respiratory syndrome (MERS),it is substantially higher than estimates from the 2009 H1N1 influenza pandemic.Our estimate of an infection fatality ratio of 0·66% in China was informed by PCR testing of international Wuhan residents returning on repatriation flights. This value was consistent with the infection fatality ratio observed in passengers on the Diamond Princess cruise ship up to March 5, 2020, although it is slightly above the upper 95% confidence limit of the age-adjusted infection fatality ratio observed by March 25 (of 712 confirmed cases, 601 have been discharged, ten have died, and 11 remain in a critical condition). This difference might be due to repatriation flight data slightly underestimating milder infections, or due to cruise passengers having better outcomes because of a potentially higher-than-average quality of health care.

Our estimates of the probability of requiring hospitalisation assume that only severe cases require hospitalisation. This assumption is clearly different from the pattern of hospitalisation that occurred in China, where hospitalisation was also used to ensure case isolation. Mortality can also be expected to vary with the underlying health of specific populations, given that the risks associated with COVID-19 will be heavily influenced by the presence of underlying comorbidities.

13 Jung S

Akhmetzhanov AR

Hayashi K

et al. Real-time estimation of the risk of death from novel coronavirus (COVID-19) infection: inference using exported cases. 14 Mizumoto K

Kagaya K

Chowell G

Yoshida-Nakaadachi-cho U Early epidemiological assessment of the transmission potential and virulence of 2019 novel coronavirus in Wuhan City: China, January–February, 2020. , 15 Famulare M 2019-nCoV: preliminary estimates of the confirmed-case-fatality-ratio and infection-fatality-ratio, and initial pandemic risk assessment. Our estimate of the case fatality ratio is substantially lower than the crude case fatality ratio obtained from China based on the cases and deaths observed to date, which is currently 3·67%, as well as many of the estimates currently in the literature. The principle reason for this difference is that the crude estimate does not take into account the severity of cases. For example, various estimates have been made from patient populations ranging from those with generally milder symptoms (for example international travellers detected through screening of travel history)through to those identified in the hospital setting.

28 Bi Q

Wu Y

Mei S

et al. Epidemiology and transmission of COVID-19 in Shenzhen China: analysis of 391 cases and 1,286 of their close contacts. It is clear from the data that have emerged from China that case fatality ratio increases substantially with age. Our results suggest a very low fatality ratio in those under the age of 20 years. As there are very few cases in this age group, it remains unclear whether this reflects a low risk of death or a difference in susceptibility, although early results indicate young people are not at lower risk of infection than adults.Serological testing in this age group will be crucial in the coming weeks to understand the significance of this age group in driving population transmission. The estimated increase in severity with age is clearly reflected in case reports, in which the mean age tends to be in the range of 50–60 years. Different surveillance systems will pick up a different age case mix, and we find that those with milder symptoms detected through a history of travel are younger on average than those detected through hospital surveillance. Our correction for this surveillance bias therefore allows us to obtain estimates that can be applied to different case mixes and demographic population structures. However, it should be noted that this correction is applicable under the assumption of a uniform infection attack rate (ie, exposure) across the population. We also assumed perfect case ascertainment outside of Wuhan in the age group with the most cases relative to their population size (50–59-year-olds); however, if many cases were missed, the case fatality ratio and infection fatality ratio estimates might be lower. In the absence of random population surveys of infection prevalence, our adjustment from case fatality ratio to infection fatality ratio relied on repatriation flight data, which was not age specific. The reported proportion of infected individuals who were asymptomatic on the Diamond Princess did not vary considerably by age, supporting this approach, but future larger representative population prevalence surveys and seroprevalence surveys will inform such estimates further.

Much of the data informing global estimates of the case fatality ratio at present are from the early outbreak in Wuhan. Given that the health system in this city was quickly overwhelmed, our estimates suggest that there is substantial under-ascertainment of cases in the younger age groups (who we estimate to have milder disease) by comparison with elsewhere in mainland China. This under-ascertainment is the main factor driving the difference between our estimate of the crude case fatality ratio from China (3·67%) and our best estimate of the overall case fatality ratio (1·38%). The case fatality ratio is likely to be strongly influenced by the availability of health-care facilities. However surprisingly, although health-care availability in Wuhan was stretched, our estimates from international cases are of a similar magnitude, suggesting relatively little difference in health outcome. Finally, as clinical knowledge of this new disease accrues, it is possible that outcomes will improve. It will therefore be important to revise these estimates as epidemics unfold.

The world is currently experiencing the early stages of a global pandemic. Although China has succeeded in containing the disease spread for 2 months, such containment is unlikely to be achievable in most countries. Thus, much of the world will experience very large community epidemics of COVID-19 over the coming weeks and months. Our estimates of the underlying infection fatality ratio of this virus will inform assessments of health effects likely to be experienced in different countries, and thus decisions around appropriate mitigation policies to be adopted.","        Skip to main content                   Home  About  Submit  ALERTS / RSS             Search for this keyword           Advanced Search                               New Results  Omitted variable bias in GLMs of neural spiking activity   View ORCID Profile Ian H.  Stevenson  doi: https://doi.org/10.1101/317511   Ian H. Stevenson 1 University of Connecticut, Department of Psychological Sciences 2 University of Connecticut, Department of Biomedical Engineering 3 CT Institute for Brain and Cognitive Sciences Find this author on Google Scholar Find this author on PubMed Search for this author on this site ORCID record for Ian H. Stevenson      Abstract Full Text Info/History Metrics Preview PDF          Abstract Generalized linear models (GLMs) have a wide range of applications in systems neuroscience describing the encoding of stimulus and behavioral variables as well as the dynamics of single neurons. However, in any given experiment, many variables that impact neural activity are not observed or not modeled. Here we demonstrate, in both theory and practice, how these omitted variables can result in biased parameter estimates for the effects that are included. In three case studies, we estimate tuning functions for common experiments in motor cortex, hippocampus, and visual cortex. We find that including traditionally omitted variables changes estimates of the original parameters and that modulation originally attributed to one variable is reduced after new variables are included. In GLMs describing single-neuron dynamics, we then demonstrate how post-spike history effects can also be biased by omitted variables. Here we find that omitted variable bias can lead to mistaken conclusions about the stability of single neuron firing. Omitted variable bias can appear in any model with confounders – where omitted variables modulate neural activity and the effects of the omitted variables covary with the included effects. Understanding how and to what extent omitted variable bias affects parameter estimates is likely to be important for interpreting the parameters and predictions of many neural encoding models. Introduction Regression models have been widely used in systems neuroscience to explain how external stimulus and task variables as well as internal state variables may relate to observed neural activity ( Brown et al., 2003 ; Kass et al., 2005 ). However, in many cases, the full set of variables that explain the activity of the observed neurons is not observed or is not even known. It is important to recognize that, in these cases, omitted variables can cause the parameter estimates for the effects that are included in a regression model to be biased. That is, parameter estimates for the modeled effects would be different if other, omitted variables were to be included in the model ( Box, 1966 ). In experiments from behaving animals ( Niell and Stryker, 2010 ; Reimer et al., 2014 ), but also in more controlled sensory tasks ( Kelly et al., 2010 ; Arandia-Romero et al., 2016 ), there is growing evidence that neural activity is affected by many more variables than are typically considered relevant ( Kandler et al., 2017 ; Stringer et al., 2018 ). At the same time, although it has long been a concern in statistics ( Pearl, 2009 ) and has received some attention in other fields ( Greenland, 1989 ; Clarke, 2005 ), omitted variable bias, as a general problem, appears underappreciated in systems neuroscience. Here we demonstrate why systematically considering omitted variable bias may be important in neural data analysis and examine how omitted variable bias can affect one popular framework for describing neural spiking activity – the generalized linear model (GLM) with Poisson observations. In general, regression methods aim to estimate variations in a response variable as a function of other variables or covariates. When the goal of modeling is to maximize prediction accuracy, such as with brain machine interfaces, interpreting the model parameters may not be a high priority. However, in many other cases, parameter estimates are, at least to some extent, interpreted and analyzed. For instance, tuning curves or receptive fields may be measured and compared under different stimulus or task conditions or before and after a manipulation. In fully controlled experiments where the covariates are assigned at random, estimated coefficients can often be interpreted as estimates of causal effects ( Gelman and Hill, 2007 ). However, for many cases in neuroscience, it may be difficult or impossible to completely control or randomize all the relevant variables. In modeling neural activity, omitted variable bias can appear in any situation where neurons are modulated by omitted variables and the omitted variables (often called confounders) are not independent from the variables included in the model – the ones whose effects we are trying to estimate. Minimizing the influence of confounding variables is a major part of most experimental design ( Rust and Movshon, 2005 ), and the statistical effects of confounding variables are well understood (Wasserman, 2004). However, when the goal of modeling is description or explanation ( Shmueli, 2010 ), the effects of these omitted variables are frequently neglected. To give a concrete example, imagine an idealized neuron in primary motor cortex (M1) whose firing, unlike typical M1 neurons ( Georgopoulos et al., 1982 ), is not at all modulated by reach direction but, instead, is modulated by reach speed ( Fig 1 ). In a typical experimental setting, an animal’s reach directions are randomized, but reach speed cannot be randomized or tightly controlled. If the average speed differs across reach directions, such a hypothetical neuron will appear to be tuned to reach direction, despite not being directly affected by direction. First, fitting a typical tuning curve for reach direction, we would infer that such a neuron has a clear preferred direction and non-zero modulation depth. On the other hand, if we then fit a second model that included both reach direction and speed, we would infer that the neuron is modulated by speed alone, and it would be apparent that the original preferred direction and modulation depth estimates were biased due to the omitted variable. Download figure Open in new tab Figure 1:  When relevant variables are omitted from the model, estimates of the included effects can be biased. Consider two hypothetical neurons tuned to an observed variable x and an omitted variable x h . Neuron 1 is not tuned to the observed variable x , but its rate is modulated by the omitted variable with true tuning curves denoted by the gray curves (top left). If x and x h covary, the apparent tuning of this neuron to x when the tuning curve is estimated using x alone will then be biased (red and blue curves, top middle). This neuron will appear to be tuned to x despite not actually being tuned to this variable. In addition, to this type of illusory tuning, there can also be more subtle biases. Here, neuron 2 is tuned to x and x h (gray curves, bottom left). However, depending on how x and x h covary, the preferred stimulus or the modulation can be misestimated. Here the true tuning to x is shown at three different fixed values of x h (three gray curves, left panels). The estimated tuning when x h is omitted are shown at center with red and blue curves corresponding to the estimates under two different joint distributions (matching borders, right). Dashed line denotes the effect of x when x h is fixed. In adding additional variables, previous studies have largely focused on the fact that including previously omitted variables improves model accuracy or the fact that neural activity is often influenced by a host of task variables. In M1, for instance, including speed improves model accuracy ( Moran and Schwartz, 1999 ), but the presence of many correlated motor variables (e.g. kinematics, end-point forces, muscle activity) makes it difficult to interpret how neurons represent movement overall ( Humphrey et al., 1970 ; Omrani et al., 2017 ). Here, instead of focusing on the advantages or complexities of models with many variables, we focus on the well-known, but under-discussed, fact that the parameters describing the original effects change as additional variables are included. The hypothetical M1 neuron above points to a more general question about regression models of neural activity. What happens when we cannot or do not include variables that are relevant to the process that we are modeling? Here we first evaluate the statistical problem of omitted variable bias in the canonical generalized linear model with Poisson observations. Then, as a case study, we examine how speed affects estimates of direction tuning of neurons in primary motor cortex, as well as, two other case studies where the spike counts are modeled as a function of external variables: orientation tuning in primary visual cortex (V1) and place tuning in the hippocampus (HC). In each of these case studies we find that commonly omitted variables (speed in M1, population activity in V1, and speed and heading in HC) can bias the estimated effects of commonly included variables (reach direction in M1, stimulus orientation/direction in V1, and place in HC). Across all three case studies, including the omitted variables reduces the estimated modulation due to typical tuning effects. We also illustrate how omitted variable bias can affect generalized linear models of spike dynamics where a post-spike history filter aims to describe refractoriness and bursting ( Truccolo et al., 2005 ). The goal of these models is typically to differentiate aspects of spike dynamics that are due to the neurons own properties (e.g. membrane time constant, resting potential, after-hyper-polarization currents) from those due to input to the neuron from other sources ( Brillinger and Segundo, 1979 ; Paninski, 2004 ). In this setting, the input to the neuron is typically not directly observed, but is approximated by stimulus or behavioral covariates, local field potential, or the activity of other neurons. Here we show that omitting the input can lead to large biases in post-spike history filters, and that including omitted variables describing the input can change the interpretation and stability of the estimated history effects. GLMs have been used in many settings to disentangle the effects of multiple, possibly correlated, stimulus or task variables ( Fernandes et al., 2014 ; Park et al., 2014 ; Runyan et al., 2017 ) and also to model neural mechanisms such as post-spike dynamics, interactions between neurons, and coupling to local fields ( Harris et al., 2003 ; Truccolo et al., 2005 ; Pillow et al., 2008 ). It is often argued that GLMs are advantageous because they have unique maximum likelihood estimates and can be more robust to non-spherical covariate distributions than other methods, such as spike-triggered averaging ( Paninski, 2004 ; Pillow, 2007 ). Although these advantages are important, GLMs are not immune to bias. Here we show how the possibility of omitted variable bias, in particular, should encourage researchers to be cautious in their interpretation of model parameters, even in cases where a GLM achieves high predictive accuracy ( Shmueli, 2010 ). Results Here we introduce the problem of omitted variable bias and examine differences between omitted variable bias in linear models and the canonical Poisson GLM. We then consider three tuning curve estimation problems: estimating direction tuning in primary motor cortex, place tuning in hippocampus, and orientation tuning in primary visual cortex and show how omitted variables in each of these three cases can alter parameter estimates. Finally, we consider a GLM that aims to describe the dynamics of post-spike history and show how omitted inputs can bias the estimated history effects and qualitatively change model stability. Omitted Variable Bias in Linear Regression and canonical Poisson GLMs When relevant variables are not included in a regression model, the estimated effects for the variables that are included can be biased ( Box, 1966 ). Omitted variable biases can cause the parameters describing the effects of the original variables to be over- or under-estimated, and model fits can change qualitatively when omitted variables are included ( Fig 1 ). To understand the problem of omitted variable bias it will be helpful to briefly review the well-known case of multiple linear regression, where the bias can be described analytically ( Box, 1966 ). In the linear setting, consider the generative model  where observations y are a linear combination of observed X and omitted X h variables plus normally-distributed i.i.d. noise ϵ∼N (0, σ ). For simplicity, we ignore the intercept term, but in the analysis that follows it may also be considered as part of X . If we then fit the (mis-specified) model without X h using maximum likelihood estimation (equivalent to the ordinary least squares solution, in this case) the estimated parameters will be  where ξ denotes the effect of the noise, and the bias ( X T X ) −1 X T X h β h will, generally, be non-zero. There will be no bias only in the cases where the omitted variables do not affect the observations ( β h = 0) or when the omitted variables and observed variables are not collinear ( X T X h = 0). Note that ( X T X ) −1 X T X h is the matrix of regression coefficients for the omitted variables using the observed variables as predictors. For linear regression, the omitted variable bias thus depends on both the extent to which the omitted variables affect the observations β h and the extent to which the omitted variables can be (linearly) predicted from the observed variables. Although there is a closed-form solution for the omitted variable bias for linear regression, the generalized linear setting is not as tractable ( Gail et al., 1984 ; Clogg et al., 1992 ; Drake and McQuarrie, 1995 ). We will consider the case of a canonical Poisson GLM, in particular, where  In the more general case, GLMs have  where g −1 (·) is the inverse link function, and y is distributed following an exponential family distribution ( McCullagh and Nelder, 1989 ). For a canonical GLM the log-likelihood takes the form  where the nonlinear function G (·) depends on both the link function and the noise model. For canonical GLMs, this log-likelihood is concave and the maximum likelihood estimate satisfies . The exact form of G (·) will depend on the model, but for linear regression G ( x ) is proportional to , and for canonical (log-link) Poisson regression G (·) = exp(·). Now, with omitted variables, instead of maximizing the correct log-likelihood, we maximize instead  For the omitted variable bias in to be 0, we need both and at the same value of β . Although, neither MLE has a closed form solution, this condition implies that, if there is no bias due to the omitted variables,  where G ′ (·) is the derivative of G (·). For linear regression this equality reduces to the OLS form derived above, and for canonical Poisson regression we have  This equality is satisfied when observations are not modulated by the omitted variables β h = 0 or, more generally, when the effect of the omitted variables δλ = exp( Xβ + X h β h ) – exp( Xβ ) is orthogonal to the included variables X . Note that with linear regression, X T X h = 0 implies that the estimates will not be biased, but here this is not the case unless X T δλ = 0 as well. Due to the structure of the canonical Poisson GLM, omitted variable bias can thus occur even in a properly randomized, controlled experiment ( Gail et al., 1984 ), It is important to note that the maximum likelihood estimates themselves are consistent. That is, the estimators converge (in probability) to their true values when the generative model is correct. The bias here is a result of the model being mis-specified. This mis-specification affects the location of the maximum and, also, the shape of the likelihood. Optimization methods, such as Newton’s method, will typically contain omitted variable bias in each parameter update. For canonical Poisson regression, for instance, the updates take the form  at iteration k where the weight matrix W is diagonal with entries W ii = λ i and ( X T WX ) −1 is the Fisher scoring matrix (inverse Hessian of the log-likelihood) at the current estimate . Since the mis-specified model will use λ = exp( Xβ ) instead of the exp( Xβ + X h β h ), both the weight matrix and the gradient X T ( y – λ ) will be biased at each step of the optimization (except when ). Traditional standard errors for the MLE will also typically be influenced by omitted variables, since . Moreover, as previous studies have shown, omitted variables can lead to misestimation of the variability in E [ y ] and dispersion var ( y ) ( Czanner et al., 2008 ; Goris et al., 2014 ). If the omitted variables affect the observations, then they will generally increase the variability of E [ y ]. Then, unless the omitted variables are perfectly predicted by the included variables, the explained variance var ( E [ y ]) of the mis-specified model will be lower than that of the full model. This may, in turn, lead to overestimates of dispersion, since var ( y ) = E [ var ( y )] + var ( E [ y ]). Omitted Variable Bias in Tuning Curve Estimation When fitting tuning curve models to spike count data, omitted variable bias can cause preferred stimuli and modulation depths to be misestimated and can even lead to completely illusory tuning ( Fig 1 ). To illustrate how omitted variable bias affects GLMs of neural spiking, not just in theory, but in practice, we consider three case studies where we fit typical tuning curve models that omit potentially relevant variables along with augmented models that include these additional variables. We first consider modeling spike counts across trials and on slow (>100ms) timescales. Here we assess 1) the tuning of neurons in motor cortex to reach direction, with speed as a potential omitted variable, 2) the tuning of neurons in hippocampus to position, with both speed and head-direction as potential omitted variables, and 3) the tuning of neurons in visual cortex to the direction of motion of a sine-wave grating, with population activity as a potential omitted variable. In each of these cases studies, we show how the omitted variables are not independent from the commonly included variables and how neural responses are modulated by the omitted variables. These two properties, together, can lead to omitted variable biases. In our first case study, we model data recorded from primary motor cortex (M1) of a macaque monkey performing a center-out, planar reaching task. In this task, speed differs systematically across reach directions ( Fig 2A ), with average speed differing by as much as 35±3% (for the targets at 45 and 225 deg relative to right, Fig 2B ). To model neural responses, we first fit a traditional tuning curve model ( Georgopoulos et al., 1982 ; Amirikian and Georgopulos, 2000 ), where the predicted responses depend only on target direction. Here we use a circular, cubic B-spline basis (5 equally spaced knots) to allow for deviations from sinusoidal firing, but, in most cases, the responses of the n=81 neurons in this experiment are well described by cosine-like tuning curves with clear modulation for reach direction. We then fit a second model that includes effects from movement speed. Here we use covariates based on ( Moran and Schwartz, 1999 ), including a linear speed effect, as well as, cosine-tuned direction-speed interactions (see Methods). This model captures the responses of individual neurons, where spike counts can increase ( Fig 2C , top) or decrease ( Fig 2C , middle) as a function of speed, and, in some cases, speed and direction appear to interact ( Fig 2C , bottom). Together, the fact that direction and speed are not independent along with the fact that neural responses appear to be modulated by speed could lead to biased parameters estimates for the model where speed is omitted. Comparing the models with and without omitted variables we find that, averaged across the population, there are only minimal shifts in the preferred direction (3±2 deg) when speed is included in the traditional tuning curve model, and there do not appear to large, systematic shifts in the population distribution of PDs (Kuiper’s test, p>0.1). At the same time, there is substantial variability between neurons in the size of the PD-shift (circular SD 32±5 deg). Across the population, modulation depth (measured using the standard deviation of the tuning curve) decreases slightly on average (3±2%), and the size of the modulation change also varies substantially between individual neurons (SD of changes 18±3%). An example neuron in Fig 1C (bottom), for instance, has a modulation decrease of 9±5% and the preferred direction changes 4±9 deg when speed is included in the model (standard error from bootstrapping). Overall, ∼10% of neurons have statistically significant changes in PD, and ∼14% have significant changes in modulation (bootstrap tests α =0.05, not corrected for multiple comparisons). For some individual neurons, at least, the parameters of the model without speed, thus, have clear omitted variable bias. However, since individual neurons have diverse speed dependencies, in this case, the average biases across the population are minimal. When speed is included in the model, model accuracy (pseudo-R 2 ) does increase slightly (p=0.01, one-sided paired t-test across neurons). The average cross-validated (jack-knife) pseudo-R 2 for the original model is 0.23±0.01 and for the model with speed 0.24±.01 ( Fig 5 ). However, it seems likely that in other experimental contexts the effects of omitting speed could be more pronounced. By requiring the animal to make reaches to the same targets at different speeds, previous studies have more clearly demonstrated that responses in M1 are modulated by speed ( Churchland et al., 2006 ). Here we demonstrate how this type of modulation can lead to omitted variable biases in the estimated parameters of typical tuning curve models without speed. Download figure Open in new tab Figure 2:  Speed as an omitted variable in M1 tuning for reach direction. A) The distribution of reach speeds differs by target direction in a center-out task. Circles denote median, boxes denote IQR. B) Speed profiles for the two targets showing the largest speed differences. Individual traces denote individual trials aligned to the half-max (black arrow). Inset shows the position of each trial with colors denoting reach direction. C) The responses of 3 M1 neurons show typical tuning for reach direction. The tuning curve estimated using direction covariates alone (black) changes when speed covariates are included (red). Red curves denote the direction effect within the full model and are generated by assuming speed is constant (equal to the mean speed across all trials). Right panels illustrate the speed dependence for the preferred direction and its opposite. Dark lines denote the estimated effect of speed under the full model. Data points show single trial data, along with the mean speed and rate for each direction (big data point). Light lines show linear trends (OLS) using only the trials from each specific target. In our second case study we examine the activity of neurons in the dorsal hippocampus of a rat foraging in an open field. Here we consider to what extent the practice of omitting speed and head direction from a place field model biases estimates of a neuron’s position tuning. As in the first case study, omitted variable bias can occur if neural activity is modulated by omitted variables and the omitted variables covary with the included variables. In the case of the hippocampus, neural activity is known to be modulated by both movement speed and head direction ( McNaughton et al., 1983 ), in addition to an animal’s position ( O’Keefe and Dostrovsky, 1971 ). Additionally, behavioral variables can be highly nonuniform across the open field ( Walsh and Cummins, 1976 ), for instance, near and far from the walls. Together the fact that the omitted variables may covary with position and the fact that neurons appears to be modulated by the omitted variables, suggest that there may be omitted variable bias. Here, in one recording during an open field foraging task we find that the average speed ( Fig 2A ) and heading ( Fig 2B ) differ extensively as a function of position. Within a given neuron’s place field, the distributions of speed and heading may be very different from their marginal distributions. Across the population of n=68 place cells (selected from 117 simultaneously recorded neurons, see Methods), average in-field speed was between 80-135% of the average overall speed (5.5cm/s), and the animal’s heading can be either more or less variable in-field (circular SD 57-80 deg) compared to overall (75 deg). As previous studies have shown, we also find that neural responses are modulated by speed and head direction. Responses due to place, speed, and heading are shown for one example neuron in Fig 3 . This neuron shows a stereotypical place-dependent response ( Fig 2B ), but splitting the observations by speed ( Fig 3C , top) or heading ( Fig 3B , bottom) by quartiles/quadrants reveals that there is also tuning to these variables. The neuron appears to increase its firing with increasing speed and responds most strongly when the rat is facing the left. These dependencies are well fit by the full model where the firing rate depends, not just on position, but also on the (log-transformed) speed and the heading ( Fig 3D , bottom). For the example place cell shown here, the location of the place-field does not change substantially when the omitted variables are included ( Fig 3E ). However, the modulation (SD of the rate map) decreases by 27%. That is, 27% of the apparent modulation due to position when it is modeled alone, can be explained by speed and heading effects. Across the population of place cells, there were no clear, systematic difference in the place field locations, but the modulation (SD of the rate map λ ( x )) decreases by 9±1% on average when speed and heading are included. Individual neurons showed substantial variability in their modulation differences (population SD 10±1%). As in M1, including the omitted variables increased spike prediction accuracy – the average cross-validated (10-fold) pseudo-R 2 was .29±.02 for the original model and .31±.02 for the model including speed and heading activity. This difference seems small, since there is large variability in pseudo-R 2 values across the population, but the average increase in pseudo-R 2 was 11±3% ( Fig 5 ). Given that neurons appear to be modulated by speed and heading, it is unsurprising that including these variables improves model fit. However, as before, it is important to note that this modulation can lead to biases in the place field estimates for the model with only position. Download figure Open in new tab Figure 3:  Speed and heading as omitted variables in hippocampal place cells. A) Average speed and heading as a function of position for a rat foraging in an open field. B) An example place cell tends to spike (red dots) when the animal is at a specific position in space. C) The activity of this neuron is modulated by the animal’s speed (top row) and heading (bottom row). Speed is split into quartiles, subplots include all headings. Heading is split into quadrants, subplots include all speeds. D) The distributions of speed and heading within the place field differ from the overall distributions, and the neuron is tuned to these variables. Blue curve shows model fit. E) After modeling the effect of speed and heading within the place field, the location of the place field does not change but the apparent modulation due to position is reduced. In our third case study, we examine the activity of neurons in a more controlled sensory experiment. Here we use data recorded from primary visual cortex (V1) of an anesthetized monkey viewing oriented sine-wave gratings moving in on of 12 different directions (see Methods). In this experiment, variability in the animal’s behavior is purposefully minimized, and, instead of considering the effect of omitting a behavioral variable, here we consider the effect of omitting a variable relating to the animal’s internal state – the total population activity. Several studies have previously shown that population activity alters neural responses in V1 ( Arieli et al., 1996 ; Kelly et al., 2010 ; Okun et al., 2015 ; Arandia-Romero et al., 2016 ). If the distribution of population activity also varies with stimulus direction, then there is the potential for omitted variable bias. Here we assess neural activity from n=90 simultaneously recorded neurons across many (2400) repeated trials with 12 different movement directions. We find that there is high trial-to-trial variability in the population rate ( Fig 4A ), and the average firing across all neurons does differs across stimulus directions, up to ∼50%. For this recording, the most extreme differences were between the 180 deg stimulus where the average rate across the population was 3.4±0.1Hz and the 60 deg stimulus where the average rate was 6.3±0.1Hz ( Fig 4B ). By adding the (log-transformed) population rate as a covariate to a more typical model of direction tuning, we find that population activity may lead to omitted variable bias in models of direction tuning alone. As in the case studies above, there do not appear to be any consistent or systematic effects on the preferred stimulus direction at the population level (Kuiper’s test, p=0.1). However, the modulation depth (measured using SD of the tuning curve) decreases substantially 15±2% when population rate is included in the model, and there is again high variability across neurons (SD 20±2%). In this case, model accuracy increases substantially when the omitted variable is included. The cross-validated (10-fold) pseudo-R 2 is .26±.02 for the original model and .43±.02 for the model including population activity, with an average increase of 164±31% ( Fig 5 ). Download figure Open in new tab Figure 4:  Population rate as an omitted variable in primary visual cortex. A) Correlated trial to trial variability. Population rasters for three trials of the same drifting grating stimulus (0 deg, red and 30 deg, orange). Neurons are sorted by overall firing rate. B) Histograms of the population rate across trials. As a population, the neurons respond at higher rates to 30 deg stimuli, but there is high trial-to-trial variability. C) The responses of 2 V1 neurons show typical tuning for direction of motion. The tuning curve estimated using direction covariates alone (black) changes when the population rate covariate are included (red). Right panels illustrate the dependence for the preferred direction and an orthogonal direction. Dark lines denote the estimated effect of speed under the full model. Data points show single trial data, along with the mean count and rate (big data point). Light lines show linear trends (OLS) using only the trials from each specific stimulus. Unlike in M1 where the effect of speed was highly diverse for different neurons, in this case study the effect of the population rate is largely consistent. Higher population rates are associated with higher firing rates, and, for most neurons, the effect of the population rate is stronger in the preferred direction(s), consistent with a multiplicative effect. Note that here, we do not include the neuron whose rate we are modeling in the calculation of the population rate. However, using the population rate as an omitted variable requires some interpretation. The population rate will certainly be affected by the tuning of the, relatively small, sample of neurons that we observe. If we have a disproportionate number of neurons tuned to a specific preferred direction, the population rate in those directions will be higher. This suggests that in a different recording, the covariation between the stimulus and the population rate could very likely be different. However, it appears that the omitted variable biases in this case are mostly driven by noise correlations, where neural activity is correlated on single trials even within the same stimulus condition, rather than stimulus correlations, where neural activity is correlated due to similar tuning. When we shuffle the data within each stimulus condition (removing noise correlations) the average change in the modulation depth is -1±2% (SD 18±3%), and the effect of the omitted variable becomes negligible. Download figure Open in new tab Figure 5:  For each of the case studies, on average, the model accuracy increases when omitted variables are included (top) and the modulation due to the original variables decreases (bottom). Scatter plots indicate cross-validated pseudo-R2 values for each neuron under the two models. Modulation denotes the standard deviation of the tuning to the original variable(s) under each model. Here, modulation values are normalized by the average rate of each neuron. Black lines denote equality. Red dashed lines denote linear fit with 0 intercept. Download figure Open in new tab Figure 6:  Estimated post-spike history filters can be heavily biased when the input is not included in the model. A) Here we simulate from an inhomogeneous Poisson model with sinusoidal input (no post-spike history effects). The input and spike responses from 20 trials are shown. Although there are no history effects in the generative model, a GLM with history effects that is missing the correct input covariate will use the history terms to capture the structure in the autocorrelation (C). Traces denote the estimated rate for the 20 trials shown above. When the history term is included in the model, but the input is not, the GLM can still reconstruct PSTH responses using the post-spike history alone. B) Post-spike filters for the models in (A) with 95% confidence bands. Note that when input is included in the model the filters correctly reconstruct the true (lack of) filter, and that there is higher uncertainty around the regions where the ISI distribution does not constrain the model. Omitted Variable Bias in the Estimation of Post-Spike History Effects In addition to modeling spike counts over trials or on relatively slow (>100ms) behavioral timescales, GLMs are also often used to describe detailed, single-trial spike dynamics on fast (<10ms) timescales. One common covariate used in these types of models is a post-spike history effect where the probably of spiking at a given time depends on the recent history of spiking. Modeling these effects allows us to describe refractoriness, bursting ( Paninski, 2004 ; Truccolo et al., 2005 ), and a whole host of other dynamics ( Weber and Pillow, 2017 ). Conceptually, the goal of these models is to disentangle the sources of rate variation based only on observations of a neuron’s spiking, with history effects, ideally, reflecting intrinsic biophysics. However, since the full synaptic input is typically not known with extracellular spike recordings there is potential for omitted variable biases. To illustrate the potential pitfalls of omitting the input to a neuron, consider using the GLM to capture single neuron dynamics in the complete absence of external covariates  where the rate λ is determined by a baseline parameter μ along with a filtered version of the neuron’s past spiking with h i ( t ) = ∑ τ>0  f i (τ) n ( t – τ). This is a perfectly acceptable model of intrinsic dynamics, but for most spike data that we observe this isolated neuron model may not provide a realistic description of a neuron receiving thousands of time-varying synaptic inputs. If we fit this model to data where the input to the neuron did vary over time,  then the history filter in the first model will attempt to capture variation in spiking due to the time-varying input, in addition to any intrinsic dynamics. For example, when x h is periodic, the estimated history filters of the original model will attempt to capture this periodic structure ( Fig 6A-B ). Just as in the tuning curve examples above, the fact that history effects covary with the input and the fact that the input modulates the neuron’s firing leads to omitted variable bias. When the input is omitted from the model, the biased history effects simply provide the best (maximum likelihood) explanation of the observed spiking ( Fig 6C ). These examples with strong, periodic input are not necessarily biologically realistic, but they make it apparent how the post-spike history can be biased by omitted input variables. In vivo, neurons instead appear to be in a high-conductance state, where membrane potential fluctuations have approximately 1/ f power spectra ( Destexhe et al., 2001 , 2003 ). When these naturalistic input statistics are used to drive the GLM, omitted variable bias can occur, as well. Here we simulate a GLM receiving 1/ f α noise input with α = 0 (white noise) 1 and 2 ( Fig 7 ). For white noise input, the MLE accurately recovers the simulated post-spike history filter when the input is omitted from the model, but when α = 1 or 2 the estimates become increasingly biased ( Fig 7A,C ). With the full model, where the input is included as a covariate, the history is recovered accurately no matter what the input statistics are. Just as in the periodic case, however, these different input statistics alter the auto-correlation, and, when the input is omitted from the model, the maximum likelihood history filter simply aims to capture these patterns. Download figure Open in new tab Figure 7:  Post-spike filters can show omitted variable bias even in a more realistic scenario. Here we simulate from a GLM with a refractory post-spike filter and drive the neurons with 1/ f α noise. Excepting the case of white noise ( α = 0), the post-spike filters estimated for the GLM without input are heavily biased (A). C) Even when the effect of the true post-spike filter is to strictly decrease the firing rate, the estimated filters can increase the firing rate. B,D) Approximate transfer functions from a quasi-renewal approximation. When the true filter is stable, the estimated filters can result in fragile dynamics. In GLMs for single-neuron dynamics, one effect of omitted variable bias is that it may lead us to misinterpret how stable a neuron’s dynamics are. Even if the true history filter only reduces the neuron’s firing rate following a spike (as in Fig 7C ), the estimated filter can be biased upwards when the input is omitted. If we were to simulate the activity of this neuron based on the biased filter, the bias could cause the neuron’s rate to diverge if the rate becomes high enough. To assess the stability of the estimated post-spike history effects quantitatively, here we make use of an quasi-renewal approximation analysis introduced in ( Gerhard et al., 2017 ). Given a history filter, this approach finds an approximate transfer function describing the neuron’s future firing rate (output) given its recent (input) firing rate (see Methods). For all estimated models, the transfer function has a stable fixed point near the neuron’s baseline firing rate. When the true input is omitted and α > 0, the estimated history filters also have an unstable fixed point where the neuron’s firing rate will diverge if the rate exceeds this point ( Gerhard et al., 2017 ). Here we find that omitted variable bias leads to apparent fragility ( Fig 7B,D ). The stable region shrinks as α increases, and even when the true dynamics are strictly stable (as in Fig 7C,D ), omitted variable bias can lead us to mistakenly conclude that the neuron has fragile dynamics. With most extracellular spike recordings, the synaptic input that the neuron receives is unknown. However, there may also be omitted variable bias when history effects are estimated from real data. In this case, the input to a neuron can be approximated by stimulus or behavioral variables, local field potentials, or the activity of simultaneously recorded neurons ( Harris et al., 2003 ; Truccolo et al., 2005 , 2010 ; Pillow et al., 2008 ; Kelly et al., 2010 ; Gerhard et al., 2013 ; Volgushev et al., 2015 ). Just as in the simulations above, including or omitting these variables can then alter the estimated history effects, even though they are not as directly related to spiking as the synaptic input itself. Here we consider total population spiking activity as a proxy for synaptic input and consider how including population activity alters the history filters when compared to a model of history alone. We examine two datasets: spontaneous activity from primary visual cortex of an anesthetized monkey with n=62 simultaneously recorded neurons and activity from dorsal hippocampus of a sleeping rate with n=39 simultaneously recorded neurons. To model population covariates we sum the spiking of all neurons, excepting the one whose spiking we aim to predict, and low-pass filter the signal (see Methods). Similar to previous results ( Okun et al., 2015 ), we find that, since neurons often have correlated fluctuations in their spiking ( Fig 8A, D ), the population rate is a good predictor for single neuron activity. Moreover, when we add population covariates to a GLM with post-spike history effects the history filter changes. In the V1 dataset, the post-spike gain decreases by 7.8±0.5% on average when population covariates are included, and 14.9±0.8% when considering only the first 50ms after a spike ( Fig 8B ). The effects of adding population covariates are less pronounced in the hippocampal dataset. The post-spike gain decreases by 2.5±0.3% on average, and 9.5±1.2% when considering only the first 50ms after the spike ( Fig 8E ). Based on the quasi-renewal approximation, all neurons in both the V1 and hippocampal datasets have fragile transfer functions where there is a stable fixed point (near the neuron’s average firing rate) and an unstable fixed point where the neuron’s rate diverges if the input becomes too strong. For V1, the average upper-limit of the stable region is 80±3Hz for the models with history only and 143±7Hz for the models with population covariates ( Fig 8C ). In the hippocampal data, the average upper-limit of the stable region is 38±6Hz for the models with history only and 75±13Hz for the models with population covariates ( Fig 8F ). Each neuron is, thus, apparently, more stable after the population covariates are included. As in the case studies using tuning curves, adding covariates also improves spike prediction accuracy. In the V1 dataset, the average log likelihood ratio relative to a homogeneous Poisson model is 2.2±0.3 bits/s for the history model and 3.3±0.3 bits/s for the model with population covariates. In hippocampus, the log likelihood ratio is 0.9±0.3 bits/s for the history model and 2.0±0.5 bits/s for the model with population covariates. The larger effects in V1 are likely explained by the fact that the population rate is predictive for many more neurons here than for the hippocampal data. In the hippocampus, only 26% of the neurons have an increase of over 0.5 bits/s when the population covariates are included, compared to 85% of neurons in V1. Altogether these results demonstrate how omitted variable bias could affect estimates of post-spike history filters in vivo. In both datasets we find that when population covariates are included in the GLM spike prediction accuracy increases, post-spike gain decreases, and apparent stability increases. Download figure Open in new tab Figure 8:  Post-spike filters estimated from real data decrease when population activity is included as a covariate. Segments of spontaneous activity are shown for V1 (A) and during sleep for hippocampus (D). Neurons are sorted by firing rate. B and E show estimated post-spike filters. Black lines denote the average filter (thick) and standard deviation (thin). For clarity, only filters for neurons with firing rates >1Hz are shown. C and F show average quasi-renewal transfer functions for the same set of neurons. All neurons appear to have fragile dynamics with one stable fixed point near the neuron’s average firing rate and an unstable fixed point, beyond which the neuron’s firing rate diverges. Including population covariates increases the region of stability. Discussion When the goal of modeling is causal inference or understanding of biological mechanisms, the potential for biases due to omitted variables is often clear. The statistical effects of confounders (Wasserman, 2004), as well, as the limits that they place on neuroscientific understanding are widely appreciated ( Jonas and Kording, 2017 ; Krakauer et al., 2017 ; Yoshihara and Yoshihara, 2018 ). However, when the goal of modeling is to create an abstract, explanation or summary of observed neural activity, the fact that omitted variables can bias these explanations is not always widely acknowledged. Here we have illustrated the potential for omitted variable bias in two types of commonly used GLMs for neural spiking activity: tuning curve models using spike counts across trials and models that capture single-neuron dynamics with a post-spike history filter. In each model, adding a previously omitted variable, as expected, improved spike prediction accuracy. However, what we emphasize here is that, when omitted variables were included, the estimates of the original parameters changed. For three case studies using tuning curves we found that by adding a traditionally omitted variable tuning curves showed less modulation due to the originally included variables. In models of single neuron dynamics, adding omitted variables led to decreased post-spike gain and greater apparent stability. Importantly, omitted variables can arise in GLMs in any situation where an omitted variable affects neural activity and the effect of the omitted variable is not independent of the included variables. The case studies here are not unique, and many studies have described how adding additional variables to a tuning curve or single neuron model can improve prediction accuracy. In M1, in addition to movement speed, joint angles, muscle activity, end-point force, and many other variables also appear to modulate neural responses ( Fetz, 1992 ; Kalaska, 2009). In addition to speed and head direction in the hippocampus, theta-band LFP, sharp-wave ripples, and environmental features, such as borders, appear to modulate neural activity ( Hartley et al., 2014 ). And in V1, there is growing evidence that population activity ( Lin et al., 2015 ) and non-visual information ( Ghazanfar and Schroeder, 2006 ) modulates neural responses. In each of these systems, neural responses are affected by many, many factors. Responding to many task variables may even be functional, allowing downstream neurons to more effectively discriminate inputs ( Fusi et al., 2016 ). In any case, it seems clear that our models do not yet capture the full complexity of neural responses ( Carandini et al., 2005 ). By omitting relevant variables, current models are likely to be not just less accurate but also biased. Parameter bias may be problematic in and of itself. However, omitted variable bias may also have an important effect on generalization performance. As noted in ( Box, 1966 ), in a new context, the effect of the omitted variables and the relationship between the omitted and included variables may be different. Since the parameters of the included variables are biased, this change can reduce generalization accuracy. This phenomena may explain, to some extent, why tuning models fit in one condition often do not generalize to others ( Graf et al., 2011 ; Oby et al., 2013 ). For models of single-neuron dynamics, omitted variable bias can also have a negative effect on the accuracy of simulations. Previous work has shown that simulating a GLM with post-spike filters estimated from data often results in unstable, diverging simulations. Although several methods for stabilizing these simulations have recently been developed ( Gerhard et al., 2017 ; Hocker and Park, 2017), one, perhaps primary, reason for this instability may be that the post-spike filters are biased due to omitted synaptic input. Since estimated post-spike filters may reflect not just intrinsic neuron properties but also the statistics of the input, interpreting and comparing post-spike filters may be difficult. Different history parameters may be different due to intrinsic biophysics ( Tripathy et al., 2013 ) or due to differing input, and resolving this ambiguity will likely involve more accurately accounting for the input itself ( Kim and Shinomoto, 2012 ). The possibility of omitted variable bias does not mean that estimated parameters, predictions, and simulations from simplified model are useless, but it may mean that we need to be cautious in interpreting these models and their outputs. When reporting the results of regression, in addition to avoiding describing associations with causal language, it may be generally useful to discuss known and potential confounds. Previous studies have already identified several specific cases of omitted variable bias where careful interpretation is necessary. For instance, omitted common input can bias estimates of interactions between neurons ( Brody, 1999 ), and omitted history effects can bias receptive field estimates ( Pillow and Simoncelli, 2003 ). In estimating peri-stimulus time histograms, omitting variables that account for trial-to-trial variation may cause biases ( Czanner et al., 2008 ) or issues with identifiability ( Amarasingham et al., 2015 ). Similarly, biases due to spike sorting errors ( Ventura, 2009 ) could be framed as a result of omitting variables related to missing/excess spikes. Since we typically do not model or observe all the variables that affect neural activity, omitted variable problems are likely to be pervasive in systems neuroscience far beyond these specific cases. Although we have focused on GLMs here, omitted variable bias can affect any model and other types of model misspecification can also result in biased parameter estimates. Adding input nonlinearities ( Ahrens et al., 2008 ; David et al., 2009 ), interaction effects ( McFarland et al., 2013 ), or higher-order terms to the GLM ( Berger et al., 2010 ; Park et al., 2013 ) may fix certain types of model misspecification, but any model that omits relevant variables is still likely to suffer from the same problems. This includes both machine learning methods that may provide better prediction accuracy than GLMs ( Benjamin et al., 2017 ) and single neuron models aiming to describe greater biophysical detail ( Herz et al., 2006 ). Unlike over-fitting or non-convergence ( Zhao and Iyengar, 2010 ), omitted variable bias will generally not be resolved by including additional data or by adding regularization. Moreover, adding one omitted variable, as we have done with the case studies here, is no guarantee that there are not other relevant variables being omitted. One approach that could potentially reduce omitted variable bias is latent variable modeling, where the effects of unknown covariates are explicitly included (constrained by simplifying assumptions). Recent work has introduced latent variables for neural activity with linear dynamics ( Smith and Brown, 2003 ; Kulkarni and Paninski, 2007 ; Paninski et al., 2010 ), switching dynamics ( Putzky et al., 2014 ), rectification ( Whiteway and Butts, 2017 ), and oscillations ( Arai and Kass, 2017 ). And these models appear to out-perform GLMs on population data in retina ( Vidne et al., 2012 ), visual ( Archer et al., 2014 ), and motor cortices ( Chase et al., 2010 ; Macke et al., 2011 ). Inferring latent variables requires making (sometimes strong) assumptions about the nature of the variables and may require observations from multiple neurons or across multiple trials, but, by approximating some of the effects of relevant omitted variables, latent variables may reduce omitted variable bias. However, generally determining when relevant variables are omitted from a model and what those variables are is not a trivial problem. There is a well-known aphorism from George EP Box that, “All models are wrong, but some are useful.” The lengthier version of this quip is, “All models are approximations. Essentially, all models are wrong, but some are useful. However, the approximate nature of the model must always be borne in mind.” GLMs are certainly useful descriptions of neural activity. They are computationally tractable, can disentangle the relative influence of multiple covariates, and often provide the core components for Bayesian decoders. Here we emphasize, however, one ubiquitous circumstance in systems neuroscience where the “approximate nature” of the models should be “borne in mind.” Namely, omitted variables can bias estimates of the included effects. Methods Neural Data All data analyzed here was previously recorded and shared by other researchers through the Collaborative Research in Computation Neuroscience (CRCNS) Data Sharing Initiative (crcns.org). Data from primary motor cortex is from CRCNS dataset DREAM-Stevenson_2011 ( Walker and Kording, 2013 ). These data were recorded using a 100-electrode Utah array (Blackrock Microsystems, 400 mm spacing, 1.5 mm length) chronically implanted in the arm area of primary motor cortex of an adult macaque monkey. The monkey made center-out reaches in a 20x20cm workspace while seated in a primate chair, grasping a two-link manipulandum in the horizontal plane (arm roughly in a sagittal plane). Each trial for the center-out task began with a hold period at a center target (0.3–0.5 s). After a go cue, subjects had 1.25 s to reach one of eight peripheral targets and then held this outer target for at least 0.2–0.5 s. Each success was rewarded with juice, and feedback (1-2cm diam) about arm position was displayed onscreen as a circular cursor. Spike sorting was performed by manual cluster cutting using an offline sorter (Plexon, Inc) with waveforms classified as single- or multi-unit based on action potential shape and minimum ISIs greater than 1.6 ms (yielding n=81 single units). Here we take model tuning curves using spike counts between 150ms before to 350ms after the speed reached its half-max. Average movement speed for each trial was calculated from 0-250ms after the speed reached its half-max (290 trials in total). For the details of the surgery, recording, and spike sorting see ( Stevenson et al., 2011 ). Hippocampal data is from CRCNS HC-3 ( Mizuseki et al., 2013 ). Here we use recording sessions ec16.19.272 and ec014.215, where a Long-Evans rat was sleeping and foraging in a 180x180cm maze, respectively. For both recordings, 12-shank silicon probes (with 8 recording sites each, 20μm separation) were implanted in CA1 (8 shanks) and EC3-5 (4 shanks) (based on histology). Spikes were sorted automatically using KlustaKwick and then manually adjusted (Klusters) yielding 85 units for the sleep data and 117 for the open field data. For the sleep data where we model post-spike history effects, the spike trains were binned at 1ms and the recording length was 27min. Here we model all neurons with firing rates >0.5Hz (n=39). For the open field data where we model place tuning, spike trains were binned at 250ms and the recording length was 93min. Place cells (n=68) were selected based on having an overall firing rate <5Hz (to rule out interneurons), a peak firing rate >2Hz, and a contiguous set of pixels (after smoothing with an isotropic Gaussian σ =8cm) of at least 200 cm 2 where the firing rate was above 10% of the peak rate. For details of the surgery, recording, and spike sorting see ( Diba and Buzsaki, 2008 ; Mizuseki et al., 2009 ). Data from primary visual cortex is from CRCNS dataset PVC-11 ( Kohn and Smith, 2016 ). Here we use spontaneous activity, during a gray screen, (Monkey 1) and responses to drifting sine-wave gratings (Monkey 1) both from an anesthetized (Sufentanil - 4-18 microg/kg/hr) adult monkey (Macaca fascicularis). Recordings were made in parafoveal V1 (RFs within 5 degrees of the fovea) using a 96-channel multi-electrode array (Blackrock Microsystems), 400 mm electrode spacing, 1mm depth. After automatic spike sorting and manual cluster adjustment, 87 and 106 units were recorded during spontaneous activity and grating presentation, respectively. Only neurons with waveform SNR>2 and firing rates >1Hz were analyzed, n=62 for spontaneous and n=90 for grating data. For the spontaneous activity we bin spike counts at 1ms and the recording length was 20min. For the drifting grating data, we analyzed spike counts from 200ms to 1.2s after stimulus onset on each trial – 12 directions, 2400 trials total. Gratings had a spatial frequency of 1.3 cyc/deg, temporal frequency of 6.25Hz, size of 8-10 deg (to cover receptive fields of all recorded neurons) and were presented for 1.25s with a 1.5s inter-trial interval between stimuli. For surgical, stimulus, and preprocessing details see ( Smith and Kohn, 2008 ; Kelly et al., 2010 ). Tuning Curve Models For the M1 data we use a circular, cubic B-spline basis with 5 equally spaced knots  where g (·) are the splines that depend on the reach direction θ , weighted by parameters β and the parameter μ defines a baseline firing rate. To include the effect of speed, we then add three covariates  where s indicates the speed, and the parameters α allow for a multiplicative speed effect as well as possible cosine-tuned speed x direction interactions as in ( Moran and Schwartz, 1999 ). For place fields in hippocampus we use isotropic Gaussian radial basis functions f (·) equally spaced (30cm) on an 6x6 square lattice with a standard deviation of 30cm  We find that the effect of speed is well modeled using the log-transformed speed s , and to model head direction-dependence we use circular, cubic B-splines g (·) with 6 equally spaced knots  For the V1 data we again use a circular, cubic B-spline basis for the direction of the sine-wave grating (7 equally spaced knots).  We find that the effect of population activity is well modeled using the total log-transformed firing rate of all neuron’s excepting the one being modeled  where z = ∑ i ≠ j log( n i + 1). In all models, to avoid overfitting, especially for low firing rate neurons, we add a small L2 penalty to the log-likelihood with a fixed hyperparameter of 10 -4 . Post-spike History Simulations and Population Rate Models In addition to capturing tuning curves, many studies have used GLMs to describe the dynamics of single spike trains ( Brillinger, 1988 ; Harris et al., 2003 ; Paninski, 2004 ; Okatan et al., 2005 ; Truccolo et al., 2005 ; Weber and Pillow, 2017 ). Here, to account for post-spike history effects, we use a GLM taking the form  where h ( t ) denotes the vector of spike history covariates representing the recent history of spiking and μ determines a baseline firing rate. Here we assume h i ( t ) = ∑ τ>0  f i (τ) n ( t – τ), and we use neuron-specific, cubic B-spline bases f (·) whose knots are determined by the quantiles of each neuron’s ISI distribution. Specifically, we choose knots spaced between 10 and 400ms (HC) or 2 and 200ms (V1), where the spacing follows equal percentile regions of the ISI distribution in that same range. This gives 6 basis functions, and coefficients α to capture the spike-history. To enforce refractoriness, we fix the coefficient of the fastest basis (which peaks at 0 and ends at 10ms) to be -5, leaving 5 coefficients to be estimated. The population rate model simply adds covariates where, for each neuron i   Here we use a set of acausal Gaussian filters for g (·) with standard deviations 20, 50, and 100ms. Note that spikes from the neuron being modeled are excluded from the population covariates. Stability analysis Here we make use of a stability analysis proposed in ( Gerhard et al., 2017 ). Briefly, we use a quasi-renewal approximation of the conditional intensity by considering the effect of the most recent spike, at time t ′, and averaging over possible spike histories preceding this spike  where H ( t – t ′) = αf ( t – t ′) and S represents the history of spiking. By assuming that S is generated from a homogeneous Poisson process with firing rate A 0 , the second term can be approximated by  Given this approximation, we can then estimate the inter-spike interval distribution as we would for a true renewal process and the steady-state distribution of inter-spike intervals is given by  and the predicted steady-state firing rate is f ( A 0 ) = 1 /E P ( τ ) [τ]. To assess stability, we can then examine how the predicted steady-state firing rate depends on the assumed rate of the homogeneous Poisson process A 0 . In particular, when f ( A 0 ) = A 0 the quasi-renewal model has a fixed-point. To allow for external input, we incorporate the average effect of the covariates X into the conditional intensity approximation  Note that, in general, adding inputs X will only change the stability of the model to the extent that these covariates change the estimate of h . References ↵ Ahrens  MB , Paninski  L , Sahani  M.  Inferring input nonlinearities in neural encoding models . Netw. Comput. Neural Syst . 19 : 35 – 67 , 2008 . OpenUrl ↵ Amarasingham  A , Geman  S , Harrison  MT . Ambiguity and nonidentifiability in the statistical analysis of neural codes . Proc. Natl. Acad. Sci. U. S. A . 112 : 6455 – 60 , 2015 . OpenUrl Abstract / FREE Full Text ↵ Amirikian  B , Georgopulos  AP . Directional tuning profiles of motor cortical cells . 36 : 73 – 79 , 2000 . ↵ Arai  K , Kass  RE . Inferring oscillatory modulation in neural spike trains . PLOS Comput. Biol . 13 : e1005596 , 2017 . OpenUrl ↵ Arandia-Romero  I , Tanabe  S , Drugowitsch  J , Kohn  A , Moreno-Bote  R.  Multiplicative and Additive Modulation of Neuronal Tuning with Population Activity Affects Encoded Information . Neuron  89 : 1305 – 1316 , 2016 . OpenUrl CrossRef PubMed ↵ Archer  EW , Koster  U , Pillow  JW , Macke  JH . Low-dimensional models of neural population activity in sensory cortical circuits . In: Advances in Neural Information Processing Systems  27 . 2014 , p. 343 – 351 . OpenUrl ↵ Arieli  A , Sterkin  A , Grinvald  A , Aertsen  A.  Dynamics of Ongoing Activity: Explanation of the Large Variability in Evoked Cortical Responses . Science  273 : 1868 – 1871 , 1996 . OpenUrl Abstract / FREE Full Text ↵ Benjamin  AS , Fernandes  HL , Tomlinson  T , Ramkumar  P , VerSteeg  C , Miller  L , Kording  KP . Modern machine learning far outperforms GLMs at predicting spikes . bioRxiv ( February 24, 2017 ).  doi: 10.1101/111450. OpenUrl CrossRef ↵ Berger  TW , Song  D , Chan  RHM , Marmarelis  VZ . The Neurobiological Basis of Cognition: Identification by Multi-Input, Multioutput Nonlinear Dynamic Modeling: A method is proposed for measuring and modeling human long-term memory formation by mathematical analysis and computer simulation of nerve-cell . Proc. IEEE  98 : 356 – 374 , 2010 . OpenUrl ↵ Box  GEP . Use and Abuse of Regression . Technometrics  8 : 625 , 1966 . OpenUrl CrossRef Web of Science ↵ Brillinger  DR . Maximum likelihood analysis of spike trains of interacting nerve cells . Biol. Cybern . 59 : 189 – 200 , 1988 . OpenUrl CrossRef PubMed Web of Science ↵ Brillinger  DR , Segundo  JP . Empirical examination of the threshold model of neuron firing . Biol. Cybern . 35 : 213 – 220 , 1979 . OpenUrl CrossRef PubMed Web of Science ↵ Brody  CD . Disambiguating Different Covariation Types . Neural Comput . 11 : 1527 – 1535 , 1999 . OpenUrl CrossRef PubMed Web of Science ↵ Brown  E , Barbieri  R , Eden  U , Frank  L.  Likelihood methods for neural data analysis . In: Computational Neuroscience: a comprehensive approach, edited by  Feng J. London: Chapman and Hall , 2003 , p. 253 – 286 . ↵ Carandini  M , Demb  JB , Mante  V , Tolhurst  DJ , Dan  Y , Olshausen  BA , Gallant  JL , Rust  NC . Do we know what the early visual system does?  J. Neurosci . 25 : 10577 , 2005 . OpenUrl Abstract / FREE Full Text ↵ Chase  SM , Schwartz  AB , Kass  RE . Latent Inputs Improve Estimates of Neural Encoding in Motor Cortex . J. Neurosci . 30 : 13873 – 13882 , 2010 . OpenUrl Abstract / FREE Full Text ↵ Churchland  MM , Santhanam  G , Shenoy  K V.  Preparatory Activity in Premotor and Motor Cortex Reflects the Speed of the Upcoming Reach . J. Neurophysiol . 96 : 3130 , 2006 . OpenUrl CrossRef PubMed Web of Science ↵ Clarke  KA . The Phantom Menace: Omitted Variable Bias in Econometric Research . Confl. Manag. Peace Sci . 22 : 341 – 352 , 2005 . OpenUrl ↵ Clogg  CC , Petkova  E , Shihadeh  ES . Statistical Methods for Analyzing Collapsibility in Regression Models . J. Educ. Stat . 17 : 51 , 1992 . OpenUrl CrossRef ↵ Czanner  G , Eden  UT , Wirth  S , Yanike  M , Suzuki  WA , Brown  EN . Analysis of between-trial and within-trial neural spiking dynamics . J. Neurophysiol . 99 : 2672 – 2693 , 2008 . OpenUrl CrossRef PubMed Web of Science ↵ David  S V , Mesgarani  N , Fritz  JB , Shamma  SA . Rapid synaptic depression explains nonlinear modulation of spectro-temporal tuning in primary auditory cortex by natural stimuli . J. Neurosci . 29 : 3374 , 2009 . OpenUrl Abstract / FREE Full Text ↵ Destexhe  A , Rudolph  M , Fellous  JM , Sejnowski  TJ . Fluctuating synaptic conductances recreate in vivo-like activity in neocortical neurons . Neuroscience  107 : 13 – 24 , 2001 . OpenUrl CrossRef PubMed Web of Science ↵ Destexhe  A , Rudolph  M , Paré  D.  The high-conductance state of neocortical neurons in vivo . Nat. Rev. Neurosci . 4 : 739 – 751 , 2003 . OpenUrl CrossRef PubMed Web of Science ↵ Diba  K , Buzsaki  G.  Hippocampal Network Dynamics Constrain the Time Lag between Pyramidal Cells across Modified Environments . J. Neurosci . 28 : 13448 – 13456 , 2008 . OpenUrl Abstract / FREE Full Text ↵ Drake  C , McQuarrie  A.  A note on the bias due to omitted confounders . Biometrika  82 : 633 – 638 , 1995 . OpenUrl CrossRef Web of Science ↵ Fernandes  HL , Stevenson  IH , Phillips  AN , Segraves  MA , Kording  KP . Saliency and saccade encoding in the frontal eye field during natural scene search . Cereb. Cortex  24 , 2014 . ↵ Fetz  EE . Are movement parameters recognizably coded in the activity of single neurons?  Behav. Brain Sci . 15 : 679 – 690 , 1992 . OpenUrl CrossRef Web of Science ↵ Fusi  S , Miller  EK , Rigotti  M.  Why neurons mix: high dimensionality for higher cognition . Curr. Opin. Neurobiol . 37 : 66 – 74 , 2016 . OpenUrl CrossRef PubMed ↵ Gail  MH , Wieand  S , Piantadosi  S.  Biased Estimates of Treatment Effect in Randomized Experiments with Nonlinear Regressions and Omitted Covariates . Biometrika  71 : 431 , 1984 . OpenUrl CrossRef Web of Science ↵ Gelman  A , Hill  J.  Data Analysis Using Regression and Multilevel/Hierarchical Models . Cambridge University Press , 2007 . ↵ Georgopoulos  AP , Kalaska  JF , Caminiti  R , Massey  JT . On the relations between the direction of two-dimensional arm movements and cell discharge in primate motor cortex . J. Neurosci . 2 : 1527 – 1537 , 1982 . OpenUrl Abstract / FREE Full Text ↵ Gerhard  F , Deger  M , Truccolo  W.  On the stability and dynamics of stochastic spiking neuron models: Nonlinear Hawkes process and point process GLMs . PLOS Comput. Biol . 13 : e1005390, 2017 . ↵ Gerhard  F , Kispersky  T , Gutierrez  GJ , Marder  E , Kramer  M , Eden  U.  Successful Reconstruction of a Physiological Circuit with Known Connectivity from Spiking Activity Alone . PLoS Comput. Biol . 9 : e1003138 , 2013 . OpenUrl CrossRef PubMed ↵ Ghazanfar  AA , Schroeder  CE . Is neocortex essentially multisensory?  Trends Cogn. Sci . 10 : 278 – 285 , 2006 . OpenUrl CrossRef PubMed Web of Science ↵ Goris  RLT , Movshon  JA , Simoncelli  EP . Partitioning neuronal variability . Nat. Neurosci . 17 : 858 – 65 , 2014 . OpenUrl CrossRef PubMed ↵ Graf  ABA , Kohn  A , Jazayeri  M , Movshon  JA . Decoding the activity of neuronal populations in macaque primary visual cortex . Nat. Neurosci . 14 : 239 – 245 , 2011 . OpenUrl CrossRef PubMed Web of Science ↵ Greenland  S.  Modeling and variable selection in epidemiologic analysis . Am. J. Public Health  79 : 340 – 9 , 1989 . OpenUrl CrossRef PubMed Web of Science ↵ Harris  KD , Csicsvari  J , Hirase  H , Dragoi  G , Buzsáki  G.  Organization of cell assemblies in the hippocampus . Nature  424 : 552 – 556 , 2003 . OpenUrl CrossRef PubMed Web of Science ↵ Hartley  T , Lever  C , Burgess  N , O’Keefe  J.  Space in the brain: how the hippocampal formation supports spatial cognition . Philos. Trans. R. Soc. Lond. B. Biol. Sci . 369 : 20120510 , 2014 . OpenUrl CrossRef PubMed ↵ Herz  AVM , Gollisch  T , Machens  CK , Jaeger  D.  Modeling single-neuron dynamics and computations: a balance of detail and abstraction . Science  314 : 80 – 5 , 2006 . OpenUrl Abstract / FREE Full Text Hocker  D , Park  IM . Multistep inference for generalized linear spiking models curbs runaway excitation . In: 2017 8th International IEEE/EMBS Conference on Neural Engineering (NER). IEEE , p. 613 – 616 . ↵ Humphrey  DR , Schmidt  EM , Thompson  WD . Predicting measures of motor performance from multiple cortical spike trains . Science  170 : 758 – 62 , 1970 . OpenUrl Abstract / FREE Full Text ↵ Jonas  E , Kording  KP . Could a Neuroscientist Understand a Microprocessor? PLOS Comput. Biol.  13 : e1005268 , 2017 . Kalaska  JF . From Intention to Action: Motor Cortex and the Control of Reaching Movements. Springer , Boston, MA , p. 139 – 178 . ↵ Kandler  S , Mao  D , McNaughton  BL , Bonin  V.  Encoding of Tactile Context in the Mouse Visual Cortex . bioRxiv ( October  6 , 2017 ).  doi: 10.1101/199364. OpenUrl CrossRef ↵ Kass  RE , Ventura  V , Brown  EN . Statistical Issues in the Analysis of Neuronal Data . J. Neurophysiol . 94 : 8 – 25 , 2005 . OpenUrl CrossRef PubMed Web of Science ↵ Kelly  RC , Smith  MA , Kass  RE , Lee  TS . Local field potentials indicate network state and account for neuronal response variability . J. Comput. Neurosci . 29 : 567 – 579 , 2010 . OpenUrl CrossRef PubMed Web of Science ↵ Kim  H , Shinomoto  S.  Estimating nonstationary input signals from a single neuronal spike train . Phys. Rev. E  86 : 051903 , 2012 . OpenUrl ↵ Kohn  A , Smith  MA . Utah array extracellular recordings of spontaneous and visually evoked activity from anesthetized macaque primary visual cortex (V1 ). CRCNS.org . 2016 . ↵ Krakauer  JW , Ghazanfar  AA , Gomez-Marin  A , MacIver  MA , Poeppel  D.  Neuroscience Needs Behavior: Correcting a Reductionist Bias . Neuron  93 : 480 – 490 , 2017 . OpenUrl CrossRef PubMed ↵ Kulkarni  JE , Paninski  L.  Common-input models for multiple neural spike-train data . Netw. Comput. Neural Syst . 18 : 375 – 407 , 2007 . OpenUrl ↵ Lin  I-C , Okun  M , Carandini  M , Harris  KD . The Nature of Shared Cortical Variability . Neuron  87 : 644 – 656 , 2015 . OpenUrl CrossRef PubMed ↵ Macke  J , Büsing  L , Cunningham  J , Yu  B , Shenoy  K , Sahani  M.  Empirical models of spiking in neural populations . In: Advances in Neural Information Processing Systems . 2011 , p. 1350 – 1358 . ↵ McCullagh  P , Nelder  JA . Generalized Linear Models . 2nd ed. CRC Press , 1989 . ↵ McFarland  JM , Cui  Y , Butts  DA . Inferring Nonlinear Neuronal Computation Based on Physiologically Plausible Inputs . PLoS Comput. Biol . 9 : e1003143 , 2013 . OpenUrl CrossRef PubMed ↵ McNaughton  BL , Barnes  CA , O’Keefe  J.  The contributions of position, direction, and velocity to single unit activity in the hippocampus of freely-moving rats . Exp. Brain Res . 52 : 41 – 49 , 1983 . OpenUrl CrossRef PubMed Web of Science ↵ Mizuseki  K , Sirota  A , Pastalkova  E , Buzsáki  G.  Theta oscillations provide temporal windows for local circuit computation in the entorhinal-hippocampal loop . Neuron  64 : 267 – 280 , 2009 . OpenUrl CrossRef PubMed Web of Science ↵ Mizuseki  K , Sirota  A , Pastalkova  E , Diba  K , Buzsáki  G.  Multiple single unit recordings from different rat hippocampal and entorhinal regions while the animals were performing multiple behavioral tasks . CRCNS.org.  2013 . ↵ Moran  DW , Schwartz  a B.  Motor cortical representation of speed and direction during reaching . J. Neurophysiol . 82 : 2676 – 2692 , 1999 . OpenUrl CrossRef PubMed Web of Science ↵ Niell  CM , Stryker  MP . Modulation of Visual Responses by Behavioral State in Mouse Visual Cortex . Neuron  65 : 472 – 479 , 2010 . OpenUrl CrossRef PubMed Web of Science ↵ O’Keefe  J , Dostrovsky  J.  The hippocampus as a spatial map: Preliminary evidence from unit activity in the freely-moving rat . Brain Res . 34 : 171 – 175 , 1971 . OpenUrl CrossRef PubMed Web of Science ↵ Oby  ER , Ethier  C , Miller  LE . Movement representation in the primary motor cortex and its contribution to generalizable EMG predictions . J. Neurophysiol . 109 : 666 – 678 , 2013 . OpenUrl CrossRef PubMed Web of Science ↵ Okatan  M , Wilson  MA , Brown  EN . Analyzing Functional Connectivity Using a Network Likelihood Model of Ensemble Neural Spiking Activity . Neural Comput . 17 : 1927 – 1961 , 2005 . OpenUrl CrossRef PubMed Web of Science ↵ Okun  M , Steinmetz  NA , Cossell  L , Iacaruso  MF , Ko  H , Barthó  P , Moore  T , Hofer  SB , Mrsic-Flogel  TD , Carandini  M , Harris  KD . Diverse coupling of neurons to populations in sensory cortex . Nature  521 : 511 – 515 , 2015 . OpenUrl CrossRef PubMed ↵ Omrani  M , Kaufman  MT , Hatsopoulos  NG , Cheney  PD . Perspectives on classical controversies about the motor cortex . J. Neurophysiol . 118 : 1828 – 1848 , 2017 . OpenUrl CrossRef PubMed ↵ Paninski  L.  Maximum likelihood estimation of cascade point-process neural encoding models . Netw. Comput. Neural Syst . 15 : 243 – 262 , 2004 . OpenUrl CrossRef ↵ Paninski  L , Ahmadian  Y , Ferreira  DG , Koyama  S , Rahnama Rad  K , Vidne  M , Vogelstein  J , Wu  W.  A new look at state-space models for neural data . J. Comput. Neurosci . 29 : 107 – 126 , 2010 . OpenUrl CrossRef PubMed Web of Science ↵ Park  IM , Archer  EW , Priebe  N , Pillow  JW . Spectral methods for neural characterization using generalized quadratic models [Online] . : 2454 – 2462 , 2013 . http://papers.nips.cc/paper/4993-spectral-methods-for-neural-characterization-using-generalized-quadratic-models [4 May. 2018]. ↵ Park  IM , Meister  MLR , Huk  AC , Pillow  JW . Encoding and decoding in parietal cortex during sensorimotor decision-making . Nat. Neurosci . 17 : 1395 – 1403 , 2014 . OpenUrl CrossRef PubMed ↵ Pearl  J.  Causal inference in statistics: An overview . Stat. Surv . 3 : 96 – 146 , 2009 . OpenUrl CrossRef ↵  Kenji Doya Alexandre  Pouget , and Rajesh  P.N.  Rao  SI. Pillow  J.  Likelihood-Based Approaches to Modeling the Neural Code. In: Bayesian brain: Probabilistic approaches to neural coding , edited by  Kenji Doya Alexandre  Pouget , and Rajesh  P.N.  Rao  SI.  MIT Press , 2007 , p. 53 – 70 . ↵ Pillow  JW , Shlens  J , Paninski  L , Sher  A , Litke  AM , Chichilnisky  EJ , Simoncelli  EP . Spatio-temporal correlations and visual signalling in a complete neuronal population . Nature  454 : 995 – 999 , 2008 . OpenUrl CrossRef PubMed Web of Science ↵ Pillow  JW , Simoncelli  EP . Biases in white noise analysis due to non-Poisson spike generation . Neurocomputing  52 – 54 : 109–115, 2003 . ↵ Putzky  P , Franzen  F , Bassetto  G , Macke  JH . A Bayesian model for identifying hierarchically organised states in neural population activity [Online]. : 3095–3103 , 2014 . http://papers.nips.cc/paper/5338-a-bayesian-model-for-identifying-hierarchically-organised-states-in-neural-population-activity [4 May. 2018]. ↵ Reimer  J , Froudarakis  E , Cadwell  CR , Yatsenko  D , Denfield  GH , Tolias  AS . Pupil Fluctuations Track Fast Switching of Cortical States during Quiet Wakefulness . Neuron  84 : 355 – 362 , 2014 . OpenUrl CrossRef PubMed ↵ Runyan  CA , Piasini  E , Panzeri  S , Harvey  CD . Distinct timescales of population coding across cortex . Nature  548 : 92 – 96 , 2017 . OpenUrl CrossRef PubMed ↵ Rust  NC , Movshon  JA . In praise of artifice . Nat. Neurosci . 8 : 1647 – 1650 , 2005 . OpenUrl CrossRef PubMed Web of Science ↵ Shmueli  G.  To Explain or to Predict?  Stat. Sci . 25 : 289 – 310 , 2010 . OpenUrl CrossRef Web of Science ↵ Smith  AC , Brown  EN . Estimating a State-Space Model from Point Process Observations . Neural Comput . 15 : 965 – 991 , 2003 . OpenUrl CrossRef PubMed Web of Science ↵ Smith  MA , Kohn  A.  Spatial and temporal scales of neuronal correlation in primary visual cortex . J. Neurosci . 28 : 12591 – 12603 , 2008 . OpenUrl Abstract / FREE Full Text ↵ Stevenson  IH , Cherian  A , London  BM , Sachs  NA , Lindberg  E , Reimer  J , Slutzky  MW , Hatsopoulos  NG , Miller  LE , Kording  KP . Statistical assessment of the stability of neural movement representations . J. Neurophysiol . 106 , 2011 . ↵ Stringer  C , Pachitariu  M , Steinmetz  N , Reddy  CB , Carandini  M , Harris  KD . Spontaneous behaviors drive multidimensional, brain-wide population activity . bioRxiv ( April 22, 2018 ).  doi: 10.1101/306019. OpenUrl CrossRef ↵ Tripathy  SJ , Padmanabhan  K , Gerkin  RC , Urban  NN . Intermediate intrinsic diversity enhances neural population coding . Proc. Natl. Acad. Sci. U. S. A . 110 : 8248 – 53 , 2013 . OpenUrl Abstract / FREE Full Text ↵ Truccolo  W , Eden  UT , Fellows  MR , Donoghue  JP , Brown  EN . A Point Process Framework for Relating Neural Spiking Activity to Spiking History, Neural Ensemble, and Extrinsic Covariate Effects . J. Neurophysiol . 93 : 1074 – 1089 , 2005 . OpenUrl CrossRef PubMed Web of Science ↵ Truccolo  W , Hochberg  LR , Donoghue  JP . Collective dynamics in human and monkey sensorimotor cortex: predicting single neuron spikes . Nat. Neurosci . 13 : 105 – 111 , 2010 . OpenUrl CrossRef PubMed Web of Science ↵ Ventura  V.  Traditional waveform based spike sorting yields biased rate code estimates . Proc. Natl. Acad. Sci . 106 : 6921 , 2009 . OpenUrl Abstract / FREE Full Text ↵ Vidne  M , Ahmadian  Y , Shlens  J , Pillow  JW , Kulkarni  J , Litke  AM , Chichilnisky  EJ , Simoncelli  E , Paninski  L.  Modeling the impact of common noise inputs on the network activity of retinal ganglion cells . J. Comput. Neurosci . 33 : 97 – 121 , 2012 . OpenUrl CrossRef PubMed ↵ Volgushev  M , Ilin  V , Stevenson  IH . Identifying and Tracking Simulated Synaptic Inputs from Neuronal Firing: Insights from In Vitro Experiments . PLOS Comput. Biol . 11 : e1004167 , 2015 . OpenUrl ↵ Walker  B , Kording  K.  The Database for Reaching Experiments and Models . PLoS One  8 : e78747 , 2013 . OpenUrl ↵ Walsh  RN , Cummins  RA . The open-field test: A critical review . Psychol. Bull . 83 : 482 – 504 , 1976 . OpenUrl CrossRef PubMed Web of Science Wasserman  L.  All of Statistics . Springer New York . ↵ Weber  AI , Pillow  JW . Capturing the Dynamical Repertoire of Single Neurons with Generalized Linear Models . Neural Comput . 29 : 3260 – 3289 , 2017 . OpenUrl ↵ Whiteway  MR , Butts  DA . Revealing unobserved factors underlying cortical activity with a rectified latent variable model applied to neural population recordings . J. Neurophysiol . 117 : 919 – 936 , 2017 . OpenUrl CrossRef PubMed ↵ Yoshihara  M , Yoshihara  M.  ‘Necessary and sufficient’ in biology is not necessarily necessary–confusions and erroneous conclusions resulting from misapplied logic in the field of biology, especially neuroscience . J. Neurogenet . 32 : 53 – 64 , 2018 . OpenUrl ↵ Zhao  M , Iyengar  S.  Nonconvergence in logistic and poisson models for neural spiking . Neural Comput . 22 : 1231 – 1244 , 2010 . OpenUrl CrossRef PubMed Web of Science View Abstract            View the discussion thread.      Back to top            Previous Next      Posted August 21, 2018.            Download PDF           Email      Thank you for your interest in spreading the word about bioRxiv. NOTE: Your email address is requested solely to identify you as the sender of this article.    Your Email *     Your Name *     Send To *   Enter multiple addresses on separate lines or separate them with commas.    You are going to email the following  Omitted variable bias in GLMs of neural spiking activity    Message Subject (Your Name) has forwarded a page to you from bioRxiv   Message Body (Your Name) thought you would like to see this page from the bioRxiv website.   Your Personal Message         CAPTCHA This question is for testing whether or not you are a human visitor and to prevent automated spam submissions.                 Share            Omitted variable bias in GLMs of neural spiking activity   Ian H.  Stevenson  bioRxiv 317511; doi: https://doi.org/10.1101/317511             Share This Article:       Copy                            Citation Tools         Omitted variable bias in GLMs of neural spiking activity   Ian H.  Stevenson  bioRxiv 317511; doi: https://doi.org/10.1101/317511      Citation Manager Formats   BibTeX Bookends EasyBib EndNote (tagged) EndNote 8 (xml) Medlars Mendeley Papers RefWorks Tagged Ref Manager RIS Zotero                       Tweet Widget Facebook Like Google Plus One     Subject Area   Neuroscience               Subject Areas          All Articles        Animal Behavior and Cognition  (1992)  Biochemistry  (3745)  Bioengineering  (2526)  Bioinformatics  (12280)  Biophysics  (5250)  Cancer Biology  (4087)  Cell Biology  (5858)  Clinical Trials  (138)  Developmental Biology  (3503)  Ecology  (5543)  Epidemiology  (2052)  Evolutionary Biology  (8377)  Genetics  (6345)  Genomics  (8079)  Immunology  (3269)  Microbiology  (9800)  Molecular Biology  (3870)  Neuroscience  (22816)  Paleontology  (166)  Pathology  (631)  Pharmacology and Toxicology  (1014)  Physiology  (1543)  Plant Biology  (3471)  Scientific Communication and Education  (811)  Synthetic Biology  (1085)  Systems Biology  (3309)  Zoology  (569)                                   "
9,proxy bias(fallacy OR bias OR heuristic) (coronavirus OR covid OR pandemic),2,https://www.brookings.edu/research/algorithmic-bias-detection-and-mitigation-best-practices-and-policies-to-reduce-consumer-harms/,"Introduction

The private and public sectors are increasingly turning to artificial intelligence (AI) systems and machine learning algorithms to automate simple and complex decision-making processes.1 The mass-scale digitization of data and the emerging technologies that use them are disrupting most economic sectors, including transportation, retail, advertising, and energy, and other areas. AI is also having an impact on democracy and governance as computerized systems are being deployed to improve accuracy and drive objectivity in government functions.

The availability of massive data sets has made it easy to derive new insights through computers. As a result, algorithms, which are a set of step-by-step instructions that computers follow to perform a task, have become more sophisticated and pervasive tools for automated decision-making.2 While algorithms are used in many contexts, we focus on computer models that make inferences from data about people, including their identities, their demographic attributes, their preferences, and their likely future behaviors, as well as the objects related to them.3

“Algorithms are harnessing volumes of macro- and micro-data to influence decisions affecting people in a range of tasks, from making movie recommendations to helping banks determine the creditworthiness of individuals.”

In the pre-algorithm world, humans and organizations made decisions in hiring, advertising, criminal sentencing, and lending. These decisions were often governed by federal, state, and local laws that regulated the decision-making processes in terms of fairness, transparency, and equity. Today, some of these decisions are entirely made or influenced by machines whose scale and statistical rigor promise unprecedented efficiencies. Algorithms are harnessing volumes of macro- and micro-data to influence decisions affecting people in a range of tasks, from making movie recommendations to helping banks determine the creditworthiness of individuals.4 In machine learning, algorithms rely on multiple data sets, or training data, that specifies what the correct outputs are for some people or objects. From that training data, it then learns a model which can be applied to other people or objects and make predictions about what the correct outputs should be for them.5

However, because machines can treat similarly-situated people and objects differently, research is starting to reveal some troubling examples in which the reality of algorithmic decision-making falls short of our expectations. Given this, some algorithms run the risk of replicating and even amplifying human biases, particularly those affecting protected groups.6 For example, automated risk assessments used by U.S. judges to determine bail and sentencing limits can generate incorrect conclusions, resulting in large cumulative effects on certain groups, like longer prison sentences or higher bails imposed on people of color.

In this example, the decision generates “bias,” a term that we define broadly as it relates to outcomes which are systematically less favorable to individuals within a particular group and where there is no relevant difference between groups that justifies such harms.7 Bias in algorithms can emanate from unrepresentative or incomplete training data or the reliance on flawed information that reflects historical inequalities. If left unchecked, biased algorithms can lead to decisions which can have a collective, disparate impact on certain groups of people even without the programmer’s intention to discriminate. The exploration of the intended and unintended consequences of algorithms is both necessary and timely, particularly since current public policies may not be sufficient to identify, mitigate, and remedy consumer impacts.

With algorithms appearing in a variety of applications, we argue that operators and other concerned stakeholders must be diligent in proactively addressing factors which contribute to bias. Surfacing and responding to algorithmic bias upfront can potentially avert harmful impacts to users and heavy liabilities against the operators and creators of algorithms, including computer programmers, government, and industry leaders. These actors comprise the audience for the series of mitigation proposals to be presented in this paper because they either build, license, distribute, or are tasked with regulating or legislating algorithmic decision-making to reduce discriminatory intent or effects.

Our research presents a framework for algorithmic hygiene, which identifies some specific causes of biases and employs best practices to identify and mitigate them. We also present a set of public policy recommendations, which promote the fair and ethical deployment of AI and machine learning technologies.

This paper draws upon the insight of 40 thought leaders from across academic disciplines, industry sectors, and civil society organizations who participated in one of two roundtables.8 Roundtable participants actively debated concepts related to algorithmic design, accountability, and fairness, as well as the technical and social trade-offs associated with various approaches to bias detection and mitigation.

Our goal is to juxtapose the issues that computer programmers and industry leaders face when developing algorithms with the concerns of policymakers and civil society groups who assess their implications. To balance the innovations of AI and machine learning algorithms with the protection of individual rights, we present a set of public policy recommendations, self-regulatory best practices, and consumer-focused strategies–all of which promote the fair and ethical deployment of these technologies.

Our public policy recommendations include the updating of nondiscrimination and civil rights laws to apply to digital practices, the use of regulatory sandboxes to foster anti-bias experimentation, and safe harbors for using sensitive information to detect and mitigate biases. We also outline a set of self-regulatory best practices, such as the development of a bias impact statement, inclusive design principles, and cross-functional work teams. Finally, we propose additional solutions focused on algorithmic literacy among users and formal feedback mechanisms to civil society groups.

The next section provides five examples of algorithms to explain the causes and sources of their biases. Later in the paper, we discuss the trade-offs between fairness and accuracy in the mitigation of algorithmic bias, followed by a robust offering of self-regulatory best practices, public policy recommendations, and consumer-driven strategies for addressing online biases. We conclude by highlighting the importance of proactively tackling the responsible and ethical use of machine learning and other automated decision-making tools.

Examples of algorithmic biases

Algorithmic bias can manifest in several ways with varying degrees of consequences for the subject group. Consider the following examples, which illustrate both a range of causes and effects that either inadvertently apply different treatment to groups or deliberately generate a disparate impact on them.

Bias in online recruitment tools

Online retailer Amazon, whose global workforce is 60 percent male and where men hold 74 percent of the company’s managerial positions, recently discontinued use of a recruiting algorithm after discovering gender bias.9 The data that engineers used to create the algorithm were derived from the resumes submitted to Amazon over a 10-year period, which were predominantly from white males. The algorithm was taught to recognize word patterns in the resumes, rather than relevant skill sets, and these data were benchmarked against the company’s predominantly male engineering department to determine an applicant’s fit. As a result, the AI software penalized any resume that contained the word “women’s” in the text and downgraded the resumes of women who attended women’s colleges, resulting in gender bias.10

Bias in word associations

Princeton University researchers used off-the-shelf machine learning AI software to analyze and link 2.2 million words. They found that European names were perceived as more pleasant than those of African-Americans, and that the words “woman” and “girl” were more likely to be associated with the arts instead of science and math, which were most likely connected to males.11 In analyzing these word-associations in the training data, the machine learning algorithm picked up on existing racial and gender biases shown by humans. If the learned associations of these algorithms were used as part of a search-engine ranking algorithm or to generate word suggestions as part of an auto-complete tool, it could have a cumulative effect of reinforcing racial and gender biases.

Bias in online ads

Latanya Sweeney, Harvard researcher and former chief technology officer at the Federal Trade Commission (FTC), found that online search queries for African-American names were more likely to return ads to that person from a service that renders arrest records, as compared to the ad results for white names.12 Her research also found that the same differential treatment occurred in the micro-targeting of higher-interest credit cards and other financial products when the computer inferred that the subjects were African-Americans, despite having similar backgrounds to whites.13 During a public presentation at a FTC hearing on big data, Sweeney demonstrated how a web site, which marketed the centennial celebration of an all-black fraternity, received continuous ad suggestions for purchasing “arrest records” or accepting high-interest credit card offerings.14

Bias in facial recognition technology

MIT researcher Joy Buolamwini found that the algorithms powering three commercially available facial recognition software systems were failing to recognize darker-skinned complexions.15 Generally, most facial recognition training data sets are estimated to be more than 75 percent male and more than 80 percent white. When the person in the photo was a white man, the software was accurate 99 percent of the time at identifying the person as male. According to Buolamwini’s research, the product error rates for the three products were less than one percent overall, but increased to more than 20 percent in one product and 34 percent in the other two in the identification of darker-skinned women as female.16 In response to Buolamwini’s facial-analysis findings, both IBM and Microsoft committed to improving the accuracy of their recognition software for darker-skinned faces.

Bias in criminal justice algorithms

Acknowledging the possibility and causes of bias is the first step in any mitigation approach.

The COMPAS (Correctional Offender Management Profiling for Alternative Sanctions) algorithm, which is used by judges to predict whether defendants should be detained or released on bail pending trial, was found to be biased against African-Americans, according to a report from ProPublica.17 The algorithm assigns a risk score to a defendant’s likelihood to commit a future offense, relying on the voluminous data available on arrest records, defendant demographics, and other variables. Compared to whites who were equally likely to re-offend, African-Americans were more likely to be assigned a higher-risk score, resulting in longer periods of detention while awaiting trial.18 Northpointe, the firm that sells the algorithm’s outputs, offers evidence to refute such claims and argues that wrong metrics are being used to assess fairness in the product, a topic that we return to later in the paper.

While these examples of bias are not exhaustive, they suggest that these problems are empirical realities and not just theoretical concerns. They also illustrate how these outcomes emerge, and in some cases, without malicious intent by the creators or operators of the algorithm. Acknowledging the possibility and causes of bias is the first step in any mitigation approach. On this point, roundtable participant Ricardo Baeza-Yates from NTENT stated that “[companies] will continue to have a problem discussing algorithmic bias if they don’t refer to the actual bias itself.”

Causes of bias

Barocas and Selbst point out that bias can creep in during all phases of a project, “…whether by specifying the problem to be solved in ways that affect classes differently, failing to recognize or address statistical biases, reproducing past prejudice, or considering an insufficiently rich set of factors.”19 Roundtable participants focused especially on bias stemming from flaws in the data used to train the algorithms. “Flawed data is a big problem,” stated roundtable participant Lucy Vasserman from Google, “…especially for the groups that businesses are working hard to protect.” While there are many causes, we focus on two of them: historical human biases and incomplete or unrepresentative data.

Historical human biases

Historical human biases are shaped by pervasive and often deeply embedded prejudices against certain groups, which can lead to their reproduction and amplification in computer models. In the COMPAS algorithm, if African-Americans are more likely to be arrested and incarcerated in the U.S. due to historical racism, disparities in policing practices, or other inequalities within the criminal justice system, these realities will be reflected in the training data and used to make suggestions about whether a defendant should be detained. If historical biases are factored into the model, it will make the same kinds of wrong judgments that people do.

The Amazon recruitment algorithm revealed a similar trajectory when men were the benchmark for professional “fit,” resulting in female applicants and their attributes being downgraded. These historical realities often find their way into the algorithm’s development and execution, and they are exacerbated by the lack of diversity which exists within the computer and data science fields.20

Further, human biases can be reinforced and perpetuated without the user’s knowledge. For example, African-Americans who are primarily the target for high-interest credit card options might find themselves clicking on this type of ad without realizing that they will continue to receive such predatory online suggestions. In this and other cases, the algorithm may never accumulate counter-factual ad suggestions (e.g., lower-interest credit options) that the consumer could be eligible for and prefer. Thus, it is important for algorithm designers and operators to watch for such potential negative feedback loops that cause an algorithm to become increasingly biased over time.

Incomplete or unrepresentative training data

Insufficient training data is another cause of algorithmic bias. If the data used to train the algorithm are more representative of some groups of people than others, the predictions from the model may also be systematically worse for unrepresented or under-representative groups. For example, in Buolamwini’s facial-analysis experiments, the poor recognition of darker-skinned faces was largely due to their statistical under-representation in the training data. That is, the algorithm presumably picked up on certain facial features, such as the distance between the eyes, the shape of the eyebrows and variations in facial skin shades, as ways to detect male and female faces. However, the facial features that were more representative in the training data were not as diverse and, therefore, less reliable to distinguish between complexions, even leading to a misidentification of darker-skinned females as males.

Turner Lee has argued that it is often the lack of diversity among the programmers designing the training sample which can lead to the under-representation of a particular group or specific physical attributes.21 Buolamwini’s findings were due to her rigor in testing, executing, and assessing a variety of proprietary facial-analysis software in different settings, correcting for the lack of diversity in their samples.

Conversely, algorithms with too much data, or an over-representation, can skew the decision toward a particular result. Researchers at Georgetown Law School found that an estimated 117 million American adults are in facial recognition networks used by law enforcement, and that African-Americans were more likely to be singled out primarily because of their over-representation in mug-shot databases.22 Consequently, African-American faces had more opportunities to be falsely matched, which produced a biased effect.

Bias detection strategies

Understanding the various causes of biases is the first step in the adoption of effective algorithmic hygiene. But, how can operators of algorithms assess whether their results are, indeed, biased? Even when flaws in the training data are corrected, the results may still be problematic because context matters during the bias detection phase.

“Even when flaws in the training data are corrected, the results may still be problematic because context matters during the bias detection phase.”

First, all detection approaches should begin with careful handling of the sensitive information of users, including data that identify a person’s membership in a federally protected group (e.g., race, gender). In some cases, operators of algorithms may also worry about a person’s membership in some other group if they are also susceptible to unfair outcomes. An examples of this could be college admission officers worrying about the algorithm’s exclusion of applicants from lower-income or rural areas; these are individuals who may be not federally protected but do have susceptibility to certain harms (e.g., financial hardships).

In the former case, systemic bias against protected classes can lead to collective, disparate impacts, which may have a basis for legally cognizable harms, such as the denial of credit, online racial profiling, or massive surveillance.23 In the latter case, the outputs of the algorithm may produce unequal outcomes or unequal error rates for different groups, but they may not violate legal prohibitions if there was no intent to discriminate.

These problematic outcomes should lead to further discussion and awareness of how algorithms work in the handling of sensitive information, and the trade-offs around fairness and accuracy in the models.

Algorithms and sensitive information

While it is intuitively appealing to think that an algorithm can be blind to sensitive attributes, this is not always the case.24 Critics have pointed out that an algorithm may classify information based on online proxies for the sensitive attributes, yielding a bias against a group even without making decisions directly based on one’s membership in that group. Barocas and Selbst define online proxies as “factors used in the scoring process of an algorithm which are mere stand-ins for protected groups, such as zip code as proxies for race, or height and weight as proxies for gender.”25 They argue that proxies often linked to algorithms can produce both errors and discriminatory outcomes, such as instances where a zip code is used to determine digital lending decisions or one’s race triggers a disparate outcome.26 Facebook’s advertising platform contained proxies that allowed housing marketers to micro-target preferred renters and buyers by clicking off data points, including zip code preferences.27 Thus, it is possible that an algorithm which is completely blind to a sensitive attribute could actually produce the same outcome as one that uses the attribute in a discriminatory manner.

“While it is intuitively appealing to think that an algorithm can be blind to sensitive attributes, this is not always the case.”

For example, Amazon made a corporate decision to exclude certain neighborhoods from its same-day Prime delivery system. Their decision relied upon the following factors: whether a particular zip code had a sufficient number of Prime members, was near a warehouse, and had sufficient people willing to deliver to that zip code.28 While these factors corresponded with the company’s profitability model, they resulted in the exclusion of poor, predominantly African-American neighborhoods, transforming these data points into proxies for racial classification. The results, even when unintended, discriminated against racial and ethnic minorities who were not included.

Similarly, a job-matching algorithm may not receive the gender field as an input, but it may produce different match scores for two resumes that differ only in the substitution of the name “Mary” for “Mark” because the algorithm is trained to make these distinctions over time.

There are also arguments that blinding the algorithm to sensitive attributes can cause algorithmic bias in some situations. Corbett-Davies and Goel point out in their research on the COMPAS algorithm that even after controlling for “legitimate” risk factors, empirically women have been found to re-offend less often than men in many jurisdictions.29 If an algorithm is forbidden from reporting a different risk assessment score for two criminal defendants who differ only in their gender, judges may be less likely to release female defendants than male defendants with equal actual risks of committing another crime before trial. Thus, blinding the algorithm from any type of sensitive attribute may not solve bias.

While roundtable participants were not in agreement on the use of online proxies in modeling, they largely agreed that operators of algorithms must be more transparent in their handling of sensitive information, especially if the potential proxy could itself be a legal classificatory harm.30 There was also discussion that the use of sensitive attributes as part of an algorithm could be a strategy for detecting and possibly curing intended and unintentional biases. Because currently doing so may be constrained by privacy regulations, such as the European Union’s General Data Protection Rules (GDPR) or proposed U.S. federal privacy legislation, the argument could be made for the use of regulatory sandboxes and safe harbors to allow the use of sensitive information when detecting and mitigating biases, both of which will be introduced as part of our policy recommendations.

Detecting bias

When detecting bias, computer programmers normally examine the set of outputs that the algorithm produces to check for anomalous results. Comparing outcomes for different groups can be a useful first step. This could even be done through simulations. Roundtable participant Rich Caruana from Microsoft suggested that companies consider the simulation of predictions (both true and false) before applying them to real-life scenarios. “We almost need a secondary data collection process because sometimes the model will [emit] something quite different,” he shared. For example, if a job-matching algorithm’s average score for male applicants is higher than that for women, further investigation and simulations could be warranted.

However, the downside of these approaches is that not all unequal outcomes are unfair. Roundtable participant Solon Barocas from Cornell University summed this up when he stated, “Maybe we find out that we have a very accurate model, but it still produces disparate outcomes. This may be unfortunate, but is it fair?” An alternative to accounting for unequal outcomes may be to look at the equality of error rates, and whether there are more mistakes for one group of people than another. On this point, Isabel Kloumann of Facebook shared that “society has expectations. One of which is not incarcerating one minority group disproportionately [as a result of an algorithm].”

As shown in the debates around the COMPAS algorithm, even error rates are not a simple litmus test for biased algorithms. Northpointe, the company that developed the COMPAS algorithm, refutes claims of racial discrimination. They argue that among defendants assigned the same high risk score, African-American and white defendants have almost equal recidivism rates, so by that measure, there is no error in the algorithm’s decision.31 In their view, judges can consider their algorithm without any reference to race in bail and release decisions.

It is not possible, in general, to have equal error rates between groups for all the different error rates.32 ProPublica focused on one error rate, while Northpointe honed in on another. Thus, some principles need to be established for which error rates should be equalized in which situations in order to be fair.

However, distinguishing between how the algorithm works with sensitive information and potential errors can be problematic for operators of algorithms, policymakers, and civil society groups.33 “Companies would be losing a lot if we don’t draw a distinction between the two,” said Julie Brill from Microsoft. At the very least, there was agreement among roundtable participants that algorithms should not perpetuate historical inequities, and that more work needs to be done to address online discrimination.34

Fairness and accuracy trade-offs

Next, a discussion of trade-offs and ethics is needed. Here, the focus should be on evaluating both societal notions of “fairness” and possible social costs. In their research of the COMPAS algorithm, Corbett-Davies, Goel, Pierson, Feller, and Huq see “an inherent tension between minimizing violent crime and satisfying common notions of fairness.”35 They conclude that optimizing for public safety yields decisions that penalize defendants of color, while satisfying legal and societal fairness definitions, and may lead to more releases of high-risk defendants, which would adversely affect public safety.36 Moreover, the negative impacts on public safety might also disproportionately affect African-American and white neighborhoods, thus creating a fairness cost as well.

If the goal is to avoid reinforcing inequalities, what, then, should developers and operators of algorithms do to mitigate potential biases? We argue that developers of algorithms should first look for ways to reduce disparities between groups without sacrificing the overall performance of the model, especially whenever there appears to be a trade-off.

A handful of roundtable participants argued that opportunities exist for improving both fairness and accuracy in algorithms. For programmers, the investigation of apparent bugs in the software may reveal why the model was not maximizing for overall accuracy. The resolution of these bugs can then improve overall accuracy. Data sets, which may be under-representative of certain groups, may need additional training data to improve accuracy in the decision-making and reduce unfair results. Buolamwini’s facial detection experiments are good examples of this type of approach to fairness and accuracy.

Roundtable participant Sarah Holland from Google pointed out the risk tolerance associated with these types of trade-offs when she shared that “[r]aising risk also involves raising equity issues.” Thus, companies and other operators of algorithms should determine if the social costs of the trade-offs are justified, the stakeholders involved are amenable to a solution through algorithms, or if human decision-makers are needed to frame the solution.

Ethical frameworks matter

What is fundamentally behind these fairness and accuracy trade-offs should be discussions around ethical frameworks and potential guardrails for machine learning tasks and systems. There are several ongoing and recent international and U.S.-based efforts to develop ethical governance standards for the use of AI.37 The 35-member Organization for Economic Cooperation and Development (OECD) is expected shortly to release its own guidelines for ethical AI.38 The European Union recently released “Ethics Guidelines for Trustworthy AI,” which delineates seven governance principles: (1) human agency and oversight, (2) technical robustness and safety, (3) privacy and data governance, (4) transparency, (5) diversity, nondiscrimination and fairness, (6) environmental and societal well-being, and (7) accountability.39 The EU’s ethical framework reflects a clear consensus that it is unethical to “unfairly discriminate.” Within these guidelines, member states link diversity and nondiscrimination with principles of fairness, enabling inclusion and diversity throughout the entire AI system’s lifecycle. Their principles interpret fairness through the lenses of equal access, inclusive design processes, and equal treatment.

Yet, even with these governmental efforts, it is still surprisingly difficult to define and measure fairness.40 While it will not always be possible to satisfy all notions of fairness at the same time, companies and other operators of algorithms must be aware that there is no simple metric to measure fairness that a software engineer can apply, especially in the design of algorithms and the determination of the appropriate trade-offs between accuracy and fairness. Fairness is a human, not a mathematical, determination, grounded in shared ethical beliefs. Thus, algorithmic decisions that may have a serious consequence for people will require human involvement.

For example, while the training data discrepancies in the COMPAS algorithm can be corrected, human interpretation of fairness still matters. For that reason, while an algorithm such as COMPAS may be a useful tool, it cannot substitute for the decision-making that lies within the discretion of the human arbiter.41 We believe that subjecting the algorithm to rigorous testing can challenge the different definitions of fairness, a useful exercise among companies and other operators of algorithms.

“It’s important for algorithm operators and developers to always be asking themselves: Will we leave some groups of people worse off as a result of the algorithm’s design or its unintended consequences?“

In the decision to create and bring algorithms to market, the ethics of likely outcomes must be considered—especially in areas where governments, civil society, or policymakers see potential for harm, and where there is a risk of perpetuating existing biases or making protected groups more vulnerable to existing societal inequalities. That is why it’s important for algorithm operators and developers to always be asking themselves: Will we leave some groups of people worse off as a result of the algorithm’s design or its unintended consequences?

We suggest that this question is one among many that the creators and operators of algorithms should consider in the design, execution, and evaluation of algorithms, which are described in the following mitigation proposals. Our first proposal addresses the updating of U.S. nondiscrimination laws to apply to the digital space.

Mitigation proposals



Nondiscrimination and other civil rights laws should be updated to interpret and redress online disparate impacts

To develop trust from policymakers, computer programmers, businesses, and other operators of algorithms must abide by U.S. laws and statutes that currently forbid discrimination in public spaces. Historically, nondiscrimination laws and statutes unambiguously define the thresholds and parameters for the disparate treatment of protected classes. The 1964 Civil Rights Act “forbade discrimination on the basis of sex as well as race in hiring, promoting, and firing.” The 1968 Fair Housing Act prohibits discrimination in the sale, rental, and financing of dwellings, and in other housing-related transactions to federally protected classes. Enacted in 1974, the Equal Credit Opportunity Act stops any creditor from discriminating against any applicant from any type of credit transaction based on protected characteristics. While these laws do not necessarily mitigate and resolve other implicit or unconscious biases that can be baked into algorithms, companies and other operators should guard against violating these statutory guardrails in the design of algorithms, as well as mitigating their implicit concern to prevent past discrimination from continuing.

Roundtable participant Wendy Anderson from the Office of Congresswoman Val Demings stated, “[T]ypically, legislators only hear when something bad happens. We need to find a way to protect those who need it without stifling innovation.” Congress can clarify how these nondiscrimination laws apply to the types of grievances recently found in the digital space, since most of these laws were written before the advent of the internet.42 Such legislative action can provide clearer guardrails that are triggered when algorithms are contributing to legally recognizable harms. Moreover, when creators and operators of algorithms understand that these may be more or less non-negotiable factors, the technical design will be more thoughtful in moving away from models that may trigger and exacerbate explicit discrimination, such as design frames that exclude rather than include certain inputs or are not checked for bias.43

Operators of algorithms must develop a bias impact statement

Once the idea for an algorithm has been vetted against nondiscrimination laws, we suggest that operators of algorithms develop a bias impact statement, which we offer as a template of questions that can be flexibly applied to guide them through the design, implementation, and monitoring phases.

As a self-regulatory practice, the bias impact statement can help probe and avert any potential biases that are baked into or are resultant from the algorithmic decision. As a best practice, operators of algorithms should brainstorm a core set of initial assumptions about the algorithm’s purpose prior to its development and execution. We propose that operators apply the bias impact statement to assess the algorithm’s purpose, process and production, where appropriate. Roundtable participants also suggested the importance of establishing a cross-functional and interdisciplinary team to create and implement the bias impact statement.

New York University’s AI Now Institute

New York University’s AI Now Institute has already introduced a model framework for governmental entities to use to create algorithmic impact assessments (AIAs), which evaluate the potential detrimental effects of an algorithm in the same manner as environmental, privacy, data, or human rights impact statements.44 While there may be differences in implementation given the type of predictive model, the AIA encompasses multiple rounds of review from internal, external, and public audiences. First, it assumes that after this review, a company will develop a list of potential harms or biases in their self-assessment, with the assistance of more technical outside experts. Second, if bias appears to have occurred, the AIA pushes for notice to be given to impacted populations and a comment period opened for response. And third, the AIA process looks to federal and other entities to support users’ right to challenge algorithmic decisions that feel unfair.

While the AIA process supports a substantive feedback loop, what may be missing is both the required forethought leading up to the decision and the oversight of the algorithm’s provisions. Moreover, our proposed bias impact statement starts with a framework that identifies which automated decisions should be subjected to such scrutiny, operator incentives, and stakeholder engagement.

Which automated decisions?

In the case of determining which automated decisions require such vetting, operators of algorithms should start with questions about whether there will be a possible negative or unintended outcome resulting from the algorithm, for whom, and the severity of consequences for members of the affected group if not detected and mitigated. Reviewing established legal protections around fair housing, employment, credit, criminal justice, and health care should serve as a starting point for determining which decisions need to be viewed with special caution in designing and testing any algorithm used to predict outcomes or make important eligibility decisions about access to a benefit. This is particularly true considering the legal prescriptions against using data that has a likelihood of disparate impact on a protected class or other established harms. Thus, we suggest that operators should be constantly questioning the potential legal, social, and economic effects and potential liabilities associated with that choice when determining which decisions should be automated and how to automate them with minimal risks.

What are the user incentives?

Incentives should also drive organizations to proactively address algorithmic bias. Conversely, operators who create and deploy algorithms that generate fairer outcomes should also be recognized by policymakers and consumers who will trust them more for their practices. When companies exercise effective algorithmic hygiene before, during, and after introducing algorithmic decision-making, they should be rewarded and potentially given a public-facing acknowledgement for best practices.

How are stakeholders being engaged?

Finally, the last element encapsulated in a bias impact statement should involve the engagement of stakeholders who could help computer programmers in the selection of inputs and outputs of certain automated decisions. “Tech succeeds when users understand the product better than its designers,” said Rich Caruana from Microsoft. Getting users engaged early and throughout the process will prompt improvements to the algorithms, which ultimately leads to improved user experiences.

Stakeholder responsibilities can also extend to civil society organizations who can add value in the conversation on the algorithm’s design. “Companies [should] engage civil society,” shared Miranda Bogen from Upturn. “Otherwise, they will go to the press and regulators with their complaints.” A possible solution for operators of algorithms could be the development of an advisory council of civil society organizations that, working alongside companies, may be helpful in defining the scope of the procedure and predicting biases based on their ground-level experiences.

The template for the bias impact statement

These three foundational elements for a bias impact statement are reflected in a discrete set of questions that operators should answer during the design phase to filter out potential biases (Table 1). As a self-regulatory framework, computer programmers and other operators of algorithms can construct this type of tool prior to the model’s design and execution.

Table 1. Design questions template for bias impact statement

What will the automated decision do? Who is the audience for the algorithm and who will be most affected by it? Do we have training data to make the correct predictions about the decision? Is the training data sufficiently diverse and reliable? What is the data lifecycle of the algorithm? Which groups are we worried about when it comes to training data errors, disparate treatment, and impact? How will potential bias be detected? How and when will the algorithm be tested? Who will be the targets for testing? What will be the threshold for measuring and correcting for bias in the algorithm, especially as it relates to protected groups? What are the operator incentives? What will we gain in the development of the algorithm? What are the potential bad outcomes and how will we know? How open (e.g., in code or intent) will we make the design process of the algorithm to internal partners, clients, and customers? What intervention will be taken if we predict that there might be bad outcomes associated with the development or deployment of the algorithm? How are other stakeholders being engaged? What’s the feedback loop for the algorithm for developers, internal partners and customers? Is there a role for civil society organizations in the design of the algorithm? Has diversity been considered in the design and execution? Will the algorithm have implications for cultural groups and play out differently in cultural contexts? Is the design team representative enough to capture these nuances and predict the application of the algorithm within different cultural contexts? If not, what steps are being taken to make these scenarios more salient and understandable to designers? Given the algorithm’s purpose, is the training data sufficiently diverse? Are there statutory guardrails that companies should be reviewing to ensure that the algorithm is both legal and ethical?

Diversity-in-design

Operators of algorithms should also consider the role of diversity within their work teams, training data, and the level of cultural sensitivity within their decision-making processes. Employing diversity in the design of algorithms upfront will trigger and potentially avoid harmful discriminatory effects on certain protected groups, especially racial and ethnic minorities. While the immediate consequences of biases in these areas may be small, the sheer quantity of digital interactions and inferences can amount to a new form of systemic bias. Therefore, the operators of algorithms should not discount the possibility or prevalence of bias and should seek to have a diverse workforce developing the algorithm, integrate inclusive spaces within their products, or employ “diversity-in-design,” where deliberate and transparent actions will be taken to ensure that cultural biases and stereotypes are addressed upfront and appropriately. Adding inclusivity into the algorithm’s design can potentially vet the cultural inclusivity and sensitivity of the algorithms for various groups and help companies avoid what can be litigious and embarrassing algorithmic outcomes.

The bias impact statement should not be an exhaustive tool. For algorithms with more at stake, ongoing review of their execution should be factored into the process. The goal here is to monitor for disparate impacts resulting from the model that border on unethical, unfair, and unjust decision-making. When the process of identifying and forecasting the purpose of the algorithm is achieved, a robust feedback loop will aid in the detection of bias, which leads to the next recommendation promoting regular audits.

Other self-regulatory best practices



Operators of algorithms should regularly audit for bias

The formal and regular auditing of algorithms to check for bias is another best practice for detecting and mitigating bias. On the importance of these audits, roundtable participant Jon Kleinberg from Cornell University shared that “[a]n algorithm has no choice but to be premeditated.” Audits prompt the review of both input data and output decisions, and when done by a third-party evaluator, they can provide insight into the algorithm’s behavior. While some audits may require technical expertise, this may not always be the case. Facial recognition software that misidentifies persons of color more than whites is an instance where a stakeholder or user can spot biased outcomes, without knowing anything about how the algorithm makes decisions. “We should expect computers to have an audit trail,” shared roundtable participant Miranda Bogen from Upturn. Developing a regular and thorough audit of the data collected for the algorithmic operation, along with responses from developers, civil society, and others impacted by the algorithm, will better detect and possibly deter biases.

“Developing a regular and thorough audit of the data collected for the algorithmic operation, along with responses from developers, civil society, and others impacted by the algorithm, will better detect and possibly deter biases.”

The experience of government officials in Allegheny County reflects the importance of third-party auditing. In 2016, the Department of Human Services launched a decision support tool, the Allegheny Family Screening Tool (AFST), to generate a score for which children are most likely to be removed from their homes within two years, or to be re-referred to the county’s child welfare office due to suspected abuse. The county took ownership of its use of the tool, worked collaboratively with the developer, and commissioned an independent evaluation of its direct and indirect effects on the maltreatment screening process, including decision accuracy, workload, and consistency. County officials also sought additional independent research from experts to determine if the software was discriminating against certain groups. In 2017, the findings did identify some statistical imbalances, with error rates higher across racial and ethnic groups. White children who were scored at the highest-risk of maltreatment were less likely to be removed from their homes compared to African-American children with similar risk scores.45 The county responded to these findings as part of the rebuild of the tool, with version two implemented in November 2018.46

Facebook recently completed a civil rights audit to determine its handling of issues and individuals from protected groups.47 After the reveal of how the platform was handling a variety of issues, including voter suppression, content moderation, privacy, and diversity, the company has committed to an updated audit around its internal infrastructure to handle civil rights grievances and address diversity in its products’ designs by default. Recent actions by Facebook to ban white nationalist content or address disinformation campaigns are some of the results of these efforts.48

Operators of algorithms must rely upon cross-functional work teams and expertise

Roundtable participants largely acknowledged the notion that organizations should employ cross-functional teams. But movement in this direction can be difficult in already-siloed organizations, despite the technical, societal, and possibly legal implications associated with the algorithm’s design and execution. Not all decisions will necessitate this type of cross-team review, but when these decisions carry risks of real harm, they should be employed. In the mitigation of bias and the management of the risks associated with the algorithm, collaborative work teams can compensate for the blind-spots often missed in smaller, segmented conversations and reviews. Bringing together experts from various departments, disciplines, and sectors will help facilitate accountability standards and strategies for mitigating online biases, including from engineering, legal, marketing, strategy, and communications.

Cross-functional work teams–whether internally driven or populated by external experts–can attempt to identify bias before and during the model’s rollout. Further, partnerships between the private sector, academics, and civil society organizations can also facilitate greater transparency in AI’s application to a variety of scenarios, particularly those that impact protected classes or are disseminated in the public interest. Kate Crawford, AI researcher and founder of the AI Now Partnership, suggested that “closed loops are not open for algorithmic auditing, for review, or for public debate” because they generally exacerbate the problems that they are trying to solve.49 Further on this point, roundtable participant Natasha Duarte from the Center for Democracy and Technology spoke to Allegheny’s challenge when she shared, “[C]ompanies should be more forthcoming with describing the limits of their tech, and government should know what questions to ask in their assessments,” which speaks to the importance of more collaboration in this area.

Increase human involvement in the design and monitoring of algorithms

Even with all the precautionary measures listed above, there is still some risk that algorithms will make biased decisions. People will continue to play a role in identifying and correcting biased outcomes long after an algorithm is developed, tested, and launched. While more data can inform automated decision-making, this process should complement rather than fully replace human judgement. Roundtable participant Alex Peysakhovich from Facebook shared, “[W]e don’t need to eliminate human moderators. We need to hire more and get them to focus on edge cases.” Such sentiment is growing increasingly important in this field as the comparative advantages of humans and algorithms become more distinguishable and the use of both improves the outcomes for online users.

However, privacy implications will arise when more humans are engaged in algorithm management, particularly if more sensitive information is involved in the model’s creation or in testing the algorithm’s predictions for bias. The timing of the roundtables, which also transpired around the adoption of the EU’s GDPR, spoke to the need for increased consumer privacy principles where users are empowered over what data they want to share with companies. As the U.S. currently debates the need for federal privacy legislation, access to and use of personal data may become even more difficult, potentially leaving algorithmic models prone to more bias. Because the values of creators and users of algorithms shift over time, humans must arbitrate conflicts between outcomes and stated goals. In addition to periodical audits, human involvement provides continuous feedback on the performance of bias mitigation efforts.

Other public policy recommendations

As indicated throughout the paper, policymakers play a critical role in identifying and mitigating biases, while ensuring that the technologies continue to make positive economic and societal benefits.

Congress should implement regulatory sandboxes and safe harbors to curb online biases

Regulatory sandboxes are perceived as one strategy for the creation of temporary reprieves from regulation to allow the technology and rules surrounding its use to evolve together. These policies could apply to algorithmic bias and other areas where the technology in question has no analog covered by existing regulations. Rather than broaden the scope of existing regulations or create rules in anticipation of potential harms, a sandbox allows for innovation both in technology and its regulation. Even in a highly regulated industry, the creation of sandboxes where innovations can be tested alongside with lighter touch regulations can yield benefits.

“Rather than broaden the scope of existing regulations or create rules in anticipation of potential harms, a sandbox allows for innovation both in technology and its regulation.”

For example, companies within the financial sector that are leveraging technology, or fintech, have shown how regulatory sandboxes can spur innovation in the development of new products and services.50 These companies make extensive use of algorithms for everything from spotting fraud to deciding to extend credit. Some of these activities mirror those of regular banks, and those would still fall under existing rules, but new ways of approaching tasks would be allowed within the sandbox.51 Because sandboxes give innovators greater leeway in developing new products and services, they will require active oversight until technology and regulations mature. The U.S. Treasury recently reported not only on the benefits that countries that have adopted fintech regulatory sandboxes have realized, but recommended that the U.S. adopt fintech sandboxes to spur innovation.52 Given the broad usefulness of algorithms to spur innovation in various regulated industries, participants in the roundtables considered the potential usefulness of extending regulatory sandboxes to other areas where algorithms can help to spur innovations.

Regulatory safe harbors could also be employed, where a regulator could specify which activities do not violate existing regulations.53 This approach has the advantage of increasing regulatory certainty for algorithm developers and operators. For example, Section 230 of the Communications Decency Act removed liability from websites for the actions of their users, a provision widely credited with the growth of internet companies like Facebook and Google. The exemption later narrowed to exclude sex trafficking with the passage of the Stop Enabling Online Sex Trafficking Act and Fight Online Sex Trafficking Act. Applying a similar approach to algorithms could exempt their operators from liabilities in certain contexts while still upholding protections in others where harms are easier to identify. In line with the previous discussion on the use of certain protected attributes, safe harbors could be considered in instances where the collection of sensitive personal information is used for the specific purposes of bias detection and mitigation.

Consumers need better algorithmic literacy

Widespread algorithmic literacy is crucial for mitigating bias. Given the increased use of algorithms in many aspects of daily life, all potential subjects of automated decisions would benefit from knowledge of how these systems function. Just as computer literacy is now considered a vital skill in the modern economy, understanding how algorithms use their data may soon become necessary.

The subjects of automated decisions deserve to know when bias negatively affects them, and how to respond when it occurs. Feedback from users can share and anticipate areas where bias can manifest in existing and future algorithms. Over time, the creators of algorithms may actively solicit feedback from a wide range of data subjects and then take steps to educate the public on how algorithms work to aid in this effort. Public agencies that regulate bias can also work to raise algorithmic literacy as part of their missions. In both the public and private sector, those that stand to lose the most from biased decision-making can also play an active role in spotting it.

Conclusion

In December 2018, President Trump signed the First Step Act, new criminal justice legislation that encourages the usage of algorithms nationwide.54 In particular, the system would use an algorithm to initially determine who can redeem earned-time credits—reductions in sentence for completion of educational, vocational, or rehabilitative programs—excluding inmates deemed higher risk. There is a likelihood that these algorithms will perpetuate racial and class disparities, which are already embedded in the criminal justice system. As a result, African-Americans and poor people in general will be more likely to serve longer prison sentences.

“When algorithms are responsibly designed, they may avoid the unfortunate consequences of amplified systemic discrimination and unethical applications.”

As outlined in the paper, these types of algorithms should be concerning if there is not a process in place that incorporates technical diligence, fairness, and equity from design to execution. That is, when algorithms are responsibly designed, they may avoid the unfortunate consequences of amplified systemic discrimination and unethical applications.

Some decisions will be best served by algorithms and other AI tools, while others may need thoughtful consideration before computer models are designed. Further, testing and review of certain algorithms will also identify, and, at best, mitigate discriminatory outcomes. For operators of algorithms seeking to reduce the risk and complications of bad outcomes for consumers, the promotion and use of the mitigation proposals can create a pathway toward algorithmic fairness, even if equity is never fully realized.

The Brookings Institution is a nonprofit organization devoted to independent research and policy solutions. Its mission is to conduct high-quality, independent research and, based on that research, to provide innovative, practical recommendations for policymakers and the public. The conclusions and recommendations of any Brookings publication are solely those of its author(s), and do not reflect the views of the Institution, its management, or its other scholars.

Amazon, Facebook, Google, IBM, and Microsoft provide general, unrestricted support to The Brookings Institution. Paul Resnick is also a consultant to Facebook, but this work is independent and his views expressed here are his own. The findings, interpretations, and conclusions posted in this piece are not influenced by any donation. Brookings recognizes that the value it provides is in its absolute commitment to quality, independence, and impact. Activities supported by its donors reflect this commitment.

Appendix: List of Roundtable Participants

Participant Organization Wendy Anderson Office of Congresswoman Val Demings Norberto Andrade Facebook Solon Barocas Cornell University Genie Barton Privacy Genie Ricardo Baeza-Yates NTENT Miranda Bogen Upturn John Brescia Better Business Bureau Julie Brill Microsoft Rich Caruana Microsoft Research Eli Cohen Brookings Institution Anupam Datta Carnegie Mellon Deven Desai Georgia Tech Natasha Duarte Center for Democracy and Technology Nadia Fawaz LinkedIn Laura Fragomeni Walmart Global eCommerce Sharad Goel Stanford University Scott Golder Cornell University Aaron Halfaker Wikimedia Sarah Holland Google Jack Karsten Brookings Institution Krishnaram Kenthapadi LinkedIn and Stanford University Jon Kleinberg Cornell University Isabel Kloumann Facebook Jake Metcalf Ethical Resolve Alex Peysakhovich Facebook Paul Resnick University of Michigan William Rinehart American Action Forum Alex Rosenblat Data and Society Jake Schneider Brookings Institution Jasjeet Sekhon University of California-Berkeley Rob Sherman Facebook JoAnn Stonier Mastercard Worldwide Nicol Turner Lee Brookings Institution Lucy Vasserman Jigsaw’s Conversation AI Project / Google Suresh Venkatasubramanian University of Utah John Verdi Future of Privacy Forum Heather West Mozilla Jason Yosinki Uber Jinyan Zang Harvard University Leila Zia Wikimedia Foundation

References

Angwin, Julia, and Terry Parris Jr. “Facebook Lets Advertisers Exclude Users by Race.” Text/html. ProPublica, October 28, 2016. https://www.propublica.org/article/facebook-lets-advertisers-exclude-users-by-race.

Angwin, Julia, Jeff Larson, Surya Mattu, and Laura Kirchner. “Machine Bias.” ProPublica, May 23, 2016. Available at https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing (last accessed April 19, 2019).

Barocas, Solon, and Andrew D. Selbst, “Big Data’s Disparate Impact,” SSRN Scholarly Paper (Rochester, NY: Social Science Research Network, 2016. Available at https://papers.ssrn.com/abstract=2477899.

Blass, Andrea, and Yuri Gurevich. Algorithms: A Quest for Absolute Definitions. Bulletin of European Association for Theoretical Computer Science 81, 2003. https://www.microsoft.com/en-us/research/wp-content/uploads/2017/01/164.pdf (last accessed April 12, 2019).

Brennan, Tim, William Dieterich, and Beate Ehret. “Evaluating the Predictive Validity of the COMPAS Risk and Needs Assessment System.” Criminal Justice and Behavior 36 (2009): 21–40.

Chessell, Mandy. “Ethics for Big Data and Analytics.” IBM, n.d. Available at https://www.ibmbigdatahub.com/sites/default/files/whitepapers_reports_file/TCG%20Study%20Report%20-%20Ethics%20for%20BD%26A.pdf (last accessed April 19, 2019).

Chodosh, Sara. “Courts use algorithms to help determine sentencing, but random people get the same results.” Popular Science, January 18, 2018. Available at https://www.popsci.com/recidivism-algorithm-random-bias (last accessed October 15, 2018).

Corbett-Davies, Sam, Emma Peirson, Avi Feller, and Sharad Goel. “A Computer Program Used for Bail and Sentencing Decisions Was Labeled Biased against Blacks. It’s Actually Not That Clear.” Washington Post (blog), October 17, 2016. Available at https://www.washingtonpost.com/news/monkey-cage/wp/2016/10/17/can-an-algorithm-be-racist-our-analysis-is-more-cautious-than-propublicas/ (last accessed April 19. 2019).

Corbett-Davies, Sam, Emma Pierson, Avi Feller, Sharad Goel, and Aziz Huq. “Algorithmic Decision Making and the Cost of Fairness.” ArXiv:1701.08230 [Cs, Stat], January 27, 2017. https://doi.org/10.1145/3097983.309809.

Courtland, Rachel. “Bias Detectives: The Researchers Striving to Make Algorithms Fair,” Nature 558, no. 7710 (June 2018): 357–60. Available at https://doi.org/10.1038/d41586-018-05469-3 (last accessed April 19, 2019).

DeAngelius, Stephen F. “Artificial intelligence: How algorithms make systems smart,” Wired Magazine, September 2014. Available at https://www.wired.com//insights/2014/09/artificial-intelligence-algorithms-2/ (last accessed April 12, 2019).

Elejalde-Ruiz, Alexia. “The end of the resume? Hiring is in the midst of technological revolution with algorithms, chatbots.” Chicago Tribune (July 19, 2018). Available at http://www.chicagotribune.com/business/ct-biz-artificial-intelligence-hiring-20180719-story.html.

Eubanks, Virginia. “A Child Abuse Prediction Model Fails Poor Families,” Wired, January 15, 2018. Available at https://www.wired.com/story/excerpt-from-automating-inequality/ (last accessed April 19, 2019).

FTC Hearing #7: The Competition and Consumer Protection Issues of Algorithms, Artificial Intelligence, and Predictive Analytics, § Federal Trade Commission (2018). https://www.ftc.gov/system/files/documents/public_events/1418693/ftc_hearings_session_7_transcript_day_2_11-14-18.pdf.

Garbade, Michael J. “Clearing the Confusion: AI vs. Machine Learning vs. Deep Learning Differences,” Towards Data Science, September 14, 2018. Available at https://towardsdatascience//clearing-the-confusion-ai-vs-machine-learning-vs-deep-learning-differences-fce69b21d5eb (last accessed April 12, 2019).

Griggs v. Duke Power Company, Oyez. Available at https//www.oyez.org/cases/1970/124 (last accessed October 1, 2018.

Guerin, Lisa. “Disparate Impact Discrimination.” www.nolo.com. Available at https://www.nolo.com/legal-encyclopedia/disparate-impact-discrimination.htm (last accessed April 24, 2019).

Hadhazy, Adam. “Biased Bots: Artificial-Intelligence Systems Echo Human Prejudices.” Princeton University, April 18, 2017. Available at https://www.princeton.edu/news/2017/04/18/biased-bots-artificial-intelligence-systems-echo-human-prejudices (last accessed April 20, 2019).

Hamilton, Isobel Asher. “Why It’s Totally Unsurprising That Amazon’s Recruitment AI Was Biased against Women.” Business Insider, October 13, 2018. Available at https://www.businessinsider.com/amazon-ai-biased-against-women-no-surprise-sandra-wachter-2018-10 (last accessed April 20, 2019).

Hardesty, Larry. “Study Finds Gender and Skin-Type Bias in Commercial Artificial-Intelligence Systems.” MIT News, February 11, 2018. Available at http://news.mit.edu/2018/study-finds-gender-skin-type-bias-artificial-intelligence-systems-0212 (last accessed April 19, 2019).

High-level Expert Group on Artificial Intelligence. “Ethics Guidelines for Trustworthy AI (Draft).” The European Commission, December 18, 2018.

Ingold, David, and Spencer Soper. “Amazon Doesn’t Consider the Race of Its Customers. Should It?” Bloomberg.com, April 21, 2016. http://www.bloomberg.com/graphics/2016-amazon-same-day/.

Kearns, Michael. “Data Intimacy, Machine Learning and Consumer Privacy.” University of Pennsylvania Law School, May 2018. Available at https://www.law.upenn.edu/live/files/7952-kearns-finalpdf (last accessed April 12, 2019).

Kleinberg, Jon, Sendhil Mullainathan, and Manish Raghavan, “Inherent Trade-Offs in the Fair Determination of Risk Scores.”In Proceedings of Innovations in Theoretical Computer Science (ITCS), 2017. Available at https://arxiv.org/pdf/1609.05807.pdf (last accessed April 19, 2019).

Larson, Jeff, Surya Mattu, and Julia Angwin. “Unintended Consequences of Geographic Targeting.” Technology Science, September 1, 2015. Available at https://techscience.org/a/2015090103/ (last accessed April 19, 2019).

Locklear, Mallory. “Facebook Releases an Update on Its Civil Rights Audit.” Engadget (blog), December 18, 2018. Available at https://www.engadget.com/2018/12/18/facebook-update-civil-rights-audit/ (last accessed April 19, 2019).

Lopez, German. “The First Step Act, Congress’s Criminal Justice Reform Bill, Explained.” Vox, December 3, 2018. Available at https://www.vox.com/future-perfect/2018/12/3/18122392/first-step-act-criminal-justice-reform-bill-congress (last accessed April 16, 2019).

Mnuchin, Steven T., and Craig S. Phillips. “A Financial System That Creates Economic Opportunities – Nonbank Financials, Fintech, and Innovation.” Washington, D.C.: U.S. Department of the Treasury, July 2018.Available at https://home.treasury.gov/sites/default/files/2018-08/A-Financial-System-that-Creates-Economic-Opportunities—Nonbank-Financials-Fintech-and-Innovation_0.pdf (last accessed April 19, 2019).

Reisman, Dillon, Jason Schultz, Kate Crawford, and Meredith Whittaker. “Algorithmic Impact Assessments: A Practical Framework for Public Agency Accountability.” New York: AI Now, April 2018.

Romei, Andrea, and Salvatore Ruggieri. “Discrimination Data Analysis: A Multi-Disciplinary Bibliography.” In Discrimination and Privacy in the Information Society, edited by Bart Custers, T Calders, B Schermer, and T Zarsky, 109–35. Studies in Applied Philosophy, Epistemology and Rational Ethics. Springer, Berlin, Heidelberg, 2013. Available at https://doi.org/10.1007/978-3-642-30487-3_6 (last accessed April 19, 2019).

Schatz, Brian. AI in Government Act of 2018, Pub. L. No. S.B. 3502 (2018). https://www.congress.gov/bill/115th-congress/senate-bill/3502.

Spielkamp, Matthias. “We Need to Shine More Light on Algorithms so They Can Help Reduce Bias, Not Perpetuate It.” MIT Technology Review. Accessed September 20, 2018. Available at https://www.technologyreview.com/s/607955/inspecting-algorithms-for-bias/ (last accessed April 19, 2019).

Stack, Liam. “Facebook Announces New Policy to Ban White Nationalist Content.” The New York Times, March 28, 2019, sec. Business. Available at https://www.nytimes.com/2019/03/27/business/facebook-white-nationalist-supremacist.html (last accessed April 19, 2019).

Sweeney, Latanya, and Jinyan Zang. “How appropriate might big data analytics decisions be when placing ads?” Powerpoint presentation presented at the Big Data: A tool for inclusion or exclusion, Federal Trade Commission conference, Washington, DC. September 15, 2014. Available at https://www.ftc.gov/systems/files/documents/public_events/313371/bigdata-slides-sweeneyzang-9_15_14.pdf (last accessed April 12, 2019).

Sweeney, Latanya. “Discrimination in online ad delivery.” Rochester, NY: Social Science Research Network, January 28, 2013. Available at https://papers.ssrn.com/abstract=2208240 (last accessed April 12, 2019).

Sydell, Laura. “It Ain’t Me, Babe: Researchers Find Flaws In Police Facial Recognition Technology.” NPR.org, October 25, 2016. Available at https://www.npr.org/sections/alltechconsidered/2016/10/25/499176469/it-aint-me-babe-researchers-find-flaws-in-police-facial-recognition (last accessed April 19, 2019).

“The Global Data Ethics Project.” Data for Democracy, n.d. https://www.datafordemocracy.org/project/global-data-ethics-project (last accessed April 19, 2019).

Tobin, Ariana. “HUD sues Facebook over housing discrimination and says the company’s algorithms have made the problem worse.” ProPublica (March 28, 2019). Available at https://www.propublica.org/article/hud-sues-facebook-housing-discrimination-advertising-algorithms (last accessed April 29, 2019).

Turner Lee, Nicol. “Inclusion in Tech: How Diversity Benefits All Americans,” § Subcommittee on Consumer Protection and Commerce, United States House Committee on Energy and Commerce (2019). Also available on Brookings web site, https://www.brookings.edu/testimonies/inclusion-in-tech-how-diversity-benefits-all-americans/ (last accessed April 29, 2019).

Turner Lee, Nicol. Detecting racial bias in algorithms and machine learning. Journal of Information, Communication and Ethics in Society 2018, Vol. 16 Issue 3, pp. 252-260. Available at https://doi.org/10.1108/JICES-06-2018-0056/ (last accessed April 29, 2019).

“Understanding bias in algorithmic design,” Impact.Engineered, September 5, 2017. Available at https://medium.com/impact-engineered/understanding-bias-in-algorithmic-design-db9847103b6e (last accessed April 12, 2019).

Vincent, James. “Amazon Reportedly Scraps Internal AI Recruiting Tool That Was Biased against Women.” The Verge, October 10, 2018. Available at https://www.theverge.com/2018/10/10/17958784/ai-recruiting-tool-bias-amazon-report (last accessed April 20, 2019).

Zafar, Muhammad Bilal, Isabel Valera Martinez, Manuel Gomez Rodriguez, and Krishna Gummadi. “Fairness Constraints: A Mechanism for Fair Classification.” In Proceedings of the 20th International Conference on Artificial Intelligence and Statistics (AISTATS). Fort Lauderdale, FL, 2017.

Zarsky, Tal. “Understanding Discrimination in the Scored Society.” SSRN Scholarly Paper. Rochester, NY: Social Science Research Network, January 15, 2015. https://papers.ssrn.com/abstract=2550248.","                 Skip to main content          Search Brookings         About Us Press Room Experts Events The Brookings Press WashU at Brookings Careers Support Brookings    Cart  0    Search Guidance for the Brookings community and the public on our response to the coronavirus (COVID-19) »  Learn more from Brookings scholars about the global response to coronavirus (COVID-19) »                  Algorithmic bias detection and mitigation: Best practices and policies to reduce consumer harms  Facebook Twitter LinkedIn Print SMS Email More Reddit AI  Policy 2020  Cities & Regions  Global Dev  Intl Affairs  U.S. Economy  U.S. Politics & Govt  More             0                Report  Algorithmic bias detection and mitigation: Best practices and policies to reduce consumer harms   Nicol Turner Lee , Paul Resnick , and Genie Barton  Wednesday, May 22, 2019                       Facebook Twitter LinkedIn Print SMS Email More Reddit  For media inquiries, contact:   Governance Studies Main Line  202.797.6090           Introduction         Nicol Turner Lee  Senior Fellow - Governance Studies  Director - Center for Technology Innovation     @drturnerlee          Paul Resnick  Professor of Information; Associate Dean for Research and Faculty Affairs, School of Information - University of Michigan     presnick          Genie Barton  Member, Research Advisory Board - International Association of Privacy Professionals     privacygenie       The private and public sectors are increasingly turning to artificial intelligence (AI) systems and machine learning algorithms to automate simple and complex decision-making processes. 1 The mass-scale digitization of data and the emerging technologies that use them are disrupting most economic sectors, including transportation, retail, advertising, and energy, and other areas. AI is also having an impact on democracy and governance as computerized systems are being deployed to improve accuracy and drive objectivity in government functions.  The availability of massive data sets has made it easy to derive new insights through computers. As a result, algorithms, which are a set of step-by-step instructions that computers follow to perform a task, have become more sophisticated and pervasive tools for automated decision-making. 2 While algorithms are used in many contexts, we focus on computer models that make inferences from data about people, including their identities, their demographic attributes, their preferences, and their likely future behaviors, as well as the objects related to them. 3  “Algorithms are harnessing volumes of macro- and micro-data to influence decisions affecting people in a range of tasks, from making movie recommendations to helping banks determine the creditworthiness of individuals.”  In the pre-algorithm world, humans and organizations made decisions in hiring, advertising, criminal sentencing, and lending. These decisions were often governed by federal, state, and local laws that regulated the decision-making processes in terms of fairness, transparency, and equity. Today, some of these decisions are entirely made or influenced by machines whose scale and statistical rigor promise unprecedented efficiencies. Algorithms are harnessing volumes of macro- and micro-data to influence decisions affecting people in a range of tasks, from making movie recommendations to helping banks determine the creditworthiness of individuals. 4 In machine learning, algorithms rely on multiple data sets, or training data, that specifies what the correct outputs are for some people or objects. From that training data, it then learns a model which can be applied to other people or objects and make predictions about what the correct outputs should be for them. 5  However, because machines can treat similarly-situated people and objects differently, research is starting to reveal some troubling examples in which the reality of algorithmic decision-making falls short of our expectations. Given this, some algorithms run the risk of replicating and even amplifying human biases, particularly those affecting protected groups. 6 For example, automated risk assessments used by U.S. judges to determine bail and sentencing limits can generate incorrect conclusions, resulting in large cumulative effects on certain groups, like longer prison sentences or higher bails imposed on people of color.  In this example, the decision generates “bias,” a term that we define broadly as it relates to outcomes which are systematically less favorable to individuals within a particular group and where there is no relevant difference between groups that justifies such harms. 7 Bias in algorithms can emanate from unrepresentative or incomplete training data or the reliance on flawed information that reflects historical inequalities. If left unchecked, biased algorithms can lead to decisions which can have a collective, disparate impact on certain groups of people even without the programmer’s intention to discriminate. The exploration of the intended and unintended consequences of algorithms is both necessary and timely, particularly since current public policies may not be sufficient to identify, mitigate, and remedy consumer impacts.  With algorithms appearing in a variety of applications, we argue that operators and other concerned stakeholders must be diligent in proactively addressing factors which contribute to bias. Surfacing and responding to algorithmic bias upfront can potentially avert harmful impacts to users and heavy liabilities against the operators and creators of algorithms, including computer programmers, government, and industry leaders. These actors comprise the audience for the series of mitigation proposals to be presented in this paper because they either build, license, distribute, or are tasked with regulating or legislating algorithmic decision-making to reduce discriminatory intent or effects.   Related             Technology & Innovation  How artificial intelligence is transforming the world   Darrell M. West and John R. Allen  Tuesday, April 24, 2018                Technology & Innovation  Trends in the Information Technology sector   Makada Henry-Nickie , Kwadwo Frimpong , and Hao Sun  Friday, March 29, 2019                Media & Journalism  How to combat fake news and disinformation   Darrell M. West  Monday, December 18, 2017        Our research presents a framework for algorithmic hygiene , which identifies some specific causes of biases and employs best practices to identify and mitigate them. We also present a set of public policy recommendations, which promote the fair and ethical deployment of AI and machine learning technologies.  This paper draws upon the insight of 40 thought leaders from across academic disciplines, industry sectors, and civil society organizations who participated in one of two roundtables. 8 Roundtable participants actively debated concepts related to algorithmic design, accountability, and fairness, as well as the technical and social trade-offs associated with various approaches to bias detection and mitigation.  Our goal is to juxtapose the issues that computer programmers and industry leaders face when developing algorithms with the concerns of policymakers and civil society groups who assess their implications. To balance the innovations of AI and machine learning algorithms with the protection of individual rights, we present a set of public policy recommendations, self-regulatory best practices, and consumer-focused strategies–all of which promote the fair and ethical deployment of these technologies.  Our public policy recommendations include the updating of nondiscrimination and civil rights laws to apply to digital practices, the use of regulatory sandboxes to foster anti-bias experimentation, and safe harbors for using sensitive information to detect and mitigate biases. We also outline a set of self-regulatory best practices, such as the development of a bias impact statement, inclusive design principles, and cross-functional work teams. Finally, we propose additional solutions focused on algorithmic literacy among users and formal feedback mechanisms to civil society groups.  The next section provides five examples of algorithms to explain the causes and sources of their biases. Later in the paper, we discuss the trade-offs between fairness and accuracy in the mitigation of algorithmic bias, followed by a robust offering of self-regulatory best practices, public policy recommendations, and consumer-driven strategies for addressing online biases. We conclude by highlighting the importance of proactively tackling the responsible and ethical use of machine learning and other automated decision-making tools.  Examples of algorithmic biases  Algorithmic bias can manifest in several ways with varying degrees of consequences for the subject group. Consider the following examples, which illustrate both a range of causes and effects that either inadvertently apply different treatment to groups or deliberately generate a disparate impact on them.  Bias in online recruitment tools  Online retailer Amazon, whose global workforce is 60 percent male and where men hold 74 percent of the company’s managerial positions, recently discontinued use of a recruiting algorithm after discovering gender bias. 9 The data that engineers used to create the algorithm were derived from the resumes submitted to Amazon over a 10-year period, which were predominantly from white males. The algorithm was taught to recognize word patterns in the resumes, rather than relevant skill sets, and these data were benchmarked against the company’s predominantly male engineering department to determine an applicant’s fit. As a result, the AI software penalized any resume that contained the word “women’s” in the text and downgraded the resumes of women who attended women’s colleges, resulting in gender bias. 10  Amazon discontinued a recruiting algorithm after discovering that it led to gender bias in its hiring. (Credit: Brian Snyder/Reuters)  Bias in word associations  Princeton University researchers used off-the-shelf machine learning AI software to analyze and link 2.2 million words. They found that European names were perceived as more pleasant than those of African-Americans, and that the words “woman” and “girl” were more likely to be associated with the arts instead of science and math, which were most likely connected to males. 11 In analyzing these word-associations in the training data, the machine learning algorithm picked up on existing racial and gender biases shown by humans. If the learned associations of these algorithms were used as part of a search-engine ranking algorithm or to generate word suggestions as part of an auto-complete tool, it could have a cumulative effect of reinforcing racial and gender biases.  Bias in online ads  Latanya Sweeney, Harvard researcher and former chief technology officer at the Federal Trade Commission (FTC), found that online search queries for African-American names were more likely to return ads to that person from a service that renders arrest records, as compared to the ad results for white names. 12 Her research also found that the same differential treatment occurred in the micro-targeting of higher-interest credit cards and other financial products when the computer inferred that the subjects were African-Americans, despite having similar backgrounds to whites. 13 During a public presentation at a FTC hearing on big data, Sweeney demonstrated how a web site, which marketed the centennial celebration of an all-black fraternity, received continuous ad suggestions for purchasing “arrest records” or accepting high-interest credit card offerings. 14  Bias in facial recognition technology  MIT researcher Joy Buolamwini found that the algorithms powering three commercially available facial recognition software systems were failing to recognize darker-skinned complexions. 15 Generally, most facial recognition training data sets are estimated to be more than 75 percent male and more than 80 percent white. When the person in the photo was a white man, the software was accurate 99 percent of the time at identifying the person as male. According to Buolamwini’s research, the product error rates for the three products were less than one percent overall, but increased to more than 20 percent in one product and 34 percent in the other two in the identification of darker-skinned women as female. 16 In response to Buolamwini’s facial-analysis findings, both IBM and Microsoft committed to improving the accuracy of their recognition software for darker-skinned faces.  Bias in criminal justice algorithms  Acknowledging the possibility and causes of bias is the first step in any mitigation approach.  The COMPAS (Correctional Offender Management Profiling for Alternative Sanctions) algorithm, which is used by judges to predict whether defendants should be detained or released on bail pending trial, was found to be biased against African-Americans, according to a report from ProPublica. 17 The algorithm assigns a risk score to a defendant’s likelihood to commit a future offense, relying on the voluminous data available on arrest records, defendant demographics, and other variables. Compared to whites who were equally likely to re-offend, African-Americans were more likely to be assigned a higher-risk score, resulting in longer periods of detention while awaiting trial. 18 Northpointe, the firm that sells the algorithm’s outputs, offers evidence to refute such claims and argues that wrong metrics are being used to assess fairness in the product, a topic that we return to later in the paper.  While these examples of bias are not exhaustive, they suggest that these problems are empirical realities and not just theoretical concerns. They also illustrate how these outcomes emerge, and in some cases, without malicious intent by the creators or operators of the algorithm. Acknowledging the possibility and causes of bias is the first step in any mitigation approach. On this point, roundtable participant Ricardo Baeza-Yates from NTENT stated that “[companies] will continue to have a problem discussing algorithmic bias if they don’t refer to the actual bias itself.”  Causes of bias  Barocas and Selbst point out that bias can creep in during all phases of a project, “…whether by specifying the problem to be solved in ways that affect classes differently, failing to recognize or address statistical biases, reproducing past prejudice, or considering an insufficiently rich set of factors.” 19 Roundtable participants focused especially on bias stemming from flaws in the data used to train the algorithms. “Flawed data is a big problem,” stated roundtable participant Lucy Vasserman from Google, “…especially for the groups that businesses are working hard to protect.” While there are many causes, we focus on two of them: historical human biases and incomplete or unrepresentative data .  Historical human biases  Historical human biases are shaped by pervasive and often deeply embedded prejudices against certain groups, which can lead to their reproduction and amplification in computer models. In the COMPAS algorithm, if African-Americans are more likely to be arrested and incarcerated in the U.S. due to historical racism, disparities in policing practices, or other inequalities within the criminal justice system, these realities will be reflected in the training data and used to make suggestions about whether a defendant should be detained. If historical biases are factored into the model, it will make the same kinds of wrong judgments that people do.  The Amazon recruitment algorithm revealed a similar trajectory when men were the benchmark for professional “fit,” resulting in female applicants and their attributes being downgraded. These historical realities often find their way into the algorithm’s development and execution, and they are exacerbated by the lack of diversity which exists within the computer and data science fields. 20  Further, human biases can be reinforced and perpetuated without the user’s knowledge. For example, African-Americans who are primarily the target for high-interest credit card options might find themselves clicking on this type of ad without realizing that they will continue to receive such predatory online suggestions. In this and other cases, the algorithm may never accumulate counter-factual ad suggestions (e.g., lower-interest credit options) that the consumer could be eligible for and prefer. Thus, it is important for algorithm designers and operators to watch for such potential negative feedback loops that cause an algorithm to become increasingly biased over time.  Incomplete or unrepresentative training data  Insufficient training data is another cause of algorithmic bias. If the data used to train the algorithm are more representative of some groups of people than others, the predictions from the model may also be systematically worse for unrepresented or under-representative groups. For example, in Buolamwini’s facial-analysis experiments, the poor recognition of darker-skinned faces was largely due to their statistical under-representation in the training data. That is, the algorithm presumably picked up on certain facial features, such as the distance between the eyes, the shape of the eyebrows and variations in facial skin shades, as ways to detect male and female faces. However, the facial features that were more representative in the training data were not as diverse and, therefore, less reliable to distinguish between complexions, even leading to a misidentification of darker-skinned females as males.  Turner Lee has argued that it is often the lack of diversity among the programmers designing the training sample which can lead to the under-representation of a particular group or specific physical attributes. 21 Buolamwini’s findings were due to her rigor in testing, executing, and assessing a variety of proprietary facial-analysis software in different settings, correcting for the lack of diversity in their samples.  Conversely, algorithms with too much data, or an over-representation, can skew the decision toward a particular result. Researchers at Georgetown Law School found that an estimated 117 million American adults are in facial recognition networks used by law enforcement, and that African-Americans were more likely to be singled out primarily because of their over-representation in mug-shot databases. 22 Consequently, African-American faces had more opportunities to be falsely matched, which produced a biased effect.  Bias detection strategies  Understanding the various causes of biases is the first step in the adoption of effective algorithmic hygiene. But, how can operators of algorithms assess whether their results are, indeed, biased? Even when flaws in the training data are corrected, the results may still be problematic because context matters during the bias detection phase.  “Even when flaws in the training data are corrected, the results may still be problematic because context matters during the bias detection phase.”  First, all detection approaches should begin with careful handling of the sensitive information of users, including data that identify a person’s membership in a federally protected group (e.g., race, gender). In some cases, operators of algorithms may also worry about a person’s membership in some other group if they are also susceptible to unfair outcomes. An examples of this could be college admission officers worrying about the algorithm’s exclusion of applicants from lower-income or rural areas; these are individuals who may be not federally protected but do have susceptibility to certain harms (e.g., financial hardships).  In the former case, systemic bias against protected classes can lead to collective, disparate impacts , which may have a basis for legally cognizable harms, such as the denial of credit, online racial profiling, or massive surveillance. 23 In the latter case, the outputs of the algorithm may produce unequal outcomes or unequal error rates for different groups, but they may not violate legal prohibitions if there was no intent to discriminate.  These problematic outcomes should lead to further discussion and awareness of how algorithms work in the handling of sensitive information, and the trade-offs around fairness and accuracy in the models.  Algorithms and sensitive information  While it is intuitively appealing to think that an algorithm can be blind to sensitive attributes, this is not always the case. 24 Critics have pointed out that an algorithm may classify information based on online proxies for the sensitive attributes, yielding a bias against a group even without making decisions directly based on one’s membership in that group. Barocas and Selbst define online proxies as “factors used in the scoring process of an algorithm which are mere stand-ins for protected groups, such as zip code as proxies for race, or height and weight as proxies for gender.” 25 They argue that proxies often linked to algorithms can produce both errors and discriminatory outcomes, such as instances where a zip code is used to determine digital lending decisions or one’s race triggers a disparate outcome. 26 Facebook’s advertising platform contained proxies that allowed housing marketers to micro-target preferred renters and buyers by clicking off data points, including zip code preferences. 27 Thus, it is possible that an algorithm which is completely blind to a sensitive attribute could actually produce the same outcome as one that uses the attribute in a discriminatory manner.  “While it is intuitively appealing to think that an algorithm can be blind to sensitive attributes, this is not always the case.”  For example, Amazon made a corporate decision to exclude certain neighborhoods from its same-day Prime delivery system. Their decision relied upon the following factors: whether a particular zip code had a sufficient number of Prime members, was near a warehouse, and had sufficient people willing to deliver to that zip code. 28 While these factors corresponded with the company’s profitability model, they resulted in the exclusion of poor, predominantly African-American neighborhoods, transforming these data points into proxies for racial classification. The results, even when unintended, discriminated against racial and ethnic minorities who were not included.  Similarly, a job-matching algorithm may not receive the gender field as an input, but it may produce different match scores for two resumes that differ only in the substitution of the name “Mary” for “Mark” because the algorithm is trained to make these distinctions over time.  There are also arguments that blinding the algorithm to sensitive attributes can cause algorithmic bias in some situations. Corbett-Davies and Goel point out in their research on the COMPAS algorithm that even after controlling for “legitimate” risk factors, empirically women have been found to re-offend less often than men in many jurisdictions. 29 If an algorithm is forbidden from reporting a different risk assessment score for two criminal defendants who differ only in their gender, judges may be less likely to release female defendants than male defendants with equal actual risks of committing another crime before trial. Thus, blinding the algorithm from any type of sensitive attribute may not solve bias.  While roundtable participants were not in agreement on the use of online proxies in modeling, they largely agreed that operators of algorithms must be more transparent in their handling of sensitive information, especially if the potential proxy could itself be a legal classificatory harm. 30 There was also discussion that the use of sensitive attributes as part of an algorithm could be a strategy for detecting and possibly curing intended and unintentional biases. Because currently doing so may be constrained by privacy regulations, such as the European Union’s General Data Protection Rules (GDPR) or proposed U.S. federal privacy legislation, the argument could be made for the use of regulatory sandboxes and safe harbors to allow the use of sensitive information when detecting and mitigating biases, both of which will be introduced as part of our policy recommendations.  Detecting bias  When detecting bias, computer programmers normally examine the set of outputs that the algorithm produces to check for anomalous results. Comparing outcomes for different groups can be a useful first step. This could even be done through simulations. Roundtable participant Rich Caruana from Microsoft suggested that companies consider the simulation of predictions (both true and false) before applying them to real-life scenarios. “We almost need a secondary data collection process because sometimes the model will [emit] something quite different,” he shared. For example, if a job-matching algorithm’s average score for male applicants is higher than that for women, further investigation and simulations could be warranted.  However, the downside of these approaches is that not all unequal outcomes are unfair. Roundtable participant Solon Barocas from Cornell University summed this up when he stated, “Maybe we find out that we have a very accurate model, but it still produces disparate outcomes. This may be unfortunate, but is it fair?” An alternative to accounting for unequal outcomes may be to look at the equality of error rates, and whether there are more mistakes for one group of people than another. On this point, Isabel Kloumann of Facebook shared that “society has expectations. One of which is not incarcerating one minority group disproportionately [as a result of an algorithm].”  As shown in the debates around the COMPAS algorithm, even error rates are not a simple litmus test for biased algorithms. Northpointe, the company that developed the COMPAS algorithm, refutes claims of racial discrimination. They argue that among defendants assigned the same high risk score, African-American and white defendants have almost equal recidivism rates, so by that measure, there is no error in the algorithm’s decision. 31 In their view, judges can consider their algorithm without any reference to race in bail and release decisions.  It is not possible, in general, to have equal error rates between groups for all the different error rates. 32 ProPublica focused on one error rate, while Northpointe honed in on another. Thus, some principles need to be established for which error rates should be equalized in which situations in order to be fair.  The COMPAS algorithm, which is used by judges to predict whether defendants should be detained or released on bail pending trial, has drawn scrutiny over claims of potential racial discrimination. (Credit: Stephen Lam/Reuters)  However, distinguishing between how the algorithm works with sensitive information and potential errors can be problematic for operators of algorithms, policymakers, and civil society groups. 33 “Companies would be losing a lot if we don’t draw a distinction between the two,” said Julie Brill from Microsoft. At the very least, there was agreement among roundtable participants that algorithms should not perpetuate historical inequities, and that more work needs to be done to address online discrimination. 34  Fairness and accuracy trade-offs  Next, a discussion of trade-offs and ethics is needed. Here, the focus should be on evaluating both societal notions of “fairness” and possible social costs. In their research of the COMPAS algorithm, Corbett-Davies, Goel, Pierson, Feller, and Huq see “an inherent tension between minimizing violent crime and satisfying common notions of fairness.” 35 They conclude that optimizing for public safety yields decisions that penalize defendants of color, while satisfying legal and societal fairness definitions, and may lead to more releases of high-risk defendants, which would adversely affect public safety. 36 Moreover, the negative impacts on public safety might also disproportionately affect African-American and white neighborhoods, thus creating a fairness cost as well.  If the goal is to avoid reinforcing inequalities, what, then, should developers and operators of algorithms do to mitigate potential biases? We argue that developers of algorithms should first look for ways to reduce disparities between groups without sacrificing the overall performance of the model, especially whenever there appears to be a trade-off.  A handful of roundtable participants argued that opportunities exist for improving both fairness and accuracy in algorithms. For programmers, the investigation of apparent bugs in the software may reveal why the model was not maximizing for overall accuracy. The resolution of these bugs can then improve overall accuracy. Data sets, which may be under-representative of certain groups, may need additional training data to improve accuracy in the decision-making and reduce unfair results. Buolamwini’s facial detection experiments are good examples of this type of approach to fairness and accuracy.  Roundtable participant Sarah Holland from Google pointed out the risk tolerance associated with these types of trade-offs when she shared that “[r]aising risk also involves raising equity issues.” Thus, companies and other operators of algorithms should determine if the social costs of the trade-offs are justified, the stakeholders involved are amenable to a solution through algorithms, or if human decision-makers are needed to frame the solution.  Ethical frameworks matter  What is fundamentally behind these fairness and accuracy trade-offs should be discussions around ethical frameworks and potential guardrails for machine learning tasks and systems. There are several ongoing and recent international and U.S.-based efforts to develop ethical governance standards for the use of AI. 37 The 35-member Organization for Economic Cooperation and Development (OECD) is expected shortly to release its own guidelines for ethical AI. 38 The European Union recently released “Ethics Guidelines for Trustworthy AI,” which delineates seven governance principles: (1) human agency and oversight, (2) technical robustness and safety, (3) privacy and data governance, (4) transparency, (5) diversity, nondiscrimination and fairness, (6) environmental and societal well-being, and (7) accountability. 39 The EU’s ethical framework reflects a clear consensus that it is unethical to “unfairly discriminate.” Within these guidelines, member states link diversity and nondiscrimination with principles of fairness, enabling inclusion and diversity throughout the entire AI system’s lifecycle. Their principles interpret fairness through the lenses of equal access, inclusive design processes, and equal treatment.  Yet, even with these governmental efforts, it is still surprisingly difficult to define and measure fairness. 40 While it will not always be possible to satisfy all notions of fairness at the same time, companies and other operators of algorithms must be aware that there is no simple metric to measure fairness that a software engineer can apply, especially in the design of algorithms and the determination of the appropriate trade-offs between accuracy and fairness. Fairness is a human, not a mathematical, determination, grounded in shared ethical beliefs. Thus, algorithmic decisions that may have a serious consequence for people will require human involvement.  For example, while the training data discrepancies in the COMPAS algorithm can be corrected, human interpretation of fairness still matters. For that reason, while an algorithm such as COMPAS may be a useful tool, it cannot substitute for the decision-making that lies within the discretion of the human arbiter. 41 We believe that subjecting the algorithm to rigorous testing can challenge the different definitions of fairness, a useful exercise among companies and other operators of algorithms.  “It’s important for algorithm operators and developers to always be asking themselves: Will we leave some groups of people worse off as a result of the algorithm’s design or its unintended consequences? “  In the decision to create and bring algorithms to market, the ethics of likely outcomes must be considered—especially in areas where governments, civil society, or policymakers see potential for harm, and where there is a risk of perpetuating existing biases or making protected groups more vulnerable to existing societal inequalities. That is why it’s important for algorithm operators and developers to always be asking themselves: Will we leave some groups of people worse off as a result of the algorithm’s design or its unintended consequences?  We suggest that this question is one among many that the creators and operators of algorithms should consider in the design, execution, and evaluation of algorithms, which are described in the following mitigation proposals. Our first proposal addresses the updating of U.S. nondiscrimination laws to apply to the digital space.  Mitigation proposals  Nondiscrimination and other civil rights laws should be updated to interpret and redress online disparate impacts  To develop trust from policymakers, computer programmers, businesses, and other operators of algorithms must abide by U.S. laws and statutes that currently forbid discrimination in public spaces. Historically, nondiscrimination laws and statutes unambiguously define the thresholds and parameters for the disparate treatment of protected classes. The 1964 Civil Rights Act “forbade discrimination on the basis of sex as well as race in hiring, promoting, and firing.” The 1968 Fair Housing Act prohibits discrimination in the sale, rental, and financing of dwellings, and in other housing-related transactions to federally protected classes. Enacted in 1974, the Equal Credit Opportunity Act stops any creditor from discriminating against any applicant from any type of credit transaction based on protected characteristics. While these laws do not necessarily mitigate and resolve other implicit or unconscious biases that can be baked into algorithms, companies and other operators should guard against violating these statutory guardrails in the design of algorithms, as well as mitigating their implicit concern to prevent past discrimination from continuing.  Roundtable participant Wendy Anderson from the Office of Congresswoman Val Demings stated, “[T]ypically, legislators only hear when something bad happens. We need to find a way to protect those who need it without stifling innovation.” Congress can clarify how these nondiscrimination laws apply to the types of grievances recently found in the digital space, since most of these laws were written before the advent of the internet. 42 Such legislative action can provide clearer guardrails that are triggered when algorithms are contributing to legally recognizable harms. Moreover, when creators and operators of algorithms understand that these may be more or less non-negotiable factors, the technical design will be more thoughtful in moving away from models that may trigger and exacerbate explicit discrimination, such as design frames that exclude rather than include certain inputs or are not checked for bias. 43  Operators of algorithms must develop a bias impact statement  Once the idea for an algorithm has been vetted against nondiscrimination laws, we suggest that operators of algorithms develop a bias impact statement, which we offer as a template of questions that can be flexibly applied to guide them through the design, implementation, and monitoring phases.  As a self-regulatory practice, the bias impact statement can help probe and avert any potential biases that are baked into or are resultant from the algorithmic decision. As a best practice, operators of algorithms should brainstorm a core set of initial assumptions about the algorithm’s purpose prior to its development and execution. We propose that operators apply the bias impact statement to assess the algorithm’s purpose, process and production, where appropriate. Roundtable participants also suggested the importance of establishing a cross-functional and interdisciplinary team to create and implement the bias impact statement.   New York University’s AI Now Institute    Related Books             Autonomous Vehicles   By Clifford Winston and Quentin Karpilow   2020                Terms of Disservice   By Dipayan Ghosh   2020                Growth in a Time of Change   Edited by Hyeon-Wook Kim and Zia Qureshi   2020        New York University’s AI Now Institute has already introduced a model framework for governmental entities to use to create algorithmic impact assessments (AIAs), which evaluate the potential detrimental effects of an algorithm in the same manner as environmental, privacy, data, or human rights impact statements. 44 While there may be differences in implementation given the type of predictive model, the AIA encompasses multiple rounds of review from internal, external, and public audiences. First, it assumes that after this review, a company will develop a list of potential harms or biases in their self-assessment, with the assistance of more technical outside experts. Second, if bias appears to have occurred, the AIA pushes for notice to be given to impacted populations and a comment period opened for response. And third, the AIA process looks to federal and other entities to support users’ right to challenge algorithmic decisions that feel unfair.  While the AIA process supports a substantive feedback loop, what may be missing is both the required forethought leading up to the decision and the oversight of the algorithm’s provisions. Moreover, our proposed bias impact statement starts with a framework that identifies which automated decisions should be subjected to such scrutiny, operator incentives, and stakeholder engagement.   Which automated decisions?   In the case of determining which automated decisions require such vetting, operators of algorithms should start with questions about whether there will be a possible negative or unintended outcome resulting from the algorithm, for whom, and the severity of consequences for members of the affected group if not detected and mitigated. Reviewing established legal protections around fair housing, employment, credit, criminal justice, and health care should serve as a starting point for determining which decisions need to be viewed with special caution in designing and testing any algorithm used to predict outcomes or make important eligibility decisions about access to a benefit. This is particularly true considering the legal prescriptions against using data that has a likelihood of disparate impact on a protected class or other established harms. Thus, we suggest that operators should be constantly questioning the potential legal, social, and economic effects and potential liabilities associated with that choice when determining which decisions should be automated and how to automate them with minimal risks.   What are the user incentives?   Incentives should also drive organizations to proactively address algorithmic bias. Conversely, operators who create and deploy algorithms that generate fairer outcomes should also be recognized by policymakers and consumers who will trust them more for their practices. When companies exercise effective algorithmic hygiene before, during, and after introducing algorithmic decision-making, they should be rewarded and potentially given a public-facing acknowledgement for best practices.   How are stakeholders being engaged?   Finally, the last element encapsulated in a bias impact statement should involve the engagement of stakeholders who could help computer programmers in the selection of inputs and outputs of certain automated decisions. “Tech succeeds when users understand the product better than its designers,” said Rich Caruana from Microsoft. Getting users engaged early and throughout the process will prompt improvements to the algorithms, which ultimately leads to improved user experiences.  Stakeholder responsibilities can also extend to civil society organizations who can add value in the conversation on the algorithm’s design. “Companies [should] engage civil society,” shared Miranda Bogen from Upturn. “Otherwise, they will go to the press and regulators with their complaints.” A possible solution for operators of algorithms could be the development of an advisory council of civil society organizations that, working alongside companies, may be helpful in defining the scope of the procedure and predicting biases based on their ground-level experiences.   The template for the bias impact statement   These three foundational elements for a bias impact statement are reflected in a discrete set of questions that operators should answer during the design phase to filter out potential biases (Table 1). As a self-regulatory framework, computer programmers and other operators of algorithms can construct this type of tool prior to the model’s design and execution.  Table 1. Design questions template for bias impact statement      What will the automated decision do?    Who is the audience for the algorithm and who will be most affected by it?    Do we have training data to make the correct predictions about the decision?    Is the training data sufficiently diverse and reliable? What is the data lifecycle of the algorithm?    Which groups are we worried about when it comes to training data errors, disparate treatment, and impact?    How will potential bias be detected?    How and when will the algorithm be tested? Who will be the targets for testing?    What will be the threshold for measuring and correcting for bias in the algorithm, especially as it relates to protected groups?    What are the operator incentives?    What will we gain in the development of the algorithm?    What are the potential bad outcomes and how will we know?    How open (e.g., in code or intent) will we make the design process of the algorithm to internal partners, clients, and customers?    What intervention will be taken if we predict that there might be bad outcomes associated with the development or deployment of the algorithm?    How are other stakeholders being engaged?    What’s the feedback loop for the algorithm for developers, internal partners and customers?    Is there a role for civil society organizations in the design of the algorithm?    Has diversity been considered in the design and execution?    Will the algorithm have implications for cultural groups and play out differently in cultural contexts?    Is the design team representative enough to capture these nuances and predict the application of the algorithm within different cultural contexts? If not, what steps are being taken to make these scenarios more salient and understandable to designers?    Given the algorithm’s purpose, is the training data sufficiently diverse?    Are there statutory guardrails that companies should be reviewing to ensure that the algorithm is both legal and ethical?      Diversity-in-design  Operators of algorithms should also consider the role of diversity within their work teams, training data, and the level of cultural sensitivity within their decision-making processes. Employing diversity in the design of algorithms upfront will trigger and potentially avoid harmful discriminatory effects on certain protected groups, especially racial and ethnic minorities. While the immediate consequences of biases in these areas may be small, the sheer quantity of digital interactions and inferences can amount to a new form of systemic bias. Therefore, the operators of algorithms should not discount the possibility or prevalence of bias and should seek to have a diverse workforce developing the algorithm, integrate inclusive spaces within their products, or employ “diversity-in-design,” where deliberate and transparent actions will be taken to ensure that cultural biases and stereotypes are addressed upfront and appropriately. Adding inclusivity into the algorithm’s design can potentially vet the cultural inclusivity and sensitivity of the algorithms for various groups and help companies avoid what can be litigious and embarrassing algorithmic outcomes.  The bias impact statement should not be an exhaustive tool. For algorithms with more at stake, ongoing review of their execution should be factored into the process. The goal here is to monitor for disparate impacts resulting from the model that border on unethical, unfair, and unjust decision-making. When the process of identifying and forecasting the purpose of the algorithm is achieved, a robust feedback loop will aid in the detection of bias, which leads to the next recommendation promoting regular audits.  Other self-regulatory best practices  Operators of algorithms should regularly audit for bias  The formal and regular auditing of algorithms to check for bias is another best practice for detecting and mitigating bias. On the importance of these audits, roundtable participant Jon Kleinberg from Cornell University shared that “[a]n algorithm has no choice but to be premeditated.” Audits prompt the review of both input data and output decisions, and when done by a third-party evaluator, they can provide insight into the algorithm’s behavior. While some audits may require technical expertise, this may not always be the case. Facial recognition software that misidentifies persons of color more than whites is an instance where a stakeholder or user can spot biased outcomes, without knowing anything about how the algorithm makes decisions. “We should expect computers to have an audit trail,” shared roundtable participant Miranda Bogen from Upturn. Developing a regular and thorough audit of the data collected for the algorithmic operation, along with responses from developers, civil society, and others impacted by the algorithm, will better detect and possibly deter biases.  “Developing a regular and thorough audit of the data collected for the algorithmic operation, along with responses from developers, civil society, and others impacted by the algorithm, will better detect and possibly deter biases.”  The experience of government officials in Allegheny County reflects the importance of third-party auditing. In 2016, the Department of Human Services launched a decision support tool, the Allegheny Family Screening Tool (AFST), to generate a score for which children are most likely to be removed from their homes within two years, or to be re-referred to the county’s child welfare office due to suspected abuse. The county took ownership of its use of the tool, worked collaboratively with the developer, and commissioned an independent evaluation of its direct and indirect effects on the maltreatment screening process, including decision accuracy, workload, and consistency. County officials also sought additional independent research from experts to determine if the software was discriminating against certain groups. In 2017, the findings did identify some statistical imbalances, with error rates higher across racial and ethnic groups. White children who were scored at the highest-risk of maltreatment were less likely to be removed from their homes compared to African-American children with similar risk scores. 45 The county responded to these findings as part of the rebuild of the tool, with version two implemented in November 2018. 46  Facebook recently completed a civil rights audit to determine its handling of issues and individuals from protected groups. 47 After the reveal of how the platform was handling a variety of issues, including voter suppression, content moderation, privacy, and diversity, the company has committed to an updated audit around its internal infrastructure to handle civil rights grievances and address diversity in its products’ designs by default. Recent actions by Facebook to ban white nationalist content or address disinformation campaigns are some of the results of these efforts. 48  Operators of algorithms must rely upon cross-functional work teams and expertise  Roundtable participants largely acknowledged the notion that organizations should employ cross-functional teams. But movement in this direction can be difficult in already-siloed organizations, despite the technical, societal, and possibly legal implications associated with the algorithm’s design and execution. Not all decisions will necessitate this type of cross-team review, but when these decisions carry risks of real harm, they should be employed. In the mitigation of bias and the management of the risks associated with the algorithm, collaborative work teams can compensate for the blind-spots often missed in smaller, segmented conversations and reviews. Bringing together experts from various departments, disciplines, and sectors will help facilitate accountability standards and strategies for mitigating online biases, including from engineering, legal, marketing, strategy, and communications.  Cross-functional work teams–whether internally driven or populated by external experts–can attempt to identify bias before and during the model’s rollout. Further, partnerships between the private sector, academics, and civil society organizations can also facilitate greater transparency in AI’s application to a variety of scenarios, particularly those that impact protected classes or are disseminated in the public interest. Kate Crawford, AI researcher and founder of the AI Now Partnership, suggested that “closed loops are not open for algorithmic auditing, for review, or for public debate” because they generally exacerbate the problems that they are trying to solve. 49 Further on this point, roundtable participant Natasha Duarte from the Center for Democracy and Technology spoke to Allegheny’s challenge when she shared, “[C]ompanies should be more forthcoming with describing the limits of their tech, and government should know what questions to ask in their assessments,” which speaks to the importance of more collaboration in this area.  Increase human involvement in the design and monitoring of algorithms  Even with all the precautionary measures listed above, there is still some risk that algorithms will make biased decisions. People will continue to play a role in identifying and correcting biased outcomes long after an algorithm is developed, tested, and launched. While more data can inform automated decision-making, this process should complement rather than fully replace human judgement. Roundtable participant Alex Peysakhovich from Facebook shared, “[W]e don’t need to eliminate human moderators. We need to hire more and get them to focus on edge cases.” Such sentiment is growing increasingly important in this field as the comparative advantages of humans and algorithms become more distinguishable and the use of both improves the outcomes for online users.  People will continue to play a role in identifying and correcting biased outcomes long after an algorithm is developed, tested, and launched. (Credit: Gabrielle Lurie/Reuters)  However, privacy implications will arise when more humans are engaged in algorithm management, particularly if more sensitive information is involved in the model’s creation or in testing the algorithm’s predictions for bias. The timing of the roundtables, which also transpired around the adoption of the EU’s GDPR, spoke to the need for increased consumer privacy principles where users are empowered over what data they want to share with companies. As the U.S. currently debates the need for federal privacy legislation, access to and use of personal data may become even more difficult, potentially leaving algorithmic models prone to more bias. Because the values of creators and users of algorithms shift over time, humans must arbitrate conflicts between outcomes and stated goals. In addition to periodical audits, human involvement provides continuous feedback on the performance of bias mitigation efforts.  Other public policy recommendations  As indicated throughout the paper, policymakers play a critical role in identifying and mitigating biases, while ensuring that the technologies continue to make positive economic and societal benefits.  Congress should implement regulatory sandboxes and safe harbors to curb online biases  Regulatory sandboxes are perceived as one strategy for the creation of temporary reprieves from regulation to allow the technology and rules surrounding its use to evolve together. These policies could apply to algorithmic bias and other areas where the technology in question has no analog covered by existing regulations. Rather than broaden the scope of existing regulations or create rules in anticipation of potential harms, a sandbox allows for innovation both in technology and its regulation. Even in a highly regulated industry, the creation of sandboxes where innovations can be tested alongside with lighter touch regulations can yield benefits.  “Rather than broaden the scope of existing regulations or create rules in anticipation of potential harms, a sandbox allows for innovation both in technology and its regulation.”  For example, companies within the financial sector that are leveraging technology, or fintech, have shown how regulatory sandboxes can spur innovation in the development of new products and services. 50 These companies make extensive use of algorithms for everything from spotting fraud to deciding to extend credit. Some of these activities mirror those of regular banks, and those would still fall under existing rules, but new ways of approaching tasks would be allowed within the sandbox. 51 Because sandboxes give innovators greater leeway in developing new products and services, they will require active oversight until technology and regulations mature. The U.S. Treasury recently reported not only on the benefits that countries that have adopted fintech regulatory sandboxes have realized, but recommended that the U.S. adopt fintech sandboxes to spur innovation. 52 Given the broad usefulness of algorithms to spur innovation in various regulated industries, participants in the roundtables considered the potential usefulness of extending regulatory sandboxes to other areas where algorithms can help to spur innovations.  Regulatory safe harbors could also be employed, where a regulator could specify which activities do not violate existing regulations. 53 This approach has the advantage of increasing regulatory certainty for algorithm developers and operators. For example, Section 230 of the Communications Decency Act removed liability from websites for the actions of their users, a provision widely credited with the growth of internet companies like Facebook and Google. The exemption later narrowed to exclude sex trafficking with the passage of the Stop Enabling Online Sex Trafficking Act and Fight Online Sex Trafficking Act. Applying a similar approach to algorithms could exempt their operators from liabilities in certain contexts while still upholding protections in others where harms are easier to identify. In line with the previous discussion on the use of certain protected attributes, safe harbors could be considered in instances where the collection of sensitive personal information is used for the specific purposes of bias detection and mitigation.  Consumers need better algorithmic literacy  Widespread algorithmic literacy is crucial for mitigating bias. Given the increased use of algorithms in many aspects of daily life, all potential subjects of automated decisions would benefit from knowledge of how these systems function. Just as computer literacy is now considered a vital skill in the modern economy, understanding how algorithms use their data may soon become necessary.  The subjects of automated decisions deserve to know when bias negatively affects them, and how to respond when it occurs. Feedback from users can share and anticipate areas where bias can manifest in existing and future algorithms. Over time, the creators of algorithms may actively solicit feedback from a wide range of data subjects and then take steps to educate the public on how algorithms work to aid in this effort. Public agencies that regulate bias can also work to raise algorithmic literacy as part of their missions. In both the public and private sector, those that stand to lose the most from biased decision-making can also play an active role in spotting it.  Conclusion  In December 2018, President Trump signed the First Step Act, new criminal justice legislation that encourages the usage of algorithms nationwide. 54 In particular, the system would use an algorithm to initially determine who can redeem earned-time credits—reductions in sentence for completion of educational, vocational, or rehabilitative programs—excluding inmates deemed higher risk. There is a likelihood that these algorithms will perpetuate racial and class disparities, which are already embedded in the criminal justice system. As a result, African-Americans and poor people in general will be more likely to serve longer prison sentences.  “When algorithms are responsibly designed, they may avoid the unfortunate consequences of amplified systemic discrimination and unethical applications.”  As outlined in the paper, these types of algorithms should be concerning if there is not a process in place that incorporates technical diligence, fairness, and equity from design to execution. That is, when algorithms are responsibly designed, they may avoid the unfortunate consequences of amplified systemic discrimination and unethical applications.  Some decisions will be best served by algorithms and other AI tools, while others may need thoughtful consideration before computer models are designed. Further, testing and review of certain algorithms will also identify, and, at best, mitigate discriminatory outcomes. For operators of algorithms seeking to reduce the risk and complications of bad outcomes for consumers, the promotion and use of the mitigation proposals can create a pathway toward algorithmic fairness, even if equity is never fully realized.   The Brookings Institution is a nonprofit organization devoted to independent research and policy solutions. Its mission is to conduct high-quality, independent research and, based on that research, to provide innovative, practical recommendations for policymakers and the public. The conclusions and recommendations of any Brookings publication are solely those of its author(s), and do not reflect the views of the Institution, its management, or its other scholars.  Amazon, Facebook, Google, IBM, and Microsoft provide general, unrestricted support to The Brookings Institution. Paul Resnick is also a consultant to Facebook, but this work is independent and his views expressed here are his own. The findings, interpretations, and conclusions posted in this piece are not influenced by any donation. Brookings recognizes that the value it provides is in its absolute commitment to quality, independence, and impact. Activities supported by its donors reflect this commitment.   Appendix: List of Roundtable Participants     Participant  Organization      Wendy Anderson  Office of Congresswoman Val Demings    Norberto Andrade  Facebook    Solon Barocas  Cornell University    Genie Barton  Privacy Genie    Ricardo Baeza-Yates  NTENT    Miranda Bogen  Upturn    John Brescia  Better Business Bureau    Julie Brill  Microsoft    Rich Caruana  Microsoft Research    Eli Cohen  Brookings Institution    Anupam Datta  Carnegie Mellon    Deven Desai  Georgia Tech    Natasha Duarte  Center for Democracy and Technology    Nadia Fawaz  LinkedIn    Laura Fragomeni  Walmart Global eCommerce    Sharad Goel  Stanford University    Scott Golder  Cornell University    Aaron Halfaker  Wikimedia    Sarah Holland  Google    Jack Karsten  Brookings Institution    Krishnaram Kenthapadi  LinkedIn and Stanford University    Jon Kleinberg  Cornell University    Isabel Kloumann  Facebook    Jake Metcalf  Ethical Resolve    Alex Peysakhovich  Facebook    Paul Resnick  University of Michigan    William Rinehart  American Action Forum    Alex Rosenblat  Data and Society    Jake Schneider  Brookings Institution    Jasjeet Sekhon  University of California-Berkeley    Rob Sherman  Facebook    JoAnn Stonier  Mastercard Worldwide    Nicol Turner Lee  Brookings Institution    Lucy Vasserman  Jigsaw’s Conversation AI Project / Google    Suresh Venkatasubramanian  University of Utah    John Verdi  Future of Privacy Forum    Heather West  Mozilla    Jason Yosinki  Uber    Jinyan Zang  Harvard University    Leila Zia  Wikimedia Foundation      References  Angwin, Julia, and Terry Parris Jr. “Facebook Lets Advertisers Exclude Users by Race.” Text/html. ProPublica, October 28, 2016. https://www.propublica.org/article/facebook-lets-advertisers-exclude-users-by-race.  Angwin, Julia, Jeff Larson, Surya Mattu, and Laura Kirchner. “Machine Bias.” ProPublica, May 23, 2016. Available at https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing (last accessed April 19, 2019).  Barocas, Solon, and Andrew D. Selbst, “Big Data’s Disparate Impact,” SSRN Scholarly Paper (Rochester, NY: Social Science Research Network, 2016. Available at https://papers.ssrn.com/abstract=2477899.  Blass, Andrea, and Yuri Gurevich. Algorithms: A Quest for Absolute Definitions. Bulletin of European Association for Theoretical Computer Science 81, 2003. https://www.microsoft.com/en-us/research/wp-content/uploads/2017/01/164.pdf (last accessed April 12, 2019).  Brennan, Tim, William Dieterich, and Beate Ehret. “Evaluating the Predictive Validity of the COMPAS Risk and Needs Assessment System.” Criminal Justice and Behavior 36 (2009): 21–40.  Chessell, Mandy. “Ethics for Big Data and Analytics.” IBM, n.d. Available at https://www.ibmbigdatahub.com/sites/default/files/whitepapers_reports_file/TCG%20Study%20Report%20-%20Ethics%20for%20BD%26A.pdf (last accessed April 19, 2019).  Chodosh, Sara. “Courts use algorithms to help determine sentencing, but random people get the same results.” Popular Science, January 18, 2018. Available at https://www.popsci.com/recidivism-algorithm-random-bias (last accessed October 15, 2018).  Corbett-Davies, Sam, Emma Peirson, Avi Feller, and Sharad Goel. “A Computer Program Used for Bail and Sentencing Decisions Was Labeled Biased against Blacks. It’s Actually Not That Clear.” Washington Post (blog), October 17, 2016. Available at https://www.washingtonpost.com/news/monkey-cage/wp/2016/10/17/can-an-algorithm-be-racist-our-analysis-is-more-cautious-than-propublicas/ (last accessed April 19. 2019).  Corbett-Davies, Sam, Emma Pierson, Avi Feller, Sharad Goel, and Aziz Huq. “Algorithmic Decision Making and the Cost of Fairness.” ArXiv:1701.08230 [Cs, Stat], January 27, 2017. https://doi.org/10.1145/3097983.309809.  Courtland, Rachel. “Bias Detectives: The Researchers Striving to Make Algorithms Fair,” Nature 558, no. 7710 (June 2018): 357–60. Available at https://doi.org/10.1038/d41586-018-05469-3 (last accessed April 19, 2019).  DeAngelius, Stephen F. “Artificial intelligence: How algorithms make systems smart,” Wired Magazine, September 2014. Available at https://www.wired.com//insights/2014/09/artificial-intelligence-algorithms-2/ (last accessed April 12, 2019).  Elejalde-Ruiz, Alexia. “The end of the resume? Hiring is in the midst of technological revolution with algorithms, chatbots.” Chicago Tribune (July 19, 2018). Available at http://www.chicagotribune.com/business/ct-biz-artificial-intelligence-hiring-20180719-story.html.  Eubanks, Virginia. “A Child Abuse Prediction Model Fails Poor Families,” Wired, January 15, 2018. Available at https://www.wired.com/story/excerpt-from-automating-inequality/ (last accessed April 19, 2019).  FTC Hearing #7: The Competition and Consumer Protection Issues of Algorithms, Artificial Intelligence, and Predictive Analytics, § Federal Trade Commission (2018). https://www.ftc.gov/system/files/documents/public_events/1418693/ftc_hearings_session_7_transcript_day_2_11-14-18.pdf.  Garbade, Michael J. “Clearing the Confusion: AI vs. Machine Learning vs. Deep Learning Differences,” Towards Data Science, September 14, 2018. Available at https://towardsdatascience//clearing-the-confusion-ai-vs-machine-learning-vs-deep-learning-differences-fce69b21d5eb (last accessed April 12, 2019).  Griggs v. Duke Power Company, Oyez. Available at https//www.oyez.org/cases/1970/124 (last accessed October 1, 2018.  Guerin, Lisa. “Disparate Impact Discrimination.” www.nolo.com. Available at https://www.nolo.com/legal-encyclopedia/disparate-impact-discrimination.htm (last accessed April 24, 2019).  Hadhazy, Adam. “Biased Bots: Artificial-Intelligence Systems Echo Human Prejudices.” Princeton University, April 18, 2017. Available at https://www.princeton.edu/news/2017/04/18/biased-bots-artificial-intelligence-systems-echo-human-prejudices (last accessed April 20, 2019).  Hamilton, Isobel Asher. “Why It’s Totally Unsurprising That Amazon’s Recruitment AI Was Biased against Women.” Business Insider, October 13, 2018. Available at https://www.businessinsider.com/amazon-ai-biased-against-women-no-surprise-sandra-wachter-2018-10 (last accessed April 20, 2019).  Hardesty, Larry. “Study Finds Gender and Skin-Type Bias in Commercial Artificial-Intelligence Systems.” MIT News, February 11, 2018. Available at http://news.mit.edu/2018/study-finds-gender-skin-type-bias-artificial-intelligence-systems-0212 (last accessed April 19, 2019).  High-level Expert Group on Artificial Intelligence. “Ethics Guidelines for Trustworthy AI (Draft).” The European Commission, December 18, 2018.  Ingold, David, and Spencer Soper. “Amazon Doesn’t Consider the Race of Its Customers. Should It?” Bloomberg.com, April 21, 2016. http://www.bloomberg.com/graphics/2016-amazon-same-day/ .  Kearns, Michael. “Data Intimacy, Machine Learning and Consumer Privacy.” University of Pennsylvania Law School, May 2018. Available at https://www.law.upenn.edu/live/files/7952-kearns-finalpdf (last accessed April 12, 2019).  Kleinberg, Jon, Sendhil Mullainathan, and Manish Raghavan, “Inherent Trade-Offs in the Fair Determination of Risk Scores.”In Proceedings of Innovations in Theoretical Computer Science (ITCS), 2017. Available at https://arxiv.org/pdf/1609.05807.pdf (last accessed April 19, 2019).  Larson, Jeff, Surya Mattu, and Julia Angwin. “Unintended Consequences of Geographic Targeting.” Technology Science, September 1, 2015. Available at https://techscience.org/a/2015090103/ (last accessed April 19, 2019).  Locklear, Mallory. “Facebook Releases an Update on Its Civil Rights Audit.” Engadget (blog), December 18, 2018. Available at https://www.engadget.com/2018/12/18/facebook-update-civil-rights-audit/ (last accessed April 19, 2019).  Lopez, German. “The First Step Act, Congress’s Criminal Justice Reform Bill, Explained.” Vox, December 3, 2018. Available at https://www.vox.com/future-perfect/2018/12/3/18122392/first-step-act-criminal-justice-reform-bill-congress (last accessed April 16, 2019).  Mnuchin, Steven T., and Craig S. Phillips. “A Financial System That Creates Economic Opportunities – Nonbank Financials, Fintech, and Innovation.” Washington, D.C.: U.S. Department of the Treasury, July 2018.Available at https://home.treasury.gov/sites/default/files/2018-08/A-Financial-System-that-Creates-Economic-Opportunities—Nonbank-Financials-Fintech-and-Innovation_0.pdf (last accessed April 19, 2019).  Reisman, Dillon, Jason Schultz, Kate Crawford, and Meredith Whittaker. “Algorithmic Impact Assessments: A Practical Framework for Public Agency Accountability.” New York: AI Now, April 2018.  Romei, Andrea, and Salvatore Ruggieri. “Discrimination Data Analysis: A Multi-Disciplinary Bibliography.” In Discrimination and Privacy in the Information Society, edited by Bart Custers, T Calders, B Schermer, and T Zarsky, 109–35. Studies in Applied Philosophy, Epistemology and Rational Ethics. Springer, Berlin, Heidelberg, 2013. Available at https://doi.org/10.1007/978-3-642-30487-3_6 (last accessed April 19, 2019).  Schatz, Brian. AI in Government Act of 2018, Pub. L. No. S.B. 3502 (2018). https://www.congress.gov/bill/115th-congress/senate-bill/3502.  Spielkamp, Matthias. “We Need to Shine More Light on Algorithms so They Can Help Reduce Bias, Not Perpetuate It.” MIT Technology Review. Accessed September 20, 2018. Available at https://www.technologyreview.com/s/607955/inspecting-algorithms-for-bias/ (last accessed April 19, 2019).  Stack, Liam. “Facebook Announces New Policy to Ban White Nationalist Content.” The New York Times, March 28, 2019, sec. Business. Available at https://www.nytimes.com/2019/03/27/business/facebook-white-nationalist-supremacist.html (last accessed April 19, 2019).  Sweeney, Latanya, and Jinyan Zang. “How appropriate might big data analytics decisions be when placing ads?” Powerpoint presentation presented at the Big Data: A tool for inclusion or exclusion, Federal Trade Commission conference, Washington, DC. September 15, 2014. Available at https://www.ftc.gov/systems/files/documents/public_events/313371/bigdata-slides-sweeneyzang-9_15_14.pdf (last accessed April 12, 2019).  Sweeney, Latanya. “Discrimination in online ad delivery.” Rochester, NY: Social Science Research Network, January 28, 2013. Available at https://papers.ssrn.com/abstract=2208240 (last accessed April 12, 2019).  Sydell, Laura. “It Ain’t Me, Babe: Researchers Find Flaws In Police Facial Recognition Technology.” NPR.org, October 25, 2016. Available at https://www.npr.org/sections/alltechconsidered/2016/10/25/499176469/it-aint-me-babe-researchers-find-flaws-in-police-facial-recognition (last accessed April 19, 2019).  “The Global Data Ethics Project.” Data for Democracy, n.d. https://www.datafordemocracy.org/project/global-data-ethics-project (last accessed April 19, 2019).  Tobin, Ariana. “HUD sues Facebook over housing discrimination and says the company’s algorithms have made the problem worse.” ProPublica (March 28, 2019). Available at https://www.propublica.org/article/hud-sues-facebook-housing-discrimination-advertising-algorithms (last accessed April 29, 2019).  Turner Lee, Nicol. “Inclusion in Tech: How Diversity Benefits All Americans,” § Subcommittee on Consumer Protection and Commerce, United States House Committee on Energy and Commerce (2019). Also available on Brookings web site, https://www.brookings.edu/testimonies/inclusion-in-tech-how-diversity-benefits-all-americans/ (last accessed April 29, 2019).  Turner Lee, Nicol. Detecting racial bias in algorithms and machine learning. Journal of Information, Communication and Ethics in Society 2018, Vol. 16 Issue 3, pp. 252-260. Available at https://doi.org/10.1108/JICES-06-2018-0056/ (last accessed April 29, 2019).  “Understanding bias in algorithmic design,” Impact.Engineered, September 5, 2017. Available at https://medium.com/impact-engineered/understanding-bias-in-algorithmic-design-db9847103b6e (last accessed April 12, 2019).  Vincent, James. “Amazon Reportedly Scraps Internal AI Recruiting Tool That Was Biased against Women.” The Verge, October 10, 2018. Available at https://www.theverge.com/2018/10/10/17958784/ai-recruiting-tool-bias-amazon-report (last accessed April 20, 2019).  Zafar, Muhammad Bilal, Isabel Valera Martinez, Manuel Gomez Rodriguez, and Krishna Gummadi. “Fairness Constraints: A Mechanism for Fair Classification.” In Proceedings of the 20th International Conference on Artificial Intelligence and Statistics (AISTATS). Fort Lauderdale, FL, 2017.  Zarsky, Tal. “Understanding Discrimination in the Scored Society.” SSRN Scholarly Paper. Rochester, NY: Social Science Research Network, January 15, 2015. https://papers.ssrn.com/abstract=2550248.     Report Produced by Center for Technology Innovation        Footnotes   Nicol Turner Lee, Fellow, Center for Technology Innovation, Brookings Institution; Paul Resnick, Michael D. Cohen Collegiate Professor of Information, Associate Dean for Research and Faculty Affairs, Professor of Information and Interim Director of Health Informatics, School of Information at the University of Michigan; Genie Barton, President, Institute for Marketplace Trust, Better Business Bureau and Member, Research Advisory Board, International Association of Privacy Professionals. The authors also acknowledge the input from the current leadership of the Better Business Bureau’s Institute for Marketplace Trust and Jinyan Zang, Harvard University.  The concepts of AI, algorithms and machine learning are often conflated and used interchangeably. In this paper, we will follow generally understood definitions of these terms as set out in publications for the general reader. See, e.g., Stephen F. DeAngelius. “Artificial intelligence: How algorithms make systems smart,” Wired Magazine, September 2014. Available at https://www.wired.com//insights/2014/09/artificial-intelligence-algorithms-2/ (last accessed April 12, 2019). See also, Michael J. Garbade. “Clearing the Confusion: AI vs. Machine Learning vs. Deep Learning Differences,” Towards Data Science, September 14, 2018. Available at https://towardsdatascience//clearing-the-confusion-ai-vs-machine-learning-vs-deep-learning-differences-fce69b21d5eb (last accessed April 12, 2019).  Andrea Blass and Yuri Gurevich. Algorithms: A Quest for Absolute Definitions. Bulletin of European Association for Theoretical Computer Science 81, 2003. https://www.microsoft.com/en-us/research/wp-content/uploads/2017/01/164.pdf (last accessed April 12, 2019).  Kearns, Michael. “Data Intimacy, Machine Learning and Consumer Privacy.” University of Pennsylvania Law School, May 2018. Available at https://www.law.upenn.edu/live/files/7952-kearns-finalpdf (last accessed April 12, 2019).  Technically, this describes what is called “supervised machine learning.”  Chodosh, Sara. “Courts use algorithms to help determine sentencing, but random people get the same results.” Popular Science, January 18, 2018. Available at https://www.popsci.com/recidivism-algorithm-random-bias (last accessed October 15, 2018).  Blog. “Understanding bias in algorithmic design,” Impact.Engineered, September 5, 2017. Available at https://medium.com/impact-engineered/understanding-bias-in-algorithmic-design-db9847103b6e (last accessed April 12, 2019). This definition is intended to include the concepts of disparate treatment and disparate impact, but the legal definitions were not designed with AI in mind. For example, the demonstration of disparate treatment does not describe the ways in which an algorithm can learn to treat similarly situated groups differently, as will be discussed later in the paper.  The recommendations offered in the paper are those of the authors and do not represent the views or a consensus of views among roundtable participants.  Hamilton, Isobel Asher. “Why It’s Totally Unsurprising That Amazon’s Recruitment AI Was Biased against Women.” Business Insider, October 13, 2018. Available at https://www.businessinsider.com/amazon-ai-biased-against-women-no-surprise-sandra-wachter-2018-10 (last accessed April 20, 2019).  Vincent, James. “Amazon Reportedly Scraps Internal AI Recruiting Tool That Was Biased against Women.” The Verge, October 10, 2018. Available at https://www.theverge.com/2018/10/10/17958784/ai-recruiting-tool-bias-amazon-report (last accessed April 20, 2019). Although Amazon scrubbed the data of the particular references that appeared to discriminate against female candidates, there was no guarantee that the algorithm could not find other ways to sort and rank male candidates higher so it was scrapped by the company.  Hadhazy, Adam. “Biased Bots: Artificial-Intelligence Systems Echo Human Prejudices.” Princeton University, April 18, 2017. Available at https://www.princeton.edu/news/2017/04/18/biased-bots-artificial-intelligence-systems-echo-human-prejudices (last accessed April 20, 2019).  Sweeney, Latanya. “Discrimination in online ad delivery.” Rochester, NY: Social Science Research Network, January 28, 2013. Available at https://papers.ssrn.com/abstract=2208240 (last accessed April 12, 2019).  Sweeney, Latanya and Jinyan Zang. “How appropriate might big data analytics decisions be when placing ads?” Powerpoint presentation presented at the Big Data: A tool for inclusion or exclusion, Federal Trade Commission conference, Washington, DC. September 15, 2014. Available at https://www.ftc.gov/systems/files/documents/public_events/313371/bigdata-slides-sweeneyzang-9_15_14.pdf (last accessed April 12, 2019).  “FTC Hearing #7: The Competition and Consumer Protection Issues of Algorithms, Artificial Intelligence, and Predictive Analytics,” § Federal Trade Commission (2018),  Hardesty, Larry. “Study Finds Gender and Skin-Type Bias in Commercial Artificial-Intelligence Systems.” MIT News, February 11, 2018. Available at http://news.mit.edu/2018/study-finds-gender-skin-type-bias-artificial-intelligence-systems-0212 (last accessed April 19, 2019). These companies were selected because they provided gender classification features in their software and the code was publicly available for testing.  Ibid.  COMPAS is a risk-and needs-assessment tool originally designed by Northpointe, Inc., to assist state corrections officials in making placement, management, and treatment decisions for offenders. Angwin, Julia, Jeff Larson, Surya Mattu, and Laura Kirchner. “Machine Bias.” ProPublica, May 23, 2016. Available at https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing (last accessed April 19, 2019). See also , Brennan, Tim, William Dieterich, and Beate Ehret. “Evaluating the Predictive Validity of the COMPAS Risk and Needs Assessment System.” Criminal Justice and Behavior 36 (2009): 21–40.  Corbett-Davies, Sam, Emma Pierson, Avi Feller, Sharad Goel, and Aziz Huq. “Algorithmic Decision Making and the Cost of Fairness.” ArXiv:1701.08230 [Cs, Stat], January 27, 2017. https://doi.org/10.1145/3097983.309809.  Solon Barocas and Andrew D. Selbst, “Big Data’s Disparate Impact,” SSRN Scholarly Paper (Rochester, NY: Social Science Research Network, 2016), https://papers.ssrn.com/abstract=2477899.  Turner Lee, Nicol. “Inclusion in Tech: How Diversity Benefits All Americans,” § Subcommittee on Consumer Protection and Commerce, United States House Committee on Energy and Commerce (2019). Also available on Brookings web site, https://www.brookings.edu/testimonies/inclusion-in-tech-how-diversity-benefits-all-americans/ (last accessed April 29, 2019).  Ibid. See also, Turner Lee, Nicol. Detecting racial bias in algorithms and machine learning. Journal of Information, Communication and Ethics in Society 2018, Vol. 16 Issue 3, pp. 252-260. Available at https://doi.org/10.1108/JICES-06-2018-0056/ (last accessed April 29, 2019).  Sydell, Laura. “It Ain’t Me, Babe: Researchers Find Flaws In Police Facial Recognition Technology.” NPR.org, October 25, 2016. Available at https://www.npr.org/sections/alltechconsidered/2016/10/25/499176469/it-aint-me-babe-researchers-find-flaws-in-police-facial-recognition (last accessed April 19, 2019).  Guerin, Lisa. “Disparate Impact Discrimination.” www.nolo.com. Available at https://www.nolo.com/legal-encyclopedia/disparate-impact-discrimination.htm (last accessed April 24, 2019). See also, Jewel v. NSA where the Electronic Frontier Foundation argues that massive (or dragnet) surveillance is illegal. Information about case available at https://www.eff.org/cases/jewel (last accessed April 19, 2019).  This is often called an anti-classification criterion that the algorithm cannot classify based on membership in the protected or sensitive classes.  Zarsky, Tal. “Understanding Discrimination in the Scored Society.” SSRN Scholarly Paper. Rochester, NY: Social Science Research Network, January 15, 2015. https://papers.ssrn.com/abstract=2550248 .  Larson, Jeff, Surya Mattu, and Julia Angwin. “Unintended Consequences of Geographic Targeting.” Technology Science, September 1, 2015. Available at https://techscience.org/a/2015090103/ (last accessed April 19, 2019).  Terry Parris Jr Julia Angwin, “Facebook Lets Advertisers Exclude Users by Race,” text/html, ProPublica, October 28, 2016. Available at https://www.propublica.org/article/facebook-lets-advertisers-exclude-users-by-race (last accessed April 19, 2019).  Amazon doesn’t consider the race of its customers. Should It? Bloomberg.com. Available at http//www.bloomberg.com/graphics/2016-amazon-same-day (last accessed April 19, 2019).  Corbett-Davies et al., “Algorithmic Decision Making and the Cost of Fairness.”  Solon Barocas and Andrew D. Selbst, “Big Data’s Disparate Impact,” SSRN Scholarly Paper (Rochester, NY: Social Science Research Network, 2016. Available at https://papers.ssrn.com/abstract=2477899.  See, Zafar, Muhammad Bilal, Isabel Valera Martinez, Manuel Gomez Rodriguez, and Krishna Gummadi. “Fairness Constraints: A Mechanism for Fair Classification.” In Proceedings of the 20th International Conference on Artificial Intelligence and Statistics (AISTATS). Fort Lauderdale, FL, 2017. See also , Spielkamp, Matthias. “We Need to Shine More Light on Algorithms so They Can Help Reduce Bias, Not Perpetuate It.” MIT Technology Review. Accessed September 20, 2018. Available at https://www.technologyreview.com/s/607955/inspecting-algorithms-for-bias/ (last accessed April 19, 2019). See also Corbett-Davies, Sam, Emma Peirson, Avi Feller, and Sharad Goel. “A Computer Program Used for Bail and Sentencing Decisions Was Labeled Biased against Blacks. It’s Actually Not That Clear.” Washington Post (blog), October 17, 2016. Available at https://www.washingtonpost.com/news/monkey-cage/wp/2016/10/17/can-an-algorithm-be-racist-our-analysis-is-more-cautious-than-propublicas/ (last accessed April 19. 2019).  Jon Kleinberg, Sendhil Mullainathan, and Manish Raghavan, “Inherent Trade-Offs in the Fair Determination of Risk Scores.”In Proceedings of Innovations in Theoretical Computer Science (ITCS), 2017. Available at https://arxiv.org/pdf/1609.05807.pdf (last accessed April 19, 2019).  This notion of disparate impact has been legally tested dating back to the 1971 U.S. Supreme Court decision, Griggs v. Duke Power Company where the defendant was found to be using intelligence test scores and high school diplomas as factors to hire more white applicants over people of color. As determined by the court decision, there was no correlation between the tests or educational requirements for the jobs in question. See, Griggs v. Duke Power Company, Oyez. Available at https//www.oyez.org/cases/1970/124 (last accessed October 1, 2018.  Various computer models are being created to combat the discriminatory effects of algorithmic bias. See , Romei, Andrea, and Salvatore Ruggieri. “Discrimination Data Analysis: A Multi-Disciplinary Bibliography.” In Discrimination and Privacy in the Information Society, edited by Bart Custers, T Calders, B Schermer, and T Zarsky, 109–35. Studies in Applied Philosophy, Epistemology and Rational Ethics. Springer, Berlin, Heidelberg, 2013. Available at https://doi.org/10.1007/978-3-642-30487-3_6 (last accessed April 19, 2019).  Corbett-Davies, Sam, Emma Pierson, Avi Feller, Sharad Goel, and Aziz Huq. “Algorithmic Decision Making and the Cost of Fairness.” ArXiv:1701.08230 [Cs, Stat], January 27, 2017. Available at https://doi.org/10.1145/3097983.309809 (last accessed April 19, 2019).  Ibid.  Schatz, Brian. AI in Government Act of 2018, Pub. L. No. S.B. 3502 (2018). https://www.congress.gov/bill/115th-congress/senate-bill/3502.  At its February meeting, the OECD announced that it had approved its expert group’s guidelines and hoped to (C.  See European Union, Digital Single Market, Ethics Guidelines for Trustworthy AI, available for download from https://ec.europa.eu/digital-single-market/en/news/ethics-guidelines-trustworthy-ai (last accessed April 19, 2019).  See High-level Expert Group on Artificial Intelligence. “Ethics Guidelines for Trustworthy AI (Draft).” The European Commission, December 18, 2018. See also , Chessell, Mandy. “Ethics for Big Data and Analytics.” IBM, n.d. Available at https://www.ibmbigdatahub.com/sites/default/files/whitepapers_reports_file/TCG%20Study%20Report%20-%20Ethics%20for%20BD%26A.pdf (last accessed April 19, 2019). https://ec.europa.eu/futurium/en/system/files/ged/ai_hleg_draft_ethics_guidelines_18_december.pdf. See also “The Global Data Ethics Project.” Data for Democracy, n.d. https://www.datafordemocracy.org/project/global-data-ethics-project (last accessed April 19, 2019).  Spielkamp, Matthias. “We need to shine more light on algorithms so they can help reduce bias, not perpetuate It.” MIT Technology Review. Available at https://www.technologyreview.com/s/607955/inspecting-algorithms-for-bias/ (last accessed September 20, 2018).  Tobin, Ariana. “HUD sues Facebook over housing discrimination and says the company’s algorithms have made the problem worse.” ProPublica (March 28, 2019). Available at https://www.propublica.org/article/hud-sues-facebook-housing-discrimination-advertising-algorithms (last accessed April 29, 2019).  Elejalde-Ruiz, Alexia. “The end of the resume? Hiring is in the midst of technological revolution with algorithms, chatbots.” Chicago Tribune (July 19, 2018). Available at http://www.chicagotribune.com/business/ct-biz-artificial-intelligence-hiring-20180719-story.html .  Reisman, Dillon, Jason Schultz, Kate Crawford, and Meredith Whittaker. “Algorithmic Impact Assessments: A Practical Framework for Public Agency Accountability.” New York: AI Now, April 2018.  Alexandra Chouldechova et al., “A Case Study of Algorithm-Assisted Decision Making in Child Maltreatment Hotline Screening Decisions,” 1st Conference on Fairness, Accountability and Transparency , n.d., 15.  Rhema Vaithianathan et al., “Section 7: Allegheny Family Screening Tool: Methodology, Version 2,” April 2019.  Locklear, Mallory. “Facebook Releases an Update on Its Civil Rights Audit.” Engadget (blog), December 18, 2018. Available at https://www.engadget.com/2018/12/18/facebook-update-civil-rights-audit/ (last accessed April 19, 2019).  Stack, Liam. “Facebook Announces New Policy to Ban White Nationalist Content.” The New York Times, March 28, 2019, sec. Business. Available at https://www.nytimes.com/2019/03/27/business/facebook-white-nationalist-supremacist.html (last accessed April 19, 2019).  Qtd. in Rachel Courtland, “Bias Detectives: The Researchers Striving to Make Algorithms Fair,” Nature 558, no. 7710 (June 2018): 357–60. Available at https://doi.org/10.1038/d41586-018-05469-3 (last accessed April 19, 2019).  Fintech regulatory sandboxes in UK , Singapore , and states in the U.S. are beginning to authorize them. They allow freedom to offer new financial products and use new technologies such as blockchain .  In March, the state of Arizona became the first U.S. state to create a “regulatory sandbox” for fintech companies , allowing them to test financial products on customers with lighter regulations. The U.K. has run a similar initiative called Project Innovate since 2014. The application of a sandbox can allow both startup companies and incumbent banks to experiment with more innovative products without worrying about how to reconcile them with existing rules.  Mnuchin, Steven T., and Craig S. Phillips. “A Financial System That Creates Economic Opportunities - Nonbank Financials, Fintech, and Innovation.” Washington, D.C.: U.S. Department of the Treasury, July 2018.Available at https://home.treasury.gov/sites/default/files/2018-08/A-Financial-System-that-Creates-Economic-Opportunities---Nonbank-Financials-Fintech-and-Innovation_0.pdf (last accessed April 19, 2019).  Another major tech-related Safe Harbor is the EU-US Privacy Shield after the previous Safe Harbor was declared invalid in the EU. Available at https://en.wikipedia.org/wiki/EU%E2%80%93US_Privacy_Shield (last accessed April 19. 2019).  Lopez, German. “The First Step Act, Congress’s Criminal Justice Reform Bill, Explained.” Vox, December 3, 2018. Available at https://www.vox.com/future-perfect/2018/12/3/18122392/first-step-act-criminal-justice-reform-bill-congress (last accessed April 16, 2019).       Related Topics   Technology & Innovation  Telecommunications & Internet                 Find us on Facebook        Find us on Twitter        Find us on YouTube        Listen to our Podcast        Browse Newsletters        Subscribe to our RSS       Languages  Español  中文  عربي      About Us  Research Programs  Find an Expert  Careers  Contact  Terms and Conditions  Brookings Privacy Policy  Copyright 2020 The Brookings Institution               Trending     U.S. Politics & Government      Topics     AI    Policy 2020    Cities & Regions    Global Dev    Intl Affairs    U.S. Economy    U.S. Politics & Govt    More      About Us    Press Room    Experts    Events    The Brookings Press    WashU at Brookings    Careers    Support Brookings     Cart  0       Get daily updates from Brookings           Enter Email                     Send to Email Address   Your Name   Your Email Address        Cancel  Post was not sent - check your email addresses!  Email check failed, please try again  Sorry, your blog cannot share posts by email.                      "
10,splitting biases(fallacy OR bias OR heuristic) (coronavirus OR covid OR pandemic),1,https://www.visualcapitalist.com/11-cognitive-biases-influence-politics/,"Cognitive Biases in the Political Arena

With the 2020 U.S. presidential election fast approaching, many people will be glued to the 24-hour news cycle to stay up to date on political developments. Yet, when searching for facts, our own cognitive biases often get in the way.

If this isn’t problematic enough, third parties can also take advantage of these biases to influence our thinking. The media, for example, can exploit our tendency to assign stereotypes to others by only providing catchy, surface-level information. Once established in our minds, these generalizations can be tough to shake off.

Such tactics can have a powerful influence on public opinion if applied consistently to a broad audience. To help us avoid these mental pitfalls, today’s infographic from PredictIt lists common cognitive biases that influence the realm of politics, beginning with the “Big Cs”.

The First C: Confirmation Bias

People exhibit confirmation bias when they seek information that only affirms their pre-existing beliefs. This can cause them to become overly rigid in their political opinions, even when presented with conflicting ideas or evidence.

When too many people fall victim to this bias, progress towards solving complex sociopolitical issues is thwarted. That’s because solving these issues in a bipartisan system requires cooperation from both sides of the spectrum.

A reluctance towards establishing a common ground is already widespread in America. According to a 2019 survey, 70% of Democrats believed their party’s leaders should “stand up” to President Trump, even if less gets done in Washington. Conversely, 51% of Republicans believed that Trump should “stand up” to Democrats.

In light of these developments, researchers have conducted studies to determine if the issue of confirmation bias is as prevalent as it seems. In one experiment, participants chose to either support or oppose a given sociopolitical issue. They were then presented with evidence that was conflicting, affirming, or a combination of both.

In all scenarios, participants were most likely to stick with their initial decisions. Of those presented with conflicting evidence, just one in five changed their stance. Furthermore, participants who maintained their initial positions became even more confident in the superiority of their decision—a testament to how influential confirmation bias can be.

The Second C: Coverage Bias

Coverage bias, in the context of politics, is a form of media bias where certain politicians or topics are disproportionately covered. In some cases, media outlets can even twist stories to fit a certain narrative.

For example, research from the University of South Florida analyzed media coverage on President Trump’s 2017 travel ban. It was discovered that primetime media hosts covered the ban through completely different perspectives.

Each host varied drastically in tone, phrasing, and facts of emphasis, […] presenting each issue in a manner that aligns with a specific partisan agenda.

—Josepher, Bryce (2017)

Charting the ideological placement of each source’s audience can help us gain a better understanding of the coverage bias at work. In other words, where do people on the left, middle, and right get their news?

The horizontal axis in this graphic corresponds to the Ideological Consistency Scale, which is composed of 10 questions. For each question, respondents are assigned a “-1” for a liberal response, “+1” for a conservative response, or a “0” for other responses. A summation of these scores places a respondent into one of five categories:

Ideological Category Ranking Consistently conservative +7 to +10 Mostly conservative +3 to +6 Mixed -2 to +2 Mostly liberal -6 to -3 Consistently liberal -10 to -7

Overcoming coverage bias—which dovetails into other biases like confirmation bias—may require us to follow a wider variety of sources, even those we may not initially agree with.

The Third C: Concision Bias

Concision bias is a type of bias where politicians or the media selectively focus on aspects of information that are easy to get across. In the process, more nuanced and delicate views get omitted from popular discourse.

A common application of concision bias is the use of sound bites, which are short clips that can be taken out of a politician’s speech. When played in isolation, these clips may leave out important context for the audience.

Without the proper context, multi-faceted issues can become extremely polarizing, and may be a reason for the growing partisan divide in America. In fact, there is less overlap in the political values of Republicans and Democrats than ever previously measured.

In 1994, just 64% of Republicans were more conservative than the median Democrat. By 2017, that margin had grown considerably, to 95% of Republicans. The same trend can be found on the other end of the spectrum. Whereas 70% of Democrats were more liberal than the median Republican in 1994, this proportion increased to 97% by 2017.

Overcoming Our Biases

Achieving full self-awareness can be difficult, especially when new biases emerge in our constantly evolving world. So where do we begin?

Simply remembering these mental pitfalls exist can be a great start—after all, we can’t fix what we don’t know. Individuals concerned about the upcoming presidential election may find it useful to focus their attention on the Big Cs, as these biases can play a significant role in shaping political beliefs. Maintaining an open mindset and diversifying the media sources we follow are two tactics that may act as a hedge.","                 Skip to main content          Search Brookings         About Us Press Room Experts Events The Brookings Press WashU at Brookings Careers Support Brookings    Cart  0    Search Guidance for the Brookings community and the public on our response to the coronavirus (COVID-19) »  Learn more from Brookings scholars about the global response to coronavirus (COVID-19) »                  Algorithmic bias detection and mitigation: Best practices and policies to reduce consumer harms  Facebook Twitter LinkedIn Print SMS Email More Reddit AI  Policy 2020  Cities & Regions  Global Dev  Intl Affairs  U.S. Economy  U.S. Politics & Govt  More             0                Report  Algorithmic bias detection and mitigation: Best practices and policies to reduce consumer harms   Nicol Turner Lee , Paul Resnick , and Genie Barton  Wednesday, May 22, 2019                       Facebook Twitter LinkedIn Print SMS Email More Reddit  For media inquiries, contact:   Governance Studies Main Line  202.797.6090           Introduction         Nicol Turner Lee  Senior Fellow - Governance Studies  Director - Center for Technology Innovation     @drturnerlee          Paul Resnick  Professor of Information; Associate Dean for Research and Faculty Affairs, School of Information - University of Michigan     presnick          Genie Barton  Member, Research Advisory Board - International Association of Privacy Professionals     privacygenie       The private and public sectors are increasingly turning to artificial intelligence (AI) systems and machine learning algorithms to automate simple and complex decision-making processes. 1 The mass-scale digitization of data and the emerging technologies that use them are disrupting most economic sectors, including transportation, retail, advertising, and energy, and other areas. AI is also having an impact on democracy and governance as computerized systems are being deployed to improve accuracy and drive objectivity in government functions.  The availability of massive data sets has made it easy to derive new insights through computers. As a result, algorithms, which are a set of step-by-step instructions that computers follow to perform a task, have become more sophisticated and pervasive tools for automated decision-making. 2 While algorithms are used in many contexts, we focus on computer models that make inferences from data about people, including their identities, their demographic attributes, their preferences, and their likely future behaviors, as well as the objects related to them. 3  “Algorithms are harnessing volumes of macro- and micro-data to influence decisions affecting people in a range of tasks, from making movie recommendations to helping banks determine the creditworthiness of individuals.”  In the pre-algorithm world, humans and organizations made decisions in hiring, advertising, criminal sentencing, and lending. These decisions were often governed by federal, state, and local laws that regulated the decision-making processes in terms of fairness, transparency, and equity. Today, some of these decisions are entirely made or influenced by machines whose scale and statistical rigor promise unprecedented efficiencies. Algorithms are harnessing volumes of macro- and micro-data to influence decisions affecting people in a range of tasks, from making movie recommendations to helping banks determine the creditworthiness of individuals. 4 In machine learning, algorithms rely on multiple data sets, or training data, that specifies what the correct outputs are for some people or objects. From that training data, it then learns a model which can be applied to other people or objects and make predictions about what the correct outputs should be for them. 5  However, because machines can treat similarly-situated people and objects differently, research is starting to reveal some troubling examples in which the reality of algorithmic decision-making falls short of our expectations. Given this, some algorithms run the risk of replicating and even amplifying human biases, particularly those affecting protected groups. 6 For example, automated risk assessments used by U.S. judges to determine bail and sentencing limits can generate incorrect conclusions, resulting in large cumulative effects on certain groups, like longer prison sentences or higher bails imposed on people of color.  In this example, the decision generates “bias,” a term that we define broadly as it relates to outcomes which are systematically less favorable to individuals within a particular group and where there is no relevant difference between groups that justifies such harms. 7 Bias in algorithms can emanate from unrepresentative or incomplete training data or the reliance on flawed information that reflects historical inequalities. If left unchecked, biased algorithms can lead to decisions which can have a collective, disparate impact on certain groups of people even without the programmer’s intention to discriminate. The exploration of the intended and unintended consequences of algorithms is both necessary and timely, particularly since current public policies may not be sufficient to identify, mitigate, and remedy consumer impacts.  With algorithms appearing in a variety of applications, we argue that operators and other concerned stakeholders must be diligent in proactively addressing factors which contribute to bias. Surfacing and responding to algorithmic bias upfront can potentially avert harmful impacts to users and heavy liabilities against the operators and creators of algorithms, including computer programmers, government, and industry leaders. These actors comprise the audience for the series of mitigation proposals to be presented in this paper because they either build, license, distribute, or are tasked with regulating or legislating algorithmic decision-making to reduce discriminatory intent or effects.   Related             Technology & Innovation  How artificial intelligence is transforming the world   Darrell M. West and John R. Allen  Tuesday, April 24, 2018                Technology & Innovation  Trends in the Information Technology sector   Makada Henry-Nickie , Kwadwo Frimpong , and Hao Sun  Friday, March 29, 2019                Media & Journalism  How to combat fake news and disinformation   Darrell M. West  Monday, December 18, 2017        Our research presents a framework for algorithmic hygiene , which identifies some specific causes of biases and employs best practices to identify and mitigate them. We also present a set of public policy recommendations, which promote the fair and ethical deployment of AI and machine learning technologies.  This paper draws upon the insight of 40 thought leaders from across academic disciplines, industry sectors, and civil society organizations who participated in one of two roundtables. 8 Roundtable participants actively debated concepts related to algorithmic design, accountability, and fairness, as well as the technical and social trade-offs associated with various approaches to bias detection and mitigation.  Our goal is to juxtapose the issues that computer programmers and industry leaders face when developing algorithms with the concerns of policymakers and civil society groups who assess their implications. To balance the innovations of AI and machine learning algorithms with the protection of individual rights, we present a set of public policy recommendations, self-regulatory best practices, and consumer-focused strategies–all of which promote the fair and ethical deployment of these technologies.  Our public policy recommendations include the updating of nondiscrimination and civil rights laws to apply to digital practices, the use of regulatory sandboxes to foster anti-bias experimentation, and safe harbors for using sensitive information to detect and mitigate biases. We also outline a set of self-regulatory best practices, such as the development of a bias impact statement, inclusive design principles, and cross-functional work teams. Finally, we propose additional solutions focused on algorithmic literacy among users and formal feedback mechanisms to civil society groups.  The next section provides five examples of algorithms to explain the causes and sources of their biases. Later in the paper, we discuss the trade-offs between fairness and accuracy in the mitigation of algorithmic bias, followed by a robust offering of self-regulatory best practices, public policy recommendations, and consumer-driven strategies for addressing online biases. We conclude by highlighting the importance of proactively tackling the responsible and ethical use of machine learning and other automated decision-making tools.  Examples of algorithmic biases  Algorithmic bias can manifest in several ways with varying degrees of consequences for the subject group. Consider the following examples, which illustrate both a range of causes and effects that either inadvertently apply different treatment to groups or deliberately generate a disparate impact on them.  Bias in online recruitment tools  Online retailer Amazon, whose global workforce is 60 percent male and where men hold 74 percent of the company’s managerial positions, recently discontinued use of a recruiting algorithm after discovering gender bias. 9 The data that engineers used to create the algorithm were derived from the resumes submitted to Amazon over a 10-year period, which were predominantly from white males. The algorithm was taught to recognize word patterns in the resumes, rather than relevant skill sets, and these data were benchmarked against the company’s predominantly male engineering department to determine an applicant’s fit. As a result, the AI software penalized any resume that contained the word “women’s” in the text and downgraded the resumes of women who attended women’s colleges, resulting in gender bias. 10  Amazon discontinued a recruiting algorithm after discovering that it led to gender bias in its hiring. (Credit: Brian Snyder/Reuters)  Bias in word associations  Princeton University researchers used off-the-shelf machine learning AI software to analyze and link 2.2 million words. They found that European names were perceived as more pleasant than those of African-Americans, and that the words “woman” and “girl” were more likely to be associated with the arts instead of science and math, which were most likely connected to males. 11 In analyzing these word-associations in the training data, the machine learning algorithm picked up on existing racial and gender biases shown by humans. If the learned associations of these algorithms were used as part of a search-engine ranking algorithm or to generate word suggestions as part of an auto-complete tool, it could have a cumulative effect of reinforcing racial and gender biases.  Bias in online ads  Latanya Sweeney, Harvard researcher and former chief technology officer at the Federal Trade Commission (FTC), found that online search queries for African-American names were more likely to return ads to that person from a service that renders arrest records, as compared to the ad results for white names. 12 Her research also found that the same differential treatment occurred in the micro-targeting of higher-interest credit cards and other financial products when the computer inferred that the subjects were African-Americans, despite having similar backgrounds to whites. 13 During a public presentation at a FTC hearing on big data, Sweeney demonstrated how a web site, which marketed the centennial celebration of an all-black fraternity, received continuous ad suggestions for purchasing “arrest records” or accepting high-interest credit card offerings. 14  Bias in facial recognition technology  MIT researcher Joy Buolamwini found that the algorithms powering three commercially available facial recognition software systems were failing to recognize darker-skinned complexions. 15 Generally, most facial recognition training data sets are estimated to be more than 75 percent male and more than 80 percent white. When the person in the photo was a white man, the software was accurate 99 percent of the time at identifying the person as male. According to Buolamwini’s research, the product error rates for the three products were less than one percent overall, but increased to more than 20 percent in one product and 34 percent in the other two in the identification of darker-skinned women as female. 16 In response to Buolamwini’s facial-analysis findings, both IBM and Microsoft committed to improving the accuracy of their recognition software for darker-skinned faces.  Bias in criminal justice algorithms  Acknowledging the possibility and causes of bias is the first step in any mitigation approach.  The COMPAS (Correctional Offender Management Profiling for Alternative Sanctions) algorithm, which is used by judges to predict whether defendants should be detained or released on bail pending trial, was found to be biased against African-Americans, according to a report from ProPublica. 17 The algorithm assigns a risk score to a defendant’s likelihood to commit a future offense, relying on the voluminous data available on arrest records, defendant demographics, and other variables. Compared to whites who were equally likely to re-offend, African-Americans were more likely to be assigned a higher-risk score, resulting in longer periods of detention while awaiting trial. 18 Northpointe, the firm that sells the algorithm’s outputs, offers evidence to refute such claims and argues that wrong metrics are being used to assess fairness in the product, a topic that we return to later in the paper.  While these examples of bias are not exhaustive, they suggest that these problems are empirical realities and not just theoretical concerns. They also illustrate how these outcomes emerge, and in some cases, without malicious intent by the creators or operators of the algorithm. Acknowledging the possibility and causes of bias is the first step in any mitigation approach. On this point, roundtable participant Ricardo Baeza-Yates from NTENT stated that “[companies] will continue to have a problem discussing algorithmic bias if they don’t refer to the actual bias itself.”  Causes of bias  Barocas and Selbst point out that bias can creep in during all phases of a project, “…whether by specifying the problem to be solved in ways that affect classes differently, failing to recognize or address statistical biases, reproducing past prejudice, or considering an insufficiently rich set of factors.” 19 Roundtable participants focused especially on bias stemming from flaws in the data used to train the algorithms. “Flawed data is a big problem,” stated roundtable participant Lucy Vasserman from Google, “…especially for the groups that businesses are working hard to protect.” While there are many causes, we focus on two of them: historical human biases and incomplete or unrepresentative data .  Historical human biases  Historical human biases are shaped by pervasive and often deeply embedded prejudices against certain groups, which can lead to their reproduction and amplification in computer models. In the COMPAS algorithm, if African-Americans are more likely to be arrested and incarcerated in the U.S. due to historical racism, disparities in policing practices, or other inequalities within the criminal justice system, these realities will be reflected in the training data and used to make suggestions about whether a defendant should be detained. If historical biases are factored into the model, it will make the same kinds of wrong judgments that people do.  The Amazon recruitment algorithm revealed a similar trajectory when men were the benchmark for professional “fit,” resulting in female applicants and their attributes being downgraded. These historical realities often find their way into the algorithm’s development and execution, and they are exacerbated by the lack of diversity which exists within the computer and data science fields. 20  Further, human biases can be reinforced and perpetuated without the user’s knowledge. For example, African-Americans who are primarily the target for high-interest credit card options might find themselves clicking on this type of ad without realizing that they will continue to receive such predatory online suggestions. In this and other cases, the algorithm may never accumulate counter-factual ad suggestions (e.g., lower-interest credit options) that the consumer could be eligible for and prefer. Thus, it is important for algorithm designers and operators to watch for such potential negative feedback loops that cause an algorithm to become increasingly biased over time.  Incomplete or unrepresentative training data  Insufficient training data is another cause of algorithmic bias. If the data used to train the algorithm are more representative of some groups of people than others, the predictions from the model may also be systematically worse for unrepresented or under-representative groups. For example, in Buolamwini’s facial-analysis experiments, the poor recognition of darker-skinned faces was largely due to their statistical under-representation in the training data. That is, the algorithm presumably picked up on certain facial features, such as the distance between the eyes, the shape of the eyebrows and variations in facial skin shades, as ways to detect male and female faces. However, the facial features that were more representative in the training data were not as diverse and, therefore, less reliable to distinguish between complexions, even leading to a misidentification of darker-skinned females as males.  Turner Lee has argued that it is often the lack of diversity among the programmers designing the training sample which can lead to the under-representation of a particular group or specific physical attributes. 21 Buolamwini’s findings were due to her rigor in testing, executing, and assessing a variety of proprietary facial-analysis software in different settings, correcting for the lack of diversity in their samples.  Conversely, algorithms with too much data, or an over-representation, can skew the decision toward a particular result. Researchers at Georgetown Law School found that an estimated 117 million American adults are in facial recognition networks used by law enforcement, and that African-Americans were more likely to be singled out primarily because of their over-representation in mug-shot databases. 22 Consequently, African-American faces had more opportunities to be falsely matched, which produced a biased effect.  Bias detection strategies  Understanding the various causes of biases is the first step in the adoption of effective algorithmic hygiene. But, how can operators of algorithms assess whether their results are, indeed, biased? Even when flaws in the training data are corrected, the results may still be problematic because context matters during the bias detection phase.  “Even when flaws in the training data are corrected, the results may still be problematic because context matters during the bias detection phase.”  First, all detection approaches should begin with careful handling of the sensitive information of users, including data that identify a person’s membership in a federally protected group (e.g., race, gender). In some cases, operators of algorithms may also worry about a person’s membership in some other group if they are also susceptible to unfair outcomes. An examples of this could be college admission officers worrying about the algorithm’s exclusion of applicants from lower-income or rural areas; these are individuals who may be not federally protected but do have susceptibility to certain harms (e.g., financial hardships).  In the former case, systemic bias against protected classes can lead to collective, disparate impacts , which may have a basis for legally cognizable harms, such as the denial of credit, online racial profiling, or massive surveillance. 23 In the latter case, the outputs of the algorithm may produce unequal outcomes or unequal error rates for different groups, but they may not violate legal prohibitions if there was no intent to discriminate.  These problematic outcomes should lead to further discussion and awareness of how algorithms work in the handling of sensitive information, and the trade-offs around fairness and accuracy in the models.  Algorithms and sensitive information  While it is intuitively appealing to think that an algorithm can be blind to sensitive attributes, this is not always the case. 24 Critics have pointed out that an algorithm may classify information based on online proxies for the sensitive attributes, yielding a bias against a group even without making decisions directly based on one’s membership in that group. Barocas and Selbst define online proxies as “factors used in the scoring process of an algorithm which are mere stand-ins for protected groups, such as zip code as proxies for race, or height and weight as proxies for gender.” 25 They argue that proxies often linked to algorithms can produce both errors and discriminatory outcomes, such as instances where a zip code is used to determine digital lending decisions or one’s race triggers a disparate outcome. 26 Facebook’s advertising platform contained proxies that allowed housing marketers to micro-target preferred renters and buyers by clicking off data points, including zip code preferences. 27 Thus, it is possible that an algorithm which is completely blind to a sensitive attribute could actually produce the same outcome as one that uses the attribute in a discriminatory manner.  “While it is intuitively appealing to think that an algorithm can be blind to sensitive attributes, this is not always the case.”  For example, Amazon made a corporate decision to exclude certain neighborhoods from its same-day Prime delivery system. Their decision relied upon the following factors: whether a particular zip code had a sufficient number of Prime members, was near a warehouse, and had sufficient people willing to deliver to that zip code. 28 While these factors corresponded with the company’s profitability model, they resulted in the exclusion of poor, predominantly African-American neighborhoods, transforming these data points into proxies for racial classification. The results, even when unintended, discriminated against racial and ethnic minorities who were not included.  Similarly, a job-matching algorithm may not receive the gender field as an input, but it may produce different match scores for two resumes that differ only in the substitution of the name “Mary” for “Mark” because the algorithm is trained to make these distinctions over time.  There are also arguments that blinding the algorithm to sensitive attributes can cause algorithmic bias in some situations. Corbett-Davies and Goel point out in their research on the COMPAS algorithm that even after controlling for “legitimate” risk factors, empirically women have been found to re-offend less often than men in many jurisdictions. 29 If an algorithm is forbidden from reporting a different risk assessment score for two criminal defendants who differ only in their gender, judges may be less likely to release female defendants than male defendants with equal actual risks of committing another crime before trial. Thus, blinding the algorithm from any type of sensitive attribute may not solve bias.  While roundtable participants were not in agreement on the use of online proxies in modeling, they largely agreed that operators of algorithms must be more transparent in their handling of sensitive information, especially if the potential proxy could itself be a legal classificatory harm. 30 There was also discussion that the use of sensitive attributes as part of an algorithm could be a strategy for detecting and possibly curing intended and unintentional biases. Because currently doing so may be constrained by privacy regulations, such as the European Union’s General Data Protection Rules (GDPR) or proposed U.S. federal privacy legislation, the argument could be made for the use of regulatory sandboxes and safe harbors to allow the use of sensitive information when detecting and mitigating biases, both of which will be introduced as part of our policy recommendations.  Detecting bias  When detecting bias, computer programmers normally examine the set of outputs that the algorithm produces to check for anomalous results. Comparing outcomes for different groups can be a useful first step. This could even be done through simulations. Roundtable participant Rich Caruana from Microsoft suggested that companies consider the simulation of predictions (both true and false) before applying them to real-life scenarios. “We almost need a secondary data collection process because sometimes the model will [emit] something quite different,” he shared. For example, if a job-matching algorithm’s average score for male applicants is higher than that for women, further investigation and simulations could be warranted.  However, the downside of these approaches is that not all unequal outcomes are unfair. Roundtable participant Solon Barocas from Cornell University summed this up when he stated, “Maybe we find out that we have a very accurate model, but it still produces disparate outcomes. This may be unfortunate, but is it fair?” An alternative to accounting for unequal outcomes may be to look at the equality of error rates, and whether there are more mistakes for one group of people than another. On this point, Isabel Kloumann of Facebook shared that “society has expectations. One of which is not incarcerating one minority group disproportionately [as a result of an algorithm].”  As shown in the debates around the COMPAS algorithm, even error rates are not a simple litmus test for biased algorithms. Northpointe, the company that developed the COMPAS algorithm, refutes claims of racial discrimination. They argue that among defendants assigned the same high risk score, African-American and white defendants have almost equal recidivism rates, so by that measure, there is no error in the algorithm’s decision. 31 In their view, judges can consider their algorithm without any reference to race in bail and release decisions.  It is not possible, in general, to have equal error rates between groups for all the different error rates. 32 ProPublica focused on one error rate, while Northpointe honed in on another. Thus, some principles need to be established for which error rates should be equalized in which situations in order to be fair.  The COMPAS algorithm, which is used by judges to predict whether defendants should be detained or released on bail pending trial, has drawn scrutiny over claims of potential racial discrimination. (Credit: Stephen Lam/Reuters)  However, distinguishing between how the algorithm works with sensitive information and potential errors can be problematic for operators of algorithms, policymakers, and civil society groups. 33 “Companies would be losing a lot if we don’t draw a distinction between the two,” said Julie Brill from Microsoft. At the very least, there was agreement among roundtable participants that algorithms should not perpetuate historical inequities, and that more work needs to be done to address online discrimination. 34  Fairness and accuracy trade-offs  Next, a discussion of trade-offs and ethics is needed. Here, the focus should be on evaluating both societal notions of “fairness” and possible social costs. In their research of the COMPAS algorithm, Corbett-Davies, Goel, Pierson, Feller, and Huq see “an inherent tension between minimizing violent crime and satisfying common notions of fairness.” 35 They conclude that optimizing for public safety yields decisions that penalize defendants of color, while satisfying legal and societal fairness definitions, and may lead to more releases of high-risk defendants, which would adversely affect public safety. 36 Moreover, the negative impacts on public safety might also disproportionately affect African-American and white neighborhoods, thus creating a fairness cost as well.  If the goal is to avoid reinforcing inequalities, what, then, should developers and operators of algorithms do to mitigate potential biases? We argue that developers of algorithms should first look for ways to reduce disparities between groups without sacrificing the overall performance of the model, especially whenever there appears to be a trade-off.  A handful of roundtable participants argued that opportunities exist for improving both fairness and accuracy in algorithms. For programmers, the investigation of apparent bugs in the software may reveal why the model was not maximizing for overall accuracy. The resolution of these bugs can then improve overall accuracy. Data sets, which may be under-representative of certain groups, may need additional training data to improve accuracy in the decision-making and reduce unfair results. Buolamwini’s facial detection experiments are good examples of this type of approach to fairness and accuracy.  Roundtable participant Sarah Holland from Google pointed out the risk tolerance associated with these types of trade-offs when she shared that “[r]aising risk also involves raising equity issues.” Thus, companies and other operators of algorithms should determine if the social costs of the trade-offs are justified, the stakeholders involved are amenable to a solution through algorithms, or if human decision-makers are needed to frame the solution.  Ethical frameworks matter  What is fundamentally behind these fairness and accuracy trade-offs should be discussions around ethical frameworks and potential guardrails for machine learning tasks and systems. There are several ongoing and recent international and U.S.-based efforts to develop ethical governance standards for the use of AI. 37 The 35-member Organization for Economic Cooperation and Development (OECD) is expected shortly to release its own guidelines for ethical AI. 38 The European Union recently released “Ethics Guidelines for Trustworthy AI,” which delineates seven governance principles: (1) human agency and oversight, (2) technical robustness and safety, (3) privacy and data governance, (4) transparency, (5) diversity, nondiscrimination and fairness, (6) environmental and societal well-being, and (7) accountability. 39 The EU’s ethical framework reflects a clear consensus that it is unethical to “unfairly discriminate.” Within these guidelines, member states link diversity and nondiscrimination with principles of fairness, enabling inclusion and diversity throughout the entire AI system’s lifecycle. Their principles interpret fairness through the lenses of equal access, inclusive design processes, and equal treatment.  Yet, even with these governmental efforts, it is still surprisingly difficult to define and measure fairness. 40 While it will not always be possible to satisfy all notions of fairness at the same time, companies and other operators of algorithms must be aware that there is no simple metric to measure fairness that a software engineer can apply, especially in the design of algorithms and the determination of the appropriate trade-offs between accuracy and fairness. Fairness is a human, not a mathematical, determination, grounded in shared ethical beliefs. Thus, algorithmic decisions that may have a serious consequence for people will require human involvement.  For example, while the training data discrepancies in the COMPAS algorithm can be corrected, human interpretation of fairness still matters. For that reason, while an algorithm such as COMPAS may be a useful tool, it cannot substitute for the decision-making that lies within the discretion of the human arbiter. 41 We believe that subjecting the algorithm to rigorous testing can challenge the different definitions of fairness, a useful exercise among companies and other operators of algorithms.  “It’s important for algorithm operators and developers to always be asking themselves: Will we leave some groups of people worse off as a result of the algorithm’s design or its unintended consequences? “  In the decision to create and bring algorithms to market, the ethics of likely outcomes must be considered—especially in areas where governments, civil society, or policymakers see potential for harm, and where there is a risk of perpetuating existing biases or making protected groups more vulnerable to existing societal inequalities. That is why it’s important for algorithm operators and developers to always be asking themselves: Will we leave some groups of people worse off as a result of the algorithm’s design or its unintended consequences?  We suggest that this question is one among many that the creators and operators of algorithms should consider in the design, execution, and evaluation of algorithms, which are described in the following mitigation proposals. Our first proposal addresses the updating of U.S. nondiscrimination laws to apply to the digital space.  Mitigation proposals  Nondiscrimination and other civil rights laws should be updated to interpret and redress online disparate impacts  To develop trust from policymakers, computer programmers, businesses, and other operators of algorithms must abide by U.S. laws and statutes that currently forbid discrimination in public spaces. Historically, nondiscrimination laws and statutes unambiguously define the thresholds and parameters for the disparate treatment of protected classes. The 1964 Civil Rights Act “forbade discrimination on the basis of sex as well as race in hiring, promoting, and firing.” The 1968 Fair Housing Act prohibits discrimination in the sale, rental, and financing of dwellings, and in other housing-related transactions to federally protected classes. Enacted in 1974, the Equal Credit Opportunity Act stops any creditor from discriminating against any applicant from any type of credit transaction based on protected characteristics. While these laws do not necessarily mitigate and resolve other implicit or unconscious biases that can be baked into algorithms, companies and other operators should guard against violating these statutory guardrails in the design of algorithms, as well as mitigating their implicit concern to prevent past discrimination from continuing.  Roundtable participant Wendy Anderson from the Office of Congresswoman Val Demings stated, “[T]ypically, legislators only hear when something bad happens. We need to find a way to protect those who need it without stifling innovation.” Congress can clarify how these nondiscrimination laws apply to the types of grievances recently found in the digital space, since most of these laws were written before the advent of the internet. 42 Such legislative action can provide clearer guardrails that are triggered when algorithms are contributing to legally recognizable harms. Moreover, when creators and operators of algorithms understand that these may be more or less non-negotiable factors, the technical design will be more thoughtful in moving away from models that may trigger and exacerbate explicit discrimination, such as design frames that exclude rather than include certain inputs or are not checked for bias. 43  Operators of algorithms must develop a bias impact statement  Once the idea for an algorithm has been vetted against nondiscrimination laws, we suggest that operators of algorithms develop a bias impact statement, which we offer as a template of questions that can be flexibly applied to guide them through the design, implementation, and monitoring phases.  As a self-regulatory practice, the bias impact statement can help probe and avert any potential biases that are baked into or are resultant from the algorithmic decision. As a best practice, operators of algorithms should brainstorm a core set of initial assumptions about the algorithm’s purpose prior to its development and execution. We propose that operators apply the bias impact statement to assess the algorithm’s purpose, process and production, where appropriate. Roundtable participants also suggested the importance of establishing a cross-functional and interdisciplinary team to create and implement the bias impact statement.   New York University’s AI Now Institute    Related Books             Autonomous Vehicles   By Clifford Winston and Quentin Karpilow   2020                Terms of Disservice   By Dipayan Ghosh   2020                Growth in a Time of Change   Edited by Hyeon-Wook Kim and Zia Qureshi   2020        New York University’s AI Now Institute has already introduced a model framework for governmental entities to use to create algorithmic impact assessments (AIAs), which evaluate the potential detrimental effects of an algorithm in the same manner as environmental, privacy, data, or human rights impact statements. 44 While there may be differences in implementation given the type of predictive model, the AIA encompasses multiple rounds of review from internal, external, and public audiences. First, it assumes that after this review, a company will develop a list of potential harms or biases in their self-assessment, with the assistance of more technical outside experts. Second, if bias appears to have occurred, the AIA pushes for notice to be given to impacted populations and a comment period opened for response. And third, the AIA process looks to federal and other entities to support users’ right to challenge algorithmic decisions that feel unfair.  While the AIA process supports a substantive feedback loop, what may be missing is both the required forethought leading up to the decision and the oversight of the algorithm’s provisions. Moreover, our proposed bias impact statement starts with a framework that identifies which automated decisions should be subjected to such scrutiny, operator incentives, and stakeholder engagement.   Which automated decisions?   In the case of determining which automated decisions require such vetting, operators of algorithms should start with questions about whether there will be a possible negative or unintended outcome resulting from the algorithm, for whom, and the severity of consequences for members of the affected group if not detected and mitigated. Reviewing established legal protections around fair housing, employment, credit, criminal justice, and health care should serve as a starting point for determining which decisions need to be viewed with special caution in designing and testing any algorithm used to predict outcomes or make important eligibility decisions about access to a benefit. This is particularly true considering the legal prescriptions against using data that has a likelihood of disparate impact on a protected class or other established harms. Thus, we suggest that operators should be constantly questioning the potential legal, social, and economic effects and potential liabilities associated with that choice when determining which decisions should be automated and how to automate them with minimal risks.   What are the user incentives?   Incentives should also drive organizations to proactively address algorithmic bias. Conversely, operators who create and deploy algorithms that generate fairer outcomes should also be recognized by policymakers and consumers who will trust them more for their practices. When companies exercise effective algorithmic hygiene before, during, and after introducing algorithmic decision-making, they should be rewarded and potentially given a public-facing acknowledgement for best practices.   How are stakeholders being engaged?   Finally, the last element encapsulated in a bias impact statement should involve the engagement of stakeholders who could help computer programmers in the selection of inputs and outputs of certain automated decisions. “Tech succeeds when users understand the product better than its designers,” said Rich Caruana from Microsoft. Getting users engaged early and throughout the process will prompt improvements to the algorithms, which ultimately leads to improved user experiences.  Stakeholder responsibilities can also extend to civil society organizations who can add value in the conversation on the algorithm’s design. “Companies [should] engage civil society,” shared Miranda Bogen from Upturn. “Otherwise, they will go to the press and regulators with their complaints.” A possible solution for operators of algorithms could be the development of an advisory council of civil society organizations that, working alongside companies, may be helpful in defining the scope of the procedure and predicting biases based on their ground-level experiences.   The template for the bias impact statement   These three foundational elements for a bias impact statement are reflected in a discrete set of questions that operators should answer during the design phase to filter out potential biases (Table 1). As a self-regulatory framework, computer programmers and other operators of algorithms can construct this type of tool prior to the model’s design and execution.  Table 1. Design questions template for bias impact statement      What will the automated decision do?    Who is the audience for the algorithm and who will be most affected by it?    Do we have training data to make the correct predictions about the decision?    Is the training data sufficiently diverse and reliable? What is the data lifecycle of the algorithm?    Which groups are we worried about when it comes to training data errors, disparate treatment, and impact?    How will potential bias be detected?    How and when will the algorithm be tested? Who will be the targets for testing?    What will be the threshold for measuring and correcting for bias in the algorithm, especially as it relates to protected groups?    What are the operator incentives?    What will we gain in the development of the algorithm?    What are the potential bad outcomes and how will we know?    How open (e.g., in code or intent) will we make the design process of the algorithm to internal partners, clients, and customers?    What intervention will be taken if we predict that there might be bad outcomes associated with the development or deployment of the algorithm?    How are other stakeholders being engaged?    What’s the feedback loop for the algorithm for developers, internal partners and customers?    Is there a role for civil society organizations in the design of the algorithm?    Has diversity been considered in the design and execution?    Will the algorithm have implications for cultural groups and play out differently in cultural contexts?    Is the design team representative enough to capture these nuances and predict the application of the algorithm within different cultural contexts? If not, what steps are being taken to make these scenarios more salient and understandable to designers?    Given the algorithm’s purpose, is the training data sufficiently diverse?    Are there statutory guardrails that companies should be reviewing to ensure that the algorithm is both legal and ethical?      Diversity-in-design  Operators of algorithms should also consider the role of diversity within their work teams, training data, and the level of cultural sensitivity within their decision-making processes. Employing diversity in the design of algorithms upfront will trigger and potentially avoid harmful discriminatory effects on certain protected groups, especially racial and ethnic minorities. While the immediate consequences of biases in these areas may be small, the sheer quantity of digital interactions and inferences can amount to a new form of systemic bias. Therefore, the operators of algorithms should not discount the possibility or prevalence of bias and should seek to have a diverse workforce developing the algorithm, integrate inclusive spaces within their products, or employ “diversity-in-design,” where deliberate and transparent actions will be taken to ensure that cultural biases and stereotypes are addressed upfront and appropriately. Adding inclusivity into the algorithm’s design can potentially vet the cultural inclusivity and sensitivity of the algorithms for various groups and help companies avoid what can be litigious and embarrassing algorithmic outcomes.  The bias impact statement should not be an exhaustive tool. For algorithms with more at stake, ongoing review of their execution should be factored into the process. The goal here is to monitor for disparate impacts resulting from the model that border on unethical, unfair, and unjust decision-making. When the process of identifying and forecasting the purpose of the algorithm is achieved, a robust feedback loop will aid in the detection of bias, which leads to the next recommendation promoting regular audits.  Other self-regulatory best practices  Operators of algorithms should regularly audit for bias  The formal and regular auditing of algorithms to check for bias is another best practice for detecting and mitigating bias. On the importance of these audits, roundtable participant Jon Kleinberg from Cornell University shared that “[a]n algorithm has no choice but to be premeditated.” Audits prompt the review of both input data and output decisions, and when done by a third-party evaluator, they can provide insight into the algorithm’s behavior. While some audits may require technical expertise, this may not always be the case. Facial recognition software that misidentifies persons of color more than whites is an instance where a stakeholder or user can spot biased outcomes, without knowing anything about how the algorithm makes decisions. “We should expect computers to have an audit trail,” shared roundtable participant Miranda Bogen from Upturn. Developing a regular and thorough audit of the data collected for the algorithmic operation, along with responses from developers, civil society, and others impacted by the algorithm, will better detect and possibly deter biases.  “Developing a regular and thorough audit of the data collected for the algorithmic operation, along with responses from developers, civil society, and others impacted by the algorithm, will better detect and possibly deter biases.”  The experience of government officials in Allegheny County reflects the importance of third-party auditing. In 2016, the Department of Human Services launched a decision support tool, the Allegheny Family Screening Tool (AFST), to generate a score for which children are most likely to be removed from their homes within two years, or to be re-referred to the county’s child welfare office due to suspected abuse. The county took ownership of its use of the tool, worked collaboratively with the developer, and commissioned an independent evaluation of its direct and indirect effects on the maltreatment screening process, including decision accuracy, workload, and consistency. County officials also sought additional independent research from experts to determine if the software was discriminating against certain groups. In 2017, the findings did identify some statistical imbalances, with error rates higher across racial and ethnic groups. White children who were scored at the highest-risk of maltreatment were less likely to be removed from their homes compared to African-American children with similar risk scores. 45 The county responded to these findings as part of the rebuild of the tool, with version two implemented in November 2018. 46  Facebook recently completed a civil rights audit to determine its handling of issues and individuals from protected groups. 47 After the reveal of how the platform was handling a variety of issues, including voter suppression, content moderation, privacy, and diversity, the company has committed to an updated audit around its internal infrastructure to handle civil rights grievances and address diversity in its products’ designs by default. Recent actions by Facebook to ban white nationalist content or address disinformation campaigns are some of the results of these efforts. 48  Operators of algorithms must rely upon cross-functional work teams and expertise  Roundtable participants largely acknowledged the notion that organizations should employ cross-functional teams. But movement in this direction can be difficult in already-siloed organizations, despite the technical, societal, and possibly legal implications associated with the algorithm’s design and execution. Not all decisions will necessitate this type of cross-team review, but when these decisions carry risks of real harm, they should be employed. In the mitigation of bias and the management of the risks associated with the algorithm, collaborative work teams can compensate for the blind-spots often missed in smaller, segmented conversations and reviews. Bringing together experts from various departments, disciplines, and sectors will help facilitate accountability standards and strategies for mitigating online biases, including from engineering, legal, marketing, strategy, and communications.  Cross-functional work teams–whether internally driven or populated by external experts–can attempt to identify bias before and during the model’s rollout. Further, partnerships between the private sector, academics, and civil society organizations can also facilitate greater transparency in AI’s application to a variety of scenarios, particularly those that impact protected classes or are disseminated in the public interest. Kate Crawford, AI researcher and founder of the AI Now Partnership, suggested that “closed loops are not open for algorithmic auditing, for review, or for public debate” because they generally exacerbate the problems that they are trying to solve. 49 Further on this point, roundtable participant Natasha Duarte from the Center for Democracy and Technology spoke to Allegheny’s challenge when she shared, “[C]ompanies should be more forthcoming with describing the limits of their tech, and government should know what questions to ask in their assessments,” which speaks to the importance of more collaboration in this area.  Increase human involvement in the design and monitoring of algorithms  Even with all the precautionary measures listed above, there is still some risk that algorithms will make biased decisions. People will continue to play a role in identifying and correcting biased outcomes long after an algorithm is developed, tested, and launched. While more data can inform automated decision-making, this process should complement rather than fully replace human judgement. Roundtable participant Alex Peysakhovich from Facebook shared, “[W]e don’t need to eliminate human moderators. We need to hire more and get them to focus on edge cases.” Such sentiment is growing increasingly important in this field as the comparative advantages of humans and algorithms become more distinguishable and the use of both improves the outcomes for online users.  People will continue to play a role in identifying and correcting biased outcomes long after an algorithm is developed, tested, and launched. (Credit: Gabrielle Lurie/Reuters)  However, privacy implications will arise when more humans are engaged in algorithm management, particularly if more sensitive information is involved in the model’s creation or in testing the algorithm’s predictions for bias. The timing of the roundtables, which also transpired around the adoption of the EU’s GDPR, spoke to the need for increased consumer privacy principles where users are empowered over what data they want to share with companies. As the U.S. currently debates the need for federal privacy legislation, access to and use of personal data may become even more difficult, potentially leaving algorithmic models prone to more bias. Because the values of creators and users of algorithms shift over time, humans must arbitrate conflicts between outcomes and stated goals. In addition to periodical audits, human involvement provides continuous feedback on the performance of bias mitigation efforts.  Other public policy recommendations  As indicated throughout the paper, policymakers play a critical role in identifying and mitigating biases, while ensuring that the technologies continue to make positive economic and societal benefits.  Congress should implement regulatory sandboxes and safe harbors to curb online biases  Regulatory sandboxes are perceived as one strategy for the creation of temporary reprieves from regulation to allow the technology and rules surrounding its use to evolve together. These policies could apply to algorithmic bias and other areas where the technology in question has no analog covered by existing regulations. Rather than broaden the scope of existing regulations or create rules in anticipation of potential harms, a sandbox allows for innovation both in technology and its regulation. Even in a highly regulated industry, the creation of sandboxes where innovations can be tested alongside with lighter touch regulations can yield benefits.  “Rather than broaden the scope of existing regulations or create rules in anticipation of potential harms, a sandbox allows for innovation both in technology and its regulation.”  For example, companies within the financial sector that are leveraging technology, or fintech, have shown how regulatory sandboxes can spur innovation in the development of new products and services. 50 These companies make extensive use of algorithms for everything from spotting fraud to deciding to extend credit. Some of these activities mirror those of regular banks, and those would still fall under existing rules, but new ways of approaching tasks would be allowed within the sandbox. 51 Because sandboxes give innovators greater leeway in developing new products and services, they will require active oversight until technology and regulations mature. The U.S. Treasury recently reported not only on the benefits that countries that have adopted fintech regulatory sandboxes have realized, but recommended that the U.S. adopt fintech sandboxes to spur innovation. 52 Given the broad usefulness of algorithms to spur innovation in various regulated industries, participants in the roundtables considered the potential usefulness of extending regulatory sandboxes to other areas where algorithms can help to spur innovations.  Regulatory safe harbors could also be employed, where a regulator could specify which activities do not violate existing regulations. 53 This approach has the advantage of increasing regulatory certainty for algorithm developers and operators. For example, Section 230 of the Communications Decency Act removed liability from websites for the actions of their users, a provision widely credited with the growth of internet companies like Facebook and Google. The exemption later narrowed to exclude sex trafficking with the passage of the Stop Enabling Online Sex Trafficking Act and Fight Online Sex Trafficking Act. Applying a similar approach to algorithms could exempt their operators from liabilities in certain contexts while still upholding protections in others where harms are easier to identify. In line with the previous discussion on the use of certain protected attributes, safe harbors could be considered in instances where the collection of sensitive personal information is used for the specific purposes of bias detection and mitigation.  Consumers need better algorithmic literacy  Widespread algorithmic literacy is crucial for mitigating bias. Given the increased use of algorithms in many aspects of daily life, all potential subjects of automated decisions would benefit from knowledge of how these systems function. Just as computer literacy is now considered a vital skill in the modern economy, understanding how algorithms use their data may soon become necessary.  The subjects of automated decisions deserve to know when bias negatively affects them, and how to respond when it occurs. Feedback from users can share and anticipate areas where bias can manifest in existing and future algorithms. Over time, the creators of algorithms may actively solicit feedback from a wide range of data subjects and then take steps to educate the public on how algorithms work to aid in this effort. Public agencies that regulate bias can also work to raise algorithmic literacy as part of their missions. In both the public and private sector, those that stand to lose the most from biased decision-making can also play an active role in spotting it.  Conclusion  In December 2018, President Trump signed the First Step Act, new criminal justice legislation that encourages the usage of algorithms nationwide. 54 In particular, the system would use an algorithm to initially determine who can redeem earned-time credits—reductions in sentence for completion of educational, vocational, or rehabilitative programs—excluding inmates deemed higher risk. There is a likelihood that these algorithms will perpetuate racial and class disparities, which are already embedded in the criminal justice system. As a result, African-Americans and poor people in general will be more likely to serve longer prison sentences.  “When algorithms are responsibly designed, they may avoid the unfortunate consequences of amplified systemic discrimination and unethical applications.”  As outlined in the paper, these types of algorithms should be concerning if there is not a process in place that incorporates technical diligence, fairness, and equity from design to execution. That is, when algorithms are responsibly designed, they may avoid the unfortunate consequences of amplified systemic discrimination and unethical applications.  Some decisions will be best served by algorithms and other AI tools, while others may need thoughtful consideration before computer models are designed. Further, testing and review of certain algorithms will also identify, and, at best, mitigate discriminatory outcomes. For operators of algorithms seeking to reduce the risk and complications of bad outcomes for consumers, the promotion and use of the mitigation proposals can create a pathway toward algorithmic fairness, even if equity is never fully realized.   The Brookings Institution is a nonprofit organization devoted to independent research and policy solutions. Its mission is to conduct high-quality, independent research and, based on that research, to provide innovative, practical recommendations for policymakers and the public. The conclusions and recommendations of any Brookings publication are solely those of its author(s), and do not reflect the views of the Institution, its management, or its other scholars.  Amazon, Facebook, Google, IBM, and Microsoft provide general, unrestricted support to The Brookings Institution. Paul Resnick is also a consultant to Facebook, but this work is independent and his views expressed here are his own. The findings, interpretations, and conclusions posted in this piece are not influenced by any donation. Brookings recognizes that the value it provides is in its absolute commitment to quality, independence, and impact. Activities supported by its donors reflect this commitment.   Appendix: List of Roundtable Participants     Participant  Organization      Wendy Anderson  Office of Congresswoman Val Demings    Norberto Andrade  Facebook    Solon Barocas  Cornell University    Genie Barton  Privacy Genie    Ricardo Baeza-Yates  NTENT    Miranda Bogen  Upturn    John Brescia  Better Business Bureau    Julie Brill  Microsoft    Rich Caruana  Microsoft Research    Eli Cohen  Brookings Institution    Anupam Datta  Carnegie Mellon    Deven Desai  Georgia Tech    Natasha Duarte  Center for Democracy and Technology    Nadia Fawaz  LinkedIn    Laura Fragomeni  Walmart Global eCommerce    Sharad Goel  Stanford University    Scott Golder  Cornell University    Aaron Halfaker  Wikimedia    Sarah Holland  Google    Jack Karsten  Brookings Institution    Krishnaram Kenthapadi  LinkedIn and Stanford University    Jon Kleinberg  Cornell University    Isabel Kloumann  Facebook    Jake Metcalf  Ethical Resolve    Alex Peysakhovich  Facebook    Paul Resnick  University of Michigan    William Rinehart  American Action Forum    Alex Rosenblat  Data and Society    Jake Schneider  Brookings Institution    Jasjeet Sekhon  University of California-Berkeley    Rob Sherman  Facebook    JoAnn Stonier  Mastercard Worldwide    Nicol Turner Lee  Brookings Institution    Lucy Vasserman  Jigsaw’s Conversation AI Project / Google    Suresh Venkatasubramanian  University of Utah    John Verdi  Future of Privacy Forum    Heather West  Mozilla    Jason Yosinki  Uber    Jinyan Zang  Harvard University    Leila Zia  Wikimedia Foundation      References  Angwin, Julia, and Terry Parris Jr. “Facebook Lets Advertisers Exclude Users by Race.” Text/html. ProPublica, October 28, 2016. https://www.propublica.org/article/facebook-lets-advertisers-exclude-users-by-race.  Angwin, Julia, Jeff Larson, Surya Mattu, and Laura Kirchner. “Machine Bias.” ProPublica, May 23, 2016. Available at https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing (last accessed April 19, 2019).  Barocas, Solon, and Andrew D. Selbst, “Big Data’s Disparate Impact,” SSRN Scholarly Paper (Rochester, NY: Social Science Research Network, 2016. Available at https://papers.ssrn.com/abstract=2477899.  Blass, Andrea, and Yuri Gurevich. Algorithms: A Quest for Absolute Definitions. Bulletin of European Association for Theoretical Computer Science 81, 2003. https://www.microsoft.com/en-us/research/wp-content/uploads/2017/01/164.pdf (last accessed April 12, 2019).  Brennan, Tim, William Dieterich, and Beate Ehret. “Evaluating the Predictive Validity of the COMPAS Risk and Needs Assessment System.” Criminal Justice and Behavior 36 (2009): 21–40.  Chessell, Mandy. “Ethics for Big Data and Analytics.” IBM, n.d. Available at https://www.ibmbigdatahub.com/sites/default/files/whitepapers_reports_file/TCG%20Study%20Report%20-%20Ethics%20for%20BD%26A.pdf (last accessed April 19, 2019).  Chodosh, Sara. “Courts use algorithms to help determine sentencing, but random people get the same results.” Popular Science, January 18, 2018. Available at https://www.popsci.com/recidivism-algorithm-random-bias (last accessed October 15, 2018).  Corbett-Davies, Sam, Emma Peirson, Avi Feller, and Sharad Goel. “A Computer Program Used for Bail and Sentencing Decisions Was Labeled Biased against Blacks. It’s Actually Not That Clear.” Washington Post (blog), October 17, 2016. Available at https://www.washingtonpost.com/news/monkey-cage/wp/2016/10/17/can-an-algorithm-be-racist-our-analysis-is-more-cautious-than-propublicas/ (last accessed April 19. 2019).  Corbett-Davies, Sam, Emma Pierson, Avi Feller, Sharad Goel, and Aziz Huq. “Algorithmic Decision Making and the Cost of Fairness.” ArXiv:1701.08230 [Cs, Stat], January 27, 2017. https://doi.org/10.1145/3097983.309809.  Courtland, Rachel. “Bias Detectives: The Researchers Striving to Make Algorithms Fair,” Nature 558, no. 7710 (June 2018): 357–60. Available at https://doi.org/10.1038/d41586-018-05469-3 (last accessed April 19, 2019).  DeAngelius, Stephen F. “Artificial intelligence: How algorithms make systems smart,” Wired Magazine, September 2014. Available at https://www.wired.com//insights/2014/09/artificial-intelligence-algorithms-2/ (last accessed April 12, 2019).  Elejalde-Ruiz, Alexia. “The end of the resume? Hiring is in the midst of technological revolution with algorithms, chatbots.” Chicago Tribune (July 19, 2018). Available at http://www.chicagotribune.com/business/ct-biz-artificial-intelligence-hiring-20180719-story.html.  Eubanks, Virginia. “A Child Abuse Prediction Model Fails Poor Families,” Wired, January 15, 2018. Available at https://www.wired.com/story/excerpt-from-automating-inequality/ (last accessed April 19, 2019).  FTC Hearing #7: The Competition and Consumer Protection Issues of Algorithms, Artificial Intelligence, and Predictive Analytics, § Federal Trade Commission (2018). https://www.ftc.gov/system/files/documents/public_events/1418693/ftc_hearings_session_7_transcript_day_2_11-14-18.pdf.  Garbade, Michael J. “Clearing the Confusion: AI vs. Machine Learning vs. Deep Learning Differences,” Towards Data Science, September 14, 2018. Available at https://towardsdatascience//clearing-the-confusion-ai-vs-machine-learning-vs-deep-learning-differences-fce69b21d5eb (last accessed April 12, 2019).  Griggs v. Duke Power Company, Oyez. Available at https//www.oyez.org/cases/1970/124 (last accessed October 1, 2018.  Guerin, Lisa. “Disparate Impact Discrimination.” www.nolo.com. Available at https://www.nolo.com/legal-encyclopedia/disparate-impact-discrimination.htm (last accessed April 24, 2019).  Hadhazy, Adam. “Biased Bots: Artificial-Intelligence Systems Echo Human Prejudices.” Princeton University, April 18, 2017. Available at https://www.princeton.edu/news/2017/04/18/biased-bots-artificial-intelligence-systems-echo-human-prejudices (last accessed April 20, 2019).  Hamilton, Isobel Asher. “Why It’s Totally Unsurprising That Amazon’s Recruitment AI Was Biased against Women.” Business Insider, October 13, 2018. Available at https://www.businessinsider.com/amazon-ai-biased-against-women-no-surprise-sandra-wachter-2018-10 (last accessed April 20, 2019).  Hardesty, Larry. “Study Finds Gender and Skin-Type Bias in Commercial Artificial-Intelligence Systems.” MIT News, February 11, 2018. Available at http://news.mit.edu/2018/study-finds-gender-skin-type-bias-artificial-intelligence-systems-0212 (last accessed April 19, 2019).  High-level Expert Group on Artificial Intelligence. “Ethics Guidelines for Trustworthy AI (Draft).” The European Commission, December 18, 2018.  Ingold, David, and Spencer Soper. “Amazon Doesn’t Consider the Race of Its Customers. Should It?” Bloomberg.com, April 21, 2016. http://www.bloomberg.com/graphics/2016-amazon-same-day/ .  Kearns, Michael. “Data Intimacy, Machine Learning and Consumer Privacy.” University of Pennsylvania Law School, May 2018. Available at https://www.law.upenn.edu/live/files/7952-kearns-finalpdf (last accessed April 12, 2019).  Kleinberg, Jon, Sendhil Mullainathan, and Manish Raghavan, “Inherent Trade-Offs in the Fair Determination of Risk Scores.”In Proceedings of Innovations in Theoretical Computer Science (ITCS), 2017. Available at https://arxiv.org/pdf/1609.05807.pdf (last accessed April 19, 2019).  Larson, Jeff, Surya Mattu, and Julia Angwin. “Unintended Consequences of Geographic Targeting.” Technology Science, September 1, 2015. Available at https://techscience.org/a/2015090103/ (last accessed April 19, 2019).  Locklear, Mallory. “Facebook Releases an Update on Its Civil Rights Audit.” Engadget (blog), December 18, 2018. Available at https://www.engadget.com/2018/12/18/facebook-update-civil-rights-audit/ (last accessed April 19, 2019).  Lopez, German. “The First Step Act, Congress’s Criminal Justice Reform Bill, Explained.” Vox, December 3, 2018. Available at https://www.vox.com/future-perfect/2018/12/3/18122392/first-step-act-criminal-justice-reform-bill-congress (last accessed April 16, 2019).  Mnuchin, Steven T., and Craig S. Phillips. “A Financial System That Creates Economic Opportunities – Nonbank Financials, Fintech, and Innovation.” Washington, D.C.: U.S. Department of the Treasury, July 2018.Available at https://home.treasury.gov/sites/default/files/2018-08/A-Financial-System-that-Creates-Economic-Opportunities—Nonbank-Financials-Fintech-and-Innovation_0.pdf (last accessed April 19, 2019).  Reisman, Dillon, Jason Schultz, Kate Crawford, and Meredith Whittaker. “Algorithmic Impact Assessments: A Practical Framework for Public Agency Accountability.” New York: AI Now, April 2018.  Romei, Andrea, and Salvatore Ruggieri. “Discrimination Data Analysis: A Multi-Disciplinary Bibliography.” In Discrimination and Privacy in the Information Society, edited by Bart Custers, T Calders, B Schermer, and T Zarsky, 109–35. Studies in Applied Philosophy, Epistemology and Rational Ethics. Springer, Berlin, Heidelberg, 2013. Available at https://doi.org/10.1007/978-3-642-30487-3_6 (last accessed April 19, 2019).  Schatz, Brian. AI in Government Act of 2018, Pub. L. No. S.B. 3502 (2018). https://www.congress.gov/bill/115th-congress/senate-bill/3502.  Spielkamp, Matthias. “We Need to Shine More Light on Algorithms so They Can Help Reduce Bias, Not Perpetuate It.” MIT Technology Review. Accessed September 20, 2018. Available at https://www.technologyreview.com/s/607955/inspecting-algorithms-for-bias/ (last accessed April 19, 2019).  Stack, Liam. “Facebook Announces New Policy to Ban White Nationalist Content.” The New York Times, March 28, 2019, sec. Business. Available at https://www.nytimes.com/2019/03/27/business/facebook-white-nationalist-supremacist.html (last accessed April 19, 2019).  Sweeney, Latanya, and Jinyan Zang. “How appropriate might big data analytics decisions be when placing ads?” Powerpoint presentation presented at the Big Data: A tool for inclusion or exclusion, Federal Trade Commission conference, Washington, DC. September 15, 2014. Available at https://www.ftc.gov/systems/files/documents/public_events/313371/bigdata-slides-sweeneyzang-9_15_14.pdf (last accessed April 12, 2019).  Sweeney, Latanya. “Discrimination in online ad delivery.” Rochester, NY: Social Science Research Network, January 28, 2013. Available at https://papers.ssrn.com/abstract=2208240 (last accessed April 12, 2019).  Sydell, Laura. “It Ain’t Me, Babe: Researchers Find Flaws In Police Facial Recognition Technology.” NPR.org, October 25, 2016. Available at https://www.npr.org/sections/alltechconsidered/2016/10/25/499176469/it-aint-me-babe-researchers-find-flaws-in-police-facial-recognition (last accessed April 19, 2019).  “The Global Data Ethics Project.” Data for Democracy, n.d. https://www.datafordemocracy.org/project/global-data-ethics-project (last accessed April 19, 2019).  Tobin, Ariana. “HUD sues Facebook over housing discrimination and says the company’s algorithms have made the problem worse.” ProPublica (March 28, 2019). Available at https://www.propublica.org/article/hud-sues-facebook-housing-discrimination-advertising-algorithms (last accessed April 29, 2019).  Turner Lee, Nicol. “Inclusion in Tech: How Diversity Benefits All Americans,” § Subcommittee on Consumer Protection and Commerce, United States House Committee on Energy and Commerce (2019). Also available on Brookings web site, https://www.brookings.edu/testimonies/inclusion-in-tech-how-diversity-benefits-all-americans/ (last accessed April 29, 2019).  Turner Lee, Nicol. Detecting racial bias in algorithms and machine learning. Journal of Information, Communication and Ethics in Society 2018, Vol. 16 Issue 3, pp. 252-260. Available at https://doi.org/10.1108/JICES-06-2018-0056/ (last accessed April 29, 2019).  “Understanding bias in algorithmic design,” Impact.Engineered, September 5, 2017. Available at https://medium.com/impact-engineered/understanding-bias-in-algorithmic-design-db9847103b6e (last accessed April 12, 2019).  Vincent, James. “Amazon Reportedly Scraps Internal AI Recruiting Tool That Was Biased against Women.” The Verge, October 10, 2018. Available at https://www.theverge.com/2018/10/10/17958784/ai-recruiting-tool-bias-amazon-report (last accessed April 20, 2019).  Zafar, Muhammad Bilal, Isabel Valera Martinez, Manuel Gomez Rodriguez, and Krishna Gummadi. “Fairness Constraints: A Mechanism for Fair Classification.” In Proceedings of the 20th International Conference on Artificial Intelligence and Statistics (AISTATS). Fort Lauderdale, FL, 2017.  Zarsky, Tal. “Understanding Discrimination in the Scored Society.” SSRN Scholarly Paper. Rochester, NY: Social Science Research Network, January 15, 2015. https://papers.ssrn.com/abstract=2550248.     Report Produced by Center for Technology Innovation        Footnotes   Nicol Turner Lee, Fellow, Center for Technology Innovation, Brookings Institution; Paul Resnick, Michael D. Cohen Collegiate Professor of Information, Associate Dean for Research and Faculty Affairs, Professor of Information and Interim Director of Health Informatics, School of Information at the University of Michigan; Genie Barton, President, Institute for Marketplace Trust, Better Business Bureau and Member, Research Advisory Board, International Association of Privacy Professionals. The authors also acknowledge the input from the current leadership of the Better Business Bureau’s Institute for Marketplace Trust and Jinyan Zang, Harvard University.  The concepts of AI, algorithms and machine learning are often conflated and used interchangeably. In this paper, we will follow generally understood definitions of these terms as set out in publications for the general reader. See, e.g., Stephen F. DeAngelius. “Artificial intelligence: How algorithms make systems smart,” Wired Magazine, September 2014. Available at https://www.wired.com//insights/2014/09/artificial-intelligence-algorithms-2/ (last accessed April 12, 2019). See also, Michael J. Garbade. “Clearing the Confusion: AI vs. Machine Learning vs. Deep Learning Differences,” Towards Data Science, September 14, 2018. Available at https://towardsdatascience//clearing-the-confusion-ai-vs-machine-learning-vs-deep-learning-differences-fce69b21d5eb (last accessed April 12, 2019).  Andrea Blass and Yuri Gurevich. Algorithms: A Quest for Absolute Definitions. Bulletin of European Association for Theoretical Computer Science 81, 2003. https://www.microsoft.com/en-us/research/wp-content/uploads/2017/01/164.pdf (last accessed April 12, 2019).  Kearns, Michael. “Data Intimacy, Machine Learning and Consumer Privacy.” University of Pennsylvania Law School, May 2018. Available at https://www.law.upenn.edu/live/files/7952-kearns-finalpdf (last accessed April 12, 2019).  Technically, this describes what is called “supervised machine learning.”  Chodosh, Sara. “Courts use algorithms to help determine sentencing, but random people get the same results.” Popular Science, January 18, 2018. Available at https://www.popsci.com/recidivism-algorithm-random-bias (last accessed October 15, 2018).  Blog. “Understanding bias in algorithmic design,” Impact.Engineered, September 5, 2017. Available at https://medium.com/impact-engineered/understanding-bias-in-algorithmic-design-db9847103b6e (last accessed April 12, 2019). This definition is intended to include the concepts of disparate treatment and disparate impact, but the legal definitions were not designed with AI in mind. For example, the demonstration of disparate treatment does not describe the ways in which an algorithm can learn to treat similarly situated groups differently, as will be discussed later in the paper.  The recommendations offered in the paper are those of the authors and do not represent the views or a consensus of views among roundtable participants.  Hamilton, Isobel Asher. “Why It’s Totally Unsurprising That Amazon’s Recruitment AI Was Biased against Women.” Business Insider, October 13, 2018. Available at https://www.businessinsider.com/amazon-ai-biased-against-women-no-surprise-sandra-wachter-2018-10 (last accessed April 20, 2019).  Vincent, James. “Amazon Reportedly Scraps Internal AI Recruiting Tool That Was Biased against Women.” The Verge, October 10, 2018. Available at https://www.theverge.com/2018/10/10/17958784/ai-recruiting-tool-bias-amazon-report (last accessed April 20, 2019). Although Amazon scrubbed the data of the particular references that appeared to discriminate against female candidates, there was no guarantee that the algorithm could not find other ways to sort and rank male candidates higher so it was scrapped by the company.  Hadhazy, Adam. “Biased Bots: Artificial-Intelligence Systems Echo Human Prejudices.” Princeton University, April 18, 2017. Available at https://www.princeton.edu/news/2017/04/18/biased-bots-artificial-intelligence-systems-echo-human-prejudices (last accessed April 20, 2019).  Sweeney, Latanya. “Discrimination in online ad delivery.” Rochester, NY: Social Science Research Network, January 28, 2013. Available at https://papers.ssrn.com/abstract=2208240 (last accessed April 12, 2019).  Sweeney, Latanya and Jinyan Zang. “How appropriate might big data analytics decisions be when placing ads?” Powerpoint presentation presented at the Big Data: A tool for inclusion or exclusion, Federal Trade Commission conference, Washington, DC. September 15, 2014. Available at https://www.ftc.gov/systems/files/documents/public_events/313371/bigdata-slides-sweeneyzang-9_15_14.pdf (last accessed April 12, 2019).  “FTC Hearing #7: The Competition and Consumer Protection Issues of Algorithms, Artificial Intelligence, and Predictive Analytics,” § Federal Trade Commission (2018),  Hardesty, Larry. “Study Finds Gender and Skin-Type Bias in Commercial Artificial-Intelligence Systems.” MIT News, February 11, 2018. Available at http://news.mit.edu/2018/study-finds-gender-skin-type-bias-artificial-intelligence-systems-0212 (last accessed April 19, 2019). These companies were selected because they provided gender classification features in their software and the code was publicly available for testing.  Ibid.  COMPAS is a risk-and needs-assessment tool originally designed by Northpointe, Inc., to assist state corrections officials in making placement, management, and treatment decisions for offenders. Angwin, Julia, Jeff Larson, Surya Mattu, and Laura Kirchner. “Machine Bias.” ProPublica, May 23, 2016. Available at https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing (last accessed April 19, 2019). See also , Brennan, Tim, William Dieterich, and Beate Ehret. “Evaluating the Predictive Validity of the COMPAS Risk and Needs Assessment System.” Criminal Justice and Behavior 36 (2009): 21–40.  Corbett-Davies, Sam, Emma Pierson, Avi Feller, Sharad Goel, and Aziz Huq. “Algorithmic Decision Making and the Cost of Fairness.” ArXiv:1701.08230 [Cs, Stat], January 27, 2017. https://doi.org/10.1145/3097983.309809.  Solon Barocas and Andrew D. Selbst, “Big Data’s Disparate Impact,” SSRN Scholarly Paper (Rochester, NY: Social Science Research Network, 2016), https://papers.ssrn.com/abstract=2477899.  Turner Lee, Nicol. “Inclusion in Tech: How Diversity Benefits All Americans,” § Subcommittee on Consumer Protection and Commerce, United States House Committee on Energy and Commerce (2019). Also available on Brookings web site, https://www.brookings.edu/testimonies/inclusion-in-tech-how-diversity-benefits-all-americans/ (last accessed April 29, 2019).  Ibid. See also, Turner Lee, Nicol. Detecting racial bias in algorithms and machine learning. Journal of Information, Communication and Ethics in Society 2018, Vol. 16 Issue 3, pp. 252-260. Available at https://doi.org/10.1108/JICES-06-2018-0056/ (last accessed April 29, 2019).  Sydell, Laura. “It Ain’t Me, Babe: Researchers Find Flaws In Police Facial Recognition Technology.” NPR.org, October 25, 2016. Available at https://www.npr.org/sections/alltechconsidered/2016/10/25/499176469/it-aint-me-babe-researchers-find-flaws-in-police-facial-recognition (last accessed April 19, 2019).  Guerin, Lisa. “Disparate Impact Discrimination.” www.nolo.com. Available at https://www.nolo.com/legal-encyclopedia/disparate-impact-discrimination.htm (last accessed April 24, 2019). See also, Jewel v. NSA where the Electronic Frontier Foundation argues that massive (or dragnet) surveillance is illegal. Information about case available at https://www.eff.org/cases/jewel (last accessed April 19, 2019).  This is often called an anti-classification criterion that the algorithm cannot classify based on membership in the protected or sensitive classes.  Zarsky, Tal. “Understanding Discrimination in the Scored Society.” SSRN Scholarly Paper. Rochester, NY: Social Science Research Network, January 15, 2015. https://papers.ssrn.com/abstract=2550248 .  Larson, Jeff, Surya Mattu, and Julia Angwin. “Unintended Consequences of Geographic Targeting.” Technology Science, September 1, 2015. Available at https://techscience.org/a/2015090103/ (last accessed April 19, 2019).  Terry Parris Jr Julia Angwin, “Facebook Lets Advertisers Exclude Users by Race,” text/html, ProPublica, October 28, 2016. Available at https://www.propublica.org/article/facebook-lets-advertisers-exclude-users-by-race (last accessed April 19, 2019).  Amazon doesn’t consider the race of its customers. Should It? Bloomberg.com. Available at http//www.bloomberg.com/graphics/2016-amazon-same-day (last accessed April 19, 2019).  Corbett-Davies et al., “Algorithmic Decision Making and the Cost of Fairness.”  Solon Barocas and Andrew D. Selbst, “Big Data’s Disparate Impact,” SSRN Scholarly Paper (Rochester, NY: Social Science Research Network, 2016. Available at https://papers.ssrn.com/abstract=2477899.  See, Zafar, Muhammad Bilal, Isabel Valera Martinez, Manuel Gomez Rodriguez, and Krishna Gummadi. “Fairness Constraints: A Mechanism for Fair Classification.” In Proceedings of the 20th International Conference on Artificial Intelligence and Statistics (AISTATS). Fort Lauderdale, FL, 2017. See also , Spielkamp, Matthias. “We Need to Shine More Light on Algorithms so They Can Help Reduce Bias, Not Perpetuate It.” MIT Technology Review. Accessed September 20, 2018. Available at https://www.technologyreview.com/s/607955/inspecting-algorithms-for-bias/ (last accessed April 19, 2019). See also Corbett-Davies, Sam, Emma Peirson, Avi Feller, and Sharad Goel. “A Computer Program Used for Bail and Sentencing Decisions Was Labeled Biased against Blacks. It’s Actually Not That Clear.” Washington Post (blog), October 17, 2016. Available at https://www.washingtonpost.com/news/monkey-cage/wp/2016/10/17/can-an-algorithm-be-racist-our-analysis-is-more-cautious-than-propublicas/ (last accessed April 19. 2019).  Jon Kleinberg, Sendhil Mullainathan, and Manish Raghavan, “Inherent Trade-Offs in the Fair Determination of Risk Scores.”In Proceedings of Innovations in Theoretical Computer Science (ITCS), 2017. Available at https://arxiv.org/pdf/1609.05807.pdf (last accessed April 19, 2019).  This notion of disparate impact has been legally tested dating back to the 1971 U.S. Supreme Court decision, Griggs v. Duke Power Company where the defendant was found to be using intelligence test scores and high school diplomas as factors to hire more white applicants over people of color. As determined by the court decision, there was no correlation between the tests or educational requirements for the jobs in question. See, Griggs v. Duke Power Company, Oyez. Available at https//www.oyez.org/cases/1970/124 (last accessed October 1, 2018.  Various computer models are being created to combat the discriminatory effects of algorithmic bias. See , Romei, Andrea, and Salvatore Ruggieri. “Discrimination Data Analysis: A Multi-Disciplinary Bibliography.” In Discrimination and Privacy in the Information Society, edited by Bart Custers, T Calders, B Schermer, and T Zarsky, 109–35. Studies in Applied Philosophy, Epistemology and Rational Ethics. Springer, Berlin, Heidelberg, 2013. Available at https://doi.org/10.1007/978-3-642-30487-3_6 (last accessed April 19, 2019).  Corbett-Davies, Sam, Emma Pierson, Avi Feller, Sharad Goel, and Aziz Huq. “Algorithmic Decision Making and the Cost of Fairness.” ArXiv:1701.08230 [Cs, Stat], January 27, 2017. Available at https://doi.org/10.1145/3097983.309809 (last accessed April 19, 2019).  Ibid.  Schatz, Brian. AI in Government Act of 2018, Pub. L. No. S.B. 3502 (2018). https://www.congress.gov/bill/115th-congress/senate-bill/3502.  At its February meeting, the OECD announced that it had approved its expert group’s guidelines and hoped to (C.  See European Union, Digital Single Market, Ethics Guidelines for Trustworthy AI, available for download from https://ec.europa.eu/digital-single-market/en/news/ethics-guidelines-trustworthy-ai (last accessed April 19, 2019).  See High-level Expert Group on Artificial Intelligence. “Ethics Guidelines for Trustworthy AI (Draft).” The European Commission, December 18, 2018. See also , Chessell, Mandy. “Ethics for Big Data and Analytics.” IBM, n.d. Available at https://www.ibmbigdatahub.com/sites/default/files/whitepapers_reports_file/TCG%20Study%20Report%20-%20Ethics%20for%20BD%26A.pdf (last accessed April 19, 2019). https://ec.europa.eu/futurium/en/system/files/ged/ai_hleg_draft_ethics_guidelines_18_december.pdf. See also “The Global Data Ethics Project.” Data for Democracy, n.d. https://www.datafordemocracy.org/project/global-data-ethics-project (last accessed April 19, 2019).  Spielkamp, Matthias. “We need to shine more light on algorithms so they can help reduce bias, not perpetuate It.” MIT Technology Review. Available at https://www.technologyreview.com/s/607955/inspecting-algorithms-for-bias/ (last accessed September 20, 2018).  Tobin, Ariana. “HUD sues Facebook over housing discrimination and says the company’s algorithms have made the problem worse.” ProPublica (March 28, 2019). Available at https://www.propublica.org/article/hud-sues-facebook-housing-discrimination-advertising-algorithms (last accessed April 29, 2019).  Elejalde-Ruiz, Alexia. “The end of the resume? Hiring is in the midst of technological revolution with algorithms, chatbots.” Chicago Tribune (July 19, 2018). Available at http://www.chicagotribune.com/business/ct-biz-artificial-intelligence-hiring-20180719-story.html .  Reisman, Dillon, Jason Schultz, Kate Crawford, and Meredith Whittaker. “Algorithmic Impact Assessments: A Practical Framework for Public Agency Accountability.” New York: AI Now, April 2018.  Alexandra Chouldechova et al., “A Case Study of Algorithm-Assisted Decision Making in Child Maltreatment Hotline Screening Decisions,” 1st Conference on Fairness, Accountability and Transparency , n.d., 15.  Rhema Vaithianathan et al., “Section 7: Allegheny Family Screening Tool: Methodology, Version 2,” April 2019.  Locklear, Mallory. “Facebook Releases an Update on Its Civil Rights Audit.” Engadget (blog), December 18, 2018. Available at https://www.engadget.com/2018/12/18/facebook-update-civil-rights-audit/ (last accessed April 19, 2019).  Stack, Liam. “Facebook Announces New Policy to Ban White Nationalist Content.” The New York Times, March 28, 2019, sec. Business. Available at https://www.nytimes.com/2019/03/27/business/facebook-white-nationalist-supremacist.html (last accessed April 19, 2019).  Qtd. in Rachel Courtland, “Bias Detectives: The Researchers Striving to Make Algorithms Fair,” Nature 558, no. 7710 (June 2018): 357–60. Available at https://doi.org/10.1038/d41586-018-05469-3 (last accessed April 19, 2019).  Fintech regulatory sandboxes in UK , Singapore , and states in the U.S. are beginning to authorize them. They allow freedom to offer new financial products and use new technologies such as blockchain .  In March, the state of Arizona became the first U.S. state to create a “regulatory sandbox” for fintech companies , allowing them to test financial products on customers with lighter regulations. The U.K. has run a similar initiative called Project Innovate since 2014. The application of a sandbox can allow both startup companies and incumbent banks to experiment with more innovative products without worrying about how to reconcile them with existing rules.  Mnuchin, Steven T., and Craig S. Phillips. “A Financial System That Creates Economic Opportunities - Nonbank Financials, Fintech, and Innovation.” Washington, D.C.: U.S. Department of the Treasury, July 2018.Available at https://home.treasury.gov/sites/default/files/2018-08/A-Financial-System-that-Creates-Economic-Opportunities---Nonbank-Financials-Fintech-and-Innovation_0.pdf (last accessed April 19, 2019).  Another major tech-related Safe Harbor is the EU-US Privacy Shield after the previous Safe Harbor was declared invalid in the EU. Available at https://en.wikipedia.org/wiki/EU%E2%80%93US_Privacy_Shield (last accessed April 19. 2019).  Lopez, German. “The First Step Act, Congress’s Criminal Justice Reform Bill, Explained.” Vox, December 3, 2018. Available at https://www.vox.com/future-perfect/2018/12/3/18122392/first-step-act-criminal-justice-reform-bill-congress (last accessed April 16, 2019).       Related Topics   Technology & Innovation  Telecommunications & Internet                 Find us on Facebook        Find us on Twitter        Find us on YouTube        Listen to our Podcast        Browse Newsletters        Subscribe to our RSS       Languages  Español  中文  عربي      About Us  Research Programs  Find an Expert  Careers  Contact  Terms and Conditions  Brookings Privacy Policy  Copyright 2020 The Brookings Institution               Trending     U.S. Politics & Government      Topics     AI    Policy 2020    Cities & Regions    Global Dev    Intl Affairs    U.S. Economy    U.S. Politics & Govt    More      About Us    Press Room    Experts    Events    The Brookings Press    WashU at Brookings    Careers    Support Brookings     Cart  0       Get daily updates from Brookings           Enter Email                     Send to Email Address   Your Name   Your Email Address        Cancel  Post was not sent - check your email addresses!  Email check failed, please try again  Sorry, your blog cannot share posts by email.                      "
10,splitting biases(fallacy OR bias OR heuristic) (coronavirus OR covid OR pandemic),2,https://www.visualcapitalist.com/50-cognitive-biases-in-the-modern-world/,"50 Cognitive Biases in the Modern World

Cognitive biases are widely accepted as something that makes us human.

Every day, systematic errors in our thought process impact the way we live and work. But in a world where everything we do is changing rapidly—from the way we store information to the way we watch TV—what really classifies as rational thinking?

It’s a question with no right or wrong answer, but to help us decide for ourselves, today’s infographic from TitleMax lists 50 cognitive biases that we may want to become privy to.

In the name of self-awareness, here’s a closer look at three recently discovered biases that we are most prone to exhibiting in the modern world.

Automation Bias

AI-infused applications are becoming incredibly good at “personalizing” our content, but will there come a time when we let algorithms make all of our decisions?

Automation bias refers to the tendency to favor the suggestions of automated systems.

Take Netflix, for example. Everything we see on the platform is the result of algorithms—even the preview images that are generated. Then, to harness the power of data and machine learning, Netflix categorizes its content into tens of thousands of micro-genres. Pairing these genre tags with a viewer’s history allows them to assign several of over 2,000 “taste profiles” to each user.

And while there’s nothing wrong with allowing Netflix to guide what we watch, there’s an enormous sea of content standing by. Estimates from 2015 claimed it would take nearly four years to watch all of Netflix’s content. Thousands more hours of content have since been added.

If we want to counter this cognitive bias, finding a new favorite series on platforms like Netflix may require some good old-fashioned human curiosity.

The Google Effect

Also known as “digital amnesia”, the aptly named Google Effect describes our tendency to forget information that can be easily accessed online.

First described in 2011 by Betsy Sparrow (Columbia University) and her colleagues, their paper described the results of several memory experiments involving technology.

In one experiment, participants typed trivia statements into a computer and were later asked to recall them. Half believed the statements were saved, and half believed the statements were erased. The results were significant: participants who assumed they could look up their statements did not make much effort to remember them.

Because search engines are continually available to us, we may often be in a state of not feeling we need to encode the information internally. When we need it, we will look it up.

– Sparrow B, et al. Science 333, 777 (2011)

Our modern brains appear to be re-prioritizing the information we hold onto. Notably, the study doesn’t suggest we’re becoming less intelligent—our ability to learn offline remains the same.

The IKEA Effect

Identified in 2011 by Michael Norton (Harvard Business School) and his colleagues, this cognitive bias refers to our tendency to attach a higher value to things we help create.

Combining the Ikea Effect with other related traits, such as our willingness to pay a premium for customization, is a strategy employed by companies seeking to increase the intrinsic value that we attach to their products.

For instance, American retailer Build-A-Bear Workshop is anchored around creating a highly interactive customer experience. With the help of staff, children (or adults) can assemble their stuffed animals from scratch, then add clothing and accessories at extra cost.

Nike also incorporates this bias into its offering. The footwear company offers a Nike By You line of customizable products, where customers pay a premium to design bespoke shoes with an extensive online configurator.

While there’s nothing necessarily wrong with our susceptibility to the Ikea Effect, understanding its significance may help us make more appropriate decisions as consumers.

What Can We Do?

As we navigate an increasingly complex world, it’s natural for us to unconsciously adopt new patterns of behavior.

Becoming aware of our cognitive biases, and their implications, can help us stay on the right course.","                 Skip to main content          Search Brookings         About Us Press Room Experts Events The Brookings Press WashU at Brookings Careers Support Brookings    Cart  0    Search Guidance for the Brookings community and the public on our response to the coronavirus (COVID-19) »  Learn more from Brookings scholars about the global response to coronavirus (COVID-19) »                  Algorithmic bias detection and mitigation: Best practices and policies to reduce consumer harms  Facebook Twitter LinkedIn Print SMS Email More Reddit AI  Policy 2020  Cities & Regions  Global Dev  Intl Affairs  U.S. Economy  U.S. Politics & Govt  More             0                Report  Algorithmic bias detection and mitigation: Best practices and policies to reduce consumer harms   Nicol Turner Lee , Paul Resnick , and Genie Barton  Wednesday, May 22, 2019                       Facebook Twitter LinkedIn Print SMS Email More Reddit  For media inquiries, contact:   Governance Studies Main Line  202.797.6090           Introduction         Nicol Turner Lee  Senior Fellow - Governance Studies  Director - Center for Technology Innovation     @drturnerlee          Paul Resnick  Professor of Information; Associate Dean for Research and Faculty Affairs, School of Information - University of Michigan     presnick          Genie Barton  Member, Research Advisory Board - International Association of Privacy Professionals     privacygenie       The private and public sectors are increasingly turning to artificial intelligence (AI) systems and machine learning algorithms to automate simple and complex decision-making processes. 1 The mass-scale digitization of data and the emerging technologies that use them are disrupting most economic sectors, including transportation, retail, advertising, and energy, and other areas. AI is also having an impact on democracy and governance as computerized systems are being deployed to improve accuracy and drive objectivity in government functions.  The availability of massive data sets has made it easy to derive new insights through computers. As a result, algorithms, which are a set of step-by-step instructions that computers follow to perform a task, have become more sophisticated and pervasive tools for automated decision-making. 2 While algorithms are used in many contexts, we focus on computer models that make inferences from data about people, including their identities, their demographic attributes, their preferences, and their likely future behaviors, as well as the objects related to them. 3  “Algorithms are harnessing volumes of macro- and micro-data to influence decisions affecting people in a range of tasks, from making movie recommendations to helping banks determine the creditworthiness of individuals.”  In the pre-algorithm world, humans and organizations made decisions in hiring, advertising, criminal sentencing, and lending. These decisions were often governed by federal, state, and local laws that regulated the decision-making processes in terms of fairness, transparency, and equity. Today, some of these decisions are entirely made or influenced by machines whose scale and statistical rigor promise unprecedented efficiencies. Algorithms are harnessing volumes of macro- and micro-data to influence decisions affecting people in a range of tasks, from making movie recommendations to helping banks determine the creditworthiness of individuals. 4 In machine learning, algorithms rely on multiple data sets, or training data, that specifies what the correct outputs are for some people or objects. From that training data, it then learns a model which can be applied to other people or objects and make predictions about what the correct outputs should be for them. 5  However, because machines can treat similarly-situated people and objects differently, research is starting to reveal some troubling examples in which the reality of algorithmic decision-making falls short of our expectations. Given this, some algorithms run the risk of replicating and even amplifying human biases, particularly those affecting protected groups. 6 For example, automated risk assessments used by U.S. judges to determine bail and sentencing limits can generate incorrect conclusions, resulting in large cumulative effects on certain groups, like longer prison sentences or higher bails imposed on people of color.  In this example, the decision generates “bias,” a term that we define broadly as it relates to outcomes which are systematically less favorable to individuals within a particular group and where there is no relevant difference between groups that justifies such harms. 7 Bias in algorithms can emanate from unrepresentative or incomplete training data or the reliance on flawed information that reflects historical inequalities. If left unchecked, biased algorithms can lead to decisions which can have a collective, disparate impact on certain groups of people even without the programmer’s intention to discriminate. The exploration of the intended and unintended consequences of algorithms is both necessary and timely, particularly since current public policies may not be sufficient to identify, mitigate, and remedy consumer impacts.  With algorithms appearing in a variety of applications, we argue that operators and other concerned stakeholders must be diligent in proactively addressing factors which contribute to bias. Surfacing and responding to algorithmic bias upfront can potentially avert harmful impacts to users and heavy liabilities against the operators and creators of algorithms, including computer programmers, government, and industry leaders. These actors comprise the audience for the series of mitigation proposals to be presented in this paper because they either build, license, distribute, or are tasked with regulating or legislating algorithmic decision-making to reduce discriminatory intent or effects.   Related             Technology & Innovation  How artificial intelligence is transforming the world   Darrell M. West and John R. Allen  Tuesday, April 24, 2018                Technology & Innovation  Trends in the Information Technology sector   Makada Henry-Nickie , Kwadwo Frimpong , and Hao Sun  Friday, March 29, 2019                Media & Journalism  How to combat fake news and disinformation   Darrell M. West  Monday, December 18, 2017        Our research presents a framework for algorithmic hygiene , which identifies some specific causes of biases and employs best practices to identify and mitigate them. We also present a set of public policy recommendations, which promote the fair and ethical deployment of AI and machine learning technologies.  This paper draws upon the insight of 40 thought leaders from across academic disciplines, industry sectors, and civil society organizations who participated in one of two roundtables. 8 Roundtable participants actively debated concepts related to algorithmic design, accountability, and fairness, as well as the technical and social trade-offs associated with various approaches to bias detection and mitigation.  Our goal is to juxtapose the issues that computer programmers and industry leaders face when developing algorithms with the concerns of policymakers and civil society groups who assess their implications. To balance the innovations of AI and machine learning algorithms with the protection of individual rights, we present a set of public policy recommendations, self-regulatory best practices, and consumer-focused strategies–all of which promote the fair and ethical deployment of these technologies.  Our public policy recommendations include the updating of nondiscrimination and civil rights laws to apply to digital practices, the use of regulatory sandboxes to foster anti-bias experimentation, and safe harbors for using sensitive information to detect and mitigate biases. We also outline a set of self-regulatory best practices, such as the development of a bias impact statement, inclusive design principles, and cross-functional work teams. Finally, we propose additional solutions focused on algorithmic literacy among users and formal feedback mechanisms to civil society groups.  The next section provides five examples of algorithms to explain the causes and sources of their biases. Later in the paper, we discuss the trade-offs between fairness and accuracy in the mitigation of algorithmic bias, followed by a robust offering of self-regulatory best practices, public policy recommendations, and consumer-driven strategies for addressing online biases. We conclude by highlighting the importance of proactively tackling the responsible and ethical use of machine learning and other automated decision-making tools.  Examples of algorithmic biases  Algorithmic bias can manifest in several ways with varying degrees of consequences for the subject group. Consider the following examples, which illustrate both a range of causes and effects that either inadvertently apply different treatment to groups or deliberately generate a disparate impact on them.  Bias in online recruitment tools  Online retailer Amazon, whose global workforce is 60 percent male and where men hold 74 percent of the company’s managerial positions, recently discontinued use of a recruiting algorithm after discovering gender bias. 9 The data that engineers used to create the algorithm were derived from the resumes submitted to Amazon over a 10-year period, which were predominantly from white males. The algorithm was taught to recognize word patterns in the resumes, rather than relevant skill sets, and these data were benchmarked against the company’s predominantly male engineering department to determine an applicant’s fit. As a result, the AI software penalized any resume that contained the word “women’s” in the text and downgraded the resumes of women who attended women’s colleges, resulting in gender bias. 10  Amazon discontinued a recruiting algorithm after discovering that it led to gender bias in its hiring. (Credit: Brian Snyder/Reuters)  Bias in word associations  Princeton University researchers used off-the-shelf machine learning AI software to analyze and link 2.2 million words. They found that European names were perceived as more pleasant than those of African-Americans, and that the words “woman” and “girl” were more likely to be associated with the arts instead of science and math, which were most likely connected to males. 11 In analyzing these word-associations in the training data, the machine learning algorithm picked up on existing racial and gender biases shown by humans. If the learned associations of these algorithms were used as part of a search-engine ranking algorithm or to generate word suggestions as part of an auto-complete tool, it could have a cumulative effect of reinforcing racial and gender biases.  Bias in online ads  Latanya Sweeney, Harvard researcher and former chief technology officer at the Federal Trade Commission (FTC), found that online search queries for African-American names were more likely to return ads to that person from a service that renders arrest records, as compared to the ad results for white names. 12 Her research also found that the same differential treatment occurred in the micro-targeting of higher-interest credit cards and other financial products when the computer inferred that the subjects were African-Americans, despite having similar backgrounds to whites. 13 During a public presentation at a FTC hearing on big data, Sweeney demonstrated how a web site, which marketed the centennial celebration of an all-black fraternity, received continuous ad suggestions for purchasing “arrest records” or accepting high-interest credit card offerings. 14  Bias in facial recognition technology  MIT researcher Joy Buolamwini found that the algorithms powering three commercially available facial recognition software systems were failing to recognize darker-skinned complexions. 15 Generally, most facial recognition training data sets are estimated to be more than 75 percent male and more than 80 percent white. When the person in the photo was a white man, the software was accurate 99 percent of the time at identifying the person as male. According to Buolamwini’s research, the product error rates for the three products were less than one percent overall, but increased to more than 20 percent in one product and 34 percent in the other two in the identification of darker-skinned women as female. 16 In response to Buolamwini’s facial-analysis findings, both IBM and Microsoft committed to improving the accuracy of their recognition software for darker-skinned faces.  Bias in criminal justice algorithms  Acknowledging the possibility and causes of bias is the first step in any mitigation approach.  The COMPAS (Correctional Offender Management Profiling for Alternative Sanctions) algorithm, which is used by judges to predict whether defendants should be detained or released on bail pending trial, was found to be biased against African-Americans, according to a report from ProPublica. 17 The algorithm assigns a risk score to a defendant’s likelihood to commit a future offense, relying on the voluminous data available on arrest records, defendant demographics, and other variables. Compared to whites who were equally likely to re-offend, African-Americans were more likely to be assigned a higher-risk score, resulting in longer periods of detention while awaiting trial. 18 Northpointe, the firm that sells the algorithm’s outputs, offers evidence to refute such claims and argues that wrong metrics are being used to assess fairness in the product, a topic that we return to later in the paper.  While these examples of bias are not exhaustive, they suggest that these problems are empirical realities and not just theoretical concerns. They also illustrate how these outcomes emerge, and in some cases, without malicious intent by the creators or operators of the algorithm. Acknowledging the possibility and causes of bias is the first step in any mitigation approach. On this point, roundtable participant Ricardo Baeza-Yates from NTENT stated that “[companies] will continue to have a problem discussing algorithmic bias if they don’t refer to the actual bias itself.”  Causes of bias  Barocas and Selbst point out that bias can creep in during all phases of a project, “…whether by specifying the problem to be solved in ways that affect classes differently, failing to recognize or address statistical biases, reproducing past prejudice, or considering an insufficiently rich set of factors.” 19 Roundtable participants focused especially on bias stemming from flaws in the data used to train the algorithms. “Flawed data is a big problem,” stated roundtable participant Lucy Vasserman from Google, “…especially for the groups that businesses are working hard to protect.” While there are many causes, we focus on two of them: historical human biases and incomplete or unrepresentative data .  Historical human biases  Historical human biases are shaped by pervasive and often deeply embedded prejudices against certain groups, which can lead to their reproduction and amplification in computer models. In the COMPAS algorithm, if African-Americans are more likely to be arrested and incarcerated in the U.S. due to historical racism, disparities in policing practices, or other inequalities within the criminal justice system, these realities will be reflected in the training data and used to make suggestions about whether a defendant should be detained. If historical biases are factored into the model, it will make the same kinds of wrong judgments that people do.  The Amazon recruitment algorithm revealed a similar trajectory when men were the benchmark for professional “fit,” resulting in female applicants and their attributes being downgraded. These historical realities often find their way into the algorithm’s development and execution, and they are exacerbated by the lack of diversity which exists within the computer and data science fields. 20  Further, human biases can be reinforced and perpetuated without the user’s knowledge. For example, African-Americans who are primarily the target for high-interest credit card options might find themselves clicking on this type of ad without realizing that they will continue to receive such predatory online suggestions. In this and other cases, the algorithm may never accumulate counter-factual ad suggestions (e.g., lower-interest credit options) that the consumer could be eligible for and prefer. Thus, it is important for algorithm designers and operators to watch for such potential negative feedback loops that cause an algorithm to become increasingly biased over time.  Incomplete or unrepresentative training data  Insufficient training data is another cause of algorithmic bias. If the data used to train the algorithm are more representative of some groups of people than others, the predictions from the model may also be systematically worse for unrepresented or under-representative groups. For example, in Buolamwini’s facial-analysis experiments, the poor recognition of darker-skinned faces was largely due to their statistical under-representation in the training data. That is, the algorithm presumably picked up on certain facial features, such as the distance between the eyes, the shape of the eyebrows and variations in facial skin shades, as ways to detect male and female faces. However, the facial features that were more representative in the training data were not as diverse and, therefore, less reliable to distinguish between complexions, even leading to a misidentification of darker-skinned females as males.  Turner Lee has argued that it is often the lack of diversity among the programmers designing the training sample which can lead to the under-representation of a particular group or specific physical attributes. 21 Buolamwini’s findings were due to her rigor in testing, executing, and assessing a variety of proprietary facial-analysis software in different settings, correcting for the lack of diversity in their samples.  Conversely, algorithms with too much data, or an over-representation, can skew the decision toward a particular result. Researchers at Georgetown Law School found that an estimated 117 million American adults are in facial recognition networks used by law enforcement, and that African-Americans were more likely to be singled out primarily because of their over-representation in mug-shot databases. 22 Consequently, African-American faces had more opportunities to be falsely matched, which produced a biased effect.  Bias detection strategies  Understanding the various causes of biases is the first step in the adoption of effective algorithmic hygiene. But, how can operators of algorithms assess whether their results are, indeed, biased? Even when flaws in the training data are corrected, the results may still be problematic because context matters during the bias detection phase.  “Even when flaws in the training data are corrected, the results may still be problematic because context matters during the bias detection phase.”  First, all detection approaches should begin with careful handling of the sensitive information of users, including data that identify a person’s membership in a federally protected group (e.g., race, gender). In some cases, operators of algorithms may also worry about a person’s membership in some other group if they are also susceptible to unfair outcomes. An examples of this could be college admission officers worrying about the algorithm’s exclusion of applicants from lower-income or rural areas; these are individuals who may be not federally protected but do have susceptibility to certain harms (e.g., financial hardships).  In the former case, systemic bias against protected classes can lead to collective, disparate impacts , which may have a basis for legally cognizable harms, such as the denial of credit, online racial profiling, or massive surveillance. 23 In the latter case, the outputs of the algorithm may produce unequal outcomes or unequal error rates for different groups, but they may not violate legal prohibitions if there was no intent to discriminate.  These problematic outcomes should lead to further discussion and awareness of how algorithms work in the handling of sensitive information, and the trade-offs around fairness and accuracy in the models.  Algorithms and sensitive information  While it is intuitively appealing to think that an algorithm can be blind to sensitive attributes, this is not always the case. 24 Critics have pointed out that an algorithm may classify information based on online proxies for the sensitive attributes, yielding a bias against a group even without making decisions directly based on one’s membership in that group. Barocas and Selbst define online proxies as “factors used in the scoring process of an algorithm which are mere stand-ins for protected groups, such as zip code as proxies for race, or height and weight as proxies for gender.” 25 They argue that proxies often linked to algorithms can produce both errors and discriminatory outcomes, such as instances where a zip code is used to determine digital lending decisions or one’s race triggers a disparate outcome. 26 Facebook’s advertising platform contained proxies that allowed housing marketers to micro-target preferred renters and buyers by clicking off data points, including zip code preferences. 27 Thus, it is possible that an algorithm which is completely blind to a sensitive attribute could actually produce the same outcome as one that uses the attribute in a discriminatory manner.  “While it is intuitively appealing to think that an algorithm can be blind to sensitive attributes, this is not always the case.”  For example, Amazon made a corporate decision to exclude certain neighborhoods from its same-day Prime delivery system. Their decision relied upon the following factors: whether a particular zip code had a sufficient number of Prime members, was near a warehouse, and had sufficient people willing to deliver to that zip code. 28 While these factors corresponded with the company’s profitability model, they resulted in the exclusion of poor, predominantly African-American neighborhoods, transforming these data points into proxies for racial classification. The results, even when unintended, discriminated against racial and ethnic minorities who were not included.  Similarly, a job-matching algorithm may not receive the gender field as an input, but it may produce different match scores for two resumes that differ only in the substitution of the name “Mary” for “Mark” because the algorithm is trained to make these distinctions over time.  There are also arguments that blinding the algorithm to sensitive attributes can cause algorithmic bias in some situations. Corbett-Davies and Goel point out in their research on the COMPAS algorithm that even after controlling for “legitimate” risk factors, empirically women have been found to re-offend less often than men in many jurisdictions. 29 If an algorithm is forbidden from reporting a different risk assessment score for two criminal defendants who differ only in their gender, judges may be less likely to release female defendants than male defendants with equal actual risks of committing another crime before trial. Thus, blinding the algorithm from any type of sensitive attribute may not solve bias.  While roundtable participants were not in agreement on the use of online proxies in modeling, they largely agreed that operators of algorithms must be more transparent in their handling of sensitive information, especially if the potential proxy could itself be a legal classificatory harm. 30 There was also discussion that the use of sensitive attributes as part of an algorithm could be a strategy for detecting and possibly curing intended and unintentional biases. Because currently doing so may be constrained by privacy regulations, such as the European Union’s General Data Protection Rules (GDPR) or proposed U.S. federal privacy legislation, the argument could be made for the use of regulatory sandboxes and safe harbors to allow the use of sensitive information when detecting and mitigating biases, both of which will be introduced as part of our policy recommendations.  Detecting bias  When detecting bias, computer programmers normally examine the set of outputs that the algorithm produces to check for anomalous results. Comparing outcomes for different groups can be a useful first step. This could even be done through simulations. Roundtable participant Rich Caruana from Microsoft suggested that companies consider the simulation of predictions (both true and false) before applying them to real-life scenarios. “We almost need a secondary data collection process because sometimes the model will [emit] something quite different,” he shared. For example, if a job-matching algorithm’s average score for male applicants is higher than that for women, further investigation and simulations could be warranted.  However, the downside of these approaches is that not all unequal outcomes are unfair. Roundtable participant Solon Barocas from Cornell University summed this up when he stated, “Maybe we find out that we have a very accurate model, but it still produces disparate outcomes. This may be unfortunate, but is it fair?” An alternative to accounting for unequal outcomes may be to look at the equality of error rates, and whether there are more mistakes for one group of people than another. On this point, Isabel Kloumann of Facebook shared that “society has expectations. One of which is not incarcerating one minority group disproportionately [as a result of an algorithm].”  As shown in the debates around the COMPAS algorithm, even error rates are not a simple litmus test for biased algorithms. Northpointe, the company that developed the COMPAS algorithm, refutes claims of racial discrimination. They argue that among defendants assigned the same high risk score, African-American and white defendants have almost equal recidivism rates, so by that measure, there is no error in the algorithm’s decision. 31 In their view, judges can consider their algorithm without any reference to race in bail and release decisions.  It is not possible, in general, to have equal error rates between groups for all the different error rates. 32 ProPublica focused on one error rate, while Northpointe honed in on another. Thus, some principles need to be established for which error rates should be equalized in which situations in order to be fair.  The COMPAS algorithm, which is used by judges to predict whether defendants should be detained or released on bail pending trial, has drawn scrutiny over claims of potential racial discrimination. (Credit: Stephen Lam/Reuters)  However, distinguishing between how the algorithm works with sensitive information and potential errors can be problematic for operators of algorithms, policymakers, and civil society groups. 33 “Companies would be losing a lot if we don’t draw a distinction between the two,” said Julie Brill from Microsoft. At the very least, there was agreement among roundtable participants that algorithms should not perpetuate historical inequities, and that more work needs to be done to address online discrimination. 34  Fairness and accuracy trade-offs  Next, a discussion of trade-offs and ethics is needed. Here, the focus should be on evaluating both societal notions of “fairness” and possible social costs. In their research of the COMPAS algorithm, Corbett-Davies, Goel, Pierson, Feller, and Huq see “an inherent tension between minimizing violent crime and satisfying common notions of fairness.” 35 They conclude that optimizing for public safety yields decisions that penalize defendants of color, while satisfying legal and societal fairness definitions, and may lead to more releases of high-risk defendants, which would adversely affect public safety. 36 Moreover, the negative impacts on public safety might also disproportionately affect African-American and white neighborhoods, thus creating a fairness cost as well.  If the goal is to avoid reinforcing inequalities, what, then, should developers and operators of algorithms do to mitigate potential biases? We argue that developers of algorithms should first look for ways to reduce disparities between groups without sacrificing the overall performance of the model, especially whenever there appears to be a trade-off.  A handful of roundtable participants argued that opportunities exist for improving both fairness and accuracy in algorithms. For programmers, the investigation of apparent bugs in the software may reveal why the model was not maximizing for overall accuracy. The resolution of these bugs can then improve overall accuracy. Data sets, which may be under-representative of certain groups, may need additional training data to improve accuracy in the decision-making and reduce unfair results. Buolamwini’s facial detection experiments are good examples of this type of approach to fairness and accuracy.  Roundtable participant Sarah Holland from Google pointed out the risk tolerance associated with these types of trade-offs when she shared that “[r]aising risk also involves raising equity issues.” Thus, companies and other operators of algorithms should determine if the social costs of the trade-offs are justified, the stakeholders involved are amenable to a solution through algorithms, or if human decision-makers are needed to frame the solution.  Ethical frameworks matter  What is fundamentally behind these fairness and accuracy trade-offs should be discussions around ethical frameworks and potential guardrails for machine learning tasks and systems. There are several ongoing and recent international and U.S.-based efforts to develop ethical governance standards for the use of AI. 37 The 35-member Organization for Economic Cooperation and Development (OECD) is expected shortly to release its own guidelines for ethical AI. 38 The European Union recently released “Ethics Guidelines for Trustworthy AI,” which delineates seven governance principles: (1) human agency and oversight, (2) technical robustness and safety, (3) privacy and data governance, (4) transparency, (5) diversity, nondiscrimination and fairness, (6) environmental and societal well-being, and (7) accountability. 39 The EU’s ethical framework reflects a clear consensus that it is unethical to “unfairly discriminate.” Within these guidelines, member states link diversity and nondiscrimination with principles of fairness, enabling inclusion and diversity throughout the entire AI system’s lifecycle. Their principles interpret fairness through the lenses of equal access, inclusive design processes, and equal treatment.  Yet, even with these governmental efforts, it is still surprisingly difficult to define and measure fairness. 40 While it will not always be possible to satisfy all notions of fairness at the same time, companies and other operators of algorithms must be aware that there is no simple metric to measure fairness that a software engineer can apply, especially in the design of algorithms and the determination of the appropriate trade-offs between accuracy and fairness. Fairness is a human, not a mathematical, determination, grounded in shared ethical beliefs. Thus, algorithmic decisions that may have a serious consequence for people will require human involvement.  For example, while the training data discrepancies in the COMPAS algorithm can be corrected, human interpretation of fairness still matters. For that reason, while an algorithm such as COMPAS may be a useful tool, it cannot substitute for the decision-making that lies within the discretion of the human arbiter. 41 We believe that subjecting the algorithm to rigorous testing can challenge the different definitions of fairness, a useful exercise among companies and other operators of algorithms.  “It’s important for algorithm operators and developers to always be asking themselves: Will we leave some groups of people worse off as a result of the algorithm’s design or its unintended consequences? “  In the decision to create and bring algorithms to market, the ethics of likely outcomes must be considered—especially in areas where governments, civil society, or policymakers see potential for harm, and where there is a risk of perpetuating existing biases or making protected groups more vulnerable to existing societal inequalities. That is why it’s important for algorithm operators and developers to always be asking themselves: Will we leave some groups of people worse off as a result of the algorithm’s design or its unintended consequences?  We suggest that this question is one among many that the creators and operators of algorithms should consider in the design, execution, and evaluation of algorithms, which are described in the following mitigation proposals. Our first proposal addresses the updating of U.S. nondiscrimination laws to apply to the digital space.  Mitigation proposals  Nondiscrimination and other civil rights laws should be updated to interpret and redress online disparate impacts  To develop trust from policymakers, computer programmers, businesses, and other operators of algorithms must abide by U.S. laws and statutes that currently forbid discrimination in public spaces. Historically, nondiscrimination laws and statutes unambiguously define the thresholds and parameters for the disparate treatment of protected classes. The 1964 Civil Rights Act “forbade discrimination on the basis of sex as well as race in hiring, promoting, and firing.” The 1968 Fair Housing Act prohibits discrimination in the sale, rental, and financing of dwellings, and in other housing-related transactions to federally protected classes. Enacted in 1974, the Equal Credit Opportunity Act stops any creditor from discriminating against any applicant from any type of credit transaction based on protected characteristics. While these laws do not necessarily mitigate and resolve other implicit or unconscious biases that can be baked into algorithms, companies and other operators should guard against violating these statutory guardrails in the design of algorithms, as well as mitigating their implicit concern to prevent past discrimination from continuing.  Roundtable participant Wendy Anderson from the Office of Congresswoman Val Demings stated, “[T]ypically, legislators only hear when something bad happens. We need to find a way to protect those who need it without stifling innovation.” Congress can clarify how these nondiscrimination laws apply to the types of grievances recently found in the digital space, since most of these laws were written before the advent of the internet. 42 Such legislative action can provide clearer guardrails that are triggered when algorithms are contributing to legally recognizable harms. Moreover, when creators and operators of algorithms understand that these may be more or less non-negotiable factors, the technical design will be more thoughtful in moving away from models that may trigger and exacerbate explicit discrimination, such as design frames that exclude rather than include certain inputs or are not checked for bias. 43  Operators of algorithms must develop a bias impact statement  Once the idea for an algorithm has been vetted against nondiscrimination laws, we suggest that operators of algorithms develop a bias impact statement, which we offer as a template of questions that can be flexibly applied to guide them through the design, implementation, and monitoring phases.  As a self-regulatory practice, the bias impact statement can help probe and avert any potential biases that are baked into or are resultant from the algorithmic decision. As a best practice, operators of algorithms should brainstorm a core set of initial assumptions about the algorithm’s purpose prior to its development and execution. We propose that operators apply the bias impact statement to assess the algorithm’s purpose, process and production, where appropriate. Roundtable participants also suggested the importance of establishing a cross-functional and interdisciplinary team to create and implement the bias impact statement.   New York University’s AI Now Institute    Related Books             Autonomous Vehicles   By Clifford Winston and Quentin Karpilow   2020                Terms of Disservice   By Dipayan Ghosh   2020                Growth in a Time of Change   Edited by Hyeon-Wook Kim and Zia Qureshi   2020        New York University’s AI Now Institute has already introduced a model framework for governmental entities to use to create algorithmic impact assessments (AIAs), which evaluate the potential detrimental effects of an algorithm in the same manner as environmental, privacy, data, or human rights impact statements. 44 While there may be differences in implementation given the type of predictive model, the AIA encompasses multiple rounds of review from internal, external, and public audiences. First, it assumes that after this review, a company will develop a list of potential harms or biases in their self-assessment, with the assistance of more technical outside experts. Second, if bias appears to have occurred, the AIA pushes for notice to be given to impacted populations and a comment period opened for response. And third, the AIA process looks to federal and other entities to support users’ right to challenge algorithmic decisions that feel unfair.  While the AIA process supports a substantive feedback loop, what may be missing is both the required forethought leading up to the decision and the oversight of the algorithm’s provisions. Moreover, our proposed bias impact statement starts with a framework that identifies which automated decisions should be subjected to such scrutiny, operator incentives, and stakeholder engagement.   Which automated decisions?   In the case of determining which automated decisions require such vetting, operators of algorithms should start with questions about whether there will be a possible negative or unintended outcome resulting from the algorithm, for whom, and the severity of consequences for members of the affected group if not detected and mitigated. Reviewing established legal protections around fair housing, employment, credit, criminal justice, and health care should serve as a starting point for determining which decisions need to be viewed with special caution in designing and testing any algorithm used to predict outcomes or make important eligibility decisions about access to a benefit. This is particularly true considering the legal prescriptions against using data that has a likelihood of disparate impact on a protected class or other established harms. Thus, we suggest that operators should be constantly questioning the potential legal, social, and economic effects and potential liabilities associated with that choice when determining which decisions should be automated and how to automate them with minimal risks.   What are the user incentives?   Incentives should also drive organizations to proactively address algorithmic bias. Conversely, operators who create and deploy algorithms that generate fairer outcomes should also be recognized by policymakers and consumers who will trust them more for their practices. When companies exercise effective algorithmic hygiene before, during, and after introducing algorithmic decision-making, they should be rewarded and potentially given a public-facing acknowledgement for best practices.   How are stakeholders being engaged?   Finally, the last element encapsulated in a bias impact statement should involve the engagement of stakeholders who could help computer programmers in the selection of inputs and outputs of certain automated decisions. “Tech succeeds when users understand the product better than its designers,” said Rich Caruana from Microsoft. Getting users engaged early and throughout the process will prompt improvements to the algorithms, which ultimately leads to improved user experiences.  Stakeholder responsibilities can also extend to civil society organizations who can add value in the conversation on the algorithm’s design. “Companies [should] engage civil society,” shared Miranda Bogen from Upturn. “Otherwise, they will go to the press and regulators with their complaints.” A possible solution for operators of algorithms could be the development of an advisory council of civil society organizations that, working alongside companies, may be helpful in defining the scope of the procedure and predicting biases based on their ground-level experiences.   The template for the bias impact statement   These three foundational elements for a bias impact statement are reflected in a discrete set of questions that operators should answer during the design phase to filter out potential biases (Table 1). As a self-regulatory framework, computer programmers and other operators of algorithms can construct this type of tool prior to the model’s design and execution.  Table 1. Design questions template for bias impact statement      What will the automated decision do?    Who is the audience for the algorithm and who will be most affected by it?    Do we have training data to make the correct predictions about the decision?    Is the training data sufficiently diverse and reliable? What is the data lifecycle of the algorithm?    Which groups are we worried about when it comes to training data errors, disparate treatment, and impact?    How will potential bias be detected?    How and when will the algorithm be tested? Who will be the targets for testing?    What will be the threshold for measuring and correcting for bias in the algorithm, especially as it relates to protected groups?    What are the operator incentives?    What will we gain in the development of the algorithm?    What are the potential bad outcomes and how will we know?    How open (e.g., in code or intent) will we make the design process of the algorithm to internal partners, clients, and customers?    What intervention will be taken if we predict that there might be bad outcomes associated with the development or deployment of the algorithm?    How are other stakeholders being engaged?    What’s the feedback loop for the algorithm for developers, internal partners and customers?    Is there a role for civil society organizations in the design of the algorithm?    Has diversity been considered in the design and execution?    Will the algorithm have implications for cultural groups and play out differently in cultural contexts?    Is the design team representative enough to capture these nuances and predict the application of the algorithm within different cultural contexts? If not, what steps are being taken to make these scenarios more salient and understandable to designers?    Given the algorithm’s purpose, is the training data sufficiently diverse?    Are there statutory guardrails that companies should be reviewing to ensure that the algorithm is both legal and ethical?      Diversity-in-design  Operators of algorithms should also consider the role of diversity within their work teams, training data, and the level of cultural sensitivity within their decision-making processes. Employing diversity in the design of algorithms upfront will trigger and potentially avoid harmful discriminatory effects on certain protected groups, especially racial and ethnic minorities. While the immediate consequences of biases in these areas may be small, the sheer quantity of digital interactions and inferences can amount to a new form of systemic bias. Therefore, the operators of algorithms should not discount the possibility or prevalence of bias and should seek to have a diverse workforce developing the algorithm, integrate inclusive spaces within their products, or employ “diversity-in-design,” where deliberate and transparent actions will be taken to ensure that cultural biases and stereotypes are addressed upfront and appropriately. Adding inclusivity into the algorithm’s design can potentially vet the cultural inclusivity and sensitivity of the algorithms for various groups and help companies avoid what can be litigious and embarrassing algorithmic outcomes.  The bias impact statement should not be an exhaustive tool. For algorithms with more at stake, ongoing review of their execution should be factored into the process. The goal here is to monitor for disparate impacts resulting from the model that border on unethical, unfair, and unjust decision-making. When the process of identifying and forecasting the purpose of the algorithm is achieved, a robust feedback loop will aid in the detection of bias, which leads to the next recommendation promoting regular audits.  Other self-regulatory best practices  Operators of algorithms should regularly audit for bias  The formal and regular auditing of algorithms to check for bias is another best practice for detecting and mitigating bias. On the importance of these audits, roundtable participant Jon Kleinberg from Cornell University shared that “[a]n algorithm has no choice but to be premeditated.” Audits prompt the review of both input data and output decisions, and when done by a third-party evaluator, they can provide insight into the algorithm’s behavior. While some audits may require technical expertise, this may not always be the case. Facial recognition software that misidentifies persons of color more than whites is an instance where a stakeholder or user can spot biased outcomes, without knowing anything about how the algorithm makes decisions. “We should expect computers to have an audit trail,” shared roundtable participant Miranda Bogen from Upturn. Developing a regular and thorough audit of the data collected for the algorithmic operation, along with responses from developers, civil society, and others impacted by the algorithm, will better detect and possibly deter biases.  “Developing a regular and thorough audit of the data collected for the algorithmic operation, along with responses from developers, civil society, and others impacted by the algorithm, will better detect and possibly deter biases.”  The experience of government officials in Allegheny County reflects the importance of third-party auditing. In 2016, the Department of Human Services launched a decision support tool, the Allegheny Family Screening Tool (AFST), to generate a score for which children are most likely to be removed from their homes within two years, or to be re-referred to the county’s child welfare office due to suspected abuse. The county took ownership of its use of the tool, worked collaboratively with the developer, and commissioned an independent evaluation of its direct and indirect effects on the maltreatment screening process, including decision accuracy, workload, and consistency. County officials also sought additional independent research from experts to determine if the software was discriminating against certain groups. In 2017, the findings did identify some statistical imbalances, with error rates higher across racial and ethnic groups. White children who were scored at the highest-risk of maltreatment were less likely to be removed from their homes compared to African-American children with similar risk scores. 45 The county responded to these findings as part of the rebuild of the tool, with version two implemented in November 2018. 46  Facebook recently completed a civil rights audit to determine its handling of issues and individuals from protected groups. 47 After the reveal of how the platform was handling a variety of issues, including voter suppression, content moderation, privacy, and diversity, the company has committed to an updated audit around its internal infrastructure to handle civil rights grievances and address diversity in its products’ designs by default. Recent actions by Facebook to ban white nationalist content or address disinformation campaigns are some of the results of these efforts. 48  Operators of algorithms must rely upon cross-functional work teams and expertise  Roundtable participants largely acknowledged the notion that organizations should employ cross-functional teams. But movement in this direction can be difficult in already-siloed organizations, despite the technical, societal, and possibly legal implications associated with the algorithm’s design and execution. Not all decisions will necessitate this type of cross-team review, but when these decisions carry risks of real harm, they should be employed. In the mitigation of bias and the management of the risks associated with the algorithm, collaborative work teams can compensate for the blind-spots often missed in smaller, segmented conversations and reviews. Bringing together experts from various departments, disciplines, and sectors will help facilitate accountability standards and strategies for mitigating online biases, including from engineering, legal, marketing, strategy, and communications.  Cross-functional work teams–whether internally driven or populated by external experts–can attempt to identify bias before and during the model’s rollout. Further, partnerships between the private sector, academics, and civil society organizations can also facilitate greater transparency in AI’s application to a variety of scenarios, particularly those that impact protected classes or are disseminated in the public interest. Kate Crawford, AI researcher and founder of the AI Now Partnership, suggested that “closed loops are not open for algorithmic auditing, for review, or for public debate” because they generally exacerbate the problems that they are trying to solve. 49 Further on this point, roundtable participant Natasha Duarte from the Center for Democracy and Technology spoke to Allegheny’s challenge when she shared, “[C]ompanies should be more forthcoming with describing the limits of their tech, and government should know what questions to ask in their assessments,” which speaks to the importance of more collaboration in this area.  Increase human involvement in the design and monitoring of algorithms  Even with all the precautionary measures listed above, there is still some risk that algorithms will make biased decisions. People will continue to play a role in identifying and correcting biased outcomes long after an algorithm is developed, tested, and launched. While more data can inform automated decision-making, this process should complement rather than fully replace human judgement. Roundtable participant Alex Peysakhovich from Facebook shared, “[W]e don’t need to eliminate human moderators. We need to hire more and get them to focus on edge cases.” Such sentiment is growing increasingly important in this field as the comparative advantages of humans and algorithms become more distinguishable and the use of both improves the outcomes for online users.  People will continue to play a role in identifying and correcting biased outcomes long after an algorithm is developed, tested, and launched. (Credit: Gabrielle Lurie/Reuters)  However, privacy implications will arise when more humans are engaged in algorithm management, particularly if more sensitive information is involved in the model’s creation or in testing the algorithm’s predictions for bias. The timing of the roundtables, which also transpired around the adoption of the EU’s GDPR, spoke to the need for increased consumer privacy principles where users are empowered over what data they want to share with companies. As the U.S. currently debates the need for federal privacy legislation, access to and use of personal data may become even more difficult, potentially leaving algorithmic models prone to more bias. Because the values of creators and users of algorithms shift over time, humans must arbitrate conflicts between outcomes and stated goals. In addition to periodical audits, human involvement provides continuous feedback on the performance of bias mitigation efforts.  Other public policy recommendations  As indicated throughout the paper, policymakers play a critical role in identifying and mitigating biases, while ensuring that the technologies continue to make positive economic and societal benefits.  Congress should implement regulatory sandboxes and safe harbors to curb online biases  Regulatory sandboxes are perceived as one strategy for the creation of temporary reprieves from regulation to allow the technology and rules surrounding its use to evolve together. These policies could apply to algorithmic bias and other areas where the technology in question has no analog covered by existing regulations. Rather than broaden the scope of existing regulations or create rules in anticipation of potential harms, a sandbox allows for innovation both in technology and its regulation. Even in a highly regulated industry, the creation of sandboxes where innovations can be tested alongside with lighter touch regulations can yield benefits.  “Rather than broaden the scope of existing regulations or create rules in anticipation of potential harms, a sandbox allows for innovation both in technology and its regulation.”  For example, companies within the financial sector that are leveraging technology, or fintech, have shown how regulatory sandboxes can spur innovation in the development of new products and services. 50 These companies make extensive use of algorithms for everything from spotting fraud to deciding to extend credit. Some of these activities mirror those of regular banks, and those would still fall under existing rules, but new ways of approaching tasks would be allowed within the sandbox. 51 Because sandboxes give innovators greater leeway in developing new products and services, they will require active oversight until technology and regulations mature. The U.S. Treasury recently reported not only on the benefits that countries that have adopted fintech regulatory sandboxes have realized, but recommended that the U.S. adopt fintech sandboxes to spur innovation. 52 Given the broad usefulness of algorithms to spur innovation in various regulated industries, participants in the roundtables considered the potential usefulness of extending regulatory sandboxes to other areas where algorithms can help to spur innovations.  Regulatory safe harbors could also be employed, where a regulator could specify which activities do not violate existing regulations. 53 This approach has the advantage of increasing regulatory certainty for algorithm developers and operators. For example, Section 230 of the Communications Decency Act removed liability from websites for the actions of their users, a provision widely credited with the growth of internet companies like Facebook and Google. The exemption later narrowed to exclude sex trafficking with the passage of the Stop Enabling Online Sex Trafficking Act and Fight Online Sex Trafficking Act. Applying a similar approach to algorithms could exempt their operators from liabilities in certain contexts while still upholding protections in others where harms are easier to identify. In line with the previous discussion on the use of certain protected attributes, safe harbors could be considered in instances where the collection of sensitive personal information is used for the specific purposes of bias detection and mitigation.  Consumers need better algorithmic literacy  Widespread algorithmic literacy is crucial for mitigating bias. Given the increased use of algorithms in many aspects of daily life, all potential subjects of automated decisions would benefit from knowledge of how these systems function. Just as computer literacy is now considered a vital skill in the modern economy, understanding how algorithms use their data may soon become necessary.  The subjects of automated decisions deserve to know when bias negatively affects them, and how to respond when it occurs. Feedback from users can share and anticipate areas where bias can manifest in existing and future algorithms. Over time, the creators of algorithms may actively solicit feedback from a wide range of data subjects and then take steps to educate the public on how algorithms work to aid in this effort. Public agencies that regulate bias can also work to raise algorithmic literacy as part of their missions. In both the public and private sector, those that stand to lose the most from biased decision-making can also play an active role in spotting it.  Conclusion  In December 2018, President Trump signed the First Step Act, new criminal justice legislation that encourages the usage of algorithms nationwide. 54 In particular, the system would use an algorithm to initially determine who can redeem earned-time credits—reductions in sentence for completion of educational, vocational, or rehabilitative programs—excluding inmates deemed higher risk. There is a likelihood that these algorithms will perpetuate racial and class disparities, which are already embedded in the criminal justice system. As a result, African-Americans and poor people in general will be more likely to serve longer prison sentences.  “When algorithms are responsibly designed, they may avoid the unfortunate consequences of amplified systemic discrimination and unethical applications.”  As outlined in the paper, these types of algorithms should be concerning if there is not a process in place that incorporates technical diligence, fairness, and equity from design to execution. That is, when algorithms are responsibly designed, they may avoid the unfortunate consequences of amplified systemic discrimination and unethical applications.  Some decisions will be best served by algorithms and other AI tools, while others may need thoughtful consideration before computer models are designed. Further, testing and review of certain algorithms will also identify, and, at best, mitigate discriminatory outcomes. For operators of algorithms seeking to reduce the risk and complications of bad outcomes for consumers, the promotion and use of the mitigation proposals can create a pathway toward algorithmic fairness, even if equity is never fully realized.   The Brookings Institution is a nonprofit organization devoted to independent research and policy solutions. Its mission is to conduct high-quality, independent research and, based on that research, to provide innovative, practical recommendations for policymakers and the public. The conclusions and recommendations of any Brookings publication are solely those of its author(s), and do not reflect the views of the Institution, its management, or its other scholars.  Amazon, Facebook, Google, IBM, and Microsoft provide general, unrestricted support to The Brookings Institution. Paul Resnick is also a consultant to Facebook, but this work is independent and his views expressed here are his own. The findings, interpretations, and conclusions posted in this piece are not influenced by any donation. Brookings recognizes that the value it provides is in its absolute commitment to quality, independence, and impact. Activities supported by its donors reflect this commitment.   Appendix: List of Roundtable Participants     Participant  Organization      Wendy Anderson  Office of Congresswoman Val Demings    Norberto Andrade  Facebook    Solon Barocas  Cornell University    Genie Barton  Privacy Genie    Ricardo Baeza-Yates  NTENT    Miranda Bogen  Upturn    John Brescia  Better Business Bureau    Julie Brill  Microsoft    Rich Caruana  Microsoft Research    Eli Cohen  Brookings Institution    Anupam Datta  Carnegie Mellon    Deven Desai  Georgia Tech    Natasha Duarte  Center for Democracy and Technology    Nadia Fawaz  LinkedIn    Laura Fragomeni  Walmart Global eCommerce    Sharad Goel  Stanford University    Scott Golder  Cornell University    Aaron Halfaker  Wikimedia    Sarah Holland  Google    Jack Karsten  Brookings Institution    Krishnaram Kenthapadi  LinkedIn and Stanford University    Jon Kleinberg  Cornell University    Isabel Kloumann  Facebook    Jake Metcalf  Ethical Resolve    Alex Peysakhovich  Facebook    Paul Resnick  University of Michigan    William Rinehart  American Action Forum    Alex Rosenblat  Data and Society    Jake Schneider  Brookings Institution    Jasjeet Sekhon  University of California-Berkeley    Rob Sherman  Facebook    JoAnn Stonier  Mastercard Worldwide    Nicol Turner Lee  Brookings Institution    Lucy Vasserman  Jigsaw’s Conversation AI Project / Google    Suresh Venkatasubramanian  University of Utah    John Verdi  Future of Privacy Forum    Heather West  Mozilla    Jason Yosinki  Uber    Jinyan Zang  Harvard University    Leila Zia  Wikimedia Foundation      References  Angwin, Julia, and Terry Parris Jr. “Facebook Lets Advertisers Exclude Users by Race.” Text/html. ProPublica, October 28, 2016. https://www.propublica.org/article/facebook-lets-advertisers-exclude-users-by-race.  Angwin, Julia, Jeff Larson, Surya Mattu, and Laura Kirchner. “Machine Bias.” ProPublica, May 23, 2016. Available at https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing (last accessed April 19, 2019).  Barocas, Solon, and Andrew D. Selbst, “Big Data’s Disparate Impact,” SSRN Scholarly Paper (Rochester, NY: Social Science Research Network, 2016. Available at https://papers.ssrn.com/abstract=2477899.  Blass, Andrea, and Yuri Gurevich. Algorithms: A Quest for Absolute Definitions. Bulletin of European Association for Theoretical Computer Science 81, 2003. https://www.microsoft.com/en-us/research/wp-content/uploads/2017/01/164.pdf (last accessed April 12, 2019).  Brennan, Tim, William Dieterich, and Beate Ehret. “Evaluating the Predictive Validity of the COMPAS Risk and Needs Assessment System.” Criminal Justice and Behavior 36 (2009): 21–40.  Chessell, Mandy. “Ethics for Big Data and Analytics.” IBM, n.d. Available at https://www.ibmbigdatahub.com/sites/default/files/whitepapers_reports_file/TCG%20Study%20Report%20-%20Ethics%20for%20BD%26A.pdf (last accessed April 19, 2019).  Chodosh, Sara. “Courts use algorithms to help determine sentencing, but random people get the same results.” Popular Science, January 18, 2018. Available at https://www.popsci.com/recidivism-algorithm-random-bias (last accessed October 15, 2018).  Corbett-Davies, Sam, Emma Peirson, Avi Feller, and Sharad Goel. “A Computer Program Used for Bail and Sentencing Decisions Was Labeled Biased against Blacks. It’s Actually Not That Clear.” Washington Post (blog), October 17, 2016. Available at https://www.washingtonpost.com/news/monkey-cage/wp/2016/10/17/can-an-algorithm-be-racist-our-analysis-is-more-cautious-than-propublicas/ (last accessed April 19. 2019).  Corbett-Davies, Sam, Emma Pierson, Avi Feller, Sharad Goel, and Aziz Huq. “Algorithmic Decision Making and the Cost of Fairness.” ArXiv:1701.08230 [Cs, Stat], January 27, 2017. https://doi.org/10.1145/3097983.309809.  Courtland, Rachel. “Bias Detectives: The Researchers Striving to Make Algorithms Fair,” Nature 558, no. 7710 (June 2018): 357–60. Available at https://doi.org/10.1038/d41586-018-05469-3 (last accessed April 19, 2019).  DeAngelius, Stephen F. “Artificial intelligence: How algorithms make systems smart,” Wired Magazine, September 2014. Available at https://www.wired.com//insights/2014/09/artificial-intelligence-algorithms-2/ (last accessed April 12, 2019).  Elejalde-Ruiz, Alexia. “The end of the resume? Hiring is in the midst of technological revolution with algorithms, chatbots.” Chicago Tribune (July 19, 2018). Available at http://www.chicagotribune.com/business/ct-biz-artificial-intelligence-hiring-20180719-story.html.  Eubanks, Virginia. “A Child Abuse Prediction Model Fails Poor Families,” Wired, January 15, 2018. Available at https://www.wired.com/story/excerpt-from-automating-inequality/ (last accessed April 19, 2019).  FTC Hearing #7: The Competition and Consumer Protection Issues of Algorithms, Artificial Intelligence, and Predictive Analytics, § Federal Trade Commission (2018). https://www.ftc.gov/system/files/documents/public_events/1418693/ftc_hearings_session_7_transcript_day_2_11-14-18.pdf.  Garbade, Michael J. “Clearing the Confusion: AI vs. Machine Learning vs. Deep Learning Differences,” Towards Data Science, September 14, 2018. Available at https://towardsdatascience//clearing-the-confusion-ai-vs-machine-learning-vs-deep-learning-differences-fce69b21d5eb (last accessed April 12, 2019).  Griggs v. Duke Power Company, Oyez. Available at https//www.oyez.org/cases/1970/124 (last accessed October 1, 2018.  Guerin, Lisa. “Disparate Impact Discrimination.” www.nolo.com. Available at https://www.nolo.com/legal-encyclopedia/disparate-impact-discrimination.htm (last accessed April 24, 2019).  Hadhazy, Adam. “Biased Bots: Artificial-Intelligence Systems Echo Human Prejudices.” Princeton University, April 18, 2017. Available at https://www.princeton.edu/news/2017/04/18/biased-bots-artificial-intelligence-systems-echo-human-prejudices (last accessed April 20, 2019).  Hamilton, Isobel Asher. “Why It’s Totally Unsurprising That Amazon’s Recruitment AI Was Biased against Women.” Business Insider, October 13, 2018. Available at https://www.businessinsider.com/amazon-ai-biased-against-women-no-surprise-sandra-wachter-2018-10 (last accessed April 20, 2019).  Hardesty, Larry. “Study Finds Gender and Skin-Type Bias in Commercial Artificial-Intelligence Systems.” MIT News, February 11, 2018. Available at http://news.mit.edu/2018/study-finds-gender-skin-type-bias-artificial-intelligence-systems-0212 (last accessed April 19, 2019).  High-level Expert Group on Artificial Intelligence. “Ethics Guidelines for Trustworthy AI (Draft).” The European Commission, December 18, 2018.  Ingold, David, and Spencer Soper. “Amazon Doesn’t Consider the Race of Its Customers. Should It?” Bloomberg.com, April 21, 2016. http://www.bloomberg.com/graphics/2016-amazon-same-day/ .  Kearns, Michael. “Data Intimacy, Machine Learning and Consumer Privacy.” University of Pennsylvania Law School, May 2018. Available at https://www.law.upenn.edu/live/files/7952-kearns-finalpdf (last accessed April 12, 2019).  Kleinberg, Jon, Sendhil Mullainathan, and Manish Raghavan, “Inherent Trade-Offs in the Fair Determination of Risk Scores.”In Proceedings of Innovations in Theoretical Computer Science (ITCS), 2017. Available at https://arxiv.org/pdf/1609.05807.pdf (last accessed April 19, 2019).  Larson, Jeff, Surya Mattu, and Julia Angwin. “Unintended Consequences of Geographic Targeting.” Technology Science, September 1, 2015. Available at https://techscience.org/a/2015090103/ (last accessed April 19, 2019).  Locklear, Mallory. “Facebook Releases an Update on Its Civil Rights Audit.” Engadget (blog), December 18, 2018. Available at https://www.engadget.com/2018/12/18/facebook-update-civil-rights-audit/ (last accessed April 19, 2019).  Lopez, German. “The First Step Act, Congress’s Criminal Justice Reform Bill, Explained.” Vox, December 3, 2018. Available at https://www.vox.com/future-perfect/2018/12/3/18122392/first-step-act-criminal-justice-reform-bill-congress (last accessed April 16, 2019).  Mnuchin, Steven T., and Craig S. Phillips. “A Financial System That Creates Economic Opportunities – Nonbank Financials, Fintech, and Innovation.” Washington, D.C.: U.S. Department of the Treasury, July 2018.Available at https://home.treasury.gov/sites/default/files/2018-08/A-Financial-System-that-Creates-Economic-Opportunities—Nonbank-Financials-Fintech-and-Innovation_0.pdf (last accessed April 19, 2019).  Reisman, Dillon, Jason Schultz, Kate Crawford, and Meredith Whittaker. “Algorithmic Impact Assessments: A Practical Framework for Public Agency Accountability.” New York: AI Now, April 2018.  Romei, Andrea, and Salvatore Ruggieri. “Discrimination Data Analysis: A Multi-Disciplinary Bibliography.” In Discrimination and Privacy in the Information Society, edited by Bart Custers, T Calders, B Schermer, and T Zarsky, 109–35. Studies in Applied Philosophy, Epistemology and Rational Ethics. Springer, Berlin, Heidelberg, 2013. Available at https://doi.org/10.1007/978-3-642-30487-3_6 (last accessed April 19, 2019).  Schatz, Brian. AI in Government Act of 2018, Pub. L. No. S.B. 3502 (2018). https://www.congress.gov/bill/115th-congress/senate-bill/3502.  Spielkamp, Matthias. “We Need to Shine More Light on Algorithms so They Can Help Reduce Bias, Not Perpetuate It.” MIT Technology Review. Accessed September 20, 2018. Available at https://www.technologyreview.com/s/607955/inspecting-algorithms-for-bias/ (last accessed April 19, 2019).  Stack, Liam. “Facebook Announces New Policy to Ban White Nationalist Content.” The New York Times, March 28, 2019, sec. Business. Available at https://www.nytimes.com/2019/03/27/business/facebook-white-nationalist-supremacist.html (last accessed April 19, 2019).  Sweeney, Latanya, and Jinyan Zang. “How appropriate might big data analytics decisions be when placing ads?” Powerpoint presentation presented at the Big Data: A tool for inclusion or exclusion, Federal Trade Commission conference, Washington, DC. September 15, 2014. Available at https://www.ftc.gov/systems/files/documents/public_events/313371/bigdata-slides-sweeneyzang-9_15_14.pdf (last accessed April 12, 2019).  Sweeney, Latanya. “Discrimination in online ad delivery.” Rochester, NY: Social Science Research Network, January 28, 2013. Available at https://papers.ssrn.com/abstract=2208240 (last accessed April 12, 2019).  Sydell, Laura. “It Ain’t Me, Babe: Researchers Find Flaws In Police Facial Recognition Technology.” NPR.org, October 25, 2016. Available at https://www.npr.org/sections/alltechconsidered/2016/10/25/499176469/it-aint-me-babe-researchers-find-flaws-in-police-facial-recognition (last accessed April 19, 2019).  “The Global Data Ethics Project.” Data for Democracy, n.d. https://www.datafordemocracy.org/project/global-data-ethics-project (last accessed April 19, 2019).  Tobin, Ariana. “HUD sues Facebook over housing discrimination and says the company’s algorithms have made the problem worse.” ProPublica (March 28, 2019). Available at https://www.propublica.org/article/hud-sues-facebook-housing-discrimination-advertising-algorithms (last accessed April 29, 2019).  Turner Lee, Nicol. “Inclusion in Tech: How Diversity Benefits All Americans,” § Subcommittee on Consumer Protection and Commerce, United States House Committee on Energy and Commerce (2019). Also available on Brookings web site, https://www.brookings.edu/testimonies/inclusion-in-tech-how-diversity-benefits-all-americans/ (last accessed April 29, 2019).  Turner Lee, Nicol. Detecting racial bias in algorithms and machine learning. Journal of Information, Communication and Ethics in Society 2018, Vol. 16 Issue 3, pp. 252-260. Available at https://doi.org/10.1108/JICES-06-2018-0056/ (last accessed April 29, 2019).  “Understanding bias in algorithmic design,” Impact.Engineered, September 5, 2017. Available at https://medium.com/impact-engineered/understanding-bias-in-algorithmic-design-db9847103b6e (last accessed April 12, 2019).  Vincent, James. “Amazon Reportedly Scraps Internal AI Recruiting Tool That Was Biased against Women.” The Verge, October 10, 2018. Available at https://www.theverge.com/2018/10/10/17958784/ai-recruiting-tool-bias-amazon-report (last accessed April 20, 2019).  Zafar, Muhammad Bilal, Isabel Valera Martinez, Manuel Gomez Rodriguez, and Krishna Gummadi. “Fairness Constraints: A Mechanism for Fair Classification.” In Proceedings of the 20th International Conference on Artificial Intelligence and Statistics (AISTATS). Fort Lauderdale, FL, 2017.  Zarsky, Tal. “Understanding Discrimination in the Scored Society.” SSRN Scholarly Paper. Rochester, NY: Social Science Research Network, January 15, 2015. https://papers.ssrn.com/abstract=2550248.     Report Produced by Center for Technology Innovation        Footnotes   Nicol Turner Lee, Fellow, Center for Technology Innovation, Brookings Institution; Paul Resnick, Michael D. Cohen Collegiate Professor of Information, Associate Dean for Research and Faculty Affairs, Professor of Information and Interim Director of Health Informatics, School of Information at the University of Michigan; Genie Barton, President, Institute for Marketplace Trust, Better Business Bureau and Member, Research Advisory Board, International Association of Privacy Professionals. The authors also acknowledge the input from the current leadership of the Better Business Bureau’s Institute for Marketplace Trust and Jinyan Zang, Harvard University.  The concepts of AI, algorithms and machine learning are often conflated and used interchangeably. In this paper, we will follow generally understood definitions of these terms as set out in publications for the general reader. See, e.g., Stephen F. DeAngelius. “Artificial intelligence: How algorithms make systems smart,” Wired Magazine, September 2014. Available at https://www.wired.com//insights/2014/09/artificial-intelligence-algorithms-2/ (last accessed April 12, 2019). See also, Michael J. Garbade. “Clearing the Confusion: AI vs. Machine Learning vs. Deep Learning Differences,” Towards Data Science, September 14, 2018. Available at https://towardsdatascience//clearing-the-confusion-ai-vs-machine-learning-vs-deep-learning-differences-fce69b21d5eb (last accessed April 12, 2019).  Andrea Blass and Yuri Gurevich. Algorithms: A Quest for Absolute Definitions. Bulletin of European Association for Theoretical Computer Science 81, 2003. https://www.microsoft.com/en-us/research/wp-content/uploads/2017/01/164.pdf (last accessed April 12, 2019).  Kearns, Michael. “Data Intimacy, Machine Learning and Consumer Privacy.” University of Pennsylvania Law School, May 2018. Available at https://www.law.upenn.edu/live/files/7952-kearns-finalpdf (last accessed April 12, 2019).  Technically, this describes what is called “supervised machine learning.”  Chodosh, Sara. “Courts use algorithms to help determine sentencing, but random people get the same results.” Popular Science, January 18, 2018. Available at https://www.popsci.com/recidivism-algorithm-random-bias (last accessed October 15, 2018).  Blog. “Understanding bias in algorithmic design,” Impact.Engineered, September 5, 2017. Available at https://medium.com/impact-engineered/understanding-bias-in-algorithmic-design-db9847103b6e (last accessed April 12, 2019). This definition is intended to include the concepts of disparate treatment and disparate impact, but the legal definitions were not designed with AI in mind. For example, the demonstration of disparate treatment does not describe the ways in which an algorithm can learn to treat similarly situated groups differently, as will be discussed later in the paper.  The recommendations offered in the paper are those of the authors and do not represent the views or a consensus of views among roundtable participants.  Hamilton, Isobel Asher. “Why It’s Totally Unsurprising That Amazon’s Recruitment AI Was Biased against Women.” Business Insider, October 13, 2018. Available at https://www.businessinsider.com/amazon-ai-biased-against-women-no-surprise-sandra-wachter-2018-10 (last accessed April 20, 2019).  Vincent, James. “Amazon Reportedly Scraps Internal AI Recruiting Tool That Was Biased against Women.” The Verge, October 10, 2018. Available at https://www.theverge.com/2018/10/10/17958784/ai-recruiting-tool-bias-amazon-report (last accessed April 20, 2019). Although Amazon scrubbed the data of the particular references that appeared to discriminate against female candidates, there was no guarantee that the algorithm could not find other ways to sort and rank male candidates higher so it was scrapped by the company.  Hadhazy, Adam. “Biased Bots: Artificial-Intelligence Systems Echo Human Prejudices.” Princeton University, April 18, 2017. Available at https://www.princeton.edu/news/2017/04/18/biased-bots-artificial-intelligence-systems-echo-human-prejudices (last accessed April 20, 2019).  Sweeney, Latanya. “Discrimination in online ad delivery.” Rochester, NY: Social Science Research Network, January 28, 2013. Available at https://papers.ssrn.com/abstract=2208240 (last accessed April 12, 2019).  Sweeney, Latanya and Jinyan Zang. “How appropriate might big data analytics decisions be when placing ads?” Powerpoint presentation presented at the Big Data: A tool for inclusion or exclusion, Federal Trade Commission conference, Washington, DC. September 15, 2014. Available at https://www.ftc.gov/systems/files/documents/public_events/313371/bigdata-slides-sweeneyzang-9_15_14.pdf (last accessed April 12, 2019).  “FTC Hearing #7: The Competition and Consumer Protection Issues of Algorithms, Artificial Intelligence, and Predictive Analytics,” § Federal Trade Commission (2018),  Hardesty, Larry. “Study Finds Gender and Skin-Type Bias in Commercial Artificial-Intelligence Systems.” MIT News, February 11, 2018. Available at http://news.mit.edu/2018/study-finds-gender-skin-type-bias-artificial-intelligence-systems-0212 (last accessed April 19, 2019). These companies were selected because they provided gender classification features in their software and the code was publicly available for testing.  Ibid.  COMPAS is a risk-and needs-assessment tool originally designed by Northpointe, Inc., to assist state corrections officials in making placement, management, and treatment decisions for offenders. Angwin, Julia, Jeff Larson, Surya Mattu, and Laura Kirchner. “Machine Bias.” ProPublica, May 23, 2016. Available at https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing (last accessed April 19, 2019). See also , Brennan, Tim, William Dieterich, and Beate Ehret. “Evaluating the Predictive Validity of the COMPAS Risk and Needs Assessment System.” Criminal Justice and Behavior 36 (2009): 21–40.  Corbett-Davies, Sam, Emma Pierson, Avi Feller, Sharad Goel, and Aziz Huq. “Algorithmic Decision Making and the Cost of Fairness.” ArXiv:1701.08230 [Cs, Stat], January 27, 2017. https://doi.org/10.1145/3097983.309809.  Solon Barocas and Andrew D. Selbst, “Big Data’s Disparate Impact,” SSRN Scholarly Paper (Rochester, NY: Social Science Research Network, 2016), https://papers.ssrn.com/abstract=2477899.  Turner Lee, Nicol. “Inclusion in Tech: How Diversity Benefits All Americans,” § Subcommittee on Consumer Protection and Commerce, United States House Committee on Energy and Commerce (2019). Also available on Brookings web site, https://www.brookings.edu/testimonies/inclusion-in-tech-how-diversity-benefits-all-americans/ (last accessed April 29, 2019).  Ibid. See also, Turner Lee, Nicol. Detecting racial bias in algorithms and machine learning. Journal of Information, Communication and Ethics in Society 2018, Vol. 16 Issue 3, pp. 252-260. Available at https://doi.org/10.1108/JICES-06-2018-0056/ (last accessed April 29, 2019).  Sydell, Laura. “It Ain’t Me, Babe: Researchers Find Flaws In Police Facial Recognition Technology.” NPR.org, October 25, 2016. Available at https://www.npr.org/sections/alltechconsidered/2016/10/25/499176469/it-aint-me-babe-researchers-find-flaws-in-police-facial-recognition (last accessed April 19, 2019).  Guerin, Lisa. “Disparate Impact Discrimination.” www.nolo.com. Available at https://www.nolo.com/legal-encyclopedia/disparate-impact-discrimination.htm (last accessed April 24, 2019). See also, Jewel v. NSA where the Electronic Frontier Foundation argues that massive (or dragnet) surveillance is illegal. Information about case available at https://www.eff.org/cases/jewel (last accessed April 19, 2019).  This is often called an anti-classification criterion that the algorithm cannot classify based on membership in the protected or sensitive classes.  Zarsky, Tal. “Understanding Discrimination in the Scored Society.” SSRN Scholarly Paper. Rochester, NY: Social Science Research Network, January 15, 2015. https://papers.ssrn.com/abstract=2550248 .  Larson, Jeff, Surya Mattu, and Julia Angwin. “Unintended Consequences of Geographic Targeting.” Technology Science, September 1, 2015. Available at https://techscience.org/a/2015090103/ (last accessed April 19, 2019).  Terry Parris Jr Julia Angwin, “Facebook Lets Advertisers Exclude Users by Race,” text/html, ProPublica, October 28, 2016. Available at https://www.propublica.org/article/facebook-lets-advertisers-exclude-users-by-race (last accessed April 19, 2019).  Amazon doesn’t consider the race of its customers. Should It? Bloomberg.com. Available at http//www.bloomberg.com/graphics/2016-amazon-same-day (last accessed April 19, 2019).  Corbett-Davies et al., “Algorithmic Decision Making and the Cost of Fairness.”  Solon Barocas and Andrew D. Selbst, “Big Data’s Disparate Impact,” SSRN Scholarly Paper (Rochester, NY: Social Science Research Network, 2016. Available at https://papers.ssrn.com/abstract=2477899.  See, Zafar, Muhammad Bilal, Isabel Valera Martinez, Manuel Gomez Rodriguez, and Krishna Gummadi. “Fairness Constraints: A Mechanism for Fair Classification.” In Proceedings of the 20th International Conference on Artificial Intelligence and Statistics (AISTATS). Fort Lauderdale, FL, 2017. See also , Spielkamp, Matthias. “We Need to Shine More Light on Algorithms so They Can Help Reduce Bias, Not Perpetuate It.” MIT Technology Review. Accessed September 20, 2018. Available at https://www.technologyreview.com/s/607955/inspecting-algorithms-for-bias/ (last accessed April 19, 2019). See also Corbett-Davies, Sam, Emma Peirson, Avi Feller, and Sharad Goel. “A Computer Program Used for Bail and Sentencing Decisions Was Labeled Biased against Blacks. It’s Actually Not That Clear.” Washington Post (blog), October 17, 2016. Available at https://www.washingtonpost.com/news/monkey-cage/wp/2016/10/17/can-an-algorithm-be-racist-our-analysis-is-more-cautious-than-propublicas/ (last accessed April 19. 2019).  Jon Kleinberg, Sendhil Mullainathan, and Manish Raghavan, “Inherent Trade-Offs in the Fair Determination of Risk Scores.”In Proceedings of Innovations in Theoretical Computer Science (ITCS), 2017. Available at https://arxiv.org/pdf/1609.05807.pdf (last accessed April 19, 2019).  This notion of disparate impact has been legally tested dating back to the 1971 U.S. Supreme Court decision, Griggs v. Duke Power Company where the defendant was found to be using intelligence test scores and high school diplomas as factors to hire more white applicants over people of color. As determined by the court decision, there was no correlation between the tests or educational requirements for the jobs in question. See, Griggs v. Duke Power Company, Oyez. Available at https//www.oyez.org/cases/1970/124 (last accessed October 1, 2018.  Various computer models are being created to combat the discriminatory effects of algorithmic bias. See , Romei, Andrea, and Salvatore Ruggieri. “Discrimination Data Analysis: A Multi-Disciplinary Bibliography.” In Discrimination and Privacy in the Information Society, edited by Bart Custers, T Calders, B Schermer, and T Zarsky, 109–35. Studies in Applied Philosophy, Epistemology and Rational Ethics. Springer, Berlin, Heidelberg, 2013. Available at https://doi.org/10.1007/978-3-642-30487-3_6 (last accessed April 19, 2019).  Corbett-Davies, Sam, Emma Pierson, Avi Feller, Sharad Goel, and Aziz Huq. “Algorithmic Decision Making and the Cost of Fairness.” ArXiv:1701.08230 [Cs, Stat], January 27, 2017. Available at https://doi.org/10.1145/3097983.309809 (last accessed April 19, 2019).  Ibid.  Schatz, Brian. AI in Government Act of 2018, Pub. L. No. S.B. 3502 (2018). https://www.congress.gov/bill/115th-congress/senate-bill/3502.  At its February meeting, the OECD announced that it had approved its expert group’s guidelines and hoped to (C.  See European Union, Digital Single Market, Ethics Guidelines for Trustworthy AI, available for download from https://ec.europa.eu/digital-single-market/en/news/ethics-guidelines-trustworthy-ai (last accessed April 19, 2019).  See High-level Expert Group on Artificial Intelligence. “Ethics Guidelines for Trustworthy AI (Draft).” The European Commission, December 18, 2018. See also , Chessell, Mandy. “Ethics for Big Data and Analytics.” IBM, n.d. Available at https://www.ibmbigdatahub.com/sites/default/files/whitepapers_reports_file/TCG%20Study%20Report%20-%20Ethics%20for%20BD%26A.pdf (last accessed April 19, 2019). https://ec.europa.eu/futurium/en/system/files/ged/ai_hleg_draft_ethics_guidelines_18_december.pdf. See also “The Global Data Ethics Project.” Data for Democracy, n.d. https://www.datafordemocracy.org/project/global-data-ethics-project (last accessed April 19, 2019).  Spielkamp, Matthias. “We need to shine more light on algorithms so they can help reduce bias, not perpetuate It.” MIT Technology Review. Available at https://www.technologyreview.com/s/607955/inspecting-algorithms-for-bias/ (last accessed September 20, 2018).  Tobin, Ariana. “HUD sues Facebook over housing discrimination and says the company’s algorithms have made the problem worse.” ProPublica (March 28, 2019). Available at https://www.propublica.org/article/hud-sues-facebook-housing-discrimination-advertising-algorithms (last accessed April 29, 2019).  Elejalde-Ruiz, Alexia. “The end of the resume? Hiring is in the midst of technological revolution with algorithms, chatbots.” Chicago Tribune (July 19, 2018). Available at http://www.chicagotribune.com/business/ct-biz-artificial-intelligence-hiring-20180719-story.html .  Reisman, Dillon, Jason Schultz, Kate Crawford, and Meredith Whittaker. “Algorithmic Impact Assessments: A Practical Framework for Public Agency Accountability.” New York: AI Now, April 2018.  Alexandra Chouldechova et al., “A Case Study of Algorithm-Assisted Decision Making in Child Maltreatment Hotline Screening Decisions,” 1st Conference on Fairness, Accountability and Transparency , n.d., 15.  Rhema Vaithianathan et al., “Section 7: Allegheny Family Screening Tool: Methodology, Version 2,” April 2019.  Locklear, Mallory. “Facebook Releases an Update on Its Civil Rights Audit.” Engadget (blog), December 18, 2018. Available at https://www.engadget.com/2018/12/18/facebook-update-civil-rights-audit/ (last accessed April 19, 2019).  Stack, Liam. “Facebook Announces New Policy to Ban White Nationalist Content.” The New York Times, March 28, 2019, sec. Business. Available at https://www.nytimes.com/2019/03/27/business/facebook-white-nationalist-supremacist.html (last accessed April 19, 2019).  Qtd. in Rachel Courtland, “Bias Detectives: The Researchers Striving to Make Algorithms Fair,” Nature 558, no. 7710 (June 2018): 357–60. Available at https://doi.org/10.1038/d41586-018-05469-3 (last accessed April 19, 2019).  Fintech regulatory sandboxes in UK , Singapore , and states in the U.S. are beginning to authorize them. They allow freedom to offer new financial products and use new technologies such as blockchain .  In March, the state of Arizona became the first U.S. state to create a “regulatory sandbox” for fintech companies , allowing them to test financial products on customers with lighter regulations. The U.K. has run a similar initiative called Project Innovate since 2014. The application of a sandbox can allow both startup companies and incumbent banks to experiment with more innovative products without worrying about how to reconcile them with existing rules.  Mnuchin, Steven T., and Craig S. Phillips. “A Financial System That Creates Economic Opportunities - Nonbank Financials, Fintech, and Innovation.” Washington, D.C.: U.S. Department of the Treasury, July 2018.Available at https://home.treasury.gov/sites/default/files/2018-08/A-Financial-System-that-Creates-Economic-Opportunities---Nonbank-Financials-Fintech-and-Innovation_0.pdf (last accessed April 19, 2019).  Another major tech-related Safe Harbor is the EU-US Privacy Shield after the previous Safe Harbor was declared invalid in the EU. Available at https://en.wikipedia.org/wiki/EU%E2%80%93US_Privacy_Shield (last accessed April 19. 2019).  Lopez, German. “The First Step Act, Congress’s Criminal Justice Reform Bill, Explained.” Vox, December 3, 2018. Available at https://www.vox.com/future-perfect/2018/12/3/18122392/first-step-act-criminal-justice-reform-bill-congress (last accessed April 16, 2019).       Related Topics   Technology & Innovation  Telecommunications & Internet                 Find us on Facebook        Find us on Twitter        Find us on YouTube        Listen to our Podcast        Browse Newsletters        Subscribe to our RSS       Languages  Español  中文  عربي      About Us  Research Programs  Find an Expert  Careers  Contact  Terms and Conditions  Brookings Privacy Policy  Copyright 2020 The Brookings Institution               Trending     U.S. Politics & Government      Topics     AI    Policy 2020    Cities & Regions    Global Dev    Intl Affairs    U.S. Economy    U.S. Politics & Govt    More      About Us    Press Room    Experts    Events    The Brookings Press    WashU at Brookings    Careers    Support Brookings     Cart  0       Get daily updates from Brookings           Enter Email                     Send to Email Address   Your Name   Your Email Address        Cancel  Post was not sent - check your email addresses!  Email check failed, please try again  Sorry, your blog cannot share posts by email.                      "
11,affect influenced(fallacy OR bias OR heuristic) (coronavirus OR covid OR pandemic),1,https://link.springer.com/article/10.1007/s40258-020-00595-4,"The outbreak of 2019 coronavirus disease (COVID-19) has become a public health emergency of international concern [1]. The number of COVID-infected individuals and related deaths continues to rise rapidly. COVID-19 is a serious threat to global health and the world economy and has caused widespread concern around the world. In the absence of approved treatments for and vaccines against COVID-19, preventive strategies and hygiene behaviors such as social distancing and stay-at-home policies, avoiding touching the face, and repeated hand washing are effective options in the fight against COVID-19 [2, 3]. During this pandemic, encouraging people to adopt and sustain preventive behaviors is a central focus of public health policies that seek to mitigate the spread of COVID-19.

Behavioral economics has recently received a great deal of attention in public policy making [4]. This field of economics uses insights from the fields of psychology, neuroscience, and cognitive sciences to explain how people’s behaviors deviate from the rational choice theory and when and why people’s short-term decisions sometimes undermine their long-term interests. The focus of this field is on better predicting and understanding people’s behaviors and choices to help formulate more effective public policies [5, 6]. It identifies biases in the decision-making process and uses them as entry points for interventions to address particular behaviors.

Behavioral economics acknowledges that people do not have infinite rationality and willpower, so they are not the rational decision makers assumed in the standard economic theory of utility maximization [6, 7]. In addition, they have limited cognitive and computational abilities, and their decisions are not based on a complete analysis of all available information [6]. These limitations lead people to apply the rules of thumb or heuristics (i.e., mental shortcuts) to make their decisions rather than conducting cost–benefit analyses when making a decision. The heuristics are generally useful but can lead to systematic mistakes (i.e., biases) in decision making that, in turn, result in suboptimal and harmful behaviors [5, 8].

Behavioral economics has shed new light on a range of risky and preventive health behaviors [9]. It also has considerable potential for providing a valuable perspective to better understand and explain COVID-19-related behaviors. While multiple biases are identified in the field of behavioral economics, in this paper we focus on six that tend to be particularly relevant to COVID-19-related behaviors: present bias, status quo bias, framing effect, optimism bias, affect heuristic, and herding behavior. It may provide useful insights into public health policies designed to reduce the spread of COVID-19 and may be helpful in developing and implementing interventions.","           Advertisement                  Search         Log in              Search SpringerLink     Search                       Download PDF          Practical Application  Published: 21 May 2020   Using Insights from Behavioral Economics to Mitigate the Spread of COVID-19  Moslem Soofi 1 , Farid Najafi 2 & Behzad Karami-Matin 2    Applied Health Economics and Health Policy  volume 18 , pages 345 – 350 ( 2020 ) Cite this article        1560 Accesses    2 Altmetric    Metrics details          Abstract The outbreak of 2019 coronavirus disease (COVID-19) has become a public health emergency of international concern. The number of COVID-infected individuals and related deaths continues to rise rapidly. Encouraging people to adopt and sustain preventive behaviors is a central focus of public health policies that seek to mitigate the spread of COVID-19. Public health policy needs improved methods to encourage people to adhere to COVID-19-preventive behaviors. In this paper, we introduce a number of insights from behavioral economics that help explain why people may behave irrationally during the COVID-19 pandemic. In particular, present bias, status quo bias, framing effect, optimism bias, affect heuristic, and herding behavior are discussed. We hope this paper will shed light on how insights from behavioral economics can enrich public health policies and interventions in the fight against COVID-19.  FormalPara Key Points for Decision Makers Behavioral economics acknowledges that people are not the rational decision makers assumed in the standard economic theory of decision making. Finite rationality and willpower lead people to apply the rules of thumb or heuristics to make their COVID-19-related decisions rather than conducting cost-benefit analyses. Therefore, they may be biased in their COVID-19-related decisions. Behavioral economics can help policy makers identify individuals’ decision biases and use them as starting points for designing COVID-19-preventive interventions. Behavioral economics interventions can help people behave rationally and make better COVID-19-related decisions.  Introduction The outbreak of 2019 coronavirus disease (COVID-19) has become a public health emergency of international concern [ 1 ]. The number of COVID-infected individuals and related deaths continues to rise rapidly. COVID-19 is a serious threat to global health and the world economy and has caused widespread concern around the world. In the absence of approved treatments for and vaccines against COVID-19, preventive strategies and hygiene behaviors such as social distancing and stay-at-home policies, avoiding touching the face, and repeated hand washing are effective options in the fight against COVID-19 [ 2 , 3 ]. During this pandemic, encouraging people to adopt and sustain preventive behaviors is a central focus of public health policies that seek to mitigate the spread of COVID-19. Behavioral economics has recently received a great deal of attention in public policy making [ 4 ]. This field of economics uses insights from the fields of psychology, neuroscience, and cognitive sciences to explain how people’s behaviors deviate from the rational choice theory and when and why people’s short-term decisions sometimes undermine their long-term interests. The focus of this field is on better predicting and understanding people’s behaviors and choices to help formulate more effective public policies [ 5 , 6 ]. It identifies biases in the decision-making process and uses them as entry points for interventions to address particular behaviors. Behavioral economics acknowledges that people do not have infinite rationality and willpower, so they are not the rational decision makers assumed in the standard economic theory of utility maximization [ 6 , 7 ]. In addition, they have limited cognitive and computational abilities, and their decisions are not based on a complete analysis of all available information [ 6 ]. These limitations lead people to apply the rules of thumb or heuristics (i.e., mental shortcuts) to make their decisions rather than conducting cost–benefit analyses when making a decision. The heuristics are generally useful but can lead to systematic mistakes (i.e., biases) in decision making that, in turn, result in suboptimal and harmful behaviors [ 5 , 8 ]. Behavioral economics has shed new light on a range of risky and preventive health behaviors [ 9 ]. It also has considerable potential for providing a valuable perspective to better understand and explain COVID-19-related behaviors. While multiple biases are identified in the field of behavioral economics, in this paper we focus on six that tend to be particularly relevant to COVID-19-related behaviors: present bias, status quo bias, framing effect, optimism bias, affect heuristic, and herding behavior. It may provide useful insights into public health policies designed to reduce the spread of COVID-19 and may be helpful in developing and implementing interventions. Present Bias In the context of intertemporal choices, the costs and benefits of our choices occur at different points in time, that is, many daily choices are a trade-off between immediate outcomes (i.e., costs and benefit) and expected future outcomes [ 10 ]. Present bias or hyperbolic discounting is the nonlinear and nonconstant tendency of many individuals to prefer a smaller sooner pay-off over a larger future pay-off [ 11 , 12 , 13 ]. Present bias may lead to time-inconsistent preferences. An individual makes a plan for tomorrow, but once tomorrow comes they may experience a preference reversal and revise their plan. Present bias has been shown to be a significant predictor of a wide variety of health behaviors [ 14 ]. Many health behaviors involve a trade-off between immediate and future outcomes. For example, smoking has both current benefits (temporary stress relief) and future costs (increased risk of lung cancer) [ 10 ]. In the case of COVID-19, not adhering to stay-at-home policies involves a trade-off between the pleasure of going to the mall or restaurant now (current benefit) and the increased risk of contracting COVID-19 in the future (uncertain future cost). Uncertain future cost means that not every excursion outside the house would result in COVID-19 infection. Thus, myopic individuals (i.e., those with present bias), who put a greater emphasis on the here and now, are less likely to adhere to COVID-19-preventive behaviors, including staying at home and hand washing. Present bias is an explanation for why people do not behave in their own best interests and why they have difficulty adhering to preventive health behaviors such as social distancing, even when they wish to adhere [ 15 ]. Although present bias may lead to suboptimal behavioral choices, it can be used to help people adhere to COVID-19-preventive behaviors [ 12 ]. For example, reducing the current costs of adherence to social distancing may help people overcome their present bias, as even small costs could outweigh any perceived future benefits of adherence. Increasing the current benefit of adherence to social distancing, such as offering small and frequent payments now, can be useful in encouraging people to adhere to COVID-19-preventive behaviors. Such interventions involving low-cost rewards have been used as ways to increase current benefits of adherence to antiretroviral medication [ 16 ], smoking cessation [ 17 ], and weight loss [ 18 ], and they have been shown to be effective in changing behaviors. To reduce COVID-19 transmission, in the short term, providing free internet access at home, temporary suspension of loan repayments (e.g., loans provided by the government to support unemployed and uninsured people to start small businesses), and providing benefit packages for vulnerable groups should be considered in stay-at-home policies to encourage people to adhere to the policy and to increase its success rate. Status Quo Bias and Default Options Status quo bias is a disproportionate preference for the current status of options and an unwillingness to change them [ 19 , 20 ]. One reason for this is that people interpret the potential disadvantages of changing the status quo as greater than the potential benefits. This bias can be turned to the advantage of encouraging health-enhancing behaviors through the use of “nudges”. The concept of “nudge” was introduced in behavioral economics to persuade individuals to behave rationally and make better choices. Thaler and Sunstein [ 5 ] defined a nudge as “any aspect of the choice architecture that influences individuals’ decision making in a predictable way without forbidding any options or changing economic incentives.” They argued that, by improving and altering the environment in which individuals make decisions—what they call the “choice architecture”—, individuals can be influenced to make smarter choices. Choice architecture can be used to build an environment in which it is easier to make optimal health choices and more difficult to select suboptimal ones. The default option is a nudge with a powerful impact on directing the behaviors of people in ways that meet their long-term interests [ 5 ]. One of the most notable examples of the default option is organ donation. Countries with an opt-out system (consent to donate is assumed, and the default option is to donate organs) have a considerably higher rate of organ donation than countries with an opt-in system (default option is not to donate organs) [ 21 ]. Positive effects from the default option have also been reported for vaccination uptake [ 22 ] and the rate of enrollment into a diabetes management program [ 23 ]. COVID-19-prevention policies can also nudge people to engage in hygiene practices such as repeated hand washing by arranging defaults in the environment where they make COVID-19-related choices. For example, soaps with toys embedded inside improved hand washing behavior in children [ 24 ]. This example is a choice architecture (i.e., nudge) that may nudge children to wash their hands more frequently, so could be used to increase hand washing during this COVID-19 outbreak. A field experiment study in India found that the installation of low-cost soap dispensers in homes improved hand washing in peri-urban and rural households [ 25 ]. Framing Effect Framing effect refers to the fact that individuals’ choices often depend on the way the choices are described, or framed, and that these choices are often affected by whether the possible outcomes are framed in terms of the gains or the losses [ 26 ]. This concept is closely associated with loss aversion, which implies that the disutility caused by a given amount of loss is about twice the utility of gaining the same amount. For example, the statements “the odds of survival after 1 month of surgery are 90%” and “the odds of mortality within 1 month of surgery are 10%” elicit different reactions. Both statements offer the same information, but many individuals react differently to the risk of surgery when presented as a 90% chance of survival versus a 10% chance of death [ 27 ]. The framing effect has application for directing individuals toward health-promoting decisions and has been examined in a wide variety of health behaviors [ 28 ]. A health message can be framed to emphasize the benefits (i.e., gain-framed message) of performing a specific behavior or to emphasize the disadvantages (i.e., loss-framed message) of not engaging in that behavior [ 29 ]. Studies have shown that loss-framed messages are often more effective for disease-detection behaviors such as uptake of cancer screening, whereas gain-framed messages are often more effective for promoting preventive behaviors. A meta-analysis of 94 studies found that health messages framed as gains or benefits were significantly more likely to increase preventive behaviors than those framed as losses [ 29 ]. It offers a helpful perspective for framing health messages regarding COVID-19 prevention. It seems that health messages intended to encourage people to engage in COVID-19-preventive behaviors (e.g., social distancing) should be framed in terms of gains, such as “If you wash your hands properly/follow social distancing policy/adhere to the stay-at-home policy, you will increase the chances of yourself and your family having a long, healthy life.” Optimism and Overconfidence People display unrealistic optimism about their vulnerability to a wide set of negative outcomes [ 30 ] and often see themselves as being at less-than-average risk of negative outcomes. Optimism bias is people’s tendency to estimate the probability of positive future outcomes as greater than average and that of negative future outcomes as less than average [ 30 , 31 ]. This may lead people to unwittingly take extra risks with their own health and more than they would if they were aware of the objective risk of health-related behavior [ 32 ]. This can help explain a wide range of risk-taking behaviors, including health-related decisions. For example, one study revealed that smokers underestimated their risk of developing lung cancer compared with that of other smokers and even non-smokers [ 33 ]. Another study found that people with a subjective risk lower than their objective risk were more likely to support the belief that there is no risk of lung cancer if you just smoke for a few years and to believe that a large number of patients with lung cancer are cured. They were even less likely to decide to stop smoking [ 34 ]. People realize the risk of getting COVID-19 from suboptimal behaviors such as not washing hands or not adhering to social distancing but are likely to believe that they are less likely than other people or their peers to get COVID-19, even if their peers adhere to preventive practices. Providing peer comparison feedback or communicating risks accurately can be helpful for addressing optimism and overconfidence bias. In addition, priming an outcome by presenting what has happened to individuals or populations that are considered peers may persuade people to adhere to preventive behaviors [ 35 ]. For example, adolescents may become more engaged in COVID-19-prevention programs if they are aware that an adolescent celebrity contracted COVID-19. A possible explanation for this may be that the COVID-19 infection of an adolescent celebrity tends to increase the perception of individuals regarding their personal risk of getting COVID-19. Affect Heuristic Affect heuristic is a person’s tendency to judge risks and benefits based on their affect, that is, different affects can produce different risk and benefit perceptions [ 36 ]. It has been shown that individuals’ affect acts as a form of information that they refer to when deciding whether to engage in particular health behaviors [ 37 , 38 ]. In particular, when people feel positive about a behavior, they judge its risks as low and benefits as high; when they feel negative about a behavior, they judge its risks as high and benefits as low [ 39 ]. Evidence has shown that, while risk and benefit appear to be often uncorrelated or even positively correlated across harmful behaviors in the real-world context (i.e., high-risk activity appears to be highly gainful), they are often negatively correlated in individuals’ judgments and decisions (i.e., high risk is associated with low profit and vice versa) [ 36 , 39 ]. A study found that test harm information about prostate-specific antigen screening for prostate cancer and magnetic resonance imaging reduced perceived test benefits [ 40 ]. A study of how affect influences individuals’ processing of messages about risks and benefits of using autonomous artificial intelligence technology to screen for skin cancer found that integral artificial intelligence affect impacted on individuals’ perception of risk and benefits based on messages provided, which then influenced the probability of using artificial intelligence technology for health [ 41 ]. If perceptions of risk and benefit are directed by affect, the provision of benefits information will switch people’s judgment of risk and vice versa [ 39 ]. Therefore, the messages that people receive about a certain behavior become an important source of information that influences their health decisions [ 37 ]. This heuristic suggests that policy maker’s efforts to create negative feelings toward not adhering to COVID-19-preventive behaviors can increase the perceived risks associated with not adhering. For example, if an individual is told that not adhering to social distancing policy might cause them to contract COVID-19, this is predicted to cause negative feelings toward not adhering, which should, in turn, reduce the perceived benefits of not adhering to social distancing. In addition, “don’t miss the opportunity to be together at home” may be helpful for encouraging people to adhere to stay-at-home policies. A controlled trial in India showed that a scalable village-level intervention based on emotional drivers of behavior was more successful at increasing hand washing than was providing information [ 42 ]. Herding Behavior and Social Influence Social norms and the behavior of peers such as friends, family members, and colleagues affect behaviors. Herding behavior occurs when people consider a certain behavior to be good or bad based on the behavior of other people and mimic their observed behaviors [ 43 ]. This characteristic of human behavior is well-established in a number of fields, particularly economics and finance [ 44 ]. One implication of this behavior is that if a policy aims to encourage people to make a health decision, then it needs to inform individuals about the behavior of other people and their peers [ 5 ]. In a real-world experiment conducted on tax compliance in Minnesota, one of the interventions informed people that more than 90% of Minnesotans had paid their taxes; this had a significant effect on tax compliance compared with other interventions [ 5 , 45 ]. To nudge people to adhere to social distancing policy, interventions should draw attention to what other people are doing [ 5 ]. For example, telling people that “the majority of the people in your neighboring city or province are following the social distancing/stay-at-home policy” may increase adherence to social distancing policy. Conclusion We have discussed insights from behavioral economics that shed light on how to help people engage in COVID-19-preventive behaviors. This paper can improve our understanding of the decision-making biases that can be applied as entry points in public health policies and interventions for the prevention of COVID-19. They may assist policy makers in identifying novel interventions to improve decision making and behaviors related to the prevention of COVID-19. We provided some policy suggestions that may be useful in the fight against COVID-19, summarized as follows: reduce the current costs or increase the current benefits of adherence to social distancing/stay-at-home policies, arrange defaults in environments where people make COVID-19-related choices (i.e., choice architecture), design gain-framed messages for COVID-19-preventive behaviors, prime contamination with COVID-19 by presenting examples pertinent to a specific population, create negative feelings toward not adhering to COVID-19-preventive behavior, and draw individual’s attention to what other individuals are doing about COVID-19-related decisions. While many health-related behaviors have been shown to be associated with the six decision biases discussed, the degree to which they impact COVID-19-preventive behaviors has not yet been empirically investigated. Future work should examine strategies such as gain-framed messages, incentives in terms of small frequent rewards, and default options or nudges to improve interventions designed to prevent not only COVID-19 but also other communicable diseases.  References 1. World Health Organization (WHO) Emergency Committee. Statement on the second meeting of the International Health Regulations (2005) Emergency Committee regarding the outbreak of novel coronavirus (2019-nCoV). Geneva: WHO. https://www.who.int/news-room/detail/30-01-2020-statement-on-the-second-meeting-of-the-international-health-regulations-(2005)-emergency-committee-regarding-the-outbreak-of-novel-coronavirus-(2019-ncov ). Accessed 30 January 2020. 2. Chen Y, Liu Q, Guo D. Emerging coronaviruses: genome structure, replication, and pathogenesis. J Med Virol. 2020;92(4):418–23. CAS  PubMed  PubMed Central  Article  Google Scholar  3. Wilder-Smith A, Freedman D. Isolation, quarantine, social distancing and community containment: pivotal role for old-style public health measures in the novel coronavirus (2019-nCoV) outbreak. J Travel Med. 2020;27(2):taaa020. 4. Congdon WJ, Shankar M. The role of behavioral economics in evidence-based policymaking. Ann Am Acad Pol Soc Sc. 2018;678(1):81–92. Article  Google Scholar  5. Thaler RH, Sunstein CR. Nudge: improving decisions about health, wealth and happiness. New Haven: Yale University Press; 2008. Google Scholar  6. Thaler RH. Behavioral economics: past, present, and future. Am Econ Rev. 2016;106(7):1577–600. Article  Google Scholar  7. Thaler RH. From cashews to nudges: The evolution of behavioral economics. Am Econ Rev. 2018;108(6):1265–87. Article  Google Scholar  8. Tversky A, Kahneman D. Judgment under uncertainty: heuristics and biases. Science. 1974;185(4157):1124–31. CAS  PubMed  Article  PubMed Central  Google Scholar  9. Bickel WK, Moody L, Higgins ST. Some current dimensions of the behavioral economics of health-related behavior change. Prev Med. 2016;92:16–23. PubMed  PubMed Central  Article  Google Scholar  10. Van der Pol M, Cairns J. Descriptive validity of alternative intertemporal models for health outcomes: an axiomatic test. Health Econ. 2011;20(7):770–82. PubMed  Article  Google Scholar  11. Laibson D. Golden eggs and hyperbolic discounting. Q J Econ. 1997;112(2):443–78. Article  Google Scholar  12. Loewenstein G, Asch DA, Friedman JY, Melichar LA, Volpp KG. Can behavioural economics make us healthier? BMJ. 2012;344:e3482. PubMed  Article  Google Scholar  13. O'Donoghue T, Rabin M. Doing it now or later. Am Econ Rev. 1999;89(1):103–24. Article  Google Scholar  14. Soofi M, Sari AA, Rezaei S, Hajizadeh M, Najafi F. Individual time preferences and obesity: a behavioral economics analysis using a quasi-hyperbolic discounting approach. Int J Soc Econ. 2019;47(1):16–26. Article  Google Scholar  15. Van Der Pol M, Hennessy D, Manns B. The role of time and risk preferences in adherence to physician advice on health behavior change. Eur J Health Econ. 2017;18(3):373–86. PubMed  Article  Google Scholar  16. Linnemayr S, Stecher C, Mukasa B. Behavioral economic incentives to improve adherence to antiretroviral medication. AIDS (London, England). 2017;31(5):719. Article  Google Scholar  17. Halpern SD, French B, Small DS, Saulsgiver K, Harhay MO, Audrain-McGovern J, et al. Randomized trial of four financial-incentive programs for smoking cessation. N Engl J Med. 2015;372:2108–17. PubMed  PubMed Central  Article  Google Scholar  18. John LK, Loewenstein G, Troxel AB, Norton L, Fassbender JE, Volpp KG. Financial incentives for extended weight loss: a randomized, controlled trial. J Gen Intern Med. 2011;26(6):621–6. PubMed  PubMed Central  Article  Google Scholar  19. Loewenstein G, Brennan T, Volpp KG. Asymmetric paternalism to improve health behaviors. JAMA. 2007;298(20):2415–7. CAS  PubMed  Article  Google Scholar  20. Samuelson W, Zeckhauser R. Status quo bias in decision making. J Risk Uncertain. 1988;1(1):7–59. Article  Google Scholar  21. Johnson EJ, Goldstein D. Do defaults save lives?: American Association for the Advancement of Science; 2003. 22. Chapman GB, Li M, Colby H, Yoon H. Opting in vs opting out of influenza vaccination. JAMA. 2010;304(1):43–4. CAS  PubMed  Article  Google Scholar  23. Aysola J, Tahirovic E, Troxel AB, Asch DA, Gangemi K, Hodlofski AT, et al. A randomized controlled trial of opt-in versus opt-out enrollment into a diabetes behavioral intervention. Am J Health Promot. 2018;32(3):745–52. PubMed  Article  Google Scholar  24. Watson J, Dreibelbis R, Aunger R, Deola C, King K, Long S, et al. Child's play: harnessing play and curiosity motives to improve child handwashing in a humanitarian setting. Int J Hyg Environ Health. 2019;222(2):177–82. PubMed  Article  Google Scholar  25. Hussam R, Rabbani A, Reggiani G, Rigol N. Habit formation and rational addiction: a field experiment in handwashing. Harvard Business School BGIE Unit working paper. 2017(18-030). 26. Tversky A, Kahneman D. Prospect theory: an analysis of decision under risk. Econometrica. 1979;47(2):263–91. Article  Google Scholar  27. Luoto J, Carman KG. Behavioral economics guidelines with applications for health interventions. Washington: Inter-American Development Bank; 2014. Google Scholar  28. Kahneman D, Slovic SP, Slovic P, Tversky A. Judgment under uncertainty: heuristics and biases. Cambridge: Cambridge University Press; 1982. 29. Gallagher KM, Updegraff JA. Health message framing effects on attitudes, intentions, and behavior: a meta-analytic review. Ann Behav Med. 2012;43(1):101–16. PubMed  Article  Google Scholar  30. Weinstein ND. Unrealistic optimism about future life events. J Pers Soc Psychol. 1980;39(5):806. Article  Google Scholar  31. Weinstein ND. Unrealistic optimism about susceptibility to health problems: conclusions from a community-wide sample. J Behav Med. 1987;10(5):481–500. CAS  PubMed  Article  Google Scholar  32. White JS, Dow WH. Intertemporal choices for health. In: Roberto Ch A, Kawachi I, editors, Behavioral economics and public health. Oxford: Oxford University Press, 2015;27:62. Google Scholar  33. Weinstein ND, Marcus SE, Moser RP. Smokers’ unrealistic optimism about their risk. Tob control. 2005;14(1):55–9. CAS  PubMed  PubMed Central  Article  Google Scholar  34. Dillard AJ, McCaul KD, Klein WM. Unrealistic optimism in smokers: Implications for smoking myth endorsement and self-protective motivation. J Health Commun. 2006;11(S1):93–102. PubMed  Article  Google Scholar  35. Matjasko JL, Cawley JH, Baker-Goering MM, Yokum DV. Applying behavioral economics to public health policy: illustrative examples and promising directions. Am J Prev Med. 2016;50(5):S13–S1919. PubMed  PubMed Central  Article  Google Scholar  36. Finucane ML, Alhakami A, Slovic P, Johnson SM. The affect heuristic in judgments of risks and benefits. J Behav Decis Mak. 2000;13(1):1–17. Article  Google Scholar  37. Schwarz N. Feelings-as-information theory. In: Van Lange P, Kruglanski A, Higgins ET, editors. Handbook of theories of social psychology. Thousand Oaks: Sage; 2011. p. 289–308. Google Scholar  38. Peters E, Lipkus I, Diefenbach MA. The functions of affect in health communications and in the construction of health preferences. J Commun. 2006;56:S140–S162162. Article  Google Scholar  39. Slovic P, Peters E. Risk perception and affect. Curr Dir Psychol Sci. 2006;15(6):322–5. Article  Google Scholar  40. Scherer LD, Shaffer VA, Caverly T, Scherer AM, Zikmund-Fisher BJ, Kullgren JT, et al. The role of the affect heuristic and cancer anxiety in responding to negative information about medical tests. Psychol Health. 2018;33(2):292–312. PubMed  Article  Google Scholar  41. Tong ST, Sopory P. Does integral affect influence intentions to use artificial intelligence for skin cancer screening? A test of the affect heuristic. Psychol Health. 2019;34(7):828–49. PubMed  Article  Google Scholar  42. Biran A, Schmidt W-P, Varadharajan KS, Rajaraman D, Kumar R, Greenland K, et al. Effect of a behaviour-change intervention on handwashing with soap in India (SuperAmma): a cluster-randomised trial. Lancet Glob Health. 2014;2(3):e145–e154154. PubMed  Article  Google Scholar  43. Ariely D. Predictably irrational. New York: Harper Audio New York; 2008. 44. Raafat RM, Chater N, Frith C. Herding in humans. Trends Cognit Sci. 2009;13(10):420–8. Article  Google Scholar  45. Coleman S. The Minnesota income tax compliance experiment: replication of the social norms experiment. Available at SSRN 1393292; 2007. Download references Author information Affiliations Social Development and Health Promotion Research Center, Health Institute, Kermanshah University of Medical Sciences, Kermanshah, Iran Moslem Soofi Research Center for Environmental Determinants of Health, Health Institute, Kermanshah University of Medical Sciences, Kermanshah, Iran Farid Najafi & Behzad Karami-Matin Authors Moslem Soofi View author publications You can also search for this author in PubMed  Google Scholar Farid Najafi View author publications You can also search for this author in PubMed  Google Scholar Behzad Karami-Matin View author publications You can also search for this author in PubMed  Google Scholar Contributions MS contributed to the conception and design of the study and drafted the manuscript. FN and BKM contributed to critical revision of the manuscript. All authors read and approved the final manuscript. Corresponding author Correspondence to Behzad Karami-Matin . Ethics declarations  Funding  No sources of funding were used to conduct this study or prepare this manuscript.  Conflict of interest  Moslem Soofi, Farid Najafi, and Behzad Karami-Matin have no conflicts of interest that are directly relevant to the content of this article.  Rights and permissions Reprints and Permissions About this article Cite this article Soofi, M., Najafi, F. & Karami-Matin, B. Using Insights from Behavioral Economics to Mitigate the Spread of COVID-19. Appl Health Econ Health Policy  18, 345–350 (2020). https://doi.org/10.1007/s40258-020-00595-4 Download citation Published : 21 May 2020 Issue Date : June 2020 DOI : https://doi.org/10.1007/s40258-020-00595-4           Download PDF                 Advertisement                 Over 10 million scientific documents at your fingertips   Switch Edition    Academic Edition    Corporate Edition         Home  Impressum  Legal information  Privacy statement  How we use cookies  Accessibility  Contact us     Not logged in  - 89.64.25.55   Not affiliated    Springer Nature       © 2020 Springer Nature Switzerland AG. Part of Springer Nature .                      "
11,affect influenced(fallacy OR bias OR heuristic) (coronavirus OR covid OR pandemic),2,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7144592/,"Access Denied

Your access to the NCBI website at www.ncbi.nlm.nih.gov has been temporarily blocked due to a possible misuse/abuse situation involving your site. This is not an indication of a security issue such as a virus or attack. It could be something as simple as a run away script or learning how to better use E-utilities, http://www.ncbi.nlm.nih.gov/books/NBK25497/, for more efficient work such that your work does not impact the ability of other researchers to also use our site. To restore access and understand how to better interact with our site to avoid this in the future, please have your system administrator contact info@ncbi.nlm.nih.gov.","           Advertisement                  Search         Log in              Search SpringerLink     Search                       Download PDF          Practical Application  Published: 21 May 2020   Using Insights from Behavioral Economics to Mitigate the Spread of COVID-19  Moslem Soofi 1 , Farid Najafi 2 & Behzad Karami-Matin 2    Applied Health Economics and Health Policy  volume 18 , pages 345 – 350 ( 2020 ) Cite this article        1560 Accesses    2 Altmetric    Metrics details          Abstract The outbreak of 2019 coronavirus disease (COVID-19) has become a public health emergency of international concern. The number of COVID-infected individuals and related deaths continues to rise rapidly. Encouraging people to adopt and sustain preventive behaviors is a central focus of public health policies that seek to mitigate the spread of COVID-19. Public health policy needs improved methods to encourage people to adhere to COVID-19-preventive behaviors. In this paper, we introduce a number of insights from behavioral economics that help explain why people may behave irrationally during the COVID-19 pandemic. In particular, present bias, status quo bias, framing effect, optimism bias, affect heuristic, and herding behavior are discussed. We hope this paper will shed light on how insights from behavioral economics can enrich public health policies and interventions in the fight against COVID-19.  FormalPara Key Points for Decision Makers Behavioral economics acknowledges that people are not the rational decision makers assumed in the standard economic theory of decision making. Finite rationality and willpower lead people to apply the rules of thumb or heuristics to make their COVID-19-related decisions rather than conducting cost-benefit analyses. Therefore, they may be biased in their COVID-19-related decisions. Behavioral economics can help policy makers identify individuals’ decision biases and use them as starting points for designing COVID-19-preventive interventions. Behavioral economics interventions can help people behave rationally and make better COVID-19-related decisions.  Introduction The outbreak of 2019 coronavirus disease (COVID-19) has become a public health emergency of international concern [ 1 ]. The number of COVID-infected individuals and related deaths continues to rise rapidly. COVID-19 is a serious threat to global health and the world economy and has caused widespread concern around the world. In the absence of approved treatments for and vaccines against COVID-19, preventive strategies and hygiene behaviors such as social distancing and stay-at-home policies, avoiding touching the face, and repeated hand washing are effective options in the fight against COVID-19 [ 2 , 3 ]. During this pandemic, encouraging people to adopt and sustain preventive behaviors is a central focus of public health policies that seek to mitigate the spread of COVID-19. Behavioral economics has recently received a great deal of attention in public policy making [ 4 ]. This field of economics uses insights from the fields of psychology, neuroscience, and cognitive sciences to explain how people’s behaviors deviate from the rational choice theory and when and why people’s short-term decisions sometimes undermine their long-term interests. The focus of this field is on better predicting and understanding people’s behaviors and choices to help formulate more effective public policies [ 5 , 6 ]. It identifies biases in the decision-making process and uses them as entry points for interventions to address particular behaviors. Behavioral economics acknowledges that people do not have infinite rationality and willpower, so they are not the rational decision makers assumed in the standard economic theory of utility maximization [ 6 , 7 ]. In addition, they have limited cognitive and computational abilities, and their decisions are not based on a complete analysis of all available information [ 6 ]. These limitations lead people to apply the rules of thumb or heuristics (i.e., mental shortcuts) to make their decisions rather than conducting cost–benefit analyses when making a decision. The heuristics are generally useful but can lead to systematic mistakes (i.e., biases) in decision making that, in turn, result in suboptimal and harmful behaviors [ 5 , 8 ]. Behavioral economics has shed new light on a range of risky and preventive health behaviors [ 9 ]. It also has considerable potential for providing a valuable perspective to better understand and explain COVID-19-related behaviors. While multiple biases are identified in the field of behavioral economics, in this paper we focus on six that tend to be particularly relevant to COVID-19-related behaviors: present bias, status quo bias, framing effect, optimism bias, affect heuristic, and herding behavior. It may provide useful insights into public health policies designed to reduce the spread of COVID-19 and may be helpful in developing and implementing interventions. Present Bias In the context of intertemporal choices, the costs and benefits of our choices occur at different points in time, that is, many daily choices are a trade-off between immediate outcomes (i.e., costs and benefit) and expected future outcomes [ 10 ]. Present bias or hyperbolic discounting is the nonlinear and nonconstant tendency of many individuals to prefer a smaller sooner pay-off over a larger future pay-off [ 11 , 12 , 13 ]. Present bias may lead to time-inconsistent preferences. An individual makes a plan for tomorrow, but once tomorrow comes they may experience a preference reversal and revise their plan. Present bias has been shown to be a significant predictor of a wide variety of health behaviors [ 14 ]. Many health behaviors involve a trade-off between immediate and future outcomes. For example, smoking has both current benefits (temporary stress relief) and future costs (increased risk of lung cancer) [ 10 ]. In the case of COVID-19, not adhering to stay-at-home policies involves a trade-off between the pleasure of going to the mall or restaurant now (current benefit) and the increased risk of contracting COVID-19 in the future (uncertain future cost). Uncertain future cost means that not every excursion outside the house would result in COVID-19 infection. Thus, myopic individuals (i.e., those with present bias), who put a greater emphasis on the here and now, are less likely to adhere to COVID-19-preventive behaviors, including staying at home and hand washing. Present bias is an explanation for why people do not behave in their own best interests and why they have difficulty adhering to preventive health behaviors such as social distancing, even when they wish to adhere [ 15 ]. Although present bias may lead to suboptimal behavioral choices, it can be used to help people adhere to COVID-19-preventive behaviors [ 12 ]. For example, reducing the current costs of adherence to social distancing may help people overcome their present bias, as even small costs could outweigh any perceived future benefits of adherence. Increasing the current benefit of adherence to social distancing, such as offering small and frequent payments now, can be useful in encouraging people to adhere to COVID-19-preventive behaviors. Such interventions involving low-cost rewards have been used as ways to increase current benefits of adherence to antiretroviral medication [ 16 ], smoking cessation [ 17 ], and weight loss [ 18 ], and they have been shown to be effective in changing behaviors. To reduce COVID-19 transmission, in the short term, providing free internet access at home, temporary suspension of loan repayments (e.g., loans provided by the government to support unemployed and uninsured people to start small businesses), and providing benefit packages for vulnerable groups should be considered in stay-at-home policies to encourage people to adhere to the policy and to increase its success rate. Status Quo Bias and Default Options Status quo bias is a disproportionate preference for the current status of options and an unwillingness to change them [ 19 , 20 ]. One reason for this is that people interpret the potential disadvantages of changing the status quo as greater than the potential benefits. This bias can be turned to the advantage of encouraging health-enhancing behaviors through the use of “nudges”. The concept of “nudge” was introduced in behavioral economics to persuade individuals to behave rationally and make better choices. Thaler and Sunstein [ 5 ] defined a nudge as “any aspect of the choice architecture that influences individuals’ decision making in a predictable way without forbidding any options or changing economic incentives.” They argued that, by improving and altering the environment in which individuals make decisions—what they call the “choice architecture”—, individuals can be influenced to make smarter choices. Choice architecture can be used to build an environment in which it is easier to make optimal health choices and more difficult to select suboptimal ones. The default option is a nudge with a powerful impact on directing the behaviors of people in ways that meet their long-term interests [ 5 ]. One of the most notable examples of the default option is organ donation. Countries with an opt-out system (consent to donate is assumed, and the default option is to donate organs) have a considerably higher rate of organ donation than countries with an opt-in system (default option is not to donate organs) [ 21 ]. Positive effects from the default option have also been reported for vaccination uptake [ 22 ] and the rate of enrollment into a diabetes management program [ 23 ]. COVID-19-prevention policies can also nudge people to engage in hygiene practices such as repeated hand washing by arranging defaults in the environment where they make COVID-19-related choices. For example, soaps with toys embedded inside improved hand washing behavior in children [ 24 ]. This example is a choice architecture (i.e., nudge) that may nudge children to wash their hands more frequently, so could be used to increase hand washing during this COVID-19 outbreak. A field experiment study in India found that the installation of low-cost soap dispensers in homes improved hand washing in peri-urban and rural households [ 25 ]. Framing Effect Framing effect refers to the fact that individuals’ choices often depend on the way the choices are described, or framed, and that these choices are often affected by whether the possible outcomes are framed in terms of the gains or the losses [ 26 ]. This concept is closely associated with loss aversion, which implies that the disutility caused by a given amount of loss is about twice the utility of gaining the same amount. For example, the statements “the odds of survival after 1 month of surgery are 90%” and “the odds of mortality within 1 month of surgery are 10%” elicit different reactions. Both statements offer the same information, but many individuals react differently to the risk of surgery when presented as a 90% chance of survival versus a 10% chance of death [ 27 ]. The framing effect has application for directing individuals toward health-promoting decisions and has been examined in a wide variety of health behaviors [ 28 ]. A health message can be framed to emphasize the benefits (i.e., gain-framed message) of performing a specific behavior or to emphasize the disadvantages (i.e., loss-framed message) of not engaging in that behavior [ 29 ]. Studies have shown that loss-framed messages are often more effective for disease-detection behaviors such as uptake of cancer screening, whereas gain-framed messages are often more effective for promoting preventive behaviors. A meta-analysis of 94 studies found that health messages framed as gains or benefits were significantly more likely to increase preventive behaviors than those framed as losses [ 29 ]. It offers a helpful perspective for framing health messages regarding COVID-19 prevention. It seems that health messages intended to encourage people to engage in COVID-19-preventive behaviors (e.g., social distancing) should be framed in terms of gains, such as “If you wash your hands properly/follow social distancing policy/adhere to the stay-at-home policy, you will increase the chances of yourself and your family having a long, healthy life.” Optimism and Overconfidence People display unrealistic optimism about their vulnerability to a wide set of negative outcomes [ 30 ] and often see themselves as being at less-than-average risk of negative outcomes. Optimism bias is people’s tendency to estimate the probability of positive future outcomes as greater than average and that of negative future outcomes as less than average [ 30 , 31 ]. This may lead people to unwittingly take extra risks with their own health and more than they would if they were aware of the objective risk of health-related behavior [ 32 ]. This can help explain a wide range of risk-taking behaviors, including health-related decisions. For example, one study revealed that smokers underestimated their risk of developing lung cancer compared with that of other smokers and even non-smokers [ 33 ]. Another study found that people with a subjective risk lower than their objective risk were more likely to support the belief that there is no risk of lung cancer if you just smoke for a few years and to believe that a large number of patients with lung cancer are cured. They were even less likely to decide to stop smoking [ 34 ]. People realize the risk of getting COVID-19 from suboptimal behaviors such as not washing hands or not adhering to social distancing but are likely to believe that they are less likely than other people or their peers to get COVID-19, even if their peers adhere to preventive practices. Providing peer comparison feedback or communicating risks accurately can be helpful for addressing optimism and overconfidence bias. In addition, priming an outcome by presenting what has happened to individuals or populations that are considered peers may persuade people to adhere to preventive behaviors [ 35 ]. For example, adolescents may become more engaged in COVID-19-prevention programs if they are aware that an adolescent celebrity contracted COVID-19. A possible explanation for this may be that the COVID-19 infection of an adolescent celebrity tends to increase the perception of individuals regarding their personal risk of getting COVID-19. Affect Heuristic Affect heuristic is a person’s tendency to judge risks and benefits based on their affect, that is, different affects can produce different risk and benefit perceptions [ 36 ]. It has been shown that individuals’ affect acts as a form of information that they refer to when deciding whether to engage in particular health behaviors [ 37 , 38 ]. In particular, when people feel positive about a behavior, they judge its risks as low and benefits as high; when they feel negative about a behavior, they judge its risks as high and benefits as low [ 39 ]. Evidence has shown that, while risk and benefit appear to be often uncorrelated or even positively correlated across harmful behaviors in the real-world context (i.e., high-risk activity appears to be highly gainful), they are often negatively correlated in individuals’ judgments and decisions (i.e., high risk is associated with low profit and vice versa) [ 36 , 39 ]. A study found that test harm information about prostate-specific antigen screening for prostate cancer and magnetic resonance imaging reduced perceived test benefits [ 40 ]. A study of how affect influences individuals’ processing of messages about risks and benefits of using autonomous artificial intelligence technology to screen for skin cancer found that integral artificial intelligence affect impacted on individuals’ perception of risk and benefits based on messages provided, which then influenced the probability of using artificial intelligence technology for health [ 41 ]. If perceptions of risk and benefit are directed by affect, the provision of benefits information will switch people’s judgment of risk and vice versa [ 39 ]. Therefore, the messages that people receive about a certain behavior become an important source of information that influences their health decisions [ 37 ]. This heuristic suggests that policy maker’s efforts to create negative feelings toward not adhering to COVID-19-preventive behaviors can increase the perceived risks associated with not adhering. For example, if an individual is told that not adhering to social distancing policy might cause them to contract COVID-19, this is predicted to cause negative feelings toward not adhering, which should, in turn, reduce the perceived benefits of not adhering to social distancing. In addition, “don’t miss the opportunity to be together at home” may be helpful for encouraging people to adhere to stay-at-home policies. A controlled trial in India showed that a scalable village-level intervention based on emotional drivers of behavior was more successful at increasing hand washing than was providing information [ 42 ]. Herding Behavior and Social Influence Social norms and the behavior of peers such as friends, family members, and colleagues affect behaviors. Herding behavior occurs when people consider a certain behavior to be good or bad based on the behavior of other people and mimic their observed behaviors [ 43 ]. This characteristic of human behavior is well-established in a number of fields, particularly economics and finance [ 44 ]. One implication of this behavior is that if a policy aims to encourage people to make a health decision, then it needs to inform individuals about the behavior of other people and their peers [ 5 ]. In a real-world experiment conducted on tax compliance in Minnesota, one of the interventions informed people that more than 90% of Minnesotans had paid their taxes; this had a significant effect on tax compliance compared with other interventions [ 5 , 45 ]. To nudge people to adhere to social distancing policy, interventions should draw attention to what other people are doing [ 5 ]. For example, telling people that “the majority of the people in your neighboring city or province are following the social distancing/stay-at-home policy” may increase adherence to social distancing policy. Conclusion We have discussed insights from behavioral economics that shed light on how to help people engage in COVID-19-preventive behaviors. This paper can improve our understanding of the decision-making biases that can be applied as entry points in public health policies and interventions for the prevention of COVID-19. They may assist policy makers in identifying novel interventions to improve decision making and behaviors related to the prevention of COVID-19. We provided some policy suggestions that may be useful in the fight against COVID-19, summarized as follows: reduce the current costs or increase the current benefits of adherence to social distancing/stay-at-home policies, arrange defaults in environments where people make COVID-19-related choices (i.e., choice architecture), design gain-framed messages for COVID-19-preventive behaviors, prime contamination with COVID-19 by presenting examples pertinent to a specific population, create negative feelings toward not adhering to COVID-19-preventive behavior, and draw individual’s attention to what other individuals are doing about COVID-19-related decisions. While many health-related behaviors have been shown to be associated with the six decision biases discussed, the degree to which they impact COVID-19-preventive behaviors has not yet been empirically investigated. Future work should examine strategies such as gain-framed messages, incentives in terms of small frequent rewards, and default options or nudges to improve interventions designed to prevent not only COVID-19 but also other communicable diseases.  References 1. World Health Organization (WHO) Emergency Committee. Statement on the second meeting of the International Health Regulations (2005) Emergency Committee regarding the outbreak of novel coronavirus (2019-nCoV). Geneva: WHO. https://www.who.int/news-room/detail/30-01-2020-statement-on-the-second-meeting-of-the-international-health-regulations-(2005)-emergency-committee-regarding-the-outbreak-of-novel-coronavirus-(2019-ncov ). Accessed 30 January 2020. 2. Chen Y, Liu Q, Guo D. Emerging coronaviruses: genome structure, replication, and pathogenesis. J Med Virol. 2020;92(4):418–23. CAS  PubMed  PubMed Central  Article  Google Scholar  3. Wilder-Smith A, Freedman D. Isolation, quarantine, social distancing and community containment: pivotal role for old-style public health measures in the novel coronavirus (2019-nCoV) outbreak. J Travel Med. 2020;27(2):taaa020. 4. Congdon WJ, Shankar M. The role of behavioral economics in evidence-based policymaking. Ann Am Acad Pol Soc Sc. 2018;678(1):81–92. Article  Google Scholar  5. Thaler RH, Sunstein CR. Nudge: improving decisions about health, wealth and happiness. New Haven: Yale University Press; 2008. Google Scholar  6. Thaler RH. Behavioral economics: past, present, and future. Am Econ Rev. 2016;106(7):1577–600. Article  Google Scholar  7. Thaler RH. From cashews to nudges: The evolution of behavioral economics. Am Econ Rev. 2018;108(6):1265–87. Article  Google Scholar  8. Tversky A, Kahneman D. Judgment under uncertainty: heuristics and biases. Science. 1974;185(4157):1124–31. CAS  PubMed  Article  PubMed Central  Google Scholar  9. Bickel WK, Moody L, Higgins ST. Some current dimensions of the behavioral economics of health-related behavior change. Prev Med. 2016;92:16–23. PubMed  PubMed Central  Article  Google Scholar  10. Van der Pol M, Cairns J. Descriptive validity of alternative intertemporal models for health outcomes: an axiomatic test. Health Econ. 2011;20(7):770–82. PubMed  Article  Google Scholar  11. Laibson D. Golden eggs and hyperbolic discounting. Q J Econ. 1997;112(2):443–78. Article  Google Scholar  12. Loewenstein G, Asch DA, Friedman JY, Melichar LA, Volpp KG. Can behavioural economics make us healthier? BMJ. 2012;344:e3482. PubMed  Article  Google Scholar  13. O'Donoghue T, Rabin M. Doing it now or later. Am Econ Rev. 1999;89(1):103–24. Article  Google Scholar  14. Soofi M, Sari AA, Rezaei S, Hajizadeh M, Najafi F. Individual time preferences and obesity: a behavioral economics analysis using a quasi-hyperbolic discounting approach. Int J Soc Econ. 2019;47(1):16–26. Article  Google Scholar  15. Van Der Pol M, Hennessy D, Manns B. The role of time and risk preferences in adherence to physician advice on health behavior change. Eur J Health Econ. 2017;18(3):373–86. PubMed  Article  Google Scholar  16. Linnemayr S, Stecher C, Mukasa B. Behavioral economic incentives to improve adherence to antiretroviral medication. AIDS (London, England). 2017;31(5):719. Article  Google Scholar  17. Halpern SD, French B, Small DS, Saulsgiver K, Harhay MO, Audrain-McGovern J, et al. Randomized trial of four financial-incentive programs for smoking cessation. N Engl J Med. 2015;372:2108–17. PubMed  PubMed Central  Article  Google Scholar  18. John LK, Loewenstein G, Troxel AB, Norton L, Fassbender JE, Volpp KG. Financial incentives for extended weight loss: a randomized, controlled trial. J Gen Intern Med. 2011;26(6):621–6. PubMed  PubMed Central  Article  Google Scholar  19. Loewenstein G, Brennan T, Volpp KG. Asymmetric paternalism to improve health behaviors. JAMA. 2007;298(20):2415–7. CAS  PubMed  Article  Google Scholar  20. Samuelson W, Zeckhauser R. Status quo bias in decision making. J Risk Uncertain. 1988;1(1):7–59. Article  Google Scholar  21. Johnson EJ, Goldstein D. Do defaults save lives?: American Association for the Advancement of Science; 2003. 22. Chapman GB, Li M, Colby H, Yoon H. Opting in vs opting out of influenza vaccination. JAMA. 2010;304(1):43–4. CAS  PubMed  Article  Google Scholar  23. Aysola J, Tahirovic E, Troxel AB, Asch DA, Gangemi K, Hodlofski AT, et al. A randomized controlled trial of opt-in versus opt-out enrollment into a diabetes behavioral intervention. Am J Health Promot. 2018;32(3):745–52. PubMed  Article  Google Scholar  24. Watson J, Dreibelbis R, Aunger R, Deola C, King K, Long S, et al. Child's play: harnessing play and curiosity motives to improve child handwashing in a humanitarian setting. Int J Hyg Environ Health. 2019;222(2):177–82. PubMed  Article  Google Scholar  25. Hussam R, Rabbani A, Reggiani G, Rigol N. Habit formation and rational addiction: a field experiment in handwashing. Harvard Business School BGIE Unit working paper. 2017(18-030). 26. Tversky A, Kahneman D. Prospect theory: an analysis of decision under risk. Econometrica. 1979;47(2):263–91. Article  Google Scholar  27. Luoto J, Carman KG. Behavioral economics guidelines with applications for health interventions. Washington: Inter-American Development Bank; 2014. Google Scholar  28. Kahneman D, Slovic SP, Slovic P, Tversky A. Judgment under uncertainty: heuristics and biases. Cambridge: Cambridge University Press; 1982. 29. Gallagher KM, Updegraff JA. Health message framing effects on attitudes, intentions, and behavior: a meta-analytic review. Ann Behav Med. 2012;43(1):101–16. PubMed  Article  Google Scholar  30. Weinstein ND. Unrealistic optimism about future life events. J Pers Soc Psychol. 1980;39(5):806. Article  Google Scholar  31. Weinstein ND. Unrealistic optimism about susceptibility to health problems: conclusions from a community-wide sample. J Behav Med. 1987;10(5):481–500. CAS  PubMed  Article  Google Scholar  32. White JS, Dow WH. Intertemporal choices for health. In: Roberto Ch A, Kawachi I, editors, Behavioral economics and public health. Oxford: Oxford University Press, 2015;27:62. Google Scholar  33. Weinstein ND, Marcus SE, Moser RP. Smokers’ unrealistic optimism about their risk. Tob control. 2005;14(1):55–9. CAS  PubMed  PubMed Central  Article  Google Scholar  34. Dillard AJ, McCaul KD, Klein WM. Unrealistic optimism in smokers: Implications for smoking myth endorsement and self-protective motivation. J Health Commun. 2006;11(S1):93–102. PubMed  Article  Google Scholar  35. Matjasko JL, Cawley JH, Baker-Goering MM, Yokum DV. Applying behavioral economics to public health policy: illustrative examples and promising directions. Am J Prev Med. 2016;50(5):S13–S1919. PubMed  PubMed Central  Article  Google Scholar  36. Finucane ML, Alhakami A, Slovic P, Johnson SM. The affect heuristic in judgments of risks and benefits. J Behav Decis Mak. 2000;13(1):1–17. Article  Google Scholar  37. Schwarz N. Feelings-as-information theory. In: Van Lange P, Kruglanski A, Higgins ET, editors. Handbook of theories of social psychology. Thousand Oaks: Sage; 2011. p. 289–308. Google Scholar  38. Peters E, Lipkus I, Diefenbach MA. The functions of affect in health communications and in the construction of health preferences. J Commun. 2006;56:S140–S162162. Article  Google Scholar  39. Slovic P, Peters E. Risk perception and affect. Curr Dir Psychol Sci. 2006;15(6):322–5. Article  Google Scholar  40. Scherer LD, Shaffer VA, Caverly T, Scherer AM, Zikmund-Fisher BJ, Kullgren JT, et al. The role of the affect heuristic and cancer anxiety in responding to negative information about medical tests. Psychol Health. 2018;33(2):292–312. PubMed  Article  Google Scholar  41. Tong ST, Sopory P. Does integral affect influence intentions to use artificial intelligence for skin cancer screening? A test of the affect heuristic. Psychol Health. 2019;34(7):828–49. PubMed  Article  Google Scholar  42. Biran A, Schmidt W-P, Varadharajan KS, Rajaraman D, Kumar R, Greenland K, et al. Effect of a behaviour-change intervention on handwashing with soap in India (SuperAmma): a cluster-randomised trial. Lancet Glob Health. 2014;2(3):e145–e154154. PubMed  Article  Google Scholar  43. Ariely D. Predictably irrational. New York: Harper Audio New York; 2008. 44. Raafat RM, Chater N, Frith C. Herding in humans. Trends Cognit Sci. 2009;13(10):420–8. Article  Google Scholar  45. Coleman S. The Minnesota income tax compliance experiment: replication of the social norms experiment. Available at SSRN 1393292; 2007. Download references Author information Affiliations Social Development and Health Promotion Research Center, Health Institute, Kermanshah University of Medical Sciences, Kermanshah, Iran Moslem Soofi Research Center for Environmental Determinants of Health, Health Institute, Kermanshah University of Medical Sciences, Kermanshah, Iran Farid Najafi & Behzad Karami-Matin Authors Moslem Soofi View author publications You can also search for this author in PubMed  Google Scholar Farid Najafi View author publications You can also search for this author in PubMed  Google Scholar Behzad Karami-Matin View author publications You can also search for this author in PubMed  Google Scholar Contributions MS contributed to the conception and design of the study and drafted the manuscript. FN and BKM contributed to critical revision of the manuscript. All authors read and approved the final manuscript. Corresponding author Correspondence to Behzad Karami-Matin . Ethics declarations  Funding  No sources of funding were used to conduct this study or prepare this manuscript.  Conflict of interest  Moslem Soofi, Farid Najafi, and Behzad Karami-Matin have no conflicts of interest that are directly relevant to the content of this article.  Rights and permissions Reprints and Permissions About this article Cite this article Soofi, M., Najafi, F. & Karami-Matin, B. Using Insights from Behavioral Economics to Mitigate the Spread of COVID-19. Appl Health Econ Health Policy  18, 345–350 (2020). https://doi.org/10.1007/s40258-020-00595-4 Download citation Published : 21 May 2020 Issue Date : June 2020 DOI : https://doi.org/10.1007/s40258-020-00595-4           Download PDF                 Advertisement                 Over 10 million scientific documents at your fingertips   Switch Edition    Academic Edition    Corporate Edition         Home  Impressum  Legal information  Privacy statement  How we use cookies  Accessibility  Contact us     Not logged in  - 89.64.25.55   Not affiliated    Springer Nature       © 2020 Springer Nature Switzerland AG. Part of Springer Nature .                      "
12,confirmation(fallacy OR bias OR heuristic) (coronavirus OR covid OR pandemic),1,https://towardsdatascience.com/overcoming-confirmation-bias-during-covid-19-51a64205eceb,"Overcoming confirmation bias during COVID-19

Why confirmation bias is the archnemesis of data science and how you can fight it

Two weeks ago, I published a decision-making guide to help you navigate the choices you’re grappling with during the COVID-19 pandemic.

This week, let’s talk about a nasty psychological effect which might be ruining your ability to cope effectively with the information you’re being flooded with daily: confirmation bias.

What is it?

“Confirmation bias is the tendency to search for, interpret, favor, and recall information in a way that confirms or strengthens one’s prior personal beliefs.” — Wikipedia

Yeah, but what is it?

This:

Illustration by Paul J.

If you’re keen to nerd out on psychology, let’s separate confirmation bias from some other phenomena:

Confirmation bias : your existing opinion changes how you perceive information.

your changes how you perceive information. Wishful blindness : your self-interest changes how you perceive information, especially in the context of ethical decision-making. (Not to be confused with willful blindness, which is a term from law.)

your changes how you perceive information, especially in the context of ethical decision-making. (Not to be confused with willful blindness, which is a term from law.) “Filter bubble” refers to intellectual isolation in online echo chambers. If an algorithm is designed to prioritize content you’ll like, and you tend to like content from people who already agree with you, then you’re less likely to be exposed to content that might change your mind.

I won’t bore you with advice on how to crawl out of an echo chamber — you’ve heard it all already. “Something something …intellectual discipline to seek… something something… people who disagree with you.” Right? Right.

Wishful blindness is a tricky one, since it’s a relatively new theory without much research on how to overcome it. Confirmation bias, though, has been on the dissection table since the 1960s, so let’s focus on that.

Filter bubbles are about skewed information, while confirmation bias is about skewed perception.

Confirmation bias is not the situation where your social media feed agrees with itself too much. It’s far more subtle: even if you’re exposed to information that disagrees with your opinion, you might not take it in. You might misremember it. You might find a reason to ignore it. You’ll keep digging until the numbers you see are the ones you wanted to see. The mind is funny like that.

Confirmation bias means we can all look at the same number and perceive it differently. A fact is no longer just a fact.

In other words, confirmation bias is the archnemesis of data science since it means that a fact is no longer just a fact, no matter how much math and science you throw into getting it. You and I can look at the same number and perceive it differently. Diligently exposing yourself to better information sources is not enough to overcome a problem that starts behind your eyes.

Confirmation bias is the archnemesis of data science.

You should be careful when it comes to trusting your own brain. (That traitor!) If you start with a strong uninformed opinion and go foraging for information, you’re likely to end up with the same opinion after you’re finished. Why bother? Unless you fight confirmation bias up front and do things in the right order, your foray into data is destined to be a gooey waste of time.

Interested in learning more about skewed perception of data? See my article on apophenia.

Confirmation bias and COVID-19

In a nutshell: If you have strong opinions about COVID-19 and then you go looking for evidence that supports them, you’ll think you see it… no matter how outlandish those opinions are. You’ll also have a harder time absorbing evidence that points in the opposite direction.

If you’re feeling scared and you go looking for information to make you feel better, confirmation bias can make you end up feeling worse instead.

To make matters worse, whenever you’re panicking and you try to soothe yourself with a nice relaxing internet session, you’re likely to find more reasons to panic.

Being quick to panic is not a virtue — it’s generally counterproductive.

While panic can give you an adrenaline surge — “confirmation bias: one weird trick to boost your at-home fitness routine” — it doesn’t help you make better decisions. It’s generally counterproductive, impairs your ability to think clearly, and does nothing good for your mood or your ability to cope with stress. No matter how bad your situation, level-headed realism will serve you better than panic.

How to fight confirmation bias

The upshot is that a strong starting opinion can mess up both your decision-making and your mood. Luckily, there are four easy ways you can fight confirmation bias:

Hold your opinions loosely.

Emphasize decisions, not opinions.

Focus on the things you have control over.

Change the order in which you approach information.

Hold your opinions loosely

This one is perhaps easier said than done, especially if you’ve been stewing in headlines for a while, but it’s good intellectual practice to keep an open mind and not to take your opinions too seriously, even if they’re based on plenty of data (hi, Bayesians).

Here’s another bias worth knowing about: overconfidence bias. (“A well-established bias in which a person’s subjective confidence in his or her judgements is reliably greater than the objective accuracy of those judgements, especially when confidence is relatively high.” — Wikipedia) Yeah, but what is it? Something you can reduce by developing the habit of holding your opinions loosely.

Holding your opinions loosely reduces their ability to skew your perception.

The best decision-makers I know are adaptable. They have the skills to take in new information and admit they were wrong. Before you run off to admire leaders who are unwavering in their judgments (steadfast and loyal, right?), take a moment to see them the way a psychologist seem them: they’re so hampered by confirmation bias that they’re unable to process new information appropriately (or they’re superb actors).

If you’re keen to curb your own tendencies towards hubris, take a page out of the scientist’s book. Ever noticed how research papers in applied science are filled with caveats, humbly reminding the reader about unknowns that could invalidate their conclusions? That’s not a bad standard to aim for.

Emphasize decisions, not opinions

In my, er, opinion, the best way to loosen the grip your opinions have on both your emotions and your decisions is to get into the habit of taking a decision-first perspective. If an opinion falls in the forest and no action is affected by it, did the opinion matter?

It’s through our decisions — our actions — that we affect the world around us.

Imagine that I believe that the Loch Ness monster is real. If my belief does not influence my decisions — interactions with the world — in any way, what harm is there in it? On the other hand, if my belief does influence my actions (consciously or subconsciously), then perhaps I should evaluate my opinion in context of the actions that it stands to influence. That involves thinking about the actions (in order of importance) before the opinion and then forming hypotheses for testing as a second step. My article on that is here.

“Will this information make me act differently?”

Whenever you find yourself receiving new information, remember to ask yourself: “is this actionable?” If yes, which of your decisions does it affect?

Focus on the things you have control over

When it comes to COVID-19, many of us already find our decision-making constrained by new rules. If we don’t take a moment to recognize that some decisions we’ve been worrying about are no longer on the table, we’ll waste unnecessary energy on opinions without a function.

Wouldn’t you prefer to focus on making decisions and plans about things that are under your control? (Or to use that time on activities that bring you joy?)

For example, here in NYC (yup, I know) I no longer have the option to go to the theatre this month. Whatever my opinion about how being in a crowded space might affect my health — I have such an opinion, loosely held, from reading WHO publications — there’s no decision that I can make which takes that information as input.

I no longer have the option to be in a crowd this month even if I wanted to. That decision is not on the table.

Perhaps I’d be better served by shifting my cognitive efforts elsewhere, towards decisions that are on the table for me, such as whether to help out remotely (yes), whether to order toilet paper online (no, that’s what all my textbooks are for), and whether to invest mountains of effort into filming an at-home video series about data science (perhaps not, unless the videos I already put on YouTube get more traffic).

Broadway is dark and Times Square is eerily empty these days. Source: Spencer Platt/Getty Images/AFP.

Stepping away from agonizing over things you can’t influence isn’t a call for ignorance. Compare the following ways to use your mental energy:

Worrying about a decision your mayor is grappling with while trying to think about how you would make it in their place. Examining your mayor’s decision skills from the perspective of a decision you’re making about them (e.g. to vote for/against the mayor’s reelection). Considering which actions you should take— if any — in response to (or in preparation for) each of the possible options your mayor is choosing between. Deciding whether you should try to exert your own influence/effort to affect the mayor’s decision or its potential consequences.

Of these, (2)-(4) are the more useful perspectives (unless your interest is academic), while (1) is good practice for students of decision-making (“How would I make this decision if I were in charge and what skills can I learn here?”) but not an efficient use of headspace for the emotionally overwhelmed.

The difference is not what information you seek. It’s how you seek it. As a bonus, you may find that you have control over more than you realize.

Notice that the difference isn’t a matter of being more or less informed. The difference is whether you’re explicitly focusing your energy on decisions that are yours to make (or influence). Obsessing over someone else’s decisions is likely to leave you feeling powerless and inundated with ambiguity (especially if the person responsible for the decision has more information than you). If you find yourself taking perspectives like (1) during a high-stress time, shifting to (2), (3), or (4) might bring you some relief. As a bonus, you may find that you have control over more than you realize.

Change the order in which you approach information

Now that you’re focused on actions and decisions, it’s time to reveal the big punchline: the strongest antidote to confirmation bias is planning your decision-making before you seek information.

Frame your decision-making in a way that prevents you from moving the goalposts after you’ve seen where the ball landed.

In other words, it’s important to frame your decision-making in a way that prevents you from moving the goalposts after you’ve seen where the ball landed. Curious to learn more? I have a general article plus a step-by-step COVID-19 decision-making guide to help you out.","           Advertisement                  Search         Log in              Search SpringerLink     Search                       Download PDF          Practical Application  Published: 21 May 2020   Using Insights from Behavioral Economics to Mitigate the Spread of COVID-19  Moslem Soofi 1 , Farid Najafi 2 & Behzad Karami-Matin 2    Applied Health Economics and Health Policy  volume 18 , pages 345 – 350 ( 2020 ) Cite this article        1560 Accesses    2 Altmetric    Metrics details          Abstract The outbreak of 2019 coronavirus disease (COVID-19) has become a public health emergency of international concern. The number of COVID-infected individuals and related deaths continues to rise rapidly. Encouraging people to adopt and sustain preventive behaviors is a central focus of public health policies that seek to mitigate the spread of COVID-19. Public health policy needs improved methods to encourage people to adhere to COVID-19-preventive behaviors. In this paper, we introduce a number of insights from behavioral economics that help explain why people may behave irrationally during the COVID-19 pandemic. In particular, present bias, status quo bias, framing effect, optimism bias, affect heuristic, and herding behavior are discussed. We hope this paper will shed light on how insights from behavioral economics can enrich public health policies and interventions in the fight against COVID-19.  FormalPara Key Points for Decision Makers Behavioral economics acknowledges that people are not the rational decision makers assumed in the standard economic theory of decision making. Finite rationality and willpower lead people to apply the rules of thumb or heuristics to make their COVID-19-related decisions rather than conducting cost-benefit analyses. Therefore, they may be biased in their COVID-19-related decisions. Behavioral economics can help policy makers identify individuals’ decision biases and use them as starting points for designing COVID-19-preventive interventions. Behavioral economics interventions can help people behave rationally and make better COVID-19-related decisions.  Introduction The outbreak of 2019 coronavirus disease (COVID-19) has become a public health emergency of international concern [ 1 ]. The number of COVID-infected individuals and related deaths continues to rise rapidly. COVID-19 is a serious threat to global health and the world economy and has caused widespread concern around the world. In the absence of approved treatments for and vaccines against COVID-19, preventive strategies and hygiene behaviors such as social distancing and stay-at-home policies, avoiding touching the face, and repeated hand washing are effective options in the fight against COVID-19 [ 2 , 3 ]. During this pandemic, encouraging people to adopt and sustain preventive behaviors is a central focus of public health policies that seek to mitigate the spread of COVID-19. Behavioral economics has recently received a great deal of attention in public policy making [ 4 ]. This field of economics uses insights from the fields of psychology, neuroscience, and cognitive sciences to explain how people’s behaviors deviate from the rational choice theory and when and why people’s short-term decisions sometimes undermine their long-term interests. The focus of this field is on better predicting and understanding people’s behaviors and choices to help formulate more effective public policies [ 5 , 6 ]. It identifies biases in the decision-making process and uses them as entry points for interventions to address particular behaviors. Behavioral economics acknowledges that people do not have infinite rationality and willpower, so they are not the rational decision makers assumed in the standard economic theory of utility maximization [ 6 , 7 ]. In addition, they have limited cognitive and computational abilities, and their decisions are not based on a complete analysis of all available information [ 6 ]. These limitations lead people to apply the rules of thumb or heuristics (i.e., mental shortcuts) to make their decisions rather than conducting cost–benefit analyses when making a decision. The heuristics are generally useful but can lead to systematic mistakes (i.e., biases) in decision making that, in turn, result in suboptimal and harmful behaviors [ 5 , 8 ]. Behavioral economics has shed new light on a range of risky and preventive health behaviors [ 9 ]. It also has considerable potential for providing a valuable perspective to better understand and explain COVID-19-related behaviors. While multiple biases are identified in the field of behavioral economics, in this paper we focus on six that tend to be particularly relevant to COVID-19-related behaviors: present bias, status quo bias, framing effect, optimism bias, affect heuristic, and herding behavior. It may provide useful insights into public health policies designed to reduce the spread of COVID-19 and may be helpful in developing and implementing interventions. Present Bias In the context of intertemporal choices, the costs and benefits of our choices occur at different points in time, that is, many daily choices are a trade-off between immediate outcomes (i.e., costs and benefit) and expected future outcomes [ 10 ]. Present bias or hyperbolic discounting is the nonlinear and nonconstant tendency of many individuals to prefer a smaller sooner pay-off over a larger future pay-off [ 11 , 12 , 13 ]. Present bias may lead to time-inconsistent preferences. An individual makes a plan for tomorrow, but once tomorrow comes they may experience a preference reversal and revise their plan. Present bias has been shown to be a significant predictor of a wide variety of health behaviors [ 14 ]. Many health behaviors involve a trade-off between immediate and future outcomes. For example, smoking has both current benefits (temporary stress relief) and future costs (increased risk of lung cancer) [ 10 ]. In the case of COVID-19, not adhering to stay-at-home policies involves a trade-off between the pleasure of going to the mall or restaurant now (current benefit) and the increased risk of contracting COVID-19 in the future (uncertain future cost). Uncertain future cost means that not every excursion outside the house would result in COVID-19 infection. Thus, myopic individuals (i.e., those with present bias), who put a greater emphasis on the here and now, are less likely to adhere to COVID-19-preventive behaviors, including staying at home and hand washing. Present bias is an explanation for why people do not behave in their own best interests and why they have difficulty adhering to preventive health behaviors such as social distancing, even when they wish to adhere [ 15 ]. Although present bias may lead to suboptimal behavioral choices, it can be used to help people adhere to COVID-19-preventive behaviors [ 12 ]. For example, reducing the current costs of adherence to social distancing may help people overcome their present bias, as even small costs could outweigh any perceived future benefits of adherence. Increasing the current benefit of adherence to social distancing, such as offering small and frequent payments now, can be useful in encouraging people to adhere to COVID-19-preventive behaviors. Such interventions involving low-cost rewards have been used as ways to increase current benefits of adherence to antiretroviral medication [ 16 ], smoking cessation [ 17 ], and weight loss [ 18 ], and they have been shown to be effective in changing behaviors. To reduce COVID-19 transmission, in the short term, providing free internet access at home, temporary suspension of loan repayments (e.g., loans provided by the government to support unemployed and uninsured people to start small businesses), and providing benefit packages for vulnerable groups should be considered in stay-at-home policies to encourage people to adhere to the policy and to increase its success rate. Status Quo Bias and Default Options Status quo bias is a disproportionate preference for the current status of options and an unwillingness to change them [ 19 , 20 ]. One reason for this is that people interpret the potential disadvantages of changing the status quo as greater than the potential benefits. This bias can be turned to the advantage of encouraging health-enhancing behaviors through the use of “nudges”. The concept of “nudge” was introduced in behavioral economics to persuade individuals to behave rationally and make better choices. Thaler and Sunstein [ 5 ] defined a nudge as “any aspect of the choice architecture that influences individuals’ decision making in a predictable way without forbidding any options or changing economic incentives.” They argued that, by improving and altering the environment in which individuals make decisions—what they call the “choice architecture”—, individuals can be influenced to make smarter choices. Choice architecture can be used to build an environment in which it is easier to make optimal health choices and more difficult to select suboptimal ones. The default option is a nudge with a powerful impact on directing the behaviors of people in ways that meet their long-term interests [ 5 ]. One of the most notable examples of the default option is organ donation. Countries with an opt-out system (consent to donate is assumed, and the default option is to donate organs) have a considerably higher rate of organ donation than countries with an opt-in system (default option is not to donate organs) [ 21 ]. Positive effects from the default option have also been reported for vaccination uptake [ 22 ] and the rate of enrollment into a diabetes management program [ 23 ]. COVID-19-prevention policies can also nudge people to engage in hygiene practices such as repeated hand washing by arranging defaults in the environment where they make COVID-19-related choices. For example, soaps with toys embedded inside improved hand washing behavior in children [ 24 ]. This example is a choice architecture (i.e., nudge) that may nudge children to wash their hands more frequently, so could be used to increase hand washing during this COVID-19 outbreak. A field experiment study in India found that the installation of low-cost soap dispensers in homes improved hand washing in peri-urban and rural households [ 25 ]. Framing Effect Framing effect refers to the fact that individuals’ choices often depend on the way the choices are described, or framed, and that these choices are often affected by whether the possible outcomes are framed in terms of the gains or the losses [ 26 ]. This concept is closely associated with loss aversion, which implies that the disutility caused by a given amount of loss is about twice the utility of gaining the same amount. For example, the statements “the odds of survival after 1 month of surgery are 90%” and “the odds of mortality within 1 month of surgery are 10%” elicit different reactions. Both statements offer the same information, but many individuals react differently to the risk of surgery when presented as a 90% chance of survival versus a 10% chance of death [ 27 ]. The framing effect has application for directing individuals toward health-promoting decisions and has been examined in a wide variety of health behaviors [ 28 ]. A health message can be framed to emphasize the benefits (i.e., gain-framed message) of performing a specific behavior or to emphasize the disadvantages (i.e., loss-framed message) of not engaging in that behavior [ 29 ]. Studies have shown that loss-framed messages are often more effective for disease-detection behaviors such as uptake of cancer screening, whereas gain-framed messages are often more effective for promoting preventive behaviors. A meta-analysis of 94 studies found that health messages framed as gains or benefits were significantly more likely to increase preventive behaviors than those framed as losses [ 29 ]. It offers a helpful perspective for framing health messages regarding COVID-19 prevention. It seems that health messages intended to encourage people to engage in COVID-19-preventive behaviors (e.g., social distancing) should be framed in terms of gains, such as “If you wash your hands properly/follow social distancing policy/adhere to the stay-at-home policy, you will increase the chances of yourself and your family having a long, healthy life.” Optimism and Overconfidence People display unrealistic optimism about their vulnerability to a wide set of negative outcomes [ 30 ] and often see themselves as being at less-than-average risk of negative outcomes. Optimism bias is people’s tendency to estimate the probability of positive future outcomes as greater than average and that of negative future outcomes as less than average [ 30 , 31 ]. This may lead people to unwittingly take extra risks with their own health and more than they would if they were aware of the objective risk of health-related behavior [ 32 ]. This can help explain a wide range of risk-taking behaviors, including health-related decisions. For example, one study revealed that smokers underestimated their risk of developing lung cancer compared with that of other smokers and even non-smokers [ 33 ]. Another study found that people with a subjective risk lower than their objective risk were more likely to support the belief that there is no risk of lung cancer if you just smoke for a few years and to believe that a large number of patients with lung cancer are cured. They were even less likely to decide to stop smoking [ 34 ]. People realize the risk of getting COVID-19 from suboptimal behaviors such as not washing hands or not adhering to social distancing but are likely to believe that they are less likely than other people or their peers to get COVID-19, even if their peers adhere to preventive practices. Providing peer comparison feedback or communicating risks accurately can be helpful for addressing optimism and overconfidence bias. In addition, priming an outcome by presenting what has happened to individuals or populations that are considered peers may persuade people to adhere to preventive behaviors [ 35 ]. For example, adolescents may become more engaged in COVID-19-prevention programs if they are aware that an adolescent celebrity contracted COVID-19. A possible explanation for this may be that the COVID-19 infection of an adolescent celebrity tends to increase the perception of individuals regarding their personal risk of getting COVID-19. Affect Heuristic Affect heuristic is a person’s tendency to judge risks and benefits based on their affect, that is, different affects can produce different risk and benefit perceptions [ 36 ]. It has been shown that individuals’ affect acts as a form of information that they refer to when deciding whether to engage in particular health behaviors [ 37 , 38 ]. In particular, when people feel positive about a behavior, they judge its risks as low and benefits as high; when they feel negative about a behavior, they judge its risks as high and benefits as low [ 39 ]. Evidence has shown that, while risk and benefit appear to be often uncorrelated or even positively correlated across harmful behaviors in the real-world context (i.e., high-risk activity appears to be highly gainful), they are often negatively correlated in individuals’ judgments and decisions (i.e., high risk is associated with low profit and vice versa) [ 36 , 39 ]. A study found that test harm information about prostate-specific antigen screening for prostate cancer and magnetic resonance imaging reduced perceived test benefits [ 40 ]. A study of how affect influences individuals’ processing of messages about risks and benefits of using autonomous artificial intelligence technology to screen for skin cancer found that integral artificial intelligence affect impacted on individuals’ perception of risk and benefits based on messages provided, which then influenced the probability of using artificial intelligence technology for health [ 41 ]. If perceptions of risk and benefit are directed by affect, the provision of benefits information will switch people’s judgment of risk and vice versa [ 39 ]. Therefore, the messages that people receive about a certain behavior become an important source of information that influences their health decisions [ 37 ]. This heuristic suggests that policy maker’s efforts to create negative feelings toward not adhering to COVID-19-preventive behaviors can increase the perceived risks associated with not adhering. For example, if an individual is told that not adhering to social distancing policy might cause them to contract COVID-19, this is predicted to cause negative feelings toward not adhering, which should, in turn, reduce the perceived benefits of not adhering to social distancing. In addition, “don’t miss the opportunity to be together at home” may be helpful for encouraging people to adhere to stay-at-home policies. A controlled trial in India showed that a scalable village-level intervention based on emotional drivers of behavior was more successful at increasing hand washing than was providing information [ 42 ]. Herding Behavior and Social Influence Social norms and the behavior of peers such as friends, family members, and colleagues affect behaviors. Herding behavior occurs when people consider a certain behavior to be good or bad based on the behavior of other people and mimic their observed behaviors [ 43 ]. This characteristic of human behavior is well-established in a number of fields, particularly economics and finance [ 44 ]. One implication of this behavior is that if a policy aims to encourage people to make a health decision, then it needs to inform individuals about the behavior of other people and their peers [ 5 ]. In a real-world experiment conducted on tax compliance in Minnesota, one of the interventions informed people that more than 90% of Minnesotans had paid their taxes; this had a significant effect on tax compliance compared with other interventions [ 5 , 45 ]. To nudge people to adhere to social distancing policy, interventions should draw attention to what other people are doing [ 5 ]. For example, telling people that “the majority of the people in your neighboring city or province are following the social distancing/stay-at-home policy” may increase adherence to social distancing policy. Conclusion We have discussed insights from behavioral economics that shed light on how to help people engage in COVID-19-preventive behaviors. This paper can improve our understanding of the decision-making biases that can be applied as entry points in public health policies and interventions for the prevention of COVID-19. They may assist policy makers in identifying novel interventions to improve decision making and behaviors related to the prevention of COVID-19. We provided some policy suggestions that may be useful in the fight against COVID-19, summarized as follows: reduce the current costs or increase the current benefits of adherence to social distancing/stay-at-home policies, arrange defaults in environments where people make COVID-19-related choices (i.e., choice architecture), design gain-framed messages for COVID-19-preventive behaviors, prime contamination with COVID-19 by presenting examples pertinent to a specific population, create negative feelings toward not adhering to COVID-19-preventive behavior, and draw individual’s attention to what other individuals are doing about COVID-19-related decisions. While many health-related behaviors have been shown to be associated with the six decision biases discussed, the degree to which they impact COVID-19-preventive behaviors has not yet been empirically investigated. Future work should examine strategies such as gain-framed messages, incentives in terms of small frequent rewards, and default options or nudges to improve interventions designed to prevent not only COVID-19 but also other communicable diseases.  References 1. World Health Organization (WHO) Emergency Committee. Statement on the second meeting of the International Health Regulations (2005) Emergency Committee regarding the outbreak of novel coronavirus (2019-nCoV). Geneva: WHO. https://www.who.int/news-room/detail/30-01-2020-statement-on-the-second-meeting-of-the-international-health-regulations-(2005)-emergency-committee-regarding-the-outbreak-of-novel-coronavirus-(2019-ncov ). Accessed 30 January 2020. 2. Chen Y, Liu Q, Guo D. Emerging coronaviruses: genome structure, replication, and pathogenesis. J Med Virol. 2020;92(4):418–23. CAS  PubMed  PubMed Central  Article  Google Scholar  3. Wilder-Smith A, Freedman D. Isolation, quarantine, social distancing and community containment: pivotal role for old-style public health measures in the novel coronavirus (2019-nCoV) outbreak. J Travel Med. 2020;27(2):taaa020. 4. Congdon WJ, Shankar M. The role of behavioral economics in evidence-based policymaking. Ann Am Acad Pol Soc Sc. 2018;678(1):81–92. Article  Google Scholar  5. Thaler RH, Sunstein CR. Nudge: improving decisions about health, wealth and happiness. New Haven: Yale University Press; 2008. Google Scholar  6. Thaler RH. Behavioral economics: past, present, and future. Am Econ Rev. 2016;106(7):1577–600. Article  Google Scholar  7. Thaler RH. From cashews to nudges: The evolution of behavioral economics. Am Econ Rev. 2018;108(6):1265–87. Article  Google Scholar  8. Tversky A, Kahneman D. Judgment under uncertainty: heuristics and biases. Science. 1974;185(4157):1124–31. CAS  PubMed  Article  PubMed Central  Google Scholar  9. Bickel WK, Moody L, Higgins ST. Some current dimensions of the behavioral economics of health-related behavior change. Prev Med. 2016;92:16–23. PubMed  PubMed Central  Article  Google Scholar  10. Van der Pol M, Cairns J. Descriptive validity of alternative intertemporal models for health outcomes: an axiomatic test. Health Econ. 2011;20(7):770–82. PubMed  Article  Google Scholar  11. Laibson D. Golden eggs and hyperbolic discounting. Q J Econ. 1997;112(2):443–78. Article  Google Scholar  12. Loewenstein G, Asch DA, Friedman JY, Melichar LA, Volpp KG. Can behavioural economics make us healthier? BMJ. 2012;344:e3482. PubMed  Article  Google Scholar  13. O'Donoghue T, Rabin M. Doing it now or later. Am Econ Rev. 1999;89(1):103–24. Article  Google Scholar  14. Soofi M, Sari AA, Rezaei S, Hajizadeh M, Najafi F. Individual time preferences and obesity: a behavioral economics analysis using a quasi-hyperbolic discounting approach. Int J Soc Econ. 2019;47(1):16–26. Article  Google Scholar  15. Van Der Pol M, Hennessy D, Manns B. The role of time and risk preferences in adherence to physician advice on health behavior change. Eur J Health Econ. 2017;18(3):373–86. PubMed  Article  Google Scholar  16. Linnemayr S, Stecher C, Mukasa B. Behavioral economic incentives to improve adherence to antiretroviral medication. AIDS (London, England). 2017;31(5):719. Article  Google Scholar  17. Halpern SD, French B, Small DS, Saulsgiver K, Harhay MO, Audrain-McGovern J, et al. Randomized trial of four financial-incentive programs for smoking cessation. N Engl J Med. 2015;372:2108–17. PubMed  PubMed Central  Article  Google Scholar  18. John LK, Loewenstein G, Troxel AB, Norton L, Fassbender JE, Volpp KG. Financial incentives for extended weight loss: a randomized, controlled trial. J Gen Intern Med. 2011;26(6):621–6. PubMed  PubMed Central  Article  Google Scholar  19. Loewenstein G, Brennan T, Volpp KG. Asymmetric paternalism to improve health behaviors. JAMA. 2007;298(20):2415–7. CAS  PubMed  Article  Google Scholar  20. Samuelson W, Zeckhauser R. Status quo bias in decision making. J Risk Uncertain. 1988;1(1):7–59. Article  Google Scholar  21. Johnson EJ, Goldstein D. Do defaults save lives?: American Association for the Advancement of Science; 2003. 22. Chapman GB, Li M, Colby H, Yoon H. Opting in vs opting out of influenza vaccination. JAMA. 2010;304(1):43–4. CAS  PubMed  Article  Google Scholar  23. Aysola J, Tahirovic E, Troxel AB, Asch DA, Gangemi K, Hodlofski AT, et al. A randomized controlled trial of opt-in versus opt-out enrollment into a diabetes behavioral intervention. Am J Health Promot. 2018;32(3):745–52. PubMed  Article  Google Scholar  24. Watson J, Dreibelbis R, Aunger R, Deola C, King K, Long S, et al. Child's play: harnessing play and curiosity motives to improve child handwashing in a humanitarian setting. Int J Hyg Environ Health. 2019;222(2):177–82. PubMed  Article  Google Scholar  25. Hussam R, Rabbani A, Reggiani G, Rigol N. Habit formation and rational addiction: a field experiment in handwashing. Harvard Business School BGIE Unit working paper. 2017(18-030). 26. Tversky A, Kahneman D. Prospect theory: an analysis of decision under risk. Econometrica. 1979;47(2):263–91. Article  Google Scholar  27. Luoto J, Carman KG. Behavioral economics guidelines with applications for health interventions. Washington: Inter-American Development Bank; 2014. Google Scholar  28. Kahneman D, Slovic SP, Slovic P, Tversky A. Judgment under uncertainty: heuristics and biases. Cambridge: Cambridge University Press; 1982. 29. Gallagher KM, Updegraff JA. Health message framing effects on attitudes, intentions, and behavior: a meta-analytic review. Ann Behav Med. 2012;43(1):101–16. PubMed  Article  Google Scholar  30. Weinstein ND. Unrealistic optimism about future life events. J Pers Soc Psychol. 1980;39(5):806. Article  Google Scholar  31. Weinstein ND. Unrealistic optimism about susceptibility to health problems: conclusions from a community-wide sample. J Behav Med. 1987;10(5):481–500. CAS  PubMed  Article  Google Scholar  32. White JS, Dow WH. Intertemporal choices for health. In: Roberto Ch A, Kawachi I, editors, Behavioral economics and public health. Oxford: Oxford University Press, 2015;27:62. Google Scholar  33. Weinstein ND, Marcus SE, Moser RP. Smokers’ unrealistic optimism about their risk. Tob control. 2005;14(1):55–9. CAS  PubMed  PubMed Central  Article  Google Scholar  34. Dillard AJ, McCaul KD, Klein WM. Unrealistic optimism in smokers: Implications for smoking myth endorsement and self-protective motivation. J Health Commun. 2006;11(S1):93–102. PubMed  Article  Google Scholar  35. Matjasko JL, Cawley JH, Baker-Goering MM, Yokum DV. Applying behavioral economics to public health policy: illustrative examples and promising directions. Am J Prev Med. 2016;50(5):S13–S1919. PubMed  PubMed Central  Article  Google Scholar  36. Finucane ML, Alhakami A, Slovic P, Johnson SM. The affect heuristic in judgments of risks and benefits. J Behav Decis Mak. 2000;13(1):1–17. Article  Google Scholar  37. Schwarz N. Feelings-as-information theory. In: Van Lange P, Kruglanski A, Higgins ET, editors. Handbook of theories of social psychology. Thousand Oaks: Sage; 2011. p. 289–308. Google Scholar  38. Peters E, Lipkus I, Diefenbach MA. The functions of affect in health communications and in the construction of health preferences. J Commun. 2006;56:S140–S162162. Article  Google Scholar  39. Slovic P, Peters E. Risk perception and affect. Curr Dir Psychol Sci. 2006;15(6):322–5. Article  Google Scholar  40. Scherer LD, Shaffer VA, Caverly T, Scherer AM, Zikmund-Fisher BJ, Kullgren JT, et al. The role of the affect heuristic and cancer anxiety in responding to negative information about medical tests. Psychol Health. 2018;33(2):292–312. PubMed  Article  Google Scholar  41. Tong ST, Sopory P. Does integral affect influence intentions to use artificial intelligence for skin cancer screening? A test of the affect heuristic. Psychol Health. 2019;34(7):828–49. PubMed  Article  Google Scholar  42. Biran A, Schmidt W-P, Varadharajan KS, Rajaraman D, Kumar R, Greenland K, et al. Effect of a behaviour-change intervention on handwashing with soap in India (SuperAmma): a cluster-randomised trial. Lancet Glob Health. 2014;2(3):e145–e154154. PubMed  Article  Google Scholar  43. Ariely D. Predictably irrational. New York: Harper Audio New York; 2008. 44. Raafat RM, Chater N, Frith C. Herding in humans. Trends Cognit Sci. 2009;13(10):420–8. Article  Google Scholar  45. Coleman S. The Minnesota income tax compliance experiment: replication of the social norms experiment. Available at SSRN 1393292; 2007. Download references Author information Affiliations Social Development and Health Promotion Research Center, Health Institute, Kermanshah University of Medical Sciences, Kermanshah, Iran Moslem Soofi Research Center for Environmental Determinants of Health, Health Institute, Kermanshah University of Medical Sciences, Kermanshah, Iran Farid Najafi & Behzad Karami-Matin Authors Moslem Soofi View author publications You can also search for this author in PubMed  Google Scholar Farid Najafi View author publications You can also search for this author in PubMed  Google Scholar Behzad Karami-Matin View author publications You can also search for this author in PubMed  Google Scholar Contributions MS contributed to the conception and design of the study and drafted the manuscript. FN and BKM contributed to critical revision of the manuscript. All authors read and approved the final manuscript. Corresponding author Correspondence to Behzad Karami-Matin . Ethics declarations  Funding  No sources of funding were used to conduct this study or prepare this manuscript.  Conflict of interest  Moslem Soofi, Farid Najafi, and Behzad Karami-Matin have no conflicts of interest that are directly relevant to the content of this article.  Rights and permissions Reprints and Permissions About this article Cite this article Soofi, M., Najafi, F. & Karami-Matin, B. Using Insights from Behavioral Economics to Mitigate the Spread of COVID-19. Appl Health Econ Health Policy  18, 345–350 (2020). https://doi.org/10.1007/s40258-020-00595-4 Download citation Published : 21 May 2020 Issue Date : June 2020 DOI : https://doi.org/10.1007/s40258-020-00595-4           Download PDF                 Advertisement                 Over 10 million scientific documents at your fingertips   Switch Edition    Academic Edition    Corporate Edition         Home  Impressum  Legal information  Privacy statement  How we use cookies  Accessibility  Contact us     Not logged in  - 89.64.25.55   Not affiliated    Springer Nature       © 2020 Springer Nature Switzerland AG. Part of Springer Nature .                      "
12,confirmation(fallacy OR bias OR heuristic) (coronavirus OR covid OR pandemic),2,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7144592/,"Access Denied

Your access to the NCBI website at www.ncbi.nlm.nih.gov has been temporarily blocked due to a possible misuse/abuse situation involving your site. This is not an indication of a security issue such as a virus or attack. It could be something as simple as a run away script or learning how to better use E-utilities, http://www.ncbi.nlm.nih.gov/books/NBK25497/, for more efficient work such that your work does not impact the ability of other researchers to also use our site. To restore access and understand how to better interact with our site to avoid this in the future, please have your system administrator contact info@ncbi.nlm.nih.gov.","           Advertisement                  Search         Log in              Search SpringerLink     Search                       Download PDF          Practical Application  Published: 21 May 2020   Using Insights from Behavioral Economics to Mitigate the Spread of COVID-19  Moslem Soofi 1 , Farid Najafi 2 & Behzad Karami-Matin 2    Applied Health Economics and Health Policy  volume 18 , pages 345 – 350 ( 2020 ) Cite this article        1560 Accesses    2 Altmetric    Metrics details          Abstract The outbreak of 2019 coronavirus disease (COVID-19) has become a public health emergency of international concern. The number of COVID-infected individuals and related deaths continues to rise rapidly. Encouraging people to adopt and sustain preventive behaviors is a central focus of public health policies that seek to mitigate the spread of COVID-19. Public health policy needs improved methods to encourage people to adhere to COVID-19-preventive behaviors. In this paper, we introduce a number of insights from behavioral economics that help explain why people may behave irrationally during the COVID-19 pandemic. In particular, present bias, status quo bias, framing effect, optimism bias, affect heuristic, and herding behavior are discussed. We hope this paper will shed light on how insights from behavioral economics can enrich public health policies and interventions in the fight against COVID-19.  FormalPara Key Points for Decision Makers Behavioral economics acknowledges that people are not the rational decision makers assumed in the standard economic theory of decision making. Finite rationality and willpower lead people to apply the rules of thumb or heuristics to make their COVID-19-related decisions rather than conducting cost-benefit analyses. Therefore, they may be biased in their COVID-19-related decisions. Behavioral economics can help policy makers identify individuals’ decision biases and use them as starting points for designing COVID-19-preventive interventions. Behavioral economics interventions can help people behave rationally and make better COVID-19-related decisions.  Introduction The outbreak of 2019 coronavirus disease (COVID-19) has become a public health emergency of international concern [ 1 ]. The number of COVID-infected individuals and related deaths continues to rise rapidly. COVID-19 is a serious threat to global health and the world economy and has caused widespread concern around the world. In the absence of approved treatments for and vaccines against COVID-19, preventive strategies and hygiene behaviors such as social distancing and stay-at-home policies, avoiding touching the face, and repeated hand washing are effective options in the fight against COVID-19 [ 2 , 3 ]. During this pandemic, encouraging people to adopt and sustain preventive behaviors is a central focus of public health policies that seek to mitigate the spread of COVID-19. Behavioral economics has recently received a great deal of attention in public policy making [ 4 ]. This field of economics uses insights from the fields of psychology, neuroscience, and cognitive sciences to explain how people’s behaviors deviate from the rational choice theory and when and why people’s short-term decisions sometimes undermine their long-term interests. The focus of this field is on better predicting and understanding people’s behaviors and choices to help formulate more effective public policies [ 5 , 6 ]. It identifies biases in the decision-making process and uses them as entry points for interventions to address particular behaviors. Behavioral economics acknowledges that people do not have infinite rationality and willpower, so they are not the rational decision makers assumed in the standard economic theory of utility maximization [ 6 , 7 ]. In addition, they have limited cognitive and computational abilities, and their decisions are not based on a complete analysis of all available information [ 6 ]. These limitations lead people to apply the rules of thumb or heuristics (i.e., mental shortcuts) to make their decisions rather than conducting cost–benefit analyses when making a decision. The heuristics are generally useful but can lead to systematic mistakes (i.e., biases) in decision making that, in turn, result in suboptimal and harmful behaviors [ 5 , 8 ]. Behavioral economics has shed new light on a range of risky and preventive health behaviors [ 9 ]. It also has considerable potential for providing a valuable perspective to better understand and explain COVID-19-related behaviors. While multiple biases are identified in the field of behavioral economics, in this paper we focus on six that tend to be particularly relevant to COVID-19-related behaviors: present bias, status quo bias, framing effect, optimism bias, affect heuristic, and herding behavior. It may provide useful insights into public health policies designed to reduce the spread of COVID-19 and may be helpful in developing and implementing interventions. Present Bias In the context of intertemporal choices, the costs and benefits of our choices occur at different points in time, that is, many daily choices are a trade-off between immediate outcomes (i.e., costs and benefit) and expected future outcomes [ 10 ]. Present bias or hyperbolic discounting is the nonlinear and nonconstant tendency of many individuals to prefer a smaller sooner pay-off over a larger future pay-off [ 11 , 12 , 13 ]. Present bias may lead to time-inconsistent preferences. An individual makes a plan for tomorrow, but once tomorrow comes they may experience a preference reversal and revise their plan. Present bias has been shown to be a significant predictor of a wide variety of health behaviors [ 14 ]. Many health behaviors involve a trade-off between immediate and future outcomes. For example, smoking has both current benefits (temporary stress relief) and future costs (increased risk of lung cancer) [ 10 ]. In the case of COVID-19, not adhering to stay-at-home policies involves a trade-off between the pleasure of going to the mall or restaurant now (current benefit) and the increased risk of contracting COVID-19 in the future (uncertain future cost). Uncertain future cost means that not every excursion outside the house would result in COVID-19 infection. Thus, myopic individuals (i.e., those with present bias), who put a greater emphasis on the here and now, are less likely to adhere to COVID-19-preventive behaviors, including staying at home and hand washing. Present bias is an explanation for why people do not behave in their own best interests and why they have difficulty adhering to preventive health behaviors such as social distancing, even when they wish to adhere [ 15 ]. Although present bias may lead to suboptimal behavioral choices, it can be used to help people adhere to COVID-19-preventive behaviors [ 12 ]. For example, reducing the current costs of adherence to social distancing may help people overcome their present bias, as even small costs could outweigh any perceived future benefits of adherence. Increasing the current benefit of adherence to social distancing, such as offering small and frequent payments now, can be useful in encouraging people to adhere to COVID-19-preventive behaviors. Such interventions involving low-cost rewards have been used as ways to increase current benefits of adherence to antiretroviral medication [ 16 ], smoking cessation [ 17 ], and weight loss [ 18 ], and they have been shown to be effective in changing behaviors. To reduce COVID-19 transmission, in the short term, providing free internet access at home, temporary suspension of loan repayments (e.g., loans provided by the government to support unemployed and uninsured people to start small businesses), and providing benefit packages for vulnerable groups should be considered in stay-at-home policies to encourage people to adhere to the policy and to increase its success rate. Status Quo Bias and Default Options Status quo bias is a disproportionate preference for the current status of options and an unwillingness to change them [ 19 , 20 ]. One reason for this is that people interpret the potential disadvantages of changing the status quo as greater than the potential benefits. This bias can be turned to the advantage of encouraging health-enhancing behaviors through the use of “nudges”. The concept of “nudge” was introduced in behavioral economics to persuade individuals to behave rationally and make better choices. Thaler and Sunstein [ 5 ] defined a nudge as “any aspect of the choice architecture that influences individuals’ decision making in a predictable way without forbidding any options or changing economic incentives.” They argued that, by improving and altering the environment in which individuals make decisions—what they call the “choice architecture”—, individuals can be influenced to make smarter choices. Choice architecture can be used to build an environment in which it is easier to make optimal health choices and more difficult to select suboptimal ones. The default option is a nudge with a powerful impact on directing the behaviors of people in ways that meet their long-term interests [ 5 ]. One of the most notable examples of the default option is organ donation. Countries with an opt-out system (consent to donate is assumed, and the default option is to donate organs) have a considerably higher rate of organ donation than countries with an opt-in system (default option is not to donate organs) [ 21 ]. Positive effects from the default option have also been reported for vaccination uptake [ 22 ] and the rate of enrollment into a diabetes management program [ 23 ]. COVID-19-prevention policies can also nudge people to engage in hygiene practices such as repeated hand washing by arranging defaults in the environment where they make COVID-19-related choices. For example, soaps with toys embedded inside improved hand washing behavior in children [ 24 ]. This example is a choice architecture (i.e., nudge) that may nudge children to wash their hands more frequently, so could be used to increase hand washing during this COVID-19 outbreak. A field experiment study in India found that the installation of low-cost soap dispensers in homes improved hand washing in peri-urban and rural households [ 25 ]. Framing Effect Framing effect refers to the fact that individuals’ choices often depend on the way the choices are described, or framed, and that these choices are often affected by whether the possible outcomes are framed in terms of the gains or the losses [ 26 ]. This concept is closely associated with loss aversion, which implies that the disutility caused by a given amount of loss is about twice the utility of gaining the same amount. For example, the statements “the odds of survival after 1 month of surgery are 90%” and “the odds of mortality within 1 month of surgery are 10%” elicit different reactions. Both statements offer the same information, but many individuals react differently to the risk of surgery when presented as a 90% chance of survival versus a 10% chance of death [ 27 ]. The framing effect has application for directing individuals toward health-promoting decisions and has been examined in a wide variety of health behaviors [ 28 ]. A health message can be framed to emphasize the benefits (i.e., gain-framed message) of performing a specific behavior or to emphasize the disadvantages (i.e., loss-framed message) of not engaging in that behavior [ 29 ]. Studies have shown that loss-framed messages are often more effective for disease-detection behaviors such as uptake of cancer screening, whereas gain-framed messages are often more effective for promoting preventive behaviors. A meta-analysis of 94 studies found that health messages framed as gains or benefits were significantly more likely to increase preventive behaviors than those framed as losses [ 29 ]. It offers a helpful perspective for framing health messages regarding COVID-19 prevention. It seems that health messages intended to encourage people to engage in COVID-19-preventive behaviors (e.g., social distancing) should be framed in terms of gains, such as “If you wash your hands properly/follow social distancing policy/adhere to the stay-at-home policy, you will increase the chances of yourself and your family having a long, healthy life.” Optimism and Overconfidence People display unrealistic optimism about their vulnerability to a wide set of negative outcomes [ 30 ] and often see themselves as being at less-than-average risk of negative outcomes. Optimism bias is people’s tendency to estimate the probability of positive future outcomes as greater than average and that of negative future outcomes as less than average [ 30 , 31 ]. This may lead people to unwittingly take extra risks with their own health and more than they would if they were aware of the objective risk of health-related behavior [ 32 ]. This can help explain a wide range of risk-taking behaviors, including health-related decisions. For example, one study revealed that smokers underestimated their risk of developing lung cancer compared with that of other smokers and even non-smokers [ 33 ]. Another study found that people with a subjective risk lower than their objective risk were more likely to support the belief that there is no risk of lung cancer if you just smoke for a few years and to believe that a large number of patients with lung cancer are cured. They were even less likely to decide to stop smoking [ 34 ]. People realize the risk of getting COVID-19 from suboptimal behaviors such as not washing hands or not adhering to social distancing but are likely to believe that they are less likely than other people or their peers to get COVID-19, even if their peers adhere to preventive practices. Providing peer comparison feedback or communicating risks accurately can be helpful for addressing optimism and overconfidence bias. In addition, priming an outcome by presenting what has happened to individuals or populations that are considered peers may persuade people to adhere to preventive behaviors [ 35 ]. For example, adolescents may become more engaged in COVID-19-prevention programs if they are aware that an adolescent celebrity contracted COVID-19. A possible explanation for this may be that the COVID-19 infection of an adolescent celebrity tends to increase the perception of individuals regarding their personal risk of getting COVID-19. Affect Heuristic Affect heuristic is a person’s tendency to judge risks and benefits based on their affect, that is, different affects can produce different risk and benefit perceptions [ 36 ]. It has been shown that individuals’ affect acts as a form of information that they refer to when deciding whether to engage in particular health behaviors [ 37 , 38 ]. In particular, when people feel positive about a behavior, they judge its risks as low and benefits as high; when they feel negative about a behavior, they judge its risks as high and benefits as low [ 39 ]. Evidence has shown that, while risk and benefit appear to be often uncorrelated or even positively correlated across harmful behaviors in the real-world context (i.e., high-risk activity appears to be highly gainful), they are often negatively correlated in individuals’ judgments and decisions (i.e., high risk is associated with low profit and vice versa) [ 36 , 39 ]. A study found that test harm information about prostate-specific antigen screening for prostate cancer and magnetic resonance imaging reduced perceived test benefits [ 40 ]. A study of how affect influences individuals’ processing of messages about risks and benefits of using autonomous artificial intelligence technology to screen for skin cancer found that integral artificial intelligence affect impacted on individuals’ perception of risk and benefits based on messages provided, which then influenced the probability of using artificial intelligence technology for health [ 41 ]. If perceptions of risk and benefit are directed by affect, the provision of benefits information will switch people’s judgment of risk and vice versa [ 39 ]. Therefore, the messages that people receive about a certain behavior become an important source of information that influences their health decisions [ 37 ]. This heuristic suggests that policy maker’s efforts to create negative feelings toward not adhering to COVID-19-preventive behaviors can increase the perceived risks associated with not adhering. For example, if an individual is told that not adhering to social distancing policy might cause them to contract COVID-19, this is predicted to cause negative feelings toward not adhering, which should, in turn, reduce the perceived benefits of not adhering to social distancing. In addition, “don’t miss the opportunity to be together at home” may be helpful for encouraging people to adhere to stay-at-home policies. A controlled trial in India showed that a scalable village-level intervention based on emotional drivers of behavior was more successful at increasing hand washing than was providing information [ 42 ]. Herding Behavior and Social Influence Social norms and the behavior of peers such as friends, family members, and colleagues affect behaviors. Herding behavior occurs when people consider a certain behavior to be good or bad based on the behavior of other people and mimic their observed behaviors [ 43 ]. This characteristic of human behavior is well-established in a number of fields, particularly economics and finance [ 44 ]. One implication of this behavior is that if a policy aims to encourage people to make a health decision, then it needs to inform individuals about the behavior of other people and their peers [ 5 ]. In a real-world experiment conducted on tax compliance in Minnesota, one of the interventions informed people that more than 90% of Minnesotans had paid their taxes; this had a significant effect on tax compliance compared with other interventions [ 5 , 45 ]. To nudge people to adhere to social distancing policy, interventions should draw attention to what other people are doing [ 5 ]. For example, telling people that “the majority of the people in your neighboring city or province are following the social distancing/stay-at-home policy” may increase adherence to social distancing policy. Conclusion We have discussed insights from behavioral economics that shed light on how to help people engage in COVID-19-preventive behaviors. This paper can improve our understanding of the decision-making biases that can be applied as entry points in public health policies and interventions for the prevention of COVID-19. They may assist policy makers in identifying novel interventions to improve decision making and behaviors related to the prevention of COVID-19. We provided some policy suggestions that may be useful in the fight against COVID-19, summarized as follows: reduce the current costs or increase the current benefits of adherence to social distancing/stay-at-home policies, arrange defaults in environments where people make COVID-19-related choices (i.e., choice architecture), design gain-framed messages for COVID-19-preventive behaviors, prime contamination with COVID-19 by presenting examples pertinent to a specific population, create negative feelings toward not adhering to COVID-19-preventive behavior, and draw individual’s attention to what other individuals are doing about COVID-19-related decisions. While many health-related behaviors have been shown to be associated with the six decision biases discussed, the degree to which they impact COVID-19-preventive behaviors has not yet been empirically investigated. Future work should examine strategies such as gain-framed messages, incentives in terms of small frequent rewards, and default options or nudges to improve interventions designed to prevent not only COVID-19 but also other communicable diseases.  References 1. World Health Organization (WHO) Emergency Committee. Statement on the second meeting of the International Health Regulations (2005) Emergency Committee regarding the outbreak of novel coronavirus (2019-nCoV). Geneva: WHO. https://www.who.int/news-room/detail/30-01-2020-statement-on-the-second-meeting-of-the-international-health-regulations-(2005)-emergency-committee-regarding-the-outbreak-of-novel-coronavirus-(2019-ncov ). Accessed 30 January 2020. 2. Chen Y, Liu Q, Guo D. Emerging coronaviruses: genome structure, replication, and pathogenesis. J Med Virol. 2020;92(4):418–23. CAS  PubMed  PubMed Central  Article  Google Scholar  3. Wilder-Smith A, Freedman D. Isolation, quarantine, social distancing and community containment: pivotal role for old-style public health measures in the novel coronavirus (2019-nCoV) outbreak. J Travel Med. 2020;27(2):taaa020. 4. Congdon WJ, Shankar M. The role of behavioral economics in evidence-based policymaking. Ann Am Acad Pol Soc Sc. 2018;678(1):81–92. Article  Google Scholar  5. Thaler RH, Sunstein CR. Nudge: improving decisions about health, wealth and happiness. New Haven: Yale University Press; 2008. Google Scholar  6. Thaler RH. Behavioral economics: past, present, and future. Am Econ Rev. 2016;106(7):1577–600. Article  Google Scholar  7. Thaler RH. From cashews to nudges: The evolution of behavioral economics. Am Econ Rev. 2018;108(6):1265–87. Article  Google Scholar  8. Tversky A, Kahneman D. Judgment under uncertainty: heuristics and biases. Science. 1974;185(4157):1124–31. CAS  PubMed  Article  PubMed Central  Google Scholar  9. Bickel WK, Moody L, Higgins ST. Some current dimensions of the behavioral economics of health-related behavior change. Prev Med. 2016;92:16–23. PubMed  PubMed Central  Article  Google Scholar  10. Van der Pol M, Cairns J. Descriptive validity of alternative intertemporal models for health outcomes: an axiomatic test. Health Econ. 2011;20(7):770–82. PubMed  Article  Google Scholar  11. Laibson D. Golden eggs and hyperbolic discounting. Q J Econ. 1997;112(2):443–78. Article  Google Scholar  12. Loewenstein G, Asch DA, Friedman JY, Melichar LA, Volpp KG. Can behavioural economics make us healthier? BMJ. 2012;344:e3482. PubMed  Article  Google Scholar  13. O'Donoghue T, Rabin M. Doing it now or later. Am Econ Rev. 1999;89(1):103–24. Article  Google Scholar  14. Soofi M, Sari AA, Rezaei S, Hajizadeh M, Najafi F. Individual time preferences and obesity: a behavioral economics analysis using a quasi-hyperbolic discounting approach. Int J Soc Econ. 2019;47(1):16–26. Article  Google Scholar  15. Van Der Pol M, Hennessy D, Manns B. The role of time and risk preferences in adherence to physician advice on health behavior change. Eur J Health Econ. 2017;18(3):373–86. PubMed  Article  Google Scholar  16. Linnemayr S, Stecher C, Mukasa B. Behavioral economic incentives to improve adherence to antiretroviral medication. AIDS (London, England). 2017;31(5):719. Article  Google Scholar  17. Halpern SD, French B, Small DS, Saulsgiver K, Harhay MO, Audrain-McGovern J, et al. Randomized trial of four financial-incentive programs for smoking cessation. N Engl J Med. 2015;372:2108–17. PubMed  PubMed Central  Article  Google Scholar  18. John LK, Loewenstein G, Troxel AB, Norton L, Fassbender JE, Volpp KG. Financial incentives for extended weight loss: a randomized, controlled trial. J Gen Intern Med. 2011;26(6):621–6. PubMed  PubMed Central  Article  Google Scholar  19. Loewenstein G, Brennan T, Volpp KG. Asymmetric paternalism to improve health behaviors. JAMA. 2007;298(20):2415–7. CAS  PubMed  Article  Google Scholar  20. Samuelson W, Zeckhauser R. Status quo bias in decision making. J Risk Uncertain. 1988;1(1):7–59. Article  Google Scholar  21. Johnson EJ, Goldstein D. Do defaults save lives?: American Association for the Advancement of Science; 2003. 22. Chapman GB, Li M, Colby H, Yoon H. Opting in vs opting out of influenza vaccination. JAMA. 2010;304(1):43–4. CAS  PubMed  Article  Google Scholar  23. Aysola J, Tahirovic E, Troxel AB, Asch DA, Gangemi K, Hodlofski AT, et al. A randomized controlled trial of opt-in versus opt-out enrollment into a diabetes behavioral intervention. Am J Health Promot. 2018;32(3):745–52. PubMed  Article  Google Scholar  24. Watson J, Dreibelbis R, Aunger R, Deola C, King K, Long S, et al. Child's play: harnessing play and curiosity motives to improve child handwashing in a humanitarian setting. Int J Hyg Environ Health. 2019;222(2):177–82. PubMed  Article  Google Scholar  25. Hussam R, Rabbani A, Reggiani G, Rigol N. Habit formation and rational addiction: a field experiment in handwashing. Harvard Business School BGIE Unit working paper. 2017(18-030). 26. Tversky A, Kahneman D. Prospect theory: an analysis of decision under risk. Econometrica. 1979;47(2):263–91. Article  Google Scholar  27. Luoto J, Carman KG. Behavioral economics guidelines with applications for health interventions. Washington: Inter-American Development Bank; 2014. Google Scholar  28. Kahneman D, Slovic SP, Slovic P, Tversky A. Judgment under uncertainty: heuristics and biases. Cambridge: Cambridge University Press; 1982. 29. Gallagher KM, Updegraff JA. Health message framing effects on attitudes, intentions, and behavior: a meta-analytic review. Ann Behav Med. 2012;43(1):101–16. PubMed  Article  Google Scholar  30. Weinstein ND. Unrealistic optimism about future life events. J Pers Soc Psychol. 1980;39(5):806. Article  Google Scholar  31. Weinstein ND. Unrealistic optimism about susceptibility to health problems: conclusions from a community-wide sample. J Behav Med. 1987;10(5):481–500. CAS  PubMed  Article  Google Scholar  32. White JS, Dow WH. Intertemporal choices for health. In: Roberto Ch A, Kawachi I, editors, Behavioral economics and public health. Oxford: Oxford University Press, 2015;27:62. Google Scholar  33. Weinstein ND, Marcus SE, Moser RP. Smokers’ unrealistic optimism about their risk. Tob control. 2005;14(1):55–9. CAS  PubMed  PubMed Central  Article  Google Scholar  34. Dillard AJ, McCaul KD, Klein WM. Unrealistic optimism in smokers: Implications for smoking myth endorsement and self-protective motivation. J Health Commun. 2006;11(S1):93–102. PubMed  Article  Google Scholar  35. Matjasko JL, Cawley JH, Baker-Goering MM, Yokum DV. Applying behavioral economics to public health policy: illustrative examples and promising directions. Am J Prev Med. 2016;50(5):S13–S1919. PubMed  PubMed Central  Article  Google Scholar  36. Finucane ML, Alhakami A, Slovic P, Johnson SM. The affect heuristic in judgments of risks and benefits. J Behav Decis Mak. 2000;13(1):1–17. Article  Google Scholar  37. Schwarz N. Feelings-as-information theory. In: Van Lange P, Kruglanski A, Higgins ET, editors. Handbook of theories of social psychology. Thousand Oaks: Sage; 2011. p. 289–308. Google Scholar  38. Peters E, Lipkus I, Diefenbach MA. The functions of affect in health communications and in the construction of health preferences. J Commun. 2006;56:S140–S162162. Article  Google Scholar  39. Slovic P, Peters E. Risk perception and affect. Curr Dir Psychol Sci. 2006;15(6):322–5. Article  Google Scholar  40. Scherer LD, Shaffer VA, Caverly T, Scherer AM, Zikmund-Fisher BJ, Kullgren JT, et al. The role of the affect heuristic and cancer anxiety in responding to negative information about medical tests. Psychol Health. 2018;33(2):292–312. PubMed  Article  Google Scholar  41. Tong ST, Sopory P. Does integral affect influence intentions to use artificial intelligence for skin cancer screening? A test of the affect heuristic. Psychol Health. 2019;34(7):828–49. PubMed  Article  Google Scholar  42. Biran A, Schmidt W-P, Varadharajan KS, Rajaraman D, Kumar R, Greenland K, et al. Effect of a behaviour-change intervention on handwashing with soap in India (SuperAmma): a cluster-randomised trial. Lancet Glob Health. 2014;2(3):e145–e154154. PubMed  Article  Google Scholar  43. Ariely D. Predictably irrational. New York: Harper Audio New York; 2008. 44. Raafat RM, Chater N, Frith C. Herding in humans. Trends Cognit Sci. 2009;13(10):420–8. Article  Google Scholar  45. Coleman S. The Minnesota income tax compliance experiment: replication of the social norms experiment. Available at SSRN 1393292; 2007. Download references Author information Affiliations Social Development and Health Promotion Research Center, Health Institute, Kermanshah University of Medical Sciences, Kermanshah, Iran Moslem Soofi Research Center for Environmental Determinants of Health, Health Institute, Kermanshah University of Medical Sciences, Kermanshah, Iran Farid Najafi & Behzad Karami-Matin Authors Moslem Soofi View author publications You can also search for this author in PubMed  Google Scholar Farid Najafi View author publications You can also search for this author in PubMed  Google Scholar Behzad Karami-Matin View author publications You can also search for this author in PubMed  Google Scholar Contributions MS contributed to the conception and design of the study and drafted the manuscript. FN and BKM contributed to critical revision of the manuscript. All authors read and approved the final manuscript. Corresponding author Correspondence to Behzad Karami-Matin . Ethics declarations  Funding  No sources of funding were used to conduct this study or prepare this manuscript.  Conflict of interest  Moslem Soofi, Farid Najafi, and Behzad Karami-Matin have no conflicts of interest that are directly relevant to the content of this article.  Rights and permissions Reprints and Permissions About this article Cite this article Soofi, M., Najafi, F. & Karami-Matin, B. Using Insights from Behavioral Economics to Mitigate the Spread of COVID-19. Appl Health Econ Health Policy  18, 345–350 (2020). https://doi.org/10.1007/s40258-020-00595-4 Download citation Published : 21 May 2020 Issue Date : June 2020 DOI : https://doi.org/10.1007/s40258-020-00595-4           Download PDF                 Advertisement                 Over 10 million scientific documents at your fingertips   Switch Edition    Academic Edition    Corporate Edition         Home  Impressum  Legal information  Privacy statement  How we use cookies  Accessibility  Contact us     Not logged in  - 89.64.25.55   Not affiliated    Springer Nature       © 2020 Springer Nature Switzerland AG. Part of Springer Nature .                      "
13,desirability of a positive event or consequence(fallacy OR bias OR heuristic) (coronavirus OR covid OR pandemic),1,https://en.wikipedia.org/wiki/Hindsight_bias,"Tendency to perceive past events as more predictable than they actually were at the time

""Hindsight"" redirects here. For other uses, see Hindsight (disambiguation)

Hindsight bias, also known as the knew-it-all-along phenomenon[1] or creeping determinism,[2] refers to the common tendency for people to perceive events that have already occurred as having been more predictable than they actually were before the events took place.[3][4] As a result, people often believe, after an event has occurred, that they would have predicted, or perhaps even would have known with a high degree of certainty, what the outcome of the event would have been, before the event occurred. Hindsight bias may cause distortions of our memories of what we knew and/or believed before an event occurred, and is a significant source of overconfidence regarding our ability to predict the outcomes of future events.[5] Examples of hindsight bias can be seen in the writings of historians describing outcomes of battles, physicians recalling clinical trials, and in judicial systems as individuals attribute responsibility on the basis of the supposed predictability of accidents.[6][7][2]

History [ edit ]

The hindsight bias, although it was not yet named, was not a new concept when it emerged in psychological research in the 1970s. In fact, it had been indirectly described numerous times by historians, philosophers, and physicians.[2] In 1973, Baruch Fischhoff attended a seminar where Paul E. Meehl stated an observation that clinicians often overestimate their ability to have foreseen the outcome of a particular case, as they claim to have known it all along.[8] Baruch, a psychology graduate student at the time, saw an opportunity in psychological research to explain these observations.[8]

In the early seventies, investigation of heuristics and biases was a large area of study in psychology, led by Amos Tversky and Daniel Kahneman.[8] Two heuristics identified by Tversky and Kahneman were of immediate importance in the development of the hindsight bias; these were the availability heuristic and the representativeness heuristic.[9] In an elaboration of these heuristics, Beyth and Fischhoff devised the first experiment directly testing the hindsight bias.[10] They asked participants to judge the likelihood of several outcomes of US president Richard Nixon's upcoming visit to Beijing (then romanized as Peking) and Moscow. Some time after president Nixon's return, participants were asked to recall (or reconstruct) the probabilities they had assigned to each possible outcome, and their perceptions of the likelihood of each outcome was greater or overestimated for events that actually had occurred.[10] This study is frequently referred to in definitions of the hindsight bias, and the title of the paper, ""I knew it would happen,"" may have contributed to the hindsight bias being interchangeable with the phrase, ""knew-it-all-along phenomenon.""

In 1975, Fischhoff developed another method for investigating the hindsight bias, which was, at the time, referred to as the ""creeping determinism hypothesis"".[2] This method involves giving participants a short story with four possible outcomes, one of which they are told is true, and are then asked to assign the likelihood of each particular outcome.[2] Participants frequently assign a higher likelihood of occurrence to whichever outcome they have been told is true.[2] Remaining relatively unmodified, this method is still used in psychological and behavioural experiments investigating aspects of the hindsight bias. Having evolved from the heuristics of Tversky and Kahneman into the creeping determinism hypothesis and finally into the hindsight bias as we now know it, the concept has many practical applications and is still at the forefront of research today. Recent studies involving the hindsight bias have investigated the effect age has on the bias, how hindsight may impact interference and confusion, and how it may affect banking and investment strategies.[11][12][13]

Factors [ edit ]

Outcome valence and intensity [ edit ]

Hindsight bias has been found to more likely to occur when the outcome of an event is negative rather than positive. [14] This is a phenomenon consistent with the general tendency for people to pay more attention to negative outcomes of events than positive outcomes. [15] In addition, hindsight bias is affected by the severity of the negative outcome. In malpractice suits, it has been found the more severe a negative outcome is, the juror's hindsight bias is more dramatic. In a perfectly objective case, the verdict would be based on the physician's standard of care instead of the outcome of the treatment; however, studies show that cases ending in severe negative outcomes (such as death) result in higher levels of hindsight bias. For example, in 1996, LaBine proposed a scenario where a psychiatric patient told a therapist that he was contemplating harming another individual. The therapist did not warn the other individual of the possible danger. Participants were each given one of three possible outcomes; the threatened individual either received no injuries, minor injuries, or serious injuries. Participants were then asked to determine if the doctor should be considered negligent. Participants in the ""serious injuries"" condition were not only more likely to rate the therapist as negligent but also rated the attack as more foreseeable. Participants in the no injuries and minor injury categories were more likely to see the therapist's actions as reasonable.[16]

Surprise [ edit ]

The role of surprise can help explain the malleability of hindsight bias. Surprise influences how the mind reconstructs pre-outcome predictions in three ways: 1. Surprise is a direct metacognitive heuristic to estimate the distance between outcome and prediction. 2. Surprise triggers a deliberate sense-making process. 3. Surprise biases this process ( the malleability of hindsight bias) by enhancing the recall of surprise-congruent information and expectancy-based hypothesis testing.[17] Pezzo's sense-making model supports two contradicting ideas of a surprising outcome. The results can show a lesser hindsight bias or possibly a reversed effect, where the individual believes the outcome wasn't a possibility at all. The outcome can also lead to the hindsight bias being magnified to have a stronger effect. The sense-making process is triggered by an initial surprise. If the sense-making process does not complete and the sensory information is not detected or coded [by the individual], the sensation is experienced as a surprise and the hindsight bias has a gradual reduction. When the sense-making process is lacking, the phenomena of reversed hindsight bias is created. Without the sense-making process being present, there is no remnant of thought about the surprise. This can lead to a sensation of not believing the outcome as a possibility.[17]

Personality [ edit ]

Along with the emotion of surprise, the personality traits of an individual affect hindsight bias. A new C model is an approach to figure out the bias and accuracy in human inferences because of their individual personality traits. This model integrates on accurate personality judgments and hindsight effects as a by-product of knowledge updating.

During the study, three processes showed potential to explain the occurrence of hindsight effects in personality judgments: 1. Changes in an individual's cue perceptions, 2. Changes in the use of more valid cues, and 3. Changes in the consistency with which an individual applies cue knowledge.

After two studies, it was clear that there were hindsight effects for each of the Big Five personality dimensions. Evidence was found that both the utilization of more valid cues and changes in cue perceptions of the individual, but not changes in the consistency with which cue knowledge is applied, account for the hindsight effects. During both of these studies, participants were presented with target pictures and were asked to judge each target's levels of the Big Five personality traits.[18]

Age [ edit ]

It is more difficult to test for hindsight bias in children than adults because the verbal methods used in experiments on adults are too complex for children to understand, let alone measure bias. Some experimental procedures have been created with visual identification to test children about their hindsight bias in a way they can grasp. Methods with visual images start by presenting a blurry image to the child that becomes clearer over time. In some conditions, the subjects know what the final object is and in others they don't. In cases where the subject knows what the object shape will become when the image is clear, they are asked to estimate the amount of time other participants of similar age will take to guess what the object is. Due to hindsight bias, the estimated times are often much lower than the actual times. This is because the participant is using their personal knowledge while making their estimate.[19]

These types of studies have presented results showing that the hindsight bias affects children as well as adults. Hindsight bias in adults and in children share a core cognitive constraint. That constraint is a tendency to be biased on one's current knowledge when, at the same time, attempting to recall or reason about a more naïve cognitive state—regardless of whether the more naïve state is one's own earlier naïve state or someone else's. Children have a theory of mind, which is their mental state of reasoning. Hindsight bias is a fundamental problem in cognitive perspective-taking. After reviewing developmental literature on hindsight bias and other limitations [of perception], it was found that some of children's limitation in the theory of mind may stem from the same core component as hindsight bias does. This key factor brings forth underlying mechanisms. A developmental approach to [hindsight bias] is necessary for a comprehensive understanding of the nature of hindsight bias in social cognition.[20]

Effects [ edit ]

Auditory distractions [ edit ]

Another topic that affects the function of hindsight bias is the auditory function of humans. To test the effects of auditory distractions on hindsight bias, four experiments were completed. Experiment one included plain words, in which low-pass filters were used to reduce the amplitude for sounds of consonants; thus making the words more degraded. In the naïve-identification task, participants were presented a warning tone before hearing the degraded words. In the hindsight estimation task, a warning tone was presented before the clear word followed by the degraded version of the word. Experiment two included words with explicit warnings of the hindsight bias. It followed the same procedure as experiment one. However, the participants were informed and asked not to complete the same error. Experiment three included full sentences of degraded words rather than individual words. Experiment four included less-degraded words in order to make the words easier to understand and identify to the participants.

By using these different techniques, it offers a different range of detection and also evaluates the ecological validity of the [expierment's] effect. In each experiment, hindsight estimates exceeded the naïve-identification rates. Therefore, knowing the identities of words caused people to overestimate others' naïve ability to identify moderately to highly degraded spoken versions of those words. People who know the outcome of an event tend to overestimate their own prior knowledge or others' naïve knowledge of the event. As a result, speakers tend to overestimate the clarity of their message while listeners tend to overestimate their understanding of ambiguous messages. This miscommunication stems from hindsight bias which then creates a feeling of inevitability. Overall, this auditory hindsight bias occurs despite people's effort to avoid it.[21]

Cognitive models [ edit ]

To understand how a person can so easily change the foundation of knowledge and belief for events after receiving new information, three cognitive models of hindsight bias have been reviewed.[22] The three models are:

SARA (Selective Activation and Reconstructive Anchoring),

RAFT (reconstruction after feedback with take the best), and

CMT (causal model theory).

SARA and RAFT focus on distortions or changes in a memory process, while CMT focuses on probability judgments of hindsight bias.

The SARA model, created by Rüdiger Pohl and associates, explains hindsight bias for descriptive information in memory and hypothetical situations.[22][23] SARA assumes that people have a set of images to draw their memories from. They suffer from the hindsight bias due to selective activation or biased sampling of that set of images. Basically, people only remember small, select amounts of information—and when asked to recall it later, use that biased image to support their own opinions about the situation. The set of images is originally processed in the brain when first experienced. When remembered, this image reactivates, and the mind can edit and alter the memory, which takes place in hindsight bias when new and correct information is presented, leading one to believe that this new information, when remembered at a later time, is the persons original memory. Due to this reactivation in the brain, a more permanent memory trace can be created. The new information acts as a memory anchor causing retrieval impairment.[24]

The RAFT model[25] explains hindsight bias with comparisons of objects. It uses knowledge-based probability then applies interpretations to those probabilities.[22] When given two choices, a person recalls the information on both topics and makes assumptions based on how reasonable they find the information. An example case is someone comparing the size of two cities. If they know one city well (e.g. because it has a popular sporting team or through personal history) and know much less about the other, their mental cues for the more popular city increase. They then ""take the best"" option in their assessment of their own probabilities. For example, they recognize a city due to knowing of its sports team, and thus they assume that that city has the highest population. ""Take the best"" refers to a cue that is viewed as most valid and becomes support for the person's interpretations. RAFT is a by-product of adaptive learning. Feedback information updates a person's knowledge base. This can lead a person to be unable to retrieve the initial information, because the information cue has been replaced by a cue that they thought was more fitting. The ""best"" cue has been replaced, and the person only remembers the answer that is most likely and believes they thought this was the best point the whole time.[22]

Both SARA and RAFT descriptions include a memory trace impairment or cognitive distortion that is caused by feedback of information and reconstruction of memory.

CMT is a non-formal theory based on work by many researchers to create a collaborative process model for hindsight bias that involves event outcomes.[22] People try to make sense of an event that has not turned out how they expected by creating causal reasoning for the starting event conditions. This can give that person the idea that the event outcome was inevitable and there was nothing that could take place to prevent it from happening. CMT can be caused by a discrepancy between a person's expectation of the event and the reality of an outcome. They consciously want to make sense of what has happened and selectively retrieve memory that supports the current outcome. This causal attribution can be motivated by wanting to feel more positive about the outcome and possibly themselves.[26]

Memory distortions [ edit ]

Hindsight bias has similarities to other memory distortions, such as misinformation effect and false autobiographical memory.[27] Misinformation effect occurs after an event is witnessed; new information received after the fact influences how the person remembers the event, and can be called post-event misinformation. This is an important issue with eyewitness testimony. False autobiographical memory takes place when suggestions or additional outside information is provided to distort and change memory of events; this can also lead to false memory syndrome. At times this can lead to creation of new memories that are completely false and have not taken place.

All three of these memory distortions contain a three-stage procedure.[27] The details of each procedure are different, but all three can result in some psychological manipulation and alteration of memory. Stage one is different between the three paradigms, although all involve an event, an event that has taken place (misinformation effect), an event that has not taken place (false autobiographical memory), and a judgment made by a person about an event that must be remembered (hindsight bias). Stage two consists of more information that is received by the person after the event has taken place. The new information given in hindsight bias is correct and presented upfront to the person, while the extra information for the other two memory distortions is wrong and presented in an indirect and possibly manipulative way. The third stage consists of recalling the starting information. The person must recall the original information with hindsight bias and misinformation effect, while a person that has a false autobiographical memory is expected to remember the incorrect information as a true memory.[27]

Cavillo (2013) tested whether there is a relationship between the amount of time the experimenters gave the participants to respond and their level of bias when recalling their initial judgements. The results showed that there is in fact a relationship; the hindsight bias index was greater among the participants asked to respond rapidly than among participants allowed more time to respond.[28]

Distortions of autobiographical memory produced by hindsight bias have also been used as a tool to study changes in students’ beliefs about paranormal phenomena after taking a university level skepticism course. In a study by Kane (2010),[29] students in Kane's skepticism class rated their level of belief in a variety of paranormal phenomena at both the beginning and at the end of the course. At the end of the course they also rated what they remembered their level of belief had been at the beginning of the course. The critical finding was that, not only did students reduce their average level of belief in paranormal phenomena by the end of the course, they also – at the end of the course – falsely remembered the level of belief they held at the beginning of the course, falsely remembering a much lower level of belief than had really been the case. It is the latter finding that is a reflection of the operation of hindsight bias.[30]

To create a false autobiographical memory, the person must believe a memory that is not real. To seem real, the information must be influenced by their own personal judgments. There is no real episode of an event to remember, so this memory construction must be logical to that person's knowledge base. Hindsight bias and misinformation effect recall a specific time and event; this is called an episodic memory process.[27] These two memory distortions both use memory-based mechanisms that involve a memory trace that has been changed. Hippocampus activation takes place when an episodic memory is recalled.[31] The memory is then available for alteration by new information. The person believes that the remembered information is the original memory trace, not an altered memory. This new memory is made from accurate information, and therefore the person does not have much motivation to admit that they were wrong originally by remembering the original memory. This can lead to motivated forgetting.

Motivated forgetting [ edit ]

Following a negative outcome of a situation, people do not want to accept responsibility. Instead of accepting their role in the event, they might either view themselves as caught up in a situation that was unforeseeable with them therefore not being the culprits (this is referred to as defensive processing) or view the situation as inevitable with there therefore being nothing that could have been done to prevent it (this is retroactive pessimism).[32] Defensive processing involves less hindsight bias, as they are playing ignorant of the event. Retroactive pessimism makes use of hindsight bias after a negative, unwanted outcome. Events in life can be hard to control or predict. It is no surprise that people want to view themselves in a more positive light and do not want to take responsibility for situations they could have altered. This leads to hindsight bias in the form of retroactive pessimism to inhibit upward counterfactual thinking, instead interpreting the outcome as succumbing to an inevitable fate.[33]

This memory inhibition that prevents a person from recalling what really happened may lead to failure to accept mistakes, and therefore may make someone unable to learn and grow to prevent repeating the mistake.[32] Hindsight bias can also lead to overconfidence in decisions without considering other options.[7] Such people see themselves as persons who remember correctly, even though they are just forgetting that they were wrong. Avoiding responsibility is common among the human population. Examples are discussed below to show the regularity and severity of hindsight bias in society.

Consequences [ edit ]

Hindsight bias has both positive and negative consequences. The bias's also play a role in the process of decision-making within the medical field.

Positive [ edit ]

Positive consequences of hindsight bias is an increase in one's confidence and performance, as long as the bias distortion is reasonable and does not create overconfidence. Another positive consequence is that one's self-assurance of their knowledge and decision-making, even if it ends up being a poor decision, can be beneficial to others; allowing others to experience new things or to learn from those who made the poor decisions.[34]

Negative [ edit ]

Hindsight bias decreases one's rational thinking because of when a person experiences strong emotions, which in turn decreases rational thinking. Another negative consequence of hindsight bias is the interference of one's ability to learn from experience, as a person is unable to look back on past decisions and learn from mistakes. A third consequence is a decrease in sensitivity toward a victim by the person who caused the wrongdoing. The person demoralizes the victim and does not allow for a correction of behaviors and actions.[34]

Medical decision-making [ edit ]

Hindsight bias may lead to overconfidence and malpractice in regards to doctors. Hindsight bias and overconfidence is often attributed to the number of years of experience the doctor has. After a procedure, doctors may have a “knew it the whole time” attitude, when in reality they may not have actually known it. In an effort to avoid hindsight bias, doctors use a computer-based decision support system that help the doctor diagnose and treat their patients correctly and accurately.[35]

Visual hindsight bias [ edit ]

Hindsight bias has also been found to affect judgments regarding the perception of visual stimuli, an effect referred to as the “I saw it all along” phenomenon.[36] This effect has been demonstrated experimentally [37] by presenting participants with initially very blurry images of celebrities. Participants then viewed the images as the images resolved to full clarity (Phase 1). Following Phase 1, participants predicted the level of blur at which a peer would be able to make an accurate identification of each celebrity. It was found that, now that the identity of the celebrities in each image was known, participants significantly overestimated the ease with which others would be able to identify the celebrities when the images were blurry.

The phenomenon of visual hindsight bias has important implications for a form of malpractice litigation that occurs in the field of radiology.[38][37] Typically, in these cases, a radiologist is charged with having failed to detect the presence of an abnormality that was actually present in a radiology image. During litigation, a different radiologist – who now knows that the image contains an abnormality – is asked to judge how likely it would be for a naive radiologist to have detected the abnormality during the initial reading of the image. This kind of judgment directly parallels the judgments made in hindsight bias studies. Consistent with the hindsight bias literature, it has been found that abnormalities are, in fact, more easily detected in hindsight than foresight.[39] In the absence of controls for hindsight bias, testifying radiologists may overestimate the ease with which the abnormality would have been detected in foresight.[37]

Attempts to decrease [ edit ]

Research suggests that people still exhibit the hindsight bias even when they are aware of it or possess the intention of eradicating it.[40] There is no solution to eliminate hindsight bias in its totality, but only ways to reduce it.[7] Some of which include considering alternative explanations or opening one's mind to different perspectives. In terms of auditory communication, the speaker would try to provide more clarity in his or her delivery and the listener may seek greater clarification.[22]

The only observable way to decrease hindsight bias in testing is to have the participant think about how alternative hypotheses could be correct. As a result, the participant doubts the correct hypothesis and reports that he or she would not have chosen it.

Given the fact that researchers' attempts to eliminate hindsight bias in its entirety have failed, some believe there is a possible combination of motivational and automatic processes in cognitive reconstruction.[41] Incentive prompts participants to use more effort to recover even the weak memory traces. This idea supports the causal model theory and the use of sense-making to understand event outcomes.[22]

Mental illness [ edit ]

Schizophrenia [ edit ]

Schizophrenia is an example of a disorder that directly affects the hindsight bias. Individuals with schizophrenia are more strongly affected by the hindsight bias than are individuals from the general public.[42]

The hindsight bias effect is a paradigm that demonstrates how recently acquired knowledge influences the recollection of past information. Recently acquired knowledge has a strange but strong influence on schizophrenic individuals in relation to information previously learned. New information combined with rejection of past memories can disconfirm behavior and delusional belief, which is typically found in patients suffering from schizophrenia.[42] This can cause faulty memory, which can lead to hindsight thinking and believing in knowing something they don't.[42] Delusion-prone individuals suffering from schizophrenia can falsely jump to conclusions.[43] Jumping to conclusions can lead to hindsight, which strongly influences the delusional conviction in individuals with schizophrenia.[43] In numerous studies, cognitive functional deficits in schizophrenic individuals impair their ability to represent and uphold contextual processing.[44]

Post-traumatic stress disorder [ edit ]

Post-traumatic stress disorder (PTSD) is the re-experiencing and avoidance of trauma-related stressors, emotions, and memories from a past event or events that has cognitive dramatizing impact on an individual.[45] PTSD can be attributed to the functional impairment of the prefrontal cortex (PFC) structure. Dysfunctions of cognitive processing of context and abnormalities that PTSD patients suffer from can affect hindsight thinking, such as in combat soldiers perceiving they could have altered outcomes of events in war.[46] The PFC and dopamine systems are parts of the brain that can be responsible for the impairment in cognitive control processing of context information. The PFC is well known for controlling the thought process in hindsight bias that something will happen when it evidently does not. Brain impairment in certain brain regions can also affect the thought process of an individual who may engage in hindsight thinking.[47]

Cognitive flashbacks and other associated features from a traumatic event can trigger severe stress and negative emotions such as unpardonable guilt. For example, studies were done on trauma-related guilt characteristics of war veterans with chronic PTSD 8.[48] Although there has been limited research, significant data suggests that hindsight bias has an effect on war veterans' personal perception of wrongdoing, in terms of guilt and responsibility from traumatic events of war. They blame themselves, and, in hindsight, perceive that they could have prevented what happened.

Examples [ edit ]

Health care system [ edit ]

Accidents are prone to happen in any human undertaking, but accidents occurring within the healthcare system seem more salient and severe due to their profound effect on the lives of those involved, sometimes resulting in the death of a patient. In the healthcare system, there are a number of methods in which specific cases where accidents happened are reviewed by others who already know the outcome of the case. These methods include morbidity and mortality conferences, autopsies, case analysis, medical malpractice claims analysis, staff interviews, and even patient observation. Hindsight bias has been shown to cause difficulties in measuring errors in these cases.[49] Many of these errors are considered preventable after the fact, clearly indicating the presence and importance of a hindsight bias in this field. There are two sides of debate in how these case reviews should be approached to best evaluate past cases: the error elimination strategy and the safety management strategy.[2] The error elimination strategy aims to find the cause of errors, relying heavily on hindsight (therefore more subject to the hindsight bias).[2] The safety management strategy relies less on hindsight (less subject to hindsight bias) and identifies possible constraints during the decision making process of that case. However, it is not immune to error.[2]

Judicial system [ edit ]

Hindsight bias results in being held to a higher standard in court. The defense is particularly susceptible to these effects, since their actions are the ones being scrutinized by the jury. Due to the hindsight bias, defendants are judged as capable of preventing the bad outcome.[50] Although much stronger for the defendants, hindsight bias also affects the plaintiffs. In cases where there is an assumption of risk, hindsight bias may contribute to the jurors perceiving the event as riskier due to the poor outcome. This may lead the jury to feel that the plaintiff should have exercised greater caution in the situation. Both of these effects can be minimized if attorneys put the jury in a position of foresight rather than hindsight through the use of language and timelines. Judges and juries are likely to mistakenly view negative events as being more foreseeable than what it really was in the moment, when looking at the situation after the fact in court.[51] Encouraging people to explicitly think about the counterfactuals was an effective means of reducing the hindsight bias.[52] In other words, people became less attached to the actual outcome and were more open to consider alternative lines of reasoning prior to the event. Judges involved in fraudulent transfer litigation cases were subject to the hindsight bias as well, resulting in an unfair advantage for the plaintiff,[53] showing that jurors are not the only ones sensitive to the effects of the hindsight bias in the courtroom.

Wikipedia [ edit ]

Since hindsight leads people to focus on information that is consistent with what happened while inconsistent information is ignored or regarded as less relevant,[54][55] it is likely included in representations about the past as well. In a study with Wikipedia articles[56] the latest article versions before the event (foresight article versions) were compared to two hindsight article versions: the first online after the event took place and another one eight weeks later. In order to be able to investigate various types of events, even including disasters (e.g., the nuclear disaster of Fukushima), for which foresight articles do not exist, the authors made use of articles about the structure that suffered damage in those instances (e.g., the article about the nuclear power plant of Fukushima). When analyzing to what extent the articles were suggestive of a particular event, they only found articles about disasters to be much more suggestive of the disaster in hindsight than in foresight – indicating hindsight bias. For the remaining event categories, however, Wikipedia articles did not show any hindsight bias. In an attempt to compare individuals' and Wikipedia's hindsight bias more directly, another study[57] came to the conclusion that Wikipedia articles are less susceptible to hindsight bias than individuals' representations.

See also [ edit ]

Curse of knowledge – Cognitive bias of assuming that others have the same background to understand

Egg of Columbus

Historian's fallacy – Assumption that decision makers of the past viewed events from the same perspective and having the same information as those subsequently analyzing the decision

Memory conformity – Phenomenon where memories or information reported by others influences an individual and is incorporated into their memory.

References [ edit ]","          Hindsight bias   From Wikipedia, the free encyclopedia    Jump to navigation  Jump to search  Tendency to perceive past events as more predictable than they actually were at the time  ""Hindsight"" redirects here. For other uses, see Hindsight (disambiguation) .   Hindsight bias , also known as the knew-it-all-along phenomenon [1] or creeping determinism , [2] refers to the common tendency for people to perceive events that have already occurred as having been more predictable than they actually were before the events took place. [3] [4] As a result, people often believe, after an event has occurred, that they would have predicted, or perhaps even would have known with a high degree of certainty, what the outcome of the event would have been, before the event occurred. Hindsight bias may cause distortions of our memories of what we knew and/or believed before an event occurred, and is a significant source of overconfidence regarding our ability to predict the outcomes of future events. [5] Examples of hindsight bias can be seen in the writings of historians describing outcomes of battles, physicians recalling clinical trials, and in judicial systems as individuals attribute responsibility on the basis of the supposed predictability of accidents. [6] [7] [2]   Contents   1  History  2  Factors   2.1  Outcome valence and intensity  2.2  Surprise  2.3  Personality  2.4  Age    3  Effects   3.1  Auditory distractions  3.2  Cognitive models  3.3  Memory distortions  3.4  Motivated forgetting    4  Consequences   4.1  Positive  4.2  Negative  4.3  Medical decision-making    5  Visual hindsight bias  6  Attempts to decrease  7  Mental illness   7.1  Schizophrenia  7.2  Post-traumatic stress disorder    8  Examples   8.1  Health care system  8.2  Judicial system  8.3  Wikipedia    9  See also  10  References  11  Further reading    History [ edit ]  The hindsight bias, although it was not yet named, was not a new concept when it emerged in psychological research in the 1970s. In fact, it had been indirectly described numerous times by historians, philosophers , and physicians. [2] In 1973, Baruch Fischhoff attended a seminar where Paul E. Meehl stated an observation that clinicians often overestimate their ability to have foreseen the outcome of a particular case, as they claim to have known it all along. [8] Baruch, a psychology graduate student at the time, saw an opportunity in psychological research to explain these observations. [8]    Daniel Kahneman  In the early seventies, investigation of heuristics and biases was a large area of study in psychology, led by Amos Tversky and Daniel Kahneman . [8] Two heuristics identified by Tversky and Kahneman were of immediate importance in the development of the hindsight bias; these were the availability heuristic and the representativeness heuristic . [9] In an elaboration of these heuristics, Beyth and Fischhoff devised the first experiment directly testing the hindsight bias. [10] They asked participants to judge the likelihood of several outcomes of US president  Richard Nixon 's upcoming visit to Beijing (then romanized as Peking ) and Moscow . Some time after president Nixon's return, participants were asked to recall (or reconstruct) the probabilities they had assigned to each possible outcome, and their perceptions of the likelihood of each outcome was greater or overestimated for events that actually had occurred. [10] This study is frequently referred to in definitions of the hindsight bias, and the title of the paper, ""I knew it would happen,"" may have contributed to the hindsight bias being interchangeable with the phrase, ""knew-it-all-along phenomenon."" In 1975, Fischhoff developed another method for investigating the hindsight bias, which was, at the time, referred to as the ""creeping determinism hypothesis"". [2] This method involves giving participants a short story with four possible outcomes, one of which they are told is true, and are then asked to assign the likelihood of each particular outcome. [2] Participants frequently assign a higher likelihood of occurrence to whichever outcome they have been told is true. [2] Remaining relatively unmodified, this method is still used in psychological and behavioural experiments investigating aspects of the hindsight bias. Having evolved from the heuristics of Tversky and Kahneman into the creeping determinism hypothesis and finally into the hindsight bias as we now know it, the concept has many practical applications and is still at the forefront of research today. Recent studies involving the hindsight bias have investigated the effect age has on the bias, how hindsight may impact interference and confusion, and how it may affect banking and investment strategies. [11] [12] [13]   Factors [ edit ]  This section may require cleanup to meet Wikipedia's quality standards . The specific problem is: unstructured Please help improve this section if you can.  ( March 2013 ) ( Learn how and when to remove this template message )  Outcome valence and intensity [ edit ]  Hindsight bias has been found to more likely to occur when the outcome of an event is negative rather than positive. [14] This is a phenomenon consistent with the general tendency for people to pay more attention to negative outcomes of events than positive outcomes. [15] In addition, hindsight bias is affected by the severity of the negative outcome. In malpractice suits, it has been found the more severe a negative outcome is, the  juror's hindsight bias is more dramatic. In a perfectly objective case, the verdict would be based on the physician's standard of care instead of the outcome of the treatment; however, studies show that cases ending in severe negative outcomes (such as death) result in higher levels of hindsight bias. For example, in 1996, LaBine proposed a scenario where a psychiatric patient told a therapist that he was contemplating harming another individual. The therapist did not warn the other individual of the possible danger. Participants were each given one of three possible outcomes; the threatened individual either received no injuries, minor injuries, or serious injuries. Participants were then asked to determine if the doctor should be considered negligent. Participants in the ""serious injuries"" condition were not only more likely to rate the therapist as negligent but also rated the attack as more foreseeable. Participants in the no injuries and minor injury categories were more likely to see the therapist's actions as reasonable. [16]   Surprise [ edit ]  The role of surprise can help explain the malleability of hindsight bias. Surprise influences how the mind reconstructs pre-outcome predictions in three ways:
1. Surprise is a direct metacognitive heuristic to estimate the distance between outcome and prediction.
2. Surprise triggers a deliberate sense-making process .
3. Surprise biases this process ( the malleability of hindsight bias) by enhancing the recall of surprise-congruent information and expectancy-based hypothesis testing. [17] Pezzo's sense-making model supports two contradicting ideas of a surprising outcome. The results can show a lesser hindsight bias or possibly a reversed effect, where the individual believes the outcome wasn't a possibility at all.  The outcome can also lead to the hindsight bias being magnified to have a stronger effect. The sense-making process is triggered by an initial surprise. If the sense-making process does not complete and the sensory information is not detected or coded [by the individual], the sensation is experienced as a surprise and the hindsight bias has a gradual reduction. When the sense-making process is lacking, the phenomena of reversed hindsight bias is created. Without the sense-making process being present, there is no remnant of thought about the surprise.  This can lead to a sensation of not believing the outcome as a possibility. [17]   Personality [ edit ]  Along with the emotion of surprise, the personality traits of an individual affect hindsight bias. A new C model is an approach to figure out the bias and accuracy in human inferences because of their individual personality traits. This model integrates on accurate personality judgments and hindsight effects as a by-product of knowledge updating. During the study, three processes showed potential to explain the occurrence of hindsight effects in personality judgments:
1. Changes in an individual's cue perceptions,
2. Changes in the use of more valid cues, and
3. Changes in the consistency with which an individual applies cue knowledge. After two studies, it was clear that there were hindsight effects for each of the Big Five personality dimensions . Evidence was found that both the utilization of more valid cues and changes in cue perceptions of the individual, but not changes in the consistency with which cue knowledge is applied, account for the hindsight effects. During both of these studies, participants were presented with target pictures and were asked to judge each target's levels of the Big Five personality traits. [18]   Age [ edit ]  It is more difficult to test for hindsight bias in children than adults because the verbal methods used in experiments on adults are too complex for children to understand, let alone measure bias. Some experimental procedures have been created with visual identification to test children about their hindsight bias in a way they can grasp. Methods with visual images start by presenting a blurry image to the child that becomes clearer over time. In some conditions, the subjects know what the final object is and in others they don't. In cases where the subject knows what the object shape will become when the image is clear, they are asked to estimate the amount of time other participants of similar age will take to guess what the object is. Due to hindsight bias, the estimated times are often much lower than the actual times.  This is because the participant is using their personal knowledge while making their estimate. [19]  These types of studies have presented results showing that the hindsight bias affects children as well as adults. Hindsight bias in adults and in children share a core cognitive constraint. That constraint is a tendency to be biased on one's current knowledge when, at the same time, attempting to recall or reason about a more naïve cognitive state—regardless of whether the more naïve state is one's own earlier naïve state or someone else's. Children have a theory of mind , which is their mental state of reasoning. Hindsight bias is a fundamental problem in cognitive perspective-taking. After reviewing developmental literature on hindsight bias and other limitations [of perception], it was found that some of children's limitation in the theory of mind may stem from the same core component as hindsight bias does. This key factor brings forth underlying mechanisms. A developmental approach to [hindsight bias] is necessary for a comprehensive understanding of the nature of hindsight bias in social cognition. [20]   Effects [ edit ]  Auditory distractions [ edit ]  Another topic that affects the function of hindsight bias is the auditory function of humans. To test the effects of auditory distractions on hindsight bias, four experiments were completed. 
Experiment one included plain words, in which low-pass filters were used to reduce the amplitude for sounds of consonants; thus making the words more degraded. In the naïve-identification task, participants were presented a warning tone before hearing the degraded words. In the hindsight estimation task, a warning tone was presented before the clear word followed by the degraded version of the word. 
Experiment two included words with explicit warnings of the hindsight bias. It followed the same procedure as experiment one.  However, the participants were informed and asked not to complete the same error. 
Experiment three included full sentences of degraded words rather than individual words. Experiment four included less-degraded words in order to make the words easier to understand and identify to the participants. By using these different techniques, it offers a different range of detection and also evaluates the ecological validity of the [expierment's] effect. In each experiment, hindsight estimates exceeded the naïve-identification rates. Therefore, knowing the identities of words caused people to overestimate others' naïve ability to identify moderately to highly degraded spoken versions of those words. People who know the outcome of an event tend to overestimate their own prior knowledge or others' naïve knowledge of the event. As a result, speakers tend to overestimate the clarity of their message while listeners tend to overestimate their understanding of ambiguous messages. This miscommunication stems from hindsight bias which then creates a feeling of inevitability. Overall, this auditory hindsight bias occurs despite people's effort to avoid it. [21]   Cognitive models [ edit ]  To understand how a person can so easily change the foundation of knowledge and belief for events after receiving new information, three cognitive models of hindsight bias have been reviewed. [22] The three models are:  SARA (Selective Activation and Reconstructive Anchoring),  RAFT (reconstruction after feedback with take the best), and  CMT (causal model theory).  SARA and RAFT focus on distortions or changes in a memory process, while CMT focuses on probability judgments of hindsight bias. The SARA model, created by Rüdiger Pohl and associates, explains hindsight bias for descriptive information in memory and hypothetical situations. [22] [23] SARA assumes that people have a set of images to draw their memories from. They suffer from the hindsight bias due to selective activation or biased sampling of that set of images. Basically, people only remember small, select amounts of information—and when asked to recall it later, use that biased image to support their own opinions about the situation. The set of images is originally processed in the brain when first experienced. When remembered, this image reactivates, and the mind can edit and alter the memory, which takes place in hindsight bias when new and correct information is presented, leading one to believe that this new information, when remembered at a later time, is the persons original memory. Due to this reactivation in the brain, a more permanent memory trace can be created. The new information acts as a memory anchor causing retrieval impairment. [24]  The RAFT model [25] explains hindsight bias with comparisons of objects. It uses knowledge-based probability then applies interpretations to those probabilities. [22] When given two choices, a person recalls the information on both topics and makes assumptions based on how reasonable they find the information. An example case is someone comparing the size of two cities. If they know one city well (e.g. because it has a popular sporting team or through personal history) and know much less about the other, their mental cues for the more popular city increase. They then ""take the best"" option in their assessment of their own probabilities. For example, they recognize a city due to knowing of its sports team, and thus they assume that that city has the highest population. ""Take the best"" refers to a cue that is viewed as most valid and becomes support for the person's interpretations. RAFT is a by-product of adaptive learning . Feedback information updates a person's knowledge base. This can lead a person to be unable to retrieve the initial information, because the information cue has been replaced by a cue that they thought was more fitting. The ""best"" cue has been replaced, and the person only remembers the answer that is most likely and believes they thought this was the best point the whole time. [22]  Both SARA and RAFT descriptions include a memory trace impairment or cognitive distortion that is caused by feedback of information and reconstruction of memory. CMT is a non-formal theory based on work by many researchers to create a collaborative process model for hindsight bias that involves event outcomes. [22] People try to make sense of an event that has not turned out how they expected by creating causal reasoning for the starting event conditions. This can give that person the idea that the event outcome was inevitable and there was nothing that could take place to prevent it from happening. CMT can be caused by a discrepancy between a person's expectation of the event and the reality of an outcome. They consciously want to make sense of what has happened and selectively retrieve memory that supports the current outcome. This causal attribution can be motivated by wanting to feel more positive about the outcome and possibly themselves. [26]   Memory distortions [ edit ]  Hindsight bias has similarities to other memory distortions, such as misinformation effect and false autobiographical memory . [27] Misinformation effect occurs after an event is witnessed; new information received after the fact influences how the person remembers the event, and can be called post-event misinformation. This is an important issue with eyewitness testimony . False autobiographical memory takes place when suggestions or additional outside information is provided to distort and change memory of events; this can also lead to false memory syndrome . At times this can lead to creation of new memories that are completely false and have not taken place. All three of these memory distortions contain a three-stage procedure. [27] The details of each procedure are different, but all three can result in some psychological manipulation and alteration of memory. Stage one is different between the three paradigms , although all involve an event, an event that has taken place (misinformation effect), an event that has not taken place (false autobiographical memory), and a judgment made by a person about an event that must be remembered (hindsight bias). Stage two consists of more information that is received by the person after the event has taken place. The new information given in hindsight bias is correct and presented upfront to the person, while the extra information for the other two memory distortions is wrong and presented in an indirect and possibly manipulative way. The third stage consists of recalling the starting information. The person must recall the original information with hindsight bias and misinformation effect, while a person that has a false autobiographical memory is expected to remember the incorrect information as a true memory. [27]  Cavillo (2013) tested whether there is a relationship between the amount of time the experimenters gave the participants to respond and their level of bias when recalling their initial judgements. The results showed that there is in fact a relationship; the hindsight bias index was greater among the participants asked to respond rapidly than among participants allowed more time to respond. [28]  Distortions of autobiographical memory produced by hindsight bias have also been used as a tool to study changes in students’ beliefs about paranormal phenomena after taking a university level skepticism course.  In a study by Kane (2010), [29] students in Kane's skepticism class rated their level of belief in a variety of paranormal phenomena at both the beginning and at the end of the course.  At the end of the course they also rated what they remembered their level of belief had been at the beginning of the course.  The critical finding was that, not only did students reduce their average level of belief in paranormal phenomena by the end of the course, they also – at the end of the course – falsely remembered the level of belief they held at the beginning of the course, falsely remembering a much lower level of belief than had really been the case.  It is the latter finding that is a reflection of the operation of hindsight bias. [30]  To create a false autobiographical memory, the person must believe a memory that is not real. To seem real, the information must be influenced by their own personal judgments. There is no real episode of an event to remember, so this memory construction must be logical to that person's knowledge base. Hindsight bias and misinformation effect recall a specific time and event; this is called an episodic memory process. [27] These two memory distortions both use memory-based mechanisms that involve a memory trace that has been changed. Hippocampus activation takes place when an episodic memory is recalled. [31] The memory is then available for alteration by new information. The person believes that the remembered information is the original memory trace, not an altered memory. This new memory is made from accurate information, and therefore the person does not have much motivation to admit that they were wrong originally by remembering the original memory. This can lead to motivated forgetting .  Motivated forgetting [ edit ]  Main article: Motivated forgetting  Following a negative outcome of a situation, people do not want to accept responsibility . Instead of accepting their role in the event, they might either view themselves as caught up in a situation that was unforeseeable with them therefore not being the culprits (this is referred to as defensive processing) or view the situation as inevitable with there therefore being nothing that could have been done to prevent it (this is retroactive pessimism). [32] Defensive processing involves less hindsight bias, as they are playing ignorant of the event. Retroactive pessimism makes use of hindsight bias after a negative, unwanted outcome. Events in life can be hard to control or predict. It is no surprise that people want to view themselves in a more positive light and do not want to take responsibility for situations they could have altered. This leads to hindsight bias in the form of retroactive pessimism to inhibit upward counterfactual thinking , instead interpreting the outcome as succumbing to an inevitable fate. [33]  This memory inhibition that prevents a person from recalling what really happened may lead to failure to accept mistakes, and therefore may make someone unable to learn and grow to prevent repeating the mistake. [32] Hindsight bias can also lead to overconfidence in decisions without considering other options. [7] Such people see themselves as persons who remember correctly, even though they are just forgetting that they were wrong. Avoiding responsibility is common among the human population. Examples are discussed below to show the regularity and severity of hindsight bias in society.  Consequences [ edit ]  Hindsight bias has both positive and negative consequences. The bias's also play a role in the process of decision-making within the medical field.  Positive [ edit ]  Positive consequences of hindsight bias is an increase in one's confidence and performance, as long as the bias distortion is reasonable and does not create overconfidence. Another positive consequence is that one's self-assurance of their knowledge and decision-making, even if it ends up being a poor decision, can be beneficial to others; allowing others to experience new things or to learn from those who made the poor decisions. [34]   Negative [ edit ]  Hindsight bias decreases one's rational thinking because of when a person experiences strong emotions, which in turn decreases rational thinking. Another negative consequence of hindsight bias is the interference of one's ability to learn from experience, as a person is unable to look back on past decisions and learn from mistakes. A third consequence is a decrease in sensitivity toward a victim by the person who caused the wrongdoing. The person demoralizes the victim and does not allow for a correction of behaviors and actions. [34]   Medical decision-making [ edit ]  Hindsight bias may lead to overconfidence and malpractice in regards to doctors. Hindsight bias and overconfidence is often attributed to the number of years of experience the doctor has. After a procedure, doctors may have a “knew it the whole time” attitude, when in reality they may not have actually known it. In an effort to avoid hindsight bias, doctors use a computer-based decision support system that help the doctor diagnose and treat their patients correctly and accurately. [35]   Visual hindsight bias [ edit ]  Hindsight bias has also been found to affect judgments regarding the perception of visual stimuli, an effect referred to as the “I saw it all along” phenomenon. [36] This effect  has been demonstrated experimentally [37] by presenting participants with initially very blurry images of celebrities.  Participants then viewed the images as the images resolved to full clarity (Phase 1). Following Phase 1, participants predicted the level of blur at which a peer would be able to make an accurate identification of each celebrity. It was found that, now that the identity of the celebrities in each image was known, participants significantly overestimated the ease with which others would be able to identify the celebrities when the images were blurry. The phenomenon of visual hindsight bias has important implications for a form of malpractice litigation that occurs in the field of radiology. [38] [37] Typically, in these cases, a radiologist is charged with having failed to detect the presence of an abnormality that was actually present in a radiology image.  During litigation, a different radiologist – who now knows that the image contains an abnormality – is asked to judge how likely it would be for a naive radiologist to have detected the abnormality during the initial reading of the image.  This kind of judgment directly parallels the judgments made in hindsight bias studies.  Consistent with the hindsight bias literature, it has been found that abnormalities are, in fact, more easily detected in hindsight than foresight. [39] In the absence of controls for hindsight bias, testifying radiologists may overestimate the ease with which the abnormality would have been detected in foresight. [37]   Attempts to decrease [ edit ]  Research suggests that people still exhibit the hindsight bias even when they are aware of it or possess the intention of eradicating it. [40] There is no solution to eliminate hindsight bias in its totality, but only ways to reduce it. [7] Some of which include considering alternative explanations or opening one's mind to different perspectives. In terms of auditory communication, the speaker would try to provide more clarity in his or her delivery and the listener may seek greater clarification. [22]  The only observable way to decrease hindsight bias in testing is to have the participant think about how alternative hypotheses could be correct. As a result, the participant doubts the correct hypothesis and reports that he or she would not have chosen it. Given the fact that researchers' attempts to eliminate hindsight bias in its entirety have failed, some believe there is a possible combination of motivational and automatic processes in cognitive reconstruction. [41] Incentive prompts participants to use more effort to recover even the weak memory traces. This idea supports the causal model theory and the use of sense-making to understand event outcomes. [22]   Mental illness [ edit ]  Schizophrenia [ edit ]  Schizophrenia is an example of a disorder that directly affects the hindsight bias. Individuals with schizophrenia are more strongly affected by the hindsight bias than are individuals from the general public. [42]  The hindsight bias effect is a paradigm that demonstrates how recently acquired knowledge influences the recollection of past information. Recently acquired knowledge has a strange but strong influence on schizophrenic individuals in relation to information previously learned. New information combined with rejection of past memories can disconfirm behavior and delusional belief, which is typically found in patients suffering from schizophrenia. [42] This can cause faulty memory, which can lead to hindsight thinking and believing in knowing something they don't. [42] Delusion-prone individuals suffering from schizophrenia can falsely jump to conclusions . [43] Jumping to conclusions can lead to hindsight, which strongly influences the delusional conviction in individuals with schizophrenia. [43] In numerous studies, cognitive functional deficits in schizophrenic individuals impair their ability to represent and uphold contextual processing. [44]   Post-traumatic stress disorder [ edit ]  Post-traumatic stress disorder (PTSD) is the re-experiencing and avoidance of trauma-related stressors, emotions, and memories from a past event or events that has cognitive dramatizing impact on an individual. [45] PTSD can be attributed to the functional impairment of the prefrontal cortex (PFC) structure. Dysfunctions of cognitive processing of context and abnormalities that PTSD patients suffer from can affect hindsight thinking, such as in combat soldiers perceiving they could have altered outcomes of events in war. [46] The PFC and dopamine systems are parts of the brain that can be responsible for the impairment in cognitive control processing of context information. The PFC is well known for controlling the thought process in hindsight bias that something will happen when it evidently does not. Brain impairment in certain brain regions can also affect the thought process of an individual who may engage in hindsight thinking. [47]  Cognitive flashbacks and other associated features from a traumatic event can trigger severe stress and negative emotions such as unpardonable guilt. For example, studies were done on trauma-related guilt characteristics of war veterans with chronic PTSD 8. [48] Although there has been limited research, significant data suggests that hindsight bias has an effect on war veterans' personal perception of wrongdoing, in terms of guilt and responsibility from traumatic events of war. They blame themselves, and, in hindsight, perceive that they could have prevented what happened.  Examples [ edit ]  Health care system [ edit ]  Accidents are prone to happen in any human undertaking, but accidents occurring within the healthcare system seem more salient and severe due to their profound effect on the lives of those involved, sometimes resulting in the death of a patient. In the healthcare system, there are a number of methods in which specific cases where accidents happened are reviewed by others who already know the outcome of the case. These methods include morbidity and mortality conferences , autopsies , case analysis, medical malpractice claims analysis, staff interviews, and even patient observation. Hindsight bias has been shown to cause difficulties in measuring errors in these cases. [49] Many of these errors are considered preventable after the fact, clearly indicating the presence and importance of a hindsight bias in this field. There are two sides of debate in how these case reviews should be approached to best evaluate past cases: the error elimination strategy and the safety management strategy. [2] The error elimination strategy aims to find the cause of errors, relying heavily on hindsight (therefore more subject to the hindsight bias). [2] The safety management strategy relies less on hindsight (less subject to hindsight bias) and identifies possible constraints during the decision making process of that case. However, it is not immune to error. [2]   Judicial system [ edit ]  Hindsight bias results in being held to a higher standard in court . The defense is particularly susceptible to these effects, since their actions are the ones being scrutinized by the jury . Due to the hindsight bias, defendants are judged as capable of preventing the bad outcome. [50] Although much stronger for the defendants, hindsight bias also affects the plaintiffs . In cases where there is an assumption of risk, hindsight bias may contribute to the jurors perceiving the event as riskier due to the poor outcome. This may lead the jury to feel that the plaintiff should have exercised greater caution in the situation. Both of these effects can be minimized if attorneys put the jury in a position of foresight rather than hindsight through the use of language and timelines. Judges and juries are likely to mistakenly view negative events as being more foreseeable than what it really was in the moment, when looking at the situation after the fact in court. [51] Encouraging people to explicitly think about the counterfactuals was an effective means of reducing the hindsight bias. [52] In other words, people became less attached to the actual outcome and were more open to consider alternative lines of reasoning prior to the event. Judges involved in fraudulent transfer litigation cases were subject to the hindsight bias as well, resulting in an unfair advantage for the plaintiff, [53] showing that jurors are not the only ones sensitive to the effects of the hindsight bias in the courtroom.  Wikipedia [ edit ]  Since hindsight leads people to focus on information that is consistent with what happened while inconsistent information is ignored or regarded as less relevant, [54] [55] it is likely included in representations about the past as well. In a study with Wikipedia articles [56] the latest article versions before the event (foresight article versions) were compared to two hindsight article versions: the first online after the event took place and another one eight weeks later. In order to be able to investigate various types of events, even including disasters (e.g., the nuclear disaster of Fukushima), for which foresight articles do not exist, the authors made use of articles about the structure that suffered damage in those instances (e.g., the article about the nuclear power plant of Fukushima). When analyzing to what extent the articles were suggestive of a particular event, they only found articles about disasters to be much more suggestive of the disaster in hindsight than in foresight – indicating hindsight bias. For the remaining event categories, however, Wikipedia articles did not show any hindsight bias. In an attempt to compare individuals' and Wikipedia's hindsight bias more directly, another study [57] came to the conclusion that Wikipedia articles are less susceptible to hindsight bias than individuals' representations.  See also [ edit ]  Curse of knowledge – Cognitive bias of assuming that others have the same background to understand  Egg of Columbus  Historian's fallacy – Assumption that decision makers of the past viewed events from the same perspective and having the same information as those subsequently analyzing the decision  Memory conformity – Phenomenon where memories or information reported by others influences an individual and is incorporated into their memory.  References [ edit ]    ^  ""I Knew It All Along…Didn't I?' – Understanding Hindsight Bias"" . APS Research News . Association for Psychological Science . Retrieved 29 January 2019 .   ^ a  b  c  d  e  f  g  h  i  Fischhoff, B. (1975). ""Hindsight ≠ foresight: The effect of outcome knowledge on judgment under uncertainty"". Journal of Experimental Psychology: Human Perception and Performance . 1 (3): 288–299. doi : 10.1037/0096-1523.1.3.288 .   ^  Roese, N. J.; Vohs, K. D. (2012). ""Hindsight bias"". Perspectives on Psychological Science . 7 (5): 411–426. doi : 10.1177/1745691612454303 . PMID  26168501 .   ^  Hoffrage, Ulrich; Pohl, Rüdiger (2003). ""Research on hindsight bias: A rich past, a productive present, and a challenging future"". Memory . 11 (4–5): 329–335. doi : 10.1080/09658210344000080 . PMID  14562866 .   ^  Boyd, Drew. ""Innovators: Beware the Hindsight Bias"" . Psychology Today . Sussex Publishers . Retrieved 29 January 2019 .   ^  Blank, H.; Nestler, S.; von Collani, G.; Fischer, V (2008). ""How many hindsight biases are there?"". Cognition . 106 (3): 1408–1440. doi : 10.1016/j.cognition.2007.07.007 . PMID  17764669 .   ^ a  b  c  Arkes, H.; Faust, D.; Guilmette, T. J.; Hart, K. (1988). ""Eliminating the hindsight bias"" . Journal of Applied Psychology . 73 (2): 305–307. doi : 10.1037/0021-9010.73.2.305 .   ^ a  b  c  Fischhoff, B (2007). ""An early history of hindsight research"". Social Cognition . 25 : 10–13. CiteSeerX  10.1.1.365.6826 . doi : 10.1521/soco.2007.25.1.10 .   ^  Tversky, A.; Kahneman, D. (1973). ""Availability: A heuristic for judging frequency and probability"". Cognitive Psychology . 5 (2): 207–232. doi : 10.1016/0010-0285(73)90033-9 .   ^ a  b  Fischhoff, Baruch; Beyth, Ruth (1975). ""I knew it would happen: Remembered probabilities of once—future things"". Organizational Behavior and Human Performance . 13 : 1–16. doi : 10.1016/0030-5073(75)90002-1 .   ^  Bernstein, D.M.; Erdfelder, E.; Meltzoff, A. N.; Peria, W.; Loftus, G. R. (2011). ""Hindsight bias from 3 to 95 years of age"" . Journal of Experimental Psychology: Learning, Memory, and Cognition . 37 (2): 378–391. doi : 10.1037/a0021971 . PMC  3084020 . PMID  21299327 .   ^  Marks, A. Z.; Arkes, H. R. (2010). ""The effects of mental contamination on the hindsight bias: Source confusion determines success in disregarding knowledge"". Journal of Behavioral Decision Making . 23 (2): 131–160. doi : 10.1002/bdm.632 .   ^  Biais, Bruno; Weber, Martin (2009). ""Hindsight Bias, Risk Perception, and Investment Performance""  (PDF) . Management Science . 55 (6): 1018–1029. doi : 10.1287/mnsc.1090.1000 .   ^  Schkade, D.; Kilbourne, L. (1991). ""Expectation-Outcome Consistency and Hindsight Bias"". Organizational Behavior and Human Decision Processes . 49 : 105–123. doi : 10.1016/0749-5978(91)90044-T .   ^  Fiske, S. (1980). ""Attention and weight in person perception: The impact of negative as well as extreme behavior"". Journal of Personality and Social Psychology . 38 (6): 889–906. doi : 10.1037/0022-3514.38.6.889 .   ^  Harley, E. M. (2007). ""Hindsight bias in legal decision making"". Social Cognition . 25 (1): 48–63. doi : 10.1521/soco.2007.25.1.48 .   ^ a  b  Müller, Patrick A.; Stahlberg, Dagmar (2007). ""The Role of Surprise in Hindsight Bias: A Metacognitive Model of Reduced and Reversed Hindsight Bias""  (PDF) . Social Cognition . 25 : 165–184. doi : 10.1521/soco.2007.25.1.165 .   ^  Nestler, Steffen; Egloff, Boris; Küfner, Albrecht C. P.; Back, Mitja D. (2012). ""An integrative lens model approach to bias and accuracy in human inferences: Hindsight effects and knowledge updating in personality judgments"". Journal of Personality and Social Psychology . 103 (4): 689–717. doi : 10.1037/a0029461 . PMID  22844973 .   ^  Bernstein, D. M.; Atance, C.; Loftus, G. R.; Meltzoff, A. (April 2004). ""We saw it all along: Visual hindsight bias in children and adults"" . Psychological Science . 15 (4): 264–267. doi : 10.1111/j.0963-7214.2004.00663.x . PMC  3640979 . PMID  15043645 .   ^  Birch, Susan A. J.; Bernstein, Daniel M. (2007). ""What Can Children Tell Us About Hindsight Bias: A Fundamental Constraint on Perspective–Taking?"". Social Cognition . 25 : 98–113. doi : 10.1521/soco.2007.25.1.98 .   ^  Bernstein, Daniel M.; Wilson, Alexander Maurice; Pernat, Nicole L. M.; Meilleur, Louise R. (2012). ""Auditory hindsight bias""  (PDF) . Psychonomic Bulletin & Review . 19 (4): 588–593. doi : 10.3758/s13423-012-0268-0 . PMID  22648656 .   ^ a  b  c  d  e  f  g  Blank, H.; Nestler, S. (2007). ""Cognitive Process Models of Hinsight Bias"". Social Cognition . 25 (1): 132–147. doi : 10.1521/soco.2007.25.1.132 .   ^  Pohl, R. F.; Eisenhauer, M.; Hardt, O. (2003). ""SARA: A Cognitive Process Model to Stimulate the Anchoring Effect and Hindsight bias"". Memory . 11 (4–5): 337–356. doi : 10.1080/09658210244000487 . PMID  14562867 .   ^  Loftus, E., F. (1991). ""Made in Memory: Distortions in Recollection After Misleading Information"". The Psychology of Learning and Motivation , 25, 187–215. New York: Academic Press   ^  Hertwig, R.; Fenselow, C.; Hoffrage, U. (2003). ""Hindsight Bias: Knowledge and Heuristics Affect our reconstruction of the Past"". Memory . 11 (4–5): 357–377. doi : 10.1080/09658210244000595 . hdl : 11858/00-001M-0000-0025-8C80-E . PMID  14562868 .   ^  Nestler, S.; Blank, H.; von Collani, G. (2008). ""A Causal Model Theory of Creeping Determinism"". Social Psychology . 39 (3): 182–188. doi : 10.1027/1864-9335.39.3.182 .   ^ a  b  c  d  Mazzoni, G.; Vannucci, M. (2007). ""Hindsight bias, the misinformation effect, and false autobiographical memories"". Social Cognition . 25 (1): 203–220. doi : 10.1521/soco.2007.25.1.203 .   ^  Calvillo, Dustin P. (2013). ""Rapid recollection of foresight judgments increases hindsight bias in a memory design"". Journal of Experimental Psychology: Learning, Memory, and Cognition . 39 (3): 959–964. doi : 10.1037/a0028579 . PMID  22582966 .   ^  Kane, M.; Core, T.J.; Hunt, R.R. (2010). ""Bias versus bias: Harnessing hindsight to reveal paranormal belief change beyond demand characteristics"". Psychonomic Bulletin & Review . 17 (2): 206–212. doi : 10.3758/PBR.17.2.206 . PMID  20382921 .   ^  Kane, M.J. (2010). ""Can people's minds be changed? How can we know?"" . Skeptic . 16 (1): 28–31.   ^  Nadel, L., Hupbach, A., Hardt, O., & Gomez, R. (2008). ""Episodic Memory: Reconsolidation"". Dere, D., Easton, A., Nadel, L., & Huston, J., P. (Eds), Handbook of Episodic Memory (pp. 43–56). The Netherlands: Elsevier.   ^ a  b  Pezzo, M.; Pezzo, S. (2007). ""Making Sense of Failure: A Motivated Model of Hindsight Bias"". Social Cognition . 25 (1): 147–165. CiteSeerX  10.1.1.319.1999 . doi : 10.1521/soco.2007.25.1.147 .   ^  Tykocinski, O. E.; Steinberg, N. (2005). ""Coping with disappointing outcomes: Retroactive pessimism and motivated inhibition of counterfactuals"". Journal of Experimental Social Psychology . 41 (5): 551–558. doi : 10.1016/j.jesp.2004.12.001 .   ^ a  b  Louie, T. A.; Rajan, M. N.; Sibley, R. E. (2007). ""Tackling the monday-morning quarterback: Applications of hindsight bias in decision-making settings"". Social Cognition . 25 (1): 32–47. doi : 10.1521/soco.2007.25.1.32 .   ^  Arkes, H. R. (2013). ""The consequences of the hindsight bias in medical decision making"". Current Directions in Psychological Science . 22 (5): 356–360. doi : 10.1177/0963721413489988 .   ^  Blank, H.; Musch, J.; Pohl, R. F. (2007). ""Hindsight Bias: On Being Wise After the Event"". Social Cognition . 25 (1): 1–9. doi : 10.1521/soco.2007.25.1.1 .   ^ a  b  c  Harley, Erin M.; Carlsen, Keri A.; Loftus, Geoffrey R. (2004). ""The 'Saw-It-All-Along' Effect: Demonstrations of Visual Hindsight Bias""  (PDF) . Journal of Experimental Psychology: Learning, Memory, and Cognition . 30 (5): 960–968. doi : 10.1037/0278-7393.30.5.960 . PMID  15355129 .   ^  Berlin (2000). ""Malpractice issues in Radiology: Hindsight bias"". American Journal of Roentgenology . 175 (3): 597–601. doi : 10.2214/ajr.175.3.1750597 . PMID  10954437 .   ^  Muhm, J.; Miller, W.; Fontana, R; Sanderson, D.; Uhlenhopp, M. (1983). ""Lung cancer detected during a screening program using four-month chest radiographs"". Radiology . 148 (3): 609–615. doi : 10.1148/radiology.148.3.6308709 . PMID  6308709 .   ^  Pohl, R.; Hell, W. (1996). ""No reduction in Hindsight Bias after Complete Information and repeated Testing"". Organizational Behavior and Human Decision Processes . 67 (1): 49–58. doi : 10.1006/obhd.1996.0064 .   ^  Hell, Wolfgang; Gigerenzer, Gerd; Gauggel, Siegfried; Mall, Maria; Müller, Michael (1988). ""Hindsight bias: An interaction of automatic and motivational factors?"". Memory & Cognition . 16 (6): 533–538. doi : 10.3758/BF03197054 . PMID  3193884 .   ^ a  b  c  Woodward, T. S.; Moritz, S.; Arnold, M. M.; Cuttler, C.; Whitman, J. C.; Lindsay, S. (2006). ""Increased hindsight bias in schizophrenia"". Neuropsychology . 20 (4): 462–467. CiteSeerX  10.1.1.708.6018 . doi : 10.1037/0894-4105.20.4.461 . PMID  16846264 .   ^ a  b  Freeman, D; Pugh, K; Garety, PA (2008). ""Jumping to conclusions and paranoid ideation in the general population"". Schizophrenia Research . 102 (1–3): 254–260. doi : 10.1016/j.schres.2008.03.020 . PMID  18442898 .   ^  Holmes, Avram J.; MacDonald, Angus; Carter, Cameron S.; Barch, Deanna M.; Andrew Stenger, V.; Cohen, Jonathan D. (2005). ""Prefrontal functioning during context processing in schizophrenia and major depression: An event-related fMRI study"". Schizophrenia Research . 76 (2–3): 199–206. doi : 10.1016/j.schres.2005.01.021 . PMID  15949653 .   ^  Brewin, C.; Dalgleish, R.; Joseph, S. (1996). ""A dual representation theory of posttraumatic stress disorder"". Psychological Review . 103 (4): 670–68. doi : 10.1037/0033-295x.103.4.670 . PMID  8888651 .   ^  Richert, K. A.; Carrion, V. G.; Karchemskiy, A.; Reiss, A. L. (2006). ""Regional differences of the prefrontal cortex in pediatric PTSD: an MRI study"". Depression and Anxiety . 23 (1): 17–25. doi : 10.1002/da.20131 . PMID  16247760 .   ^  Braver, Todd S.; Barch, Deanna M.; Keys, Beth A.; Carter, Cameron S.; Cohen, Jonathan D.; Kaye, Jeffrey A.; Janowsky, Jeri S.; Taylor, Stephan F.; Yesavage, Jerome A.; Mumenthaler, Martin S.; Jagust, William J.; Reed, Bruce R. (2001). ""Context processing in older adults: Evidence for a theory relating cognitive control to neurobiology in healthy aging"". Journal of Experimental Psychology . 130 (4): 746–763. CiteSeerX  10.1.1.599.6420 . doi : 10.1037/0096-3445.130.4.746 .   ^  Beckham, Jean C.; Feldman, Michelle E.; Kirby, Angela C. (1998). ""Atrocities Exposure in Vietnam Combat Veterans with Chronic Posttraumatic Stress Disorder: Relationship to Combat Exposure, Symptom Severity, Guilt, and Interpersonal Violence"". Journal of Traumatic Stress . 11 (4): 777–785. doi : 10.1023/a:1024453618638 . PMID  9870228 .   ^  Hurwitz, B., & Sheikh, A. (2009). Healthcare Errors and Patient Safety . Hoboken, NJ: Blackwell Publishing.   ^  Starr, V. H., & McCormick, M. (2001). Jury Selection (Third Edition). Aspen Law and Business   ^  Oeberst, A.; Goeckenjan, I. (2016). ""When being wise after the event results in injustice: Evidence for hindsight bias in judges' negligence assessments"". Psychology, Public Policy, and Law . 22 (3): 271–279. doi : 10.1037/law0000091 .   ^  Peterson, R. L. (2007). Inside the Investor's Brain: the power of mind over money . Hoboken, NJ: Wiley Publishing.   ^  Simkovic, M., & Kaminetzky, B. (2010). ""Leverage Buyout Bankruptcies, the Problem of Hindsight Bias, and the Credit Default Swap Solution"". Seton Hall Public Research Paper: August 29, 2010.   ^  Fischhoff, Baruch (1975). ""Hindsight is not equal to foresight: The effect of outcome knowledge on judgment under uncertainty"". Journal of Experimental Psychology: Human Perception and Performance . 1 (3): 288–299. doi : 10.1037/0096-1523.1.3.288 . ISSN  0096-1523 .   ^  Nestler, Steffen; Blank, Hartmut; von Collani, Gernot (2008). ""Hindsight bias doesn't always come easy: Causal models, cognitive effort, and creeping determinism"". Journal of Experimental Psychology: Learning, Memory, and Cognition . 34 (5): 1043–1054. doi : 10.1037/0278-7393.34.5.1043 . ISSN  1939-1285 .   ^  Oeberst, Aileen; von der Beck, Ina; D. Back, Mitja; Cress, Ulrike; Nestler, Steffen (17 April 2017). ""Biases in the production and reception of collective knowledge: the case of hindsight bias in Wikipedia"". Psychological Research . 82 (5): 1010–1026. doi : 10.1007/s00426-017-0865-7 . ISSN  0340-0727 . PMID  28417198 .   ^  Oeberst, Aileen; von der Beck, Ina; Cress, Ulrike; Nestler, Steffen (20 March 2019). ""Wikipedia outperforms individuals when it comes to hindsight bias"". Psychological Research . doi : 10.1007/s00426-019-01165-7 . ISSN  0340-0727 . PMID  30895365 .    Further reading [ edit ]  Excerpt from: David G. Myers, Exploring Social Psychology . New York: McGraw-Hill, 1994, pp. 15–19. (More discussion of Paul Lazarsfeld's experimental questions.)  Ken Fisher, Forecasting (Macro and Micro) and Future Concepts , on Market Analysis (4/7/06)  Iraq War Naysayers May Have Hindsight Bias . Shankar Vedantam. The Washington Post .  Why Hindsight Can Damage Foresight . Paul Goodwin. Foresight: The International Journal of Applied Forecasting , Spring 2010.  Social Cognition (2007) Vol. 25, Special Issue: The Hindsight Bias  v t e Biases Cognitive biases  Actor–observer  Acquiescence  Ambiguity  Anchoring  Attentional  Attribution  Authority  Automation  Belief  Blind spot  Choice-supportive  Confirmation  Congruence  Cultural  Distinction  Dunning–Kruger  Egocentric  Emotional  Extrinsic incentives  Fading affect  Framing  Correspondence  Halo effect  Hindsight  Horn effect  Hostile attribution  Impact  Implicit  In-group  Mere-exposure effect  Negativity  Normalcy  Omission  Optimism  Out-group homogeneity  Outcome  Overton window  Precision  Present  Pro-innovation  Response  Restraint  Self-serving  Social comparison  Status quo  Time-saving  Trait ascription  von Restorff effect  Zero-risk  In animals  Statistical biases  Estimator  Forecast  Healthy user  Information  Psychological  Lead time  Length time  Non-response  Observer  Omitted-variable  Participation  Recall  Sampling  Selection  Self-selection  Social desirability  Spectrum  Survivorship  Systematic error  Systemic  Verification  Wet  Other biases  Academic  Funding  FUTON  Inductive  Infrastructure  Inherent  In education  Media  False balance  Vietnam War  Norway  South Asia  Sweden  United States  Arab–Israeli conflict  Ukraine  Net  Political bias  Publication  Reporting  White hat  Bias reduction  Cognitive bias mitigation  Debiasing  Heuristics in judgment and decision-making  Lists: General  ·  Memory  v t e Human memory Basic concepts  Encoding  Storage  Recall  Attention  Consolidation  Neuroanatomy  Types Sensory  Echoic  Eidetic  Eyewitness  Haptic  Iconic  Motor learning  Visual  Short-term  "" The Magical Number Seven, Plus or Minus Two ""  Working memory  Intermediate    Long-term  Active recall  Autobiographical  Explicit  Declarative  Episodic  Semantic  Flashbulb  Hyperthymesia  Implicit  Meaningful learning  Personal-event  Procedural  Rote learning  Selective retention  Tip of the tongue  Forgetting  Amnesia  anterograde  childhood  post-traumatic  psychogenic  retrograde  transient global  Decay theory  Forgetting curve  Interference theory  Memory inhibition  Motivated forgetting  Repressed memory  Retrieval-induced forgetting  Selective amnesia  Weapon focus  Memory errors  Confabulation  False memory  Hindsight bias  Imagination inflation  List of memory biases  Memory conformity  Mere-exposure effect  Misattribution of memory  Misinformation effect  Source-monitoring error  Wernicke–Korsakoff syndrome  Research  Art of memory  Memory and aging  Deese–Roediger–McDermott paradigm  Exceptional memory  Indirect tests of memory  Lost in the mall technique  Memory disorder  Memory implantation  Methods used to study memory  The Seven Sins of Memory  Effects of exercise on memory  In society  Collective memory  Cultural memory  False memory syndrome  Memory and social interactions  Memory sport  Politics of memory  Shas Pollak  World Memory Championships  Related topics  Absent-mindedness  Atkinson–Shiffrin memory model  Context-dependent memory  Childhood memory  Cryptomnesia  Effects of alcohol  Emotion and memory  Exosomatic memory  Flashbacks  Free recall  Involuntary memory  Levels-of-processing effect  Memory and trauma  Memory improvement  Metamemory  Mnemonic  Muscle memory  Priming  Intertrial  Prospective memory  Recovered-memory therapy  Retrospective memory  Sleep and memory  State-dependent memory  Transactive memory  People  Robert A. Bjork  Stephen J. Ceci  Susan Clancy  Hermann Ebbinghaus  Sigmund Freud  Patricia Goldman-Rakic  Jonathan Hancock  Judith Lewis Herman  HM (patient)  Ivan Izquierdo  Marcia K. Johnson  Eric Kandel  KC (patient)  Elizabeth Loftus  Geoffrey Loftus  Chris Marker  James McGaugh  Paul R. McHugh  Eleanor Maguire  George Armitage Miller  Brenda Milner  Lynn Nadel  Dominic O'Brien  Ben Pridmore  Henry L. Roediger III  Steven Rose  Cosmos Rossellius  Daniel Schacter  Richard Shiffrin  Arthur P. Shimamura  Andriy Slyusarchuk  Larry Squire  Susumu Tonegawa  Anne Treisman  Endel Tulving  Robert Stickgold  Clive Wearing    Psychology portal   Philosophy portal       Retrieved from "" https://en.wikipedia.org/w/index.php?title=Hindsight_bias&oldid=961736349 ""  Categories : Cognitive biases Memory biases Prospect theory Error Hidden categories: Articles with short description Use dmy dates from November 2016 Articles needing cleanup from March 2013 All pages needing cleanup Cleanup tagged articles with a reason field from March 2013 Wikipedia pages needing cleanup from March 2013         Navigation menu      Personal tools      Not logged in Talk Contributions Create account Log in        Namespaces      Article Talk        Variants              Views      Read Edit View history        More           Search                     Navigation      Main page Contents Current events Random article About Wikipedia Contact us Donate Wikipedia store       Contribute      Help Community portal Recent changes Upload file       Tools      What links here Related changes Upload file Special pages Permanent link Page information Wikidata item Cite this page       Print/export      Download as PDF Printable version       Languages      العربية Български Deutsch Español فارسی Français עברית 日本語 Pälzisch Polski Русский Svenska Українська 中文  Edit links        This page was last edited on 10 June 2020, at 03:55 (UTC) .  Text is available under the Creative Commons Attribution-ShareAlike License ;
additional terms may apply.  By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc. , a non-profit organization.    Privacy policy  About Wikipedia  Disclaimers  Contact Wikipedia  Developers  Statistics  Cookie statement  Mobile view          "
13,desirability of a positive event or consequence(fallacy OR bias OR heuristic) (coronavirus OR covid OR pandemic),2,https://en.wikipedia.org/wiki/List_of_cognitive_biases,"Name Description

Agent detection The inclination to presume the purposeful intervention of a sentient or intelligent agent.

Ambiguity effect The tendency to avoid options for which the probability of a favorable outcome is unknown.[11]

Anchoring or focalism The tendency to rely too heavily, or ""anchor"", on one trait or piece of information when making decisions (usually the first piece of information acquired on that subject).[12][13]

Anthropocentric thinking The tendency to use human analogies as a basis for reasoning about other, less familiar, biological phenomena.[14]

Anthropomorphism or personification The tendency to characterize animals, objects, and abstract concepts as possessing human-like traits, emotions, and intentions.[15] The opposite bias, of not attributing feelings or thoughts to another person, is dehumanised perception,[16] a type of objectification.

Attentional bias The tendency of perception to be affected by recurring thoughts.[17]

Attribute substitution Occurs when a judgment has to be made (of a target attribute) that is computationally complex, and instead a more easily calculated heuristic attribute is substituted. This substitution is thought of as taking place in the automatic intuitive judgment system, rather than the more self-aware reflective system.

Automation bias The tendency to depend excessively on automated systems which can lead to erroneous automated information overriding correct decisions.[18]

Availability heuristic The tendency to overestimate the likelihood of events with greater ""availability"" in memory, which can be influenced by how recent the memories are or how unusual or emotionally charged they may be.[19]

Availability cascade A self-reinforcing process in which a collective belief gains more and more plausibility through its increasing repetition in public discourse (or ""repeat something long enough and it will become true"").[20]

Backfire effect The reaction to disconfirming evidence by strengthening one's previous beliefs.[21] cf. Continued influence effect.

Bandwagon effect The tendency to do (or believe) things because many other people do (or believe) the same. Related to groupthink and herd behavior.[22]

Base rate fallacy or Base rate neglect The tendency to ignore general information and focus on information only pertaining to the specific case, even when the general information is more important.[23]

Belief bias An effect where someone's evaluation of the logical strength of an argument is biased by the believability of the conclusion.[24]

Ben Franklin effect A person who has performed a favor for someone is more likely to do another favor for that person than they would be if they had received a favor from that person.[25]

Berkson's paradox The tendency to misinterpret statistical experiments involving conditional probabilities.[26]

Bias blind spot The tendency to see oneself as less biased than other people, or to be able to identify more cognitive biases in others than in oneself.[27]

Choice-supportive bias The tendency to remember one's choices as better than they actually were.[28]

Clustering illusion The tendency to overestimate the importance of small runs, streaks, or clusters in large samples of random data (that is, seeing phantom patterns).[13]

Compassion fade The predisposition to behave more compassionately towards a small number of identifiable victims than to a large number of anonymous ones.[29]

Confirmation bias The tendency to search for, interpret, focus on and remember information in a way that confirms one's preconceptions.[30]

Congruence bias The tendency to test hypotheses exclusively through direct testing, instead of testing possible alternative hypotheses.[13]

Conjunction fallacy The tendency to assume that specific conditions are more probable than a more general version of those same conditions. For example, subjects in one experiment perceived the probability of a woman being both a bank teller and a feminist as more likely than the probability of her being a bank teller.[31]

Continued influence effect The tendency to believe previously learned misinformation even after it has been corrected. Misinformation can still influence inferences one generates after a correction has occurred.[34] cf. Backfire effect

Contrast effect The enhancement or reduction of a certain stimulus' perception when compared with a recently observed, contrasting object.[35]

Courtesy bias The tendency to give an opinion that is more socially correct than one's true opinion, so as to avoid offending anyone.[36]

Curse of knowledge When better-informed people find it extremely difficult to think about problems from the perspective of lesser-informed people.[37]

Declinism The predisposition to view the past favorably (rosy retrospection) and future negatively.[38]

Decoy effect Preferences for either option A or B change in favor of option B when option C is presented, which is completely dominated by option B (inferior in all respects) and partially dominated by option A.[39]

Default effect When given a choice between several options, the tendency to favor the default one.[40]

Denomination effect The tendency to spend more money when it is denominated in small amounts (e.g., coins) rather than large amounts (e.g., bills).[41]

Disposition effect The tendency to sell an asset that has accumulated in value and resist selling an asset that has declined in value.

Distinction bias The tendency to view two options as more dissimilar when evaluating them simultaneously than when evaluating them separately.[42]

Dread aversion Just as losses yield double the emotional impact of gains, dread yields double the emotional impact of savouring.[43]

Dunning–Kruger effect The tendency for unskilled individuals to overestimate their own ability and the tendency for experts to underestimate their own ability.[44]

Duration neglect The neglect of the duration of an episode in determining its value.[45]

Empathy gap The tendency to underestimate the influence or strength of feelings, in either oneself or others.[46]

End-of-history illusion The age-independent belief that one will change less in the future than one has in the past.[47]

Endowment effect The tendency for people to demand much more to give up an object than they would be willing to pay to acquire it.[48]

Exaggerated expectation The tendency to expect or predict more extreme outcomes than those outcomes that actually happen.[6]

Experimenter's or expectation bias The tendency for experimenters to believe, certify, and publish data that agree with their expectations for the outcome of an experiment, and to disbelieve, discard, or downgrade the corresponding weightings for data that appear to conflict with those expectations.[49]

Focusing effect The tendency to place too much importance on one aspect of an event.[50]

Forer effect or Barnum effect The observation that individuals will give high accuracy ratings to descriptions of their personality that supposedly are tailored specifically for them, but are in fact vague and general enough to apply to a wide range of people. This effect can provide a partial explanation for the widespread acceptance of some beliefs and practices, such as astrology, fortune telling, graphology, and some types of personality tests.[51]

Form function attribution bias In human–robot interaction, the tendency of people to make systematic errors when interacting with a robot. People may base their expectations and perceptions of a robot on its appearance (form) and attribute functions which do not necessarily mirror the true functions of the robot.[52]

Framing effect Drawing different conclusions from the same information, depending on how that information is presented.

Frequency illusion or Baader–Meinhof phenomenon The frequency illusion is that once something has been noticed then every instance of that thing is noticed, leading to the belief it has a high frequency of occurrence (a form of selection bias).[53] The Baader–Meinhof phenomenon is the illusion where something that has recently come to one's attention suddenly seems to appear with improbable frequency shortly afterwards.[54] The Baader–Meinhof phenomenon is sometimes conflated with frequency illusion and the recency illusion.[55] It was named after an incidence of frequency illusion in which the Baader–Meinhof Group was mentioned.[56]

Functional fixedness Limits a person to using an object only in the way it is traditionally used.[57]

Gambler's fallacy The tendency to think that future probabilities are altered by past events, when in reality they are unchanged. The fallacy arises from an erroneous conceptualization of the law of large numbers. For example, ""I've flipped heads with this coin five times consecutively, so the chance of tails coming out on the sixth flip is much greater than heads.""[58]

Gender bias A widely held[59] set of implicit biases that discriminate against a gender (typically women[60]). For example, the assumption that women are less suited to jobs requiring high intellectual ability[61]. Or the assumption that people or animals are male in the absence of any indicators of gender.[62]

Groupthink The psychological phenomenon that occurs within a group of people in which the desire for harmony or conformity in the group results in an irrational or dysfunctional decision-making outcome. Group members try to minimize conflict and reach a consensus decision without critical evaluation of alternative viewpoints by actively suppressing dissenting viewpoints, and by isolating themselves from outside influences.

Hard–easy effect The tendency to overestimate one's ability to accomplish hard tasks, and underestimate one's ability to accomplish easy tasks[6][63][64][65]

Hindsight bias Sometimes called the ""I-knew-it-all-along"" effect, the tendency to see past events as being predictable[66] at the time those events happened.

Hostile attribution bias The ""hostile attribution bias"" is the tendency to interpret others' behaviors as having hostile intent, even when the behavior is ambiguous or benign.[67]

Hot-hand fallacy The ""hot-hand fallacy"" (also known as the ""hot hand phenomenon"" or ""hot hand"") is the belief that a person who has experienced success with a random event has a greater chance of further success in additional attempts.

Hyperbolic discounting Discounting is the tendency for people to have a stronger preference for more immediate payoffs relative to later payoffs. Hyperbolic discounting leads to choices that are inconsistent over time – people make choices today that their future selves would prefer not to have made, despite using the same reasoning.[68] Also known as current moment bias, present-bias, and related to Dynamic inconsistency. A good example of this: a study showed that when making food choices for the coming week, 74% of participants chose fruit, whereas when the food choice was for the current day, 70% chose chocolate.

IKEA effect The tendency for people to place a disproportionately high value on objects that they partially assembled themselves, such as furniture from IKEA, regardless of the quality of the end product.[69]

Illicit transference Occurs when a term in the distributive (referring to every member of a class) and collective (referring to the class itself as a whole) sense are treated as equivalent. The two variants of this fallacy are the fallacy of composition and the fallacy of division.

Illusion of control The tendency to overestimate one's degree of influence over other external events.[70]

Illusion of validity Believing that one's judgments are accurate, especially when available information is consistent or inter-correlated.[71]

Illusory correlation Inaccurately perceiving a relationship between two unrelated events.[72][73]

Illusory truth effect A tendency to believe that a statement is true if it is easier to process, or if it has been stated multiple times, regardless of its actual veracity. These are specific cases of truthiness.

Impact bias The tendency to overestimate the length or the intensity of the impact of future feeling states.[74]

Implicit association The speed with which people can match words depends on how closely they are associated.

Information bias The tendency to seek information even when it cannot affect action.[75]

Insensitivity to sample size The tendency to under-expect variation in small samples.

Interoceptive bias The tendency for sensory input about the body itself to affect one's judgement about external, unrelated circumstances. (As for example, in parole judges who are more lenient when fed and rested.) [76][77][78][79]

Irrational escalation or Escalation of commitment The phenomenon where people justify increased investment in a decision, based on the cumulative prior investment, despite new evidence suggesting that the decision was probably wrong. Also known as the sunk cost fallacy.

Law of the instrument An over-reliance on a familiar tool or methods, ignoring or under-valuing alternative approaches. ""If all you have is a hammer, everything looks like a nail.""

Less-is-better effect The tendency to prefer a smaller set to a larger set judged separately, but not jointly.

Look-elsewhere effect An apparently statistically significant observation may have actually arisen by chance because of the size of the parameter space to be searched.

Loss aversion The perceived disutility of giving up an object is greater than the utility associated with acquiring it.[80] (see also Sunk cost effects and endowment effect).

Mere exposure effect The tendency to express undue liking for things merely because of familiarity with them.[81]

Money illusion The tendency to concentrate on the nominal value (face value) of money rather than its value in terms of purchasing power.[82]

Moral credential effect Occurs when someone who does something good gives themselves permission to be less good in the future.

Neglect of probability The tendency to completely disregard probability when making a decision under uncertainty.[86]

Normalcy bias The refusal to plan for, or react to, a disaster which has never happened before.

Not invented here Aversion to contact with or use of products, research, standards, or knowledge developed outside a group. Related to IKEA effect.

Observer-expectancy effect When a researcher expects a given result and therefore unconsciously manipulates an experiment or misinterprets data in order to find it (see also subject-expectancy effect).

Omission bias The tendency to judge harmful actions (commissions) as worse, or less moral, than equally harmful inactions (omissions).[87]

Ostrich effect Ignoring an obvious (negative) situation.

Outcome bias The tendency to judge a decision by its eventual outcome instead of based on the quality of the decision at the time it was made.

Overconfidence effect Excessive confidence in one's own answers to questions. For example, for certain types of questions, answers that people rate as ""99% certain"" turn out to be wrong 40% of the time.[6][90][91][92]

Pareidolia A vague and random stimulus (often an image or sound) is perceived as significant, e.g., seeing images of animals or faces in clouds, the man in the moon, and hearing non-existent hidden messages on records played in reverse.

Pygmalion effect The phenomenon whereby others' expectations of a target person affect the target person's performance.

Pessimism bias The tendency for some people, especially those suffering from depression, to overestimate the likelihood of negative things happening to them.

Plan continuation bias Failure to recognize that the original plan of action is no longer appropriate for a changing situation or for a situation that is different than anticipated.[93]

Planning fallacy The tendency to underestimate task-completion times.[74]

Present bias The tendency of people to give stronger weight to payoffs that are closer to the present time when considering trade-offs between two future moments.[94]

Plant blindness The tendency to ignore plants in their environment and a failure to recognize and appreciate the utility of plants to life on earth.[95]

Pro-innovation bias The tendency to have an excessive optimism towards an invention or innovation's usefulness throughout society, while often failing to identify its limitations and weaknesses.

Projection bias The tendency to overestimate how much our future selves share one's current preferences, thoughts and values, thus leading to sub-optimal choices.[96][97][84]

Pseudocertainty effect The tendency to make risk-averse choices if the expected outcome is positive, but make risk-seeking choices to avoid negative outcomes.[98]

Reactance The urge to do the opposite of what someone wants you to do out of a need to resist a perceived attempt to constrain your freedom of choice (see also Reverse psychology).

Reactive devaluation Devaluing proposals only because they purportedly originated with an adversary.

Recency illusion The illusion that a phenomenon one has noticed only recently is itself recent. Often used to refer to linguistic phenomena; the illusion that a word or language usage that one has noticed only recently is an innovation when it is in fact long-established (see also frequency illusion).

Systematic Bias Judgement that arises when targets of differentiating judgement become subject to effects of regression that are not equivalent. [99]

Restraint bias The tendency to overestimate one's ability to show restraint in the face of temptation.

Rhyme as reason effect Rhyming statements are perceived as more truthful. A famous example being used in the O.J Simpson trial with the defense's use of the phrase ""If the gloves don't fit, then you must acquit.""

Risk compensation / Peltzman effect The tendency to take greater risks when perceived safety increases.

Salience bias The tendency to focus on items that are more prominent or emotionally striking and ignore those that are unremarkable, even though this difference is often irrelevant by objective standards.

Selection bias The tendency to notice something more when something causes us to be more aware of it, such as when we buy a car, we tend to notice similar cars more often than we did before. They are not suddenly more common – we just are noticing them more. Also called the Observational Selection Bias.

Selective perception The tendency for expectations to affect perception.

Semmelweis reflex The tendency to reject new evidence that contradicts a paradigm.[33]

Sexual overperception bias / Sexual underperception bias The tendency to over-/underestimate sexual interest of another person in oneself.

Social comparison bias The tendency, when making decisions, to favour potential candidates who don't compete with one's own particular strengths.[100]

Social desirability bias The tendency to over-report socially desirable characteristics or behaviours in oneself and under-report socially undesirable characteristics or behaviours.[101] See also: § Courtesy bias.

Stereotyping Expecting a member of a group to have certain characteristics without having actual information about that individual.

Subadditivity effect The tendency to judge probability of the whole to be less than the probabilities of the parts.[104]

Subjective validation Perception that something is true if a subject's belief demands it to be true. Also assigns perceived connections between coincidences.

Surrogation Losing sight of the strategic construct that a measure is intended to represent, and subsequently acting as though the measure is the construct of interest.

Survivorship bias Concentrating on the people or things that ""survived"" some process and inadvertently overlooking those that didn't because of their lack of visibility.

Time-saving bias Underestimations of the time that could be saved (or lost) when increasing (or decreasing) from a relatively low speed and overestimations of the time that could be saved (or lost) when increasing (or decreasing) from a relatively high speed.

Third-person effect A hypothesized tendency to believe that mass communicated media messages have a greater effect on others than on themselves. As of 2020, the third-person effect has yet to be reliably demonstrated in a scientific context.

Parkinson's law of triviality The tendency to give disproportionate weight to trivial issues. Also known as bikeshedding, this bias explains why an organization may avoid specialized or complex subjects, such as the design of a nuclear reactor, and instead focus on something easy to grasp or rewarding to the average participant, such as the design of an adjacent bike shed.[105]

Unit bias The standard suggested amount of consumption (e.g., food serving size) is perceived to be appropriate, and a person would consume it all even if it is too much for this particular person.[106]

Weber–Fechner law Difficulty in comparing small differences in large quantities.

Well travelled road effect Underestimation of the duration taken to traverse oft-traveled routes and overestimation of the duration taken to traverse less familiar routes.

Women are wonderful effect A tendency to associate more positive attributes with women than with men.

Zero-risk bias Preference for reducing a small risk to zero over a greater reduction in a larger risk.","          List of cognitive biases   From Wikipedia, the free encyclopedia    Jump to navigation  Jump to search  Systematic patterns of deviation from norm or rationality in judgment   The loss aversion cognitive bias has been shown in monkeys  Cognitive biases are systematic patterns of deviation from norm and\or rationality in judgment. They are often studied in psychology and behavioral economics . [1]  Although the reality of most of these biases is confirmed by reproducible research, [2] [3] there are often controversies about how to classify these biases or how to explain them. [4]  Gerd Gigerenzer has criticized the framing of cognitive biases as errors in judgment, and favors interpreting them as arising from rational deviations from logical thought. [5]  Explanations include information-processing rules (i.e., mental shortcuts), called heuristics , that the brain uses to produce decisions or judgments. Biases have a variety of forms and appear as cognitive (""cold"") bias, such as mental noise, [6] or motivational (""hot"") bias, such as when beliefs are distorted by wishful thinking . Both effects can be present at the same time. [7] [8]  There are also controversies over some of these biases as to whether they count as useless or irrational , or whether they result in useful attitudes or behavior. For example, when getting to know others, people tend to ask leading questions which seem biased towards confirming their assumptions about the person. However, this kind of confirmation bias has also been argued to be an example of social skill ; a way to establish a connection with the other person. [9]  Although this research overwhelmingly involves human subjects, some findings that demonstrate bias have been found in non-human animals as well. For example, loss aversion has been shown in monkeys and hyperbolic discounting has been observed in rats, pigeons, and monkeys. [10]   Contents   1  Decision-making, belief, and behavioral biases  2  Social biases  3  Memory errors and biases  4  See also  5  Footnotes  6  References    Decision-making, belief, and behavioral biases [ edit ]  Many of these biases affect belief formation, business and economic decisions, and human behavior in general.    Name  Description   Agent detection   The inclination to presume the purposeful intervention of a sentient or intelligent agent .   Ambiguity effect   The tendency to avoid options for which the probability of a favorable outcome is unknown. [11]    Anchoring or focalism  The tendency to rely too heavily, or ""anchor"", on one trait or piece of information when making decisions (usually the first piece of information acquired on that subject). [12] [13]    Anthropocentric thinking   The tendency to use human analogies as a basis for reasoning about other, less familiar, biological phenomena. [14]    Anthropomorphism or personification  The tendency to characterize animals, objects, and abstract concepts as possessing human-like traits, emotions, and intentions. [15] The opposite bias, of not attributing feelings or thoughts to another person, is dehumanised perception , [16] a type of objectification .   Attentional bias   The tendency of perception to be affected by recurring thoughts. [17]    Attribute substitution   Occurs when a judgment has to be made (of a target attribute) that is computationally complex, and instead a more easily calculated heuristic attribute is substituted. This substitution is thought of as taking place in the automatic intuitive judgment system, rather than the more self-aware reflective system.   Automation bias   The tendency to depend excessively on automated systems which can lead to erroneous automated information overriding correct decisions. [18]    Availability heuristic   The tendency to overestimate the likelihood of events with greater ""availability"" in memory, which can be influenced by how recent the memories are or how unusual or emotionally charged they may be. [19]    Availability cascade   A self-reinforcing process in which a collective belief gains more and more plausibility through its increasing repetition in public discourse (or ""repeat something long enough and it will become true""). [20]    Backfire effect   The reaction to disconfirming evidence by strengthening one's previous beliefs. [21] cf. Continued influence effect .   Bandwagon effect   The tendency to do (or believe) things because many other people do (or believe) the same. Related to groupthink and herd behavior . [22]    Base rate fallacy or Base rate neglect  The tendency to ignore general information and focus on information only pertaining to the specific case, even when the general information is more important. [23]    Belief bias   An effect where someone's evaluation of the logical strength of an argument is biased by the believability of the conclusion. [24]    Ben Franklin effect   A person who has performed a favor for someone is more likely to do another favor for that person than they would be if they had received a favor from that person. [25]    Berkson's paradox   The tendency to misinterpret statistical experiments involving conditional probabilities. [26]    Bias blind spot   The tendency to see oneself as less biased than other people, or to be able to identify more cognitive biases in others than in oneself. [27]    Choice-supportive bias   The tendency to remember one's choices as better than they actually were. [28]    Clustering illusion   The tendency to overestimate the importance of small runs, streaks, or clusters in large samples of random data (that is, seeing phantom patterns). [13]    Compassion fade   The predisposition to behave more compassionately towards a small number of identifiable victims than to a large number of anonymous ones. [29]    Confirmation bias   The tendency to search for, interpret, focus on and remember information in a way that confirms one's preconceptions. [30]    Congruence bias   The tendency to test hypotheses exclusively through direct testing, instead of testing possible alternative hypotheses. [13]    Conjunction fallacy   The tendency to assume that specific conditions are more probable than a more general version of those same conditions. For example, subjects in one experiment perceived the probability of a woman being both a bank teller and a feminist as more likely than the probability of her being a bank teller. [31]    Conservatism (belief revision)   The tendency to revise one's belief insufficiently when presented with new evidence. [6] [32] [33]    Continued influence effect   The tendency to believe previously learned misinformation even after it has been corrected. Misinformation can still influence inferences one generates after a correction has occurred. [34] cf. Backfire effect    Contrast effect   The enhancement or reduction of a certain stimulus' perception when compared with a recently observed, contrasting object. [35]    Courtesy bias  The tendency to give an opinion that is more socially correct than one's true opinion, so as to avoid offending anyone. [36]    Curse of knowledge   When better-informed people find it extremely difficult to think about problems from the perspective of lesser-informed people. [37]    Declinism   The predisposition to view the past favorably ( rosy retrospection ) and future negatively. [38]    Decoy effect   Preferences for either option A or B change in favor of option B when option C is presented, which is completely dominated by option B (inferior in all respects) and partially dominated by option A. [39]    Default effect   When given a choice between several options, the tendency to favor the default one. [40]    Denomination effect   The tendency to spend more money when it is denominated in small amounts (e.g., coins) rather than large amounts (e.g., bills). [41]    Disposition effect   The tendency to sell an asset that has accumulated in value and resist selling an asset that has declined in value.   Distinction bias   The tendency to view two options as more dissimilar when evaluating them simultaneously than when evaluating them separately. [42]    Dread aversion  Just as losses yield double the emotional impact of gains, dread yields double the emotional  impact of savouring. [43]    Dunning–Kruger effect   The tendency for unskilled individuals to overestimate their own ability and the tendency for experts to underestimate their own ability. [44]    Duration neglect   The neglect of the duration of an episode in determining its value. [45]    Empathy gap   The tendency to underestimate the influence or strength of feelings, in either oneself or others. [46]    End-of-history illusion   The age-independent belief that one will change less in the future than one has in the past. [47]    Endowment effect   The tendency for people to demand much more to give up an object than they would be willing to pay to acquire it. [48]    Exaggerated expectation   The tendency to expect or predict more extreme outcomes than those outcomes that actually happen. [6]    Experimenter's or expectation bias   The tendency for experimenters to believe, certify, and publish data that agree with their expectations for the outcome of an experiment, and to disbelieve, discard, or downgrade the corresponding weightings for data that appear to conflict with those expectations. [49]    Focusing effect   The tendency to place too much importance on one aspect of an event. [50]    Forer effect or Barnum effect   The observation that individuals will give high accuracy ratings to descriptions of their personality that supposedly are tailored specifically for them, but are in fact vague and general enough to apply to a wide range of people. This effect can provide a partial explanation for the widespread acceptance of some beliefs and practices, such as astrology, fortune telling, graphology, and some types of personality tests. [51]    Form function attribution bias  In human–robot interaction , the tendency of people to make systematic errors when interacting with a robot. People may base their expectations and perceptions of a robot on its appearance (form) and attribute functions which do not necessarily mirror the true functions of the robot. [52]    Framing effect   Drawing different conclusions from the same information, depending on how that information is presented.   Frequency illusion or Baader–Meinhof phenomenon  The frequency illusion is that once something has been noticed then every instance of that thing is noticed, leading to the belief it has a high frequency of occurrence (a form of selection bias ). [53] The Baader–Meinhof phenomenon is the illusion where something that has recently come to one's attention suddenly seems to appear with improbable frequency shortly afterwards. [54] The Baader–Meinhof phenomenon is sometimes conflated with frequency illusion and the recency illusion . [55] It was named after an incidence of frequency illusion in which the Baader–Meinhof Group was mentioned. [56]    Functional fixedness   Limits a person to using an object only in the way it is traditionally used. [57]    Gambler's fallacy   The tendency to think that future probabilities are altered by past events, when in reality they are unchanged. The fallacy arises from an erroneous conceptualization of the law of large numbers . For example, ""I've flipped heads with this coin five times consecutively, so the chance of tails coming out on the sixth flip is much greater than heads."" [58]    Gender bias   A widely held [59] set of implicit biases that discriminate against a gender (typically women [60] ). For example, the assumption that women are less suited to jobs requiring high intellectual ability [61] . Or the assumption that people or animals are male in the absence of any indicators of gender. [62]    Groupthink   The psychological phenomenon that occurs within a group of people in which the desire for harmony or conformity in the group results in an irrational or dysfunctional decision-making outcome. Group members try to minimize conflict and reach a consensus decision without critical evaluation of alternative viewpoints by actively suppressing dissenting viewpoints, and by isolating themselves from outside influences.   Hard–easy effect   The tendency to overestimate one's ability to accomplish hard tasks, and underestimate one's ability to accomplish easy tasks [6] [63] [64] [65]    Hindsight bias   Sometimes called the ""I-knew-it-all-along"" effect, the tendency to see past events as being predictable [66] at the time those events happened.   Hostile attribution bias   The ""hostile attribution bias"" is the tendency to interpret others' behaviors as having hostile intent, even when the behavior is ambiguous or benign. [67]    Hot-hand fallacy   The ""hot-hand fallacy"" (also known as the ""hot hand phenomenon"" or ""hot hand"") is the belief that a person who has experienced success with a random event has a greater chance of further success in additional attempts.   Hyperbolic discounting   Discounting is the tendency for people to have a stronger preference for more immediate payoffs relative to later payoffs. Hyperbolic discounting leads to choices that are inconsistent over time – people make choices today that their future selves would prefer not to have made, despite using the same reasoning. [68] Also known as current moment bias, present-bias, and related to Dynamic inconsistency . A good example of this: a study showed that when making food choices for the coming week, 74% of participants chose fruit, whereas when the food choice was for the current day, 70% chose chocolate.   IKEA effect   The tendency for people to place a disproportionately high value on objects that they partially assembled themselves, such as furniture from IKEA , regardless of the quality of the end product. [69]    Illicit transference   Occurs when a term in the distributive (referring to every member of a class) and collective (referring to the class itself as a whole) sense are treated as equivalent. The two variants of this fallacy are the fallacy of composition and the fallacy of division .   Illusion of control   The tendency to overestimate one's degree of influence over other external events. [70]    Illusion of validity   Believing that one's judgments are accurate, especially when available information is consistent or inter-correlated. [71]    Illusory correlation   Inaccurately perceiving a relationship between two unrelated events. [72] [73]    Illusory truth effect   A tendency to believe that a statement is true if it is easier to process , or if it has been stated multiple times , regardless of its actual veracity. These are specific cases of truthiness .   Impact bias   The tendency to overestimate the length or the intensity of the impact of future feeling states. [74]    Implicit association   The speed with which people can match words depends on how closely they are associated.   Information bias   The tendency to seek information even when it cannot affect action. [75]    Insensitivity to sample size   The tendency to under-expect variation in small samples.   Interoceptive bias  The tendency for sensory input about the body itself to affect one's judgement about external, unrelated circumstances. (As for example, in parole judges who are more lenient when fed and rested.) [76] [77] [78] [79]    Irrational escalation or Escalation of commitment   The phenomenon where people justify increased investment in a decision, based on the cumulative prior investment, despite new evidence suggesting that the decision was probably wrong. Also known as the sunk cost fallacy.   Law of the instrument   An over-reliance on a familiar tool or methods, ignoring or under-valuing alternative approaches. ""If all you have is a hammer, everything looks like a nail.""   Less-is-better effect   The tendency to prefer a smaller set to a larger set judged separately, but not jointly.   Look-elsewhere effect   An apparently statistically significant observation may have actually arisen by chance because of the size of the parameter space to be searched.   Loss aversion   The perceived disutility of giving up an object is greater than the utility associated with acquiring it. [80] (see also Sunk cost effects and endowment effect).   Mere exposure effect   The tendency to express undue liking for things merely because of familiarity with them. [81]    Money illusion   The tendency to concentrate on the nominal value (face value) of money rather than its value in terms of purchasing power. [82]    Moral credential effect   Occurs when someone who does something good gives themselves permission to be less good in the future.   Negativity bias or Negativity effect  Psychological phenomenon by which humans have a greater recall of unpleasant memories compared with positive memories. [83] [84] (see also actor-observer bias, group attribution error , positivity effect, and negativity effect ). [85]    Neglect of probability   The tendency to completely disregard probability when making a decision under uncertainty. [86]    Normalcy bias   The refusal to plan for, or react to, a disaster which has never happened before.   Not invented here   Aversion to contact with or use of products, research, standards, or knowledge developed outside a group. Related to IKEA effect .   Observer-expectancy effect   When a researcher expects a given result and therefore unconsciously manipulates an experiment or misinterprets data in order to find it (see also subject-expectancy effect ).   Omission bias   The tendency to judge harmful actions (commissions) as worse, or less moral, than equally harmful inactions (omissions). [87]    Optimism bias   The tendency to be over-optimistic, underestimating greatly the probability of undesirable outcomes and overestimating favorable and pleasing outcomes (see also wishful thinking , valence effect , positive outcome bias ). [88] [89]    Ostrich effect   Ignoring an obvious (negative) situation.   Outcome bias   The tendency to judge a decision by its eventual outcome instead of based on the quality of the decision at the time it was made.   Overconfidence effect   Excessive confidence in one's own answers to questions. For example, for certain types of questions, answers that people rate as ""99% certain"" turn out to be wrong 40% of the time. [6] [90] [91] [92]    Pareidolia   A vague and random stimulus (often an image or sound) is perceived as significant, e.g., seeing images of animals or faces in clouds, the man in the moon , and hearing non-existent hidden messages on records played in reverse .   Pygmalion effect   The phenomenon whereby others' expectations of a target person affect the target person's performance.   Pessimism bias   The tendency for some people, especially those suffering from depression , to overestimate the likelihood of negative things happening to them.   Plan continuation bias   Failure to recognize that the original plan of action is no longer appropriate for a changing situation or for a situation that is different than anticipated. [93]    Planning fallacy   The tendency to underestimate task-completion times. [74]    Present bias   The tendency of people to give stronger weight to payoffs that are closer to the present time when considering trade-offs between two future moments. [94]    Plant blindness   The tendency to ignore plants in their environment and a failure to recognize and appreciate the utility of plants to life on earth. [95]    Pro-innovation bias   The tendency to have an excessive optimism towards an invention or innovation's usefulness throughout society, while often failing to identify its limitations and weaknesses.   Projection bias   The tendency to overestimate how much our future selves share one's current preferences, thoughts and values, thus leading to sub-optimal choices. [96] [97] [84]    Pseudocertainty effect   The tendency to make risk-averse choices if the expected outcome is positive, but make risk-seeking choices to avoid negative outcomes. [98]    Reactance   The urge to do the opposite of what someone wants you to do out of a need to resist a perceived attempt to constrain your freedom of choice (see also Reverse psychology ).   Reactive devaluation   Devaluing proposals only because they purportedly originated with an adversary.   Recency illusion   The illusion that a phenomenon one has noticed only recently is itself recent. Often used to refer to linguistic phenomena; the illusion that a word or language usage that one has noticed only recently is an innovation when it is in fact long-established (see also frequency illusion ).   Systematic Bias  Judgement that arises when targets of differentiating judgement become subject to effects of regression that are not equivalent. [99]    Restraint bias   The tendency to overestimate one's ability to show restraint in the face of temptation.   Rhyme as reason effect   Rhyming statements are perceived as more truthful. A famous example being used in the O.J Simpson trial with the defense's use of the phrase ""If the gloves don't fit, then you must acquit.""   Risk compensation / Peltzman effect   The tendency to take greater risks when perceived safety increases.   Salience bias   The tendency to focus on items that are more prominent or emotionally striking and ignore those that are unremarkable, even though this difference is often irrelevant by objective standards.   Selection bias   The tendency to notice something more when something causes us to be more aware of it, such as when we buy a car, we tend to notice similar cars more often than we did before. They are not suddenly more common – we just are noticing them more. Also called the Observational Selection Bias.   Selective perception   The tendency for expectations to affect perception.   Semmelweis reflex   The tendency to reject new evidence that contradicts a paradigm. [33]    Sexual overperception bias / Sexual underperception bias   The tendency to over-/underestimate sexual interest of another person in oneself.   Social comparison bias   The tendency, when making decisions, to favour potential candidates who don't compete with one's own particular strengths. [100]    Social desirability bias   The tendency to over-report socially desirable characteristics or behaviours in oneself and under-report socially undesirable characteristics or behaviours. [101] See also: § Courtesy bias .   Status quo bias   The tendency to like things to stay relatively the same (see also loss aversion , endowment effect , and system justification ). [102] [103]    Stereotyping   Expecting a member of a group to have certain characteristics without having actual information about that individual.   Subadditivity effect   The tendency to judge probability of the whole to be less than the probabilities of the parts. [104]    Subjective validation   Perception that something is true if a subject's belief demands it to be true. Also assigns perceived connections between coincidences.   Surrogation   Losing sight of the strategic construct that a measure is intended to represent, and subsequently acting as though the measure is the construct of interest.   Survivorship bias   Concentrating on the people or things that ""survived"" some process and inadvertently overlooking those that didn't because of their lack of visibility.   Time-saving bias   Underestimations of the time that could be saved (or lost) when increasing (or decreasing) from a relatively low speed and overestimations of the time that could be saved (or lost) when increasing (or decreasing) from a relatively high speed.   Third-person effect   A hypothesized tendency to believe that mass communicated media messages have a greater effect on others than on themselves. As of 2020, the third-person effect has yet to be reliably demonstrated in a scientific context.   Parkinson's law of triviality   The tendency to give disproportionate weight to trivial issues. Also known as bikeshedding, this bias explains why an organization may avoid specialized or complex subjects, such as the design of a nuclear reactor, and instead focus on something easy to grasp or rewarding to the average participant, such as the design of an adjacent bike shed. [105]    Unit bias   The standard suggested amount of consumption (e.g., food serving size) is perceived to be appropriate, and a person would consume it all even if it is too much for this particular person. [106]    Weber–Fechner law   Difficulty in comparing small differences in large quantities.   Well travelled road effect   Underestimation of the duration taken to traverse oft-traveled routes and overestimation of the duration taken to traverse less familiar routes.   Women are wonderful effect   A tendency to associate more positive attributes with women than with men.   Zero-risk bias   Preference for reducing a small risk to zero over a greater reduction in a larger risk.   Zero-sum bias   A bias whereby a situation is incorrectly perceived to be like a zero-sum game (i.e., one person gains at the expense of another).  Social biases [ edit ]  Most of these biases are labeled as attributional biases .    Name  Description   Actor-observer bias   The tendency for explanations of other individuals' behaviors to overemphasize the influence of their personality and underemphasize the influence of their situation (see also Fundamental attribution error ), and for explanations of one's own behaviors to do the opposite (that is, to overemphasize the influence of our situation and underemphasize the influence of our own personality).   Authority bias   The tendency to attribute greater accuracy to the opinion of an authority figure (unrelated to its content) and be more influenced by that opinion. [107]    Cheerleader effect   The tendency for people to appear more attractive in a group than in isolation. [108]    Defensive attribution hypothesis   Attributing more blame to a harm-doer as the outcome becomes more severe or as personal or situational similarity to the victim increases.   Egocentric bias   Occurs when people claim more responsibility for themselves for the results of a joint action than an outside observer would credit them with.   Extrinsic incentives bias   An exception to the fundamental attribution error , when people view others as having (situational) extrinsic motivations and (dispositional) intrinsic motivations for oneself   False consensus effect   The tendency for people to overestimate the degree to which others agree with them. [109]    False uniqueness bias   The tendency of people to see their projects and themselves as more singular than they actually are. [110]    Fundamental attribution error   The tendency for people to over-emphasize personality-based explanations for behaviors observed in others while under-emphasizing the role and power of situational influences on the same behavior [84] (see also actor-observer bias, group attribution error , positivity effect, and negativity effect ). [85]    Group attribution error   The biased belief that the characteristics of an individual group member are reflective of the group as a whole or the tendency to assume that group decision outcomes reflect the preferences of group members, even when information is available that clearly suggests otherwise.   Halo effect   The tendency for a person's positive or negative traits to ""spill over"" from one personality area to another in others' perceptions of them (see also physical attractiveness stereotype ). [111]    Illusion of asymmetric insight   People perceive their knowledge of their peers to surpass their peers' knowledge of them. [112]    Illusion of external agency   When people view self-generated preferences as instead being caused by insightful, effective and benevolent agents.   Illusion of transparency   People overestimate others' ability to know themselves, and they also overestimate their ability to know others.   Illusory superiority   Overestimating one's desirable qualities, and underestimating undesirable qualities, relative to other people. (Also known as ""Lake Wobegon effect"", ""better-than-average effect"", or ""superiority bias"".) [113]    Ingroup bias   The tendency for people to give preferential treatment to others they perceive to be members of their own groups.   Intentionality bias  Tendency to judge human action to intentional rather than accidental. [114]    Just-world hypothesis   The tendency for people to want to believe that the world is fundamentally just, causing them to rationalize an otherwise inexplicable injustice as deserved by the victim(s).   Moral luck   The tendency for people to ascribe greater or lesser moral standing based on the outcome of an event.   Naïve cynicism   Expecting more egocentric bias in others than in oneself.   Naïve realism   The belief that we see reality as it really is – objectively and without bias; that the facts are plain for all to see; that rational people will agree with us; and that those who don't are either uninformed, lazy, irrational, or biased.   Outgroup homogeneity bias   Individuals see members of their own group as being relatively more varied than members of other groups. [115]    Puritanical bias   Refers to the tendency to attribute cause of an undesirable outcome or wrongdoing by an individual to a moral deficiency or lack of self control rather than taking into account the impact of broader societal determinants . [116]    Self-serving bias   The tendency to claim more responsibility for successes than failures. It may also manifest itself as a tendency for people to evaluate ambiguous information in a way beneficial to their interests (see also group-serving bias ). [117]    Shared information bias   Known as the tendency for group members to spend more time and energy discussing information that all members are already familiar with (i.e., shared information), and less time and energy discussing information that only some members are aware of (i.e., unshared information). [118]    System justification   The tendency to defend and bolster the status quo. Existing social, economic, and political arrangements tend to be preferred, and alternatives disparaged, sometimes even at the expense of individual and collective self-interest. (See also status quo bias.)   Trait ascription bias   The tendency for people to view themselves as relatively variable in terms of personality, behavior, and mood while viewing others as much more predictable.   Ultimate attribution error   Similar to the fundamental attribution error, in this error a person is likely to make an internal attribution to an entire group instead of the individuals within the group.   Worse-than-average effect   A tendency to believe ourselves to be worse than others at tasks which are difficult. [119]   Memory errors and biases [ edit ]  Main article: List of memory biases  In psychology  and  cognitive science , a memory bias is a cognitive bias that either enhances or impairs the recall of a memory (either the chances that the memory will be recalled at all, or the amount of time it takes for it to be recalled, or both), or that alters the content of a reported memory. There are many types of memory bias, including:    Name  Description   Bizarreness effect   Bizarre material is better remembered than common material.   Conservatism or Regressive bias  Tendency to remember high values and high likelihoods/probabilities/frequencies as lower than they actually were and low ones as higher than they actually were. Based on the evidence, memories are not extreme enough. [120] [121]    Consistency bias   Incorrectly remembering one's past attitudes and behaviour as resembling present attitudes and behaviour. [122]    Context effect   That cognition and memory are dependent on context, such that out-of-context memories are more difficult to retrieve than in-context memories (e.g., recall time and accuracy for a work-related memory will be lower at home, and vice versa).   Cross-race effect   The tendency for people of one race to have difficulty identifying members of a race other than their own.   Cryptomnesia   A form of misattribution where a memory is mistaken for imagination, because there is no subjective experience of it being a memory. [123]    Egocentric bias   Recalling the past in a self-serving manner, e.g., remembering one's exam grades as being better than they were, or remembering a caught fish as bigger than it really was.   Fading affect bias   A bias in which the emotion associated with unpleasant memories fades more quickly than the emotion associated with positive events. [124]    False memory   A form of misattribution where imagination is mistaken for a memory.   Generation effect (Self-generation effect)  That self-generated information is remembered best. For instance, people are better able to recall memories of statements that they have generated than similar statements generated by others.   Google effect   The tendency to forget information that can be found readily online by using Internet search engines.   Humor effect   That humorous items are more easily remembered than non-humorous ones, which might be explained by the distinctiveness of humor, the increased cognitive processing time to understand the humor, or the emotional arousal caused by the humor. [125]    Lag effect  The phenomenon whereby learning is greater when studying is spread out over time, as opposed to studying the same amount of time in a single session. See also spacing effect .   Leveling and sharpening   Memory distortions introduced by the loss of details in a recollection over time, often concurrent with sharpening or selective recollection of certain details that take on exaggerated significance in relation to the details or aspects of the experience lost through leveling. Both biases may be reinforced over time, and by repeated recollection or re-telling of a memory. [126]    Levels-of-processing effect   That different methods of encoding information into memory have different levels of effectiveness. [127]    List-length effect   A smaller percentage of items are remembered in a longer list, but as the length of the list increases, the absolute number of items remembered increases as well. For example, consider a list of 30 items (""L30"") and a list of 100 items (""L100""). An individual may remember 15 items from L30, or 50%, whereas the individual may remember 40 items from L100, or 40%. Although the percent of L30 items remembered (50%) is greater than the percent of L100 (40%), more L100 items (40) are remembered than L30 items (15). [128] [ further explanation needed ]    Misinformation effect   Memory becoming less accurate because of interference from post-event information . [129]    Modality effect   That memory recall is higher for the last items of a list when the list items were received via speech than when they were received through writing.   Mood-congruent memory bias   The improved recall of information congruent with one's current mood.   Next-in-line effect   When taking turns speaking in a group using a predetermined order (e.g. going clockwise around a room, taking numbers, etc.) people tend to have diminished recall for the words of the person who spoke immediately before them. [130]    Part-list cueing effect   That being shown some items from a list and later retrieving one item causes it to become harder to retrieve the other items. [131]    Peak-end rule   That people seem to perceive not the sum of an experience but the average of how it was at its peak (e.g., pleasant or unpleasant) and how it ended.   Picture superiority effect   The notion that concepts that are learned by viewing pictures are more easily and frequently recalled than are concepts that are learned by viewing their written word form counterparts. [132] [133] [134] [135] [136] [137]    Positivity effect ( Socioemotional selectivity theory )  That older adults favor positive over negative information in their memories.   Serial position effect   That items near the end of a sequence are the easiest to recall, followed by the items at the beginning of a sequence; items in the middle are the least likely to be remembered. [138]    Processing difficulty effect   That information that takes longer to read and is thought about more (processed with more difficulty) is more easily remembered. [139]    Reminiscence bump   The recalling of more personal events from adolescence and early adulthood than personal events from other lifetime periods. [140]    Self-relevance effect   That memories relating to the self are better recalled than similar information relating to others.   Source confusion   Confusing episodic memories with other information, creating distorted memories. [141]    Spacing effect   That information is better recalled if exposure to it is repeated over a long span of time rather than a short one.   Spotlight effect   The tendency to overestimate the amount that other people notice your appearance or behavior.   Stereotypical bias   Memory distorted towards stereotypes (e.g., racial or gender).   Suffix effect   Diminishment of the recency effect because a sound item is appended to the list that the subject is not required to recall. [142] [143]    Suggestibility   A form of misattribution where ideas suggested by a questioner are mistaken for memory.   Tachypsychia   When time perceived by the individual either lengthens, making events appear to slow down, or contracts. [144]    Telescoping effect   The tendency to displace recent events backward in time and remote events forward in time, so that recent events appear more remote, and remote events, more recent.   Testing effect   The fact that you more easily remember information you have read by rewriting it instead of rereading it. [145]    Tip of the tongue phenomenon  When a subject is able to recall parts of an item, or related information, but is frustratingly unable to recall the whole item. This is thought to be an instance of ""blocking"" where multiple similar memories are being recalled and interfere with each other. [123]    Travis Syndrome   Overestimating the significance of the present. [146] It is related to chronological snobbery with possibly an appeal to novelty  logical fallacy being part of the bias.   Verbatim effect   That the ""gist"" of what someone has said is better remembered than the verbatim wording. [147] This is because memories are representations, not exact copies.   von Restorff effect   That an item that sticks out is more likely to be remembered than other items. [148]    Zeigarnik effect   That uncompleted or interrupted tasks are remembered better than completed ones.  See also [ edit ]    Psychology portal  Society portal  Philosophy portal   Affective forecasting – Predicting someone's future emotions (affect)  Anecdotal evidence – Evidence relying on personal testimony  Apophenia – Tendency to perceive connections between unrelated things  Attribution (psychology) – The process by which individuals explain the causes of behavior and events  Black swan theory – Theory of response to surprise events  Chronostasis – Distortion in the perception of time  Cognitive distortion – An exaggerated or irrational thought pattern involved in the onset and perpetuation of psychopathological states  Defence mechanism – Unconscious psychological mechanism that reduces anxiety arising from unacceptable or potentially harmful stimuli  Dysrationalia – Inability to think and behave rationally despite adequate intelligence  Fear, uncertainty, and doubt – Tactic used to influence opinion by disseminating negative, dubious, or false information  Feedback – Process where information about current status is used to influence future status  Impostor syndrome – Psychological pattern of doubting one's accomplishments and fearing being exposed as a ""fraud""  List of common misconceptions – Wikipedia list article  List of fallacies – Types of reasoning that are logically incorrect  List of maladaptive schemas  List of memory biases  List of psychological effects – Wikipedia list article  iSheep#Behavioural patterns – Derogatory marketing term (references multiple cognitive biasses)  Media bias – Bias or perceived bias of journalists and news producers within the mass media in the selection of events and stories that are reported and how they are covered  Mind projection fallacy – An informal fallacy that the way one sees the world reflects the way the world really is  Motivated reasoning – Using emotionally-biased reasoning to produce justifications or make decisions  Observational error , also known as Systematic bias  Outline of public relations – Overview of and topical guide to public relations  Outline of thought – Overview of and topical guide to thought  Pollyanna principle – The tendency of people to remember pleasant events more than unpleasant ones  Positive feedback – Destabilising process that occurs in a feedback loop  Prevalence effect  Propaganda – Form of communication intended to sway the audience through presenting only one side of the argument  Publication bias – Higher probability of publishing results showing a significant finding  Recall bias – Systematic error caused by differences in the accuracy or completeness of the recollections retrieved  Self-handicapping – Cognitive strategy by which people avoid effort in the hopes of keeping potential failure from hurting self-esteem   Footnotes [ edit ]    ^  Haselton MG, Nettle D, Andrews PW (2005). ""The evolution of cognitive bias.""  (PDF) .  In Buss DM (ed.). The Handbook of Evolutionary Psychology . Hoboken, NJ, US: John Wiley & Sons Inc. pp. 724–746.   ^  ""Cognitive Bias – Association for Psychological Science"" . www.psychologicalscience.org . Retrieved 2018-10-10 .   ^  Thomas O (2018-01-19). ""Two decades of cognitive bias research in entrepreneurship: What do we know and where do we go from here?"". Management Review Quarterly . 68 (2): 107–143. doi : 10.1007/s11301-018-0135-9 . ISSN  2198-1620 .   ^  Dougherty MR, Gettys CF, Ogden EE (1999). ""MINERVA-DM: A memory processes model for judgments of likelihood""  (PDF) . Psychological Review . 106 (1): 180–209. doi : 10.1037/0033-295x.106.1.180 .   ^  Gigerenzer G (2006). ""Bounded and Rational"".  In Stainton RJ (ed.). Contemporary Debates in Cognitive Science . Blackwell. p. 129. ISBN  978-1-4051-1304-5 .   ^ a  b  c  d  e  Fielder, Klaus (October 2014). ""Regressive Judgment: Implications of a Universal Property of the Empirical World""  (PDF) . Sage Journals – via Google Scholar. Lay summary .   ^  MacCoun RJ (1998). ""Biases in the interpretation and use of research results""  (PDF) . Annual Review of Psychology . 49 (1): 259–87. doi : 10.1146/annurev.psych.49.1.259 . PMID  15012470 .   ^  Nickerson RS (1998). ""Confirmation Bias: A Ubiquitous Phenomenon in Many Guises""  (PDF) . Review of General Psychology . 2 (2): 175–220 [198]. doi : 10.1037/1089-2680.2.2.175 .   ^  Dardenne B, Leyens JP (1995). ""Confirmation Bias as a Social Skill"" . Personality and Social Psychology Bulletin . 21 (11): 1229–1239. doi : 10.1177/01461672952111011 .   ^  Alexander WH, Brown JW (June 2010). ""Hyperbolically discounted temporal difference learning"" . Neural Computation . 22 (6): 1511–27. doi : 10.1162/neco.2010.08-09-1080 . PMC  3005720 . PMID  20100071 .   ^  Baron 1994 , p. 372   ^  Zhang Y, Lewis M, Pellon M, Coleman P (2007). ""A Preliminary Research on Modeling Cognitive Agents for Social Environments in Multi-Agent Systems""  (PDF) : 116–123.  Cite journal requires |journal= ( help )   ^ a  b  c  Iverson GL, Brooks BL, Holdnack JA (2008). ""Misdiagnosis of Cognitive Impairment in Forensic Neuropsychology"".  In Heilbronner RL (ed.). Neuropsychology in the Courtroom: Expert Analysis of Reports and Testimony . New York: Guilford Press. p. 248. ISBN  9781593856342 . CS1 maint: ref=harv ( link )   ^  Coley JD, Tanner KD (2012). ""Common origins of diverse misconceptions: cognitive principles and the development of biology thinking"" . CBE Life Sciences Education . 11 (3): 209–15. doi : 10.1187/cbe.12-06-0074 . PMC  3433289 . PMID  22949417 .   ^  ""The Real Reason We Dress Pets Like People"" . LiveScience.com . Retrieved 2015-11-16 .   ^  Harris LT, Fiske ST (January 2011). ""Dehumanized Perception: A Psychological Means to Facilitate Atrocities, Torture, and Genocide?"" . Zeitschrift Fur Psychologie . 219 (3): 175–181. doi : 10.1027/2151-2604/a000065 . PMC  3915417 . PMID  24511459 .   ^  Bar-Haim Y, Lamy D, Pergamin L, Bakermans-Kranenburg MJ, van IJzendoorn MH (January 2007). ""Threat-related attentional bias in anxious and nonanxious individuals: a meta-analytic study"" . Psychological Bulletin . 133 (1): 1–24. doi : 10.1037/0033-2909.133.1.1 . PMID  17201568 .   ^  Goddard K, Roudsari A, Wyatt JC (2011). ""Automation Bias – A Hidden Issue for Clinical Decision Support System Use"" . International Perspectives in Health Informatics . Studies in Health Technology and Informatics. 164 . IOS Press. doi : 10.3233/978-1-60750-709-3-17 .   ^  Schwarz N, Bless H, Strack F, Klumpp G, Rittenauer-Schatka H, Simons A (1991). ""Ease of Retrieval as Information: Another Look at the Availability Heuristic""  (PDF) . Journal of Personality and Social Psychology . 61 (2): 195–202. doi : 10.1037/0022-3514.61.2.195 . Archived from the original  (PDF) on 9 February 2014 . Retrieved 19 Oct 2014 .   ^  Kuran T, Sunstein CR (1998). ""Availability Cascades and Risk Regulation"" . Stanford Law Review . 51 (4): 683–768. doi : 10.2307/1229439 . JSTOR  1229439 .   ^  Sanna LJ, Schwarz N, Stocker SL (2002). ""When debiasing backfires: Accessible content and accessibility experiences in debiasing hindsight""  (PDF) . Journal of Experimental Psychology: Learning, Memory, and Cognition . 28 (3): 497–502. CiteSeerX  10.1.1.387.5964 . doi : 10.1037/0278-7393.28.3.497 . ISSN  0278-7393 .   ^  Colman A (2003). Oxford Dictionary of Psychology . New York: Oxford University Press. p. 77 . ISBN  978-0-19-280632-1 .   ^  Baron 1994 , pp. 224–228   ^  Klauer KC, Musch J, Naumer B (October 2000). ""On belief bias in syllogistic reasoning"". Psychological Review . 107 (4): 852–84. doi : 10.1037/0033-295X.107.4.852 . PMID  11089409 .   ^  ""Harness the power of the 'Ben Franklin Effect' to get someone to like you"" . Business Insider . Retrieved 2018-10-10 .   ^  ""Berkson's Paradox | Brilliant Math & Science Wiki"" . brilliant.org . Retrieved 2018-10-10 .   ^  Pronin E, Kugler MB (July 2007). ""Valuing thoughts, ignoring behavior: The introspection illusion as a source of the bias blind spot"". Journal of Experimental Social Psychology . 43 (4): 565–578. doi : 10.1016/j.jesp.2006.05.011 . ISSN  0022-1031 .   ^  Mather M, Shafir E, Johnson MK (March 2000). ""Misremembrance of options past: source monitoring and choice""  (PDF) . Psychological Science . 11 (2): 132–8. doi : 10.1111/1467-9280.00228 . PMID  11273420 . Archived  (PDF) from the original on 2009-01-17.   ^  Västfjäll D, Slovic P, Mayorga M, Peters E (18 June 2014). ""Compassion fade: affect and charity are greatest for a single child in need"" . PLOS ONE . 9 (6): e100115. Bibcode : 2014PLoSO...9j0115V . doi : 10.1371/journal.pone.0100115 . PMC  4062481 . PMID  24940738 .   ^  Oswald ME, Grosjean S (2004). ""Confirmation Bias"" .  In Pohl RF (ed.). Cognitive Illusions: A Handbook on Fallacies and Biases in Thinking, Judgement and Memory . Hove, UK: Psychology Press. pp. 79–96 . ISBN  978-1-84169-351-4 . OCLC  55124398 .   ^  Fisk JE (2004). ""Conjunction fallacy"" .  In Pohl RF (ed.). Cognitive Illusions: A Handbook on Fallacies and Biases in Thinking, Judgement and Memory . Hove, UK: Psychology Press. pp. 23–42 . ISBN  978-1-84169-351-4 . OCLC  55124398 .   ^  DuCharme WW (1970). ""Response bias explanation of conservative human inference"". Journal of Experimental Psychology . 85 (1): 66–74. doi : 10.1037/h0029546 . hdl : 2060/19700009379 .   ^ a  b  Edwards W (1968). ""Conservatism in human information processing"".  In Kleinmuntz B (ed.). Formal representation of human judgment . New York: Wiley. pp. 17–52.   ^  Johnson HM, Seifert CM (November 1994). ""Sources of the continued influence effect: When misinformation in memory affects later inferences"". Journal of Experimental Psychology: Learning, Memory, and Cognition . 20 (6): 1420–1436. doi : 10.1037/0278-7393.20.6.1420 .   ^  Plous 1993 , pp. 38–41   ^  Ciccarelli S, White J (2014). Psychology (4th ed.). Pearson Education, Inc. p. 62. ISBN  978-0205973354 .   ^  Ackerman MS, ed. (2003). Sharing expertise beyond knowledge management (online ed.). Cambridge, Massachusetts: MIT Press. p. 7 . ISBN  9780262011952 .   ^  Quartz SR, The State Of The World Isn't Nearly As Bad As You Think , Edge Foundation, Inc. , retrieved 2016-02-17   ^  ""Evolution and cognitive biases: the decoy effect"" . FutureLearn . Retrieved 2018-10-10 .   ^  ""The Default Effect: How to Leverage Bias and Influence Behavior"" . Influence at Work. 2012-01-11 . Retrieved 2018-10-10 .   ^  Why We Spend Coins Faster Than Bills by Chana Joffe-Walt. All Things Considered , 12 May 2009.   ^  Hsee CK, Zhang J (May 2004). ""Distinction bias: misprediction and mischoice due to joint evaluation"". Journal of Personality and Social Psychology . 86 (5): 680–95. CiteSeerX  10.1.1.484.9171 . doi : 10.1037/0022-3514.86.5.680 . PMID  15161394 .   ^  de Meza D, Dawson C (January 24, 2018). ""Wishful Thinking, Prudent Behavior: The Evolutionary Origin of Optimism, Loss Aversion and Disappointment Aversion"". SSRN  3108432 .  Cite journal requires |journal= ( help )   ^  Kruger J, Dunning D (December 1999). ""Unskilled and unaware of it: how difficulties in recognizing one's own incompetence lead to inflated self-assessments"". Journal of Personality and Social Psychology . 77 (6): 1121–34. CiteSeerX  10.1.1.64.2655 . doi : 10.1037/0022-3514.77.6.1121 . PMID  10626367 .   ^  Duration Neglect in Retrospective Evaluations of Affective Episodes  Archived 2017-08-08 at the Wayback Machine | Journal of Personality and Social Psychology   ^  ""Understanding and Mastering the Empathy Gap"" . Psychology Today .   ^  Quoidbach J, Gilbert DT , Wilson TD (January 2013). ""The end of history illusion""  (PDF) . Science . 339 (6115): 96–8. Bibcode : 2013Sci...339...96Q . doi : 10.1126/science.1229294 . PMID  23288539 . Archived from the original  (PDF) on 2013-01-13. Young people, middle-aged people, and older people all believed they had changed a lot in the past but would change relatively little in the future.   ^  ( Kahneman, Knetsch & Thaler 1991 , p. 193) Richard Thaler coined the term ""endowment effect.""   ^  Jeng M (2006). ""A selected history of expectation bias in physics"". American Journal of Physics . 74 (7): 578–583. arXiv : physics/0508199 . Bibcode : 2006AmJPh..74..578J . doi : 10.1119/1.2186333 .   ^  Kahneman D, Krueger AB, Schkade D, Schwarz N, Stone AA (June 2006). ""Would you be happier if you were richer? A focusing illusion""  (PDF) . Science . 312 (5782): 1908–10. Bibcode : 2006Sci...312.1908K . CiteSeerX  10.1.1.373.2683 . doi : 10.1126/science.1129688 . PMID  16809528 .   ^  ""The Barnum Demonstration"" . psych.fullerton.edu . Retrieved 2018-10-10 .   ^  Haring KS, Watanabe K, Velonaki M, Tossell CC, Finomore V (2018). ""FFAB-The Form Function Attribution Bias in Human Robot Interaction"". IEEE Transactions on Cognitive and Developmental Systems . 10 (4): 843–851. doi : 10.1109/TCDS.2018.2851569 .   ^  Zwicky A (2005-08-07). ""Just Between Dr. Language and I"" . Language Log .   ^  ""The Baader-Meinhof Phenomenon"" . Damn Interesting . Retrieved 2020-02-16 .   ^  ""What's the Baader-Meinhof phenomenon?"" . howstuffworks.com . 20 March 2015 . Retrieved 15 April 2018 .   ^  ""What's in a name?"" . twincities.com . St. Paul Pioneer Press . Retrieved June 5, 2020 . As you might guess, the phenomenon is named after an incident in which I was talking to a friend about the Baader-Meinhof gang (and this was many years after they were in the news). The next day, my friend phoned me and referred me to an article in that day’s newspaper in which the Baader-Meinhof gang was mentioned.   ^  ""The Psychology Guide: What Does Functional Fixedness Mean?"" . PsycholoGenie . Retrieved 2018-10-10 .   ^  Investopedia Staff (2006-10-29). ""Gambler's Fallacy/Monte Carlo Fallacy"" . Investopedia . Retrieved 2018-10-10 .   ^  ""GSNI | Human Development Reports"" . hdr.undp.org . Retrieved 2020-06-10 .   ^  Abel, Martin (September 2019). ""Do Workers Discriminate against Female Bosses?""  (PDF) . IZA Institute of Labor Economics .   ^  Bian, Lin; Leslie, Sarah-Jane; Cimpian, Andrei (November 2018). ""Evidence of bias against girls and women in contexts that emphasize intellectual ability"" . American Psychologist . 73 (9): 1139–1153. doi : 10.1037/amp0000427 . ISSN  1935-990X .   ^  Hamilton, Mykol C. (1991). ""Masculine Bias in the Attribution of Personhood: People = Male, Male = People"" . Psychology of Women Quarterly . 15 (3): 393–402. doi : 10.1111/j.1471-6402.1991.tb00415.x . ISSN  0361-6843 .   ^  Lichtenstein S, Fischhoff B (1977). ""Do those who know more also know more about how much they know?"". Organizational Behavior and Human Performance . 20 (2): 159–183. doi : 10.1016/0030-5073(77)90001-0 .   ^  Merkle EC (February 2009). ""The disutility of the hard-easy effect in choice confidence"". Psychonomic Bulletin & Review . 16 (1): 204–13. doi : 10.3758/PBR.16.1.204 . PMID  19145033 .   ^  Juslin P, Winman A, Olsson H (April 2000). ""Naive empiricism and dogmatism in confidence research: a critical examination of the hard-easy effect"". Psychological Review . 107 (2): 384–96. doi : 10.1037/0033-295x.107.2.384 . PMID  10789203 .   ^  Pohl RF (2004). ""Hindsight Bias"" .  In Pohl RF (ed.). Cognitive Illusions: A Handbook on Fallacies and Biases in Thinking, Judgement and Memory . Hove, UK: Psychology Press. pp. 363–378 . ISBN  978-1-84169-351-4 . OCLC  55124398 .   ^  Anderson KB, Graham LM (2007). Hostile Attribution Bias . Encyclopedia of Social Psychology . SAGE Publications, Inc. pp. 446–447. doi : 10.4135/9781412956253 . ISBN  9781412916707 .   ^  Laibson D (1997). ""Golden Eggs and Hyperbolic Discounting"". Quarterly Journal of Economics . 112 (2): 443–477. CiteSeerX  10.1.1.337.3544 . doi : 10.1162/003355397555253 .   ^  The “IKEA Effect”: When Labor Leads to Love | Harvard Business School   ^  Thompson SC (1999). ""Illusions of Control: How We Overestimate Our Personal Influence"". Current Directions in Psychological Science . 8 (6): 187–190. doi : 10.1111/1467-8721.00044 . ISSN  0963-7214 . JSTOR  20182602 . CS1 maint: ref=harv ( link )   ^  Dierkes M, Antal AB, Child J, Ikujiro Nonaka (2003). Handbook of Organizational Learning and Knowledge . Oxford University Press. p. 22. ISBN  978-0-19-829582-2 . Retrieved 9 September 2013 .   ^  Tversky A, Kahneman D (September 1974). ""Judgment under Uncertainty: Heuristics and Biases"". Science . 185 (4157): 1124–31. Bibcode : 1974Sci...185.1124T . doi : 10.1126/science.185.4157.1124 . PMID  17835457 .   ^  Fiedler K (1991). ""The tricky nature of skewed frequency tables: An information loss account of distinctiveness-based illusory correlations"". Journal of Personality and Social Psychology . 60 (1): 24–36. doi : 10.1037/0022-3514.60.1.24 .   ^ a  b  Sanna LJ, Schwarz N (July 2004). ""Integrating temporal biases: the interplay of focal thoughts and accessibility experiences"" . Psychological Science . 15 (7): 474–81. doi : 10.1111/j.0956-7976.2004.00704.x . PMID  15200632 .   ^  Baron 1994 , pp. 258–259   ^  Danziger S, Levav J, Avnaim-Pesso L (April 2011). ""Extraneous factors in judicial decisions"" . Proceedings of the National Academy of Sciences of the United States of America . 108 (17): 6889–92. Bibcode : 2011PNAS..108.6889D . doi : 10.1073/pnas.1018033108 . PMC  3084045 . PMID  21482790 .   ^  Zaman J, De Peuter S, Van Diest I, Van den Bergh O, Vlaeyen JW (November 2016). ""Interoceptive cues predicting exteroceptive events"". International Journal of Psychophysiology . 109 : 100–106. doi : 10.1016/j.ijpsycho.2016.09.003 . PMID  27616473 .   ^  Barrett LF, Simmons WK (July 2015). ""Interoceptive predictions in the brain"" . Nature Reviews. Neuroscience . 16 (7): 419–29. doi : 10.1038/nrn3950 . PMC  4731102 . PMID  26016744 .   ^  Damasio AR (October 1996). ""The somatic marker hypothesis and the possible functions of the prefrontal cortex"" . Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences . 351 (1346): 1413–20. doi : 10.1098/rstb.1996.0125 . PMID  8941953 .   ^  ( Kahneman, Knetsch & Thaler 1991 , p. 193) Daniel Kahneman, together with Amos Tversky, coined the term ""loss aversion.""   ^  Bornstein RF, Crave-Lemley C (2004). ""Mere exposure effect"" .  In Pohl RF (ed.). Cognitive Illusions: A Handbook on Fallacies and Biases in Thinking, Judgement and Memory . Hove, UK: Psychology Press. pp. 215–234 . ISBN  978-1-84169-351-4 . OCLC  55124398 .   ^  Shafir E, Diamond P, Tversky A (2000). ""Money Illusion"".  In Kahneman D, Tversky A (eds.). Choices, values, and frames . Cambridge University Press. pp. 335–355. ISBN  978-0-521-62749-8 .   ^  Haizlip J, May N, Schorling J, Williams A, Plews-Ogan M (September 2012). ""Perspective: the negativity bias, medical education, and the culture of academic medicine: why culture change is hard"". Academic Medicine . 87 (9): 1205–9. doi : 10.1097/ACM.0b013e3182628f03 . PMID  22836850 .   ^ a  b  c  Trofimova I (2014). ""Observer bias: an interaction of temperament traits with biases in the semantic perception of lexical material"" . PLOS ONE . 9 (1): e85677. Bibcode : 2014PLoSO...985677T . doi : 10.1371/journal.pone.0085677 . PMC  3903487 . PMID  24475048 .   ^ a  b  Sutherland 2007 , pp. 138–139   ^  Baron 1994 , p. 353   ^  Baron 1994 , p. 386   ^  Baron 1994 , p. 44   ^  Hardman 2009 , p. 104   ^  Adams PA, Adams JK (December 1960). ""Confidence in the recognition and reproduction of words difficult to spell"". The American Journal of Psychology . 73 (4): 544–52. doi : 10.2307/1419942 . JSTOR  1419942 . PMID  13681411 .   ^  Hoffrage U (2004). ""Overconfidence"" .  In Rüdiger Pohl (ed.). Cognitive Illusions: a handbook on fallacies and biases in thinking, judgement and memory . Psychology Press. ISBN  978-1-84169-351-4 .   ^  Sutherland 2007 , pp. 172–178   ^  Tuccio, William (2011-01-01). ""Heuristics to Improve Human Factors Performance in Aviation"". Journal of Aviation/Aerospace Education & Research . 20 (3). doi : 10.15394/jaaer.2011.1640 . ISSN  2329-258X .   ^  O'Donoghue T, Rabin M (1999). ""Doing it now or later"" . American Economic Review . 89 (1): 103–124. doi : 10.1257/aer.89.1.103 .   ^  Balas B, Momsen JL (September 2014).  Holt EA (ed.). ""Attention ""blinks"" differently for plants and animals"" . CBE Life Sciences Education . 13 (3): 437–43. doi : 10.1187/cbe.14-05-0080 . PMC  4152205 . PMID  25185227 .   ^  Hsee CK, Hastie R (January 2006). ""Decision and experience: why don't we choose what makes us happy?""  (PDF) . Trends in Cognitive Sciences . 10 (1): 31–7. CiteSeerX  10.1.1.178.7054 . doi : 10.1016/j.tics.2005.11.007 . PMID  16318925 . Archived  (PDF) from the original on 2015-04-20.   ^  Trofimova I (October 1999). ""An investigation of how people of different age, sex, and temperament estimate the world"". Psychological Reports . 85 (2): 533–52. doi : 10.2466/pr0.1999.85.2.533 . PMID  10611787 .   ^  Hardman 2009 , p. 137   ^  Fiedler, Klaus; Unkelbach, Christian (2014-10-01). ""Regressive Judgment: Implications of a Universal Property of the Empirical World"" . Current Directions in Psychological Science . 23 (5): 361–367. doi : 10.1177/0963721414546330 . ISSN  0963-7214 .   ^  Garcia SM, Song H, Tesser A (November 2010). ""Tainted recommendations: The social comparison bias"". Organizational Behavior and Human Decision Processes . 113 (2): 97–101. doi : 10.1016/j.obhdp.2010.06.002 . ISSN  0749-5978 . Lay summary – BPS Research Digest (2010-10-30).   ^  Dalton D, Ortegren M (2011). ""Gender differences in ethics research: The importance of controlling for the social desirability response bias"". Journal of Business Ethics . 103 (1): 73–93. doi : 10.1007/s10551-011-0843-8 .   ^  Kahneman, Knetsch & Thaler 1991 , p. 193   ^  Baron 1994 , p. 382   ^  Baron, J. (in preparation). Thinking and Deciding , 4th edition. New York: Cambridge University Press.   ^  Forsyth DR (2009). Group Dynamics (5th ed.). Cengage Learning. p. 317. ISBN  978-0-495-59952-4 .   ^  ""Penn Psychologists Believe 'Unit Bias' Determines The Acceptable Amount To Eat"" . ScienceDaily (November 21, 2005)   ^  Milgram S (October 1963). ""Behavioral Study of Obedience"". Journal of Abnormal Psychology . 67 (4): 371–8. doi : 10.1037/h0040525 . PMID  14049516 .   ^  Walker D, Vul E (January 2014). ""Hierarchical encoding makes individuals in a group seem more attractive"". Psychological Science . 25 (1): 230–5. doi : 10.1177/0956797613497969 . PMID  24163333 .   ^  Marks G, Miller N (1987). ""Ten years of research on the false-consensus effect: An empirical and theoretical review"". Psychological Bulletin . 102 (1): 72–90. doi : 10.1037/0033-2909.102.1.72 .   ^  ""False Uniqueness Bias (SOCIAL PSYCHOLOGY) – IResearchNet"" . 2016-01-13.   ^  Baron 1994 , p. 275   ^  Pronin E, Kruger J, Savitsky K, Ross L (October 2001). ""You don't know me, but I know you: the illusion of asymmetric insight"". Journal of Personality and Social Psychology . 81 (4): 639–56. doi : 10.1037/0022-3514.81.4.639 . PMID  11642351 .   ^  Hoorens V (1993). ""Self-enhancement and Superiority Biases in Social Comparison"". European Review of Social Psychology . 4 (1): 113–139. doi : 10.1080/14792779343000040 .   ^  Rosset, Evelyn (2008-09-01). ""It's no accident: Our bias for intentional explanations"". Cognition . 108 (3): 771–780. doi : 10.1016/j.cognition.2008.07.001 . ISSN  0010-0277 . PMID  18692779 .   ^  Plous 2006 , p. 206 harvnb error: no target: CITEREFPlous2006 ( help )   ^  Kokkoris, Michail (2020-01-16). ""The Dark Side of Self-Control"" . Harvard Business Review . Retrieved 17 January 2020 .   ^  Plous 2006 , p. 185 harvnb error: no target: CITEREFPlous2006 ( help )   ^  Forsyth DR (2009). Group Dynamics (5th ed.). Pacific Grove, CA: Brooks/Cole.   ^  Kruger J (August 1999). ""Lake Wobegon be gone! The ""below-average effect"" and the egocentric nature of comparative ability judgments"". Journal of Personality and Social Psychology . 77 (2): 221–32. doi : 10.1037/0022-3514.77.2.221 . PMID  10474208 .   ^  Attneave F (August 1953). ""Psychological probability as a function of experienced frequency"". Journal of Experimental Psychology . 46 (2): 81–6. doi : 10.1037/h0057955 . PMID  13084849 .   ^  Fischhoff B, Slovic P, Lichtenstein S (1977). ""Knowing with certainty: The appropriateness of extreme confidence"" . Journal of Experimental Psychology: Human Perception and Performance . 3 (4): 552–564. doi : 10.1037/0096-1523.3.4.552 .   ^  Cacioppo J (2002). Foundations in social neuroscience . Cambridge, Mass: MIT Press. pp. 130–132. ISBN  978-0262531955 .   ^ a  b  Schacter DL (March 1999). ""The seven sins of memory. Insights from psychology and cognitive neuroscience"" . The American Psychologist . 54 (3): 182–203. doi : 10.1037/0003-066X.54.3.182 . PMID  10199218 .   ^  Schmidt SR (July 1994). ""Effects of humor on sentence memory""  (PDF) . Journal of Experimental Psychology. Learning, Memory, and Cognition . 20 (4): 953–67. doi : 10.1037/0278-7393.20.4.953 . PMID  8064254 . Archived from the original  (PDF) on 2016-03-15 . Retrieved 2015-04-19 .   ^  Schmidt SR (2003). ""Life Is Pleasant—and Memory Helps to Keep It That Way!""  (PDF) . Review of General Psychology . 7 (2): 203–210. doi : 10.1037/1089-2680.7.2.203 .   ^  Koriat A, Goldsmith M, Pansky A (2000). ""Toward a psychology of memory accuracy"". Annual Review of Psychology . 51 (1): 481–537. doi : 10.1146/annurev.psych.51.1.481 . PMID  10751979 .   ^  Craik & Lockhart, 1972   ^  Kinnell A, Dennis S (February 2011). ""The list length effect in recognition memory: an analysis of potential confounds"". Memory & Cognition . 39 (2): 348–63. doi : 10.3758/s13421-010-0007-6 . PMID  21264573 .   ^  Wayne Weiten (2010). Psychology: Themes and Variations . Cengage Learning. p. 338. ISBN  978-0-495-60197-5 .   ^  Wayne Weiten (2007). Psychology: Themes and Variations . Cengage Learning. p. 260. ISBN  978-0-495-09303-9 .   ^  Slamecka NJ (April 1968). ""An examination of trace storage in free recall"". Journal of Experimental Psychology . 76 (4): 504–13. doi : 10.1037/h0025695 . PMID  5650563 .   ^  Shepard RN (1967). ""Recognition memory for words, sentences, and pictures"". Journal of Learning and Verbal Behavior . 6 : 156–163. doi : 10.1016/s0022-5371(67)80067-7 .   ^  McBride DM, Dosher BA (2002). ""A comparison of conscious and automatic memory processes for picture and word stimuli: a process dissociation analysis"". Consciousness and Cognition . 11 (3): 423–460. doi : 10.1016/s1053-8100(02)00007-7 . PMID  12435377 .   ^  Defetyer MA, Russo R, McPartlin PL (2009). ""The picture superiority effect in recognition memory: a developmental study using the response signal procedure"". Cognitive Development . 24 (3): 265–273. doi : 10.1016/j.cogdev.2009.05.002 .   ^  Whitehouse AJ, Maybery MT, Durkin K (2006). ""The development of the picture-superiority effect"". British Journal of Developmental Psychology . 24 (4): 767–773. doi : 10.1348/026151005X74153 .   ^  Ally BA, Gold CA, Budson AE (January 2009). ""The picture superiority effect in patients with Alzheimer's disease and mild cognitive impairment"" . Neuropsychologia . 47 (2): 595–8. doi : 10.1016/j.neuropsychologia.2008.10.010 . PMC  2763351 . PMID  18992266 .   ^  Curran T, Doyle J (May 2011). ""Picture superiority doubly dissociates the ERP correlates of recollection and familiarity"". Journal of Cognitive Neuroscience . 23 (5): 1247–62. doi : 10.1162/jocn.2010.21464 . PMID  20350169 .   ^  Martin GN, Carlson NR, Buskist W (2007). Psychology (3rd ed.). Pearson Education. pp. 309–310. ISBN  978-0-273-71086-8 .   ^  O'Brien EJ, Myers JL (1985). ""When comprehension difficulty improves memory for text"". Journal of Experimental Psychology: Learning, Memory, and Cognition . 11 (1): 12–21. doi : 10.1037/0278-7393.11.1.12 .   ^  Rubin, Wetzler & Nebes, 1986; Rubin, Rahhal & Poon, 1998   ^  Lieberman DA (8 December 2011). Human Learning and Memory . Cambridge University Press. p. 432. ISBN  978-1-139-50253-5 .   ^  Morton, Crowder & Prussin, 1971   ^  Pitt I, Edwards AD (2003). Design of Speech-Based Devices: A Practical Guide . Springer. p. 26. ISBN  978-1-85233-436-9 .   ^  Stetson C, Fiesta MP, Eagleman DM (December 2007). ""Does time really slow down during a frightening event?"" . PLOS ONE . 2 (12): e1295. Bibcode : 2007PLoSO...2.1295S . doi : 10.1371/journal.pone.0001295 . PMC  2110887 . PMID  18074019 .   ^  Goldstein EB (2010-06-21). Cognitive Psychology: Connecting Mind, Research and Everyday Experience . Cengage Learning. p. 231. ISBN  978-1-133-00912-2 .   ^  ""Not everyone is in such awe of the internet"" . Evening Standard . Evening Standard. 2011-03-23 . Retrieved 28 October 2015 .   ^  Poppenk, Walia, Joanisse, Danckert, & Köhler, 2006   ^  Von Restorff, H (1933). ""Über die Wirkung von Bereichsbildungen im Spurenfeld (The effects of field formation in the trace field) "" "". Psychological Research . 18 (1): 299–342. doi : 10.1007/bf02409636 .    References [ edit ]   Baron J (1994). Thinking and deciding (2nd ed.). Cambridge University Press. ISBN  978-0-521-43732-5 . CS1 maint: ref=harv ( link )  Baron J (2000). Thinking and deciding (3rd ed.). New York: Cambridge University Press. ISBN  978-0-521-65030-4 . CS1 maint: ref=harv ( link )  Bishop MA, Trout JD (2004). Epistemology and the Psychology of Human Judgment . New York: Oxford University Press . ISBN  978-0-19-516229-5 . CS1 maint: ref=harv ( link )  Gilovich T (1993). How We Know What Isn't So: The Fallibility of Human Reason in Everyday Life . New York: The Free Press. ISBN  978-0-02-911706-4 . CS1 maint: ref=harv ( link )  Gilovich T, Griffin D, Kahneman D (2002). Heuristics and biases: The psychology of intuitive judgment . Cambridge, UK: Cambridge University Press. ISBN  978-0-521-79679-8 . CS1 maint: ref=harv ( link )  Greenwald AG (1980). ""The Totalitarian Ego: Fabrication and Revision of Personal History""  (PDF) . American Psychologist . 35 (7): 603–618. doi : 10.1037/0003-066x.35.7.603 . ISSN  0003-066X . CS1 maint: ref=harv ( link )  Hardman D (2009). Judgment and decision making: psychological perspectives . Wiley-Blackwell. ISBN  978-1-4051-2398-3 . CS1 maint: ref=harv ( link )  Kahneman D, Slovic P, Tversky A (1982). Judgment under Uncertainty: Heuristics and Biases . Science . 185 . Cambridge, UK: Cambridge University Press. pp. 1124–31. doi : 10.1126/science.185.4157.1124 . ISBN  978-0-521-28414-1 . PMID  17835457 . CS1 maint: ref=harv ( link )  Kahneman D, Knetsch JL, Thaler RH (1991). ""Anomalies: The Endowment Effect, Loss Aversion, and Status Quo Bias""  (PDF) . The Journal of Economic Perspectives . 5 (1): 193–206. doi : 10.1257/jep.5.1.193 . Archived from the original  (PDF) on November 24, 2012. CS1 maint: ref=harv ( link )  Plous S (1993). The Psychology of Judgment and Decision Making . New York: McGraw-Hill. ISBN  978-0-07-050477-6 . CS1 maint: ref=harv ( link )  Pohl RF (2017). Cognitive illusions: Intriguing phenomena in thinking, judgment and memory (2nd ed.). London and New York: Routledge. ISBN  978-1-138-90341-8 .  Schacter DL (March 1999). ""The seven sins of memory. Insights from psychology and cognitive neuroscience""  (PDF) . The American Psychologist . 54 (3): 182–203. doi : 10.1037/0003-066X.54.3.182 . PMID  10199218 . Archived from the original  (PDF) on May 13, 2013. CS1 maint: ref=harv ( link )  Sutherland S (2007). Irrationality . Pinter & Martin. ISBN  978-1-905177-07-3 . CS1 maint: ref=harv ( link )  Tetlock PE (2005). Expert Political Judgment: how good is it? how can we know? . Princeton: Princeton University Press. ISBN  978-0-691-12302-8 . CS1 maint: ref=harv ( link )  Virine L, Trumper M (2007). Project Decisions: The Art and Science . Vienna, VA: Management Concepts. ISBN  978-1-56726-217-9 . CS1 maint: ref=harv ( link )   v t e Biases Cognitive biases  Actor–observer  Acquiescence  Ambiguity  Anchoring  Attentional  Attribution  Authority  Automation  Belief  Blind spot  Choice-supportive  Confirmation  Congruence  Cultural  Distinction  Dunning–Kruger  Egocentric  Emotional  Extrinsic incentives  Fading affect  Framing  Correspondence  Halo effect  Hindsight  Horn effect  Hostile attribution  Impact  Implicit  In-group  Mere-exposure effect  Negativity  Normalcy  Omission  Optimism  Out-group homogeneity  Outcome  Overton window  Precision  Present  Pro-innovation  Response  Restraint  Self-serving  Social comparison  Status quo  Time-saving  Trait ascription  von Restorff effect  Zero-risk  In animals  Statistical biases  Estimator  Forecast  Healthy user  Information  Psychological  Lead time  Length time  Non-response  Observer  Omitted-variable  Participation  Recall  Sampling  Selection  Self-selection  Social desirability  Spectrum  Survivorship  Systematic error  Systemic  Verification  Wet  Other biases  Academic  Funding  FUTON  Inductive  Infrastructure  Inherent  In education  Media  False balance  Vietnam War  Norway  South Asia  Sweden  United States  Arab–Israeli conflict  Ukraine  Net  Political bias  Publication  Reporting  White hat  Bias reduction  Cognitive bias mitigation  Debiasing  Heuristics in judgment and decision-making  Lists: General  ·  Memory      Retrieved from "" https://en.wikipedia.org/w/index.php?title=List_of_cognitive_biases&oldid=965490136 ""  Categories : Cognitive biases Psychology lists Behavioral finance Cognitive science lists Hidden categories: CS1 errors: missing periodical CS1 maint: ref=harv Webarchive template wayback links Harv and Sfn no-target errors Articles with short description Wikipedia articles needing clarification from November 2013         Navigation menu      Personal tools      Not logged in Talk Contributions Create account Log in        Namespaces      Article Talk        Variants              Views      Read Edit View history        More           Search                     Navigation      Main page Contents Current events Random article About Wikipedia Contact us Donate Wikipedia store       Contribute      Help Community portal Recent changes Upload file       Tools      What links here Related changes Upload file Special pages Permanent link Page information Wikidata item Cite this page       Print/export      Download as PDF Printable version       Languages      العربية Azərbaycanca Català Deutsch Español فارسی Polski Português Русский ไทย Українська 中文  Edit links        This page was last edited on 1 July 2020, at 16:47 (UTC) .  Text is available under the Creative Commons Attribution-ShareAlike License ;
additional terms may apply.  By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc. , a non-profit organization.    Privacy policy  About Wikipedia  Disclaimers  Contact Wikipedia  Developers  Statistics  Cookie statement  Mobile view          "
14,undesirability of a negative event or consequence(fallacy OR bias OR heuristic) (coronavirus OR covid OR pandemic),1,https://www.sciencedirect.com/science/article/pii/S0960982211011912,"The ability to anticipate is a hallmark of cognition. Inferences about what will occur in the future are critical to decision making, enabling us to prepare our actions so as to avoid harm and gain reward. Given the importance of these future projections, one might expect the brain to possess accurate, unbiased foresight. Humans, however, exhibit a pervasive and surprising bias: when it comes to predicting what will happen to us tomorrow, next week, or fifty years from now, we overestimate the likelihood of positive events, and underestimate the likelihood of negative events. For example, we underrate our chances of getting divorced, being in a car accident, or suffering from cancer. We also expect to live longer than objective measures would warrant, overestimate our success in the job market, and believe that our children will be especially talented. This phenomenon is known as the optimism bias, and it is one of the most consistent, prevalent, and robust biases documented in psychology and behavioral economics.","          List of cognitive biases   From Wikipedia, the free encyclopedia    Jump to navigation  Jump to search  Systematic patterns of deviation from norm or rationality in judgment   The loss aversion cognitive bias has been shown in monkeys  Cognitive biases are systematic patterns of deviation from norm and\or rationality in judgment. They are often studied in psychology and behavioral economics . [1]  Although the reality of most of these biases is confirmed by reproducible research, [2] [3] there are often controversies about how to classify these biases or how to explain them. [4]  Gerd Gigerenzer has criticized the framing of cognitive biases as errors in judgment, and favors interpreting them as arising from rational deviations from logical thought. [5]  Explanations include information-processing rules (i.e., mental shortcuts), called heuristics , that the brain uses to produce decisions or judgments. Biases have a variety of forms and appear as cognitive (""cold"") bias, such as mental noise, [6] or motivational (""hot"") bias, such as when beliefs are distorted by wishful thinking . Both effects can be present at the same time. [7] [8]  There are also controversies over some of these biases as to whether they count as useless or irrational , or whether they result in useful attitudes or behavior. For example, when getting to know others, people tend to ask leading questions which seem biased towards confirming their assumptions about the person. However, this kind of confirmation bias has also been argued to be an example of social skill ; a way to establish a connection with the other person. [9]  Although this research overwhelmingly involves human subjects, some findings that demonstrate bias have been found in non-human animals as well. For example, loss aversion has been shown in monkeys and hyperbolic discounting has been observed in rats, pigeons, and monkeys. [10]   Contents   1  Decision-making, belief, and behavioral biases  2  Social biases  3  Memory errors and biases  4  See also  5  Footnotes  6  References    Decision-making, belief, and behavioral biases [ edit ]  Many of these biases affect belief formation, business and economic decisions, and human behavior in general.    Name  Description   Agent detection   The inclination to presume the purposeful intervention of a sentient or intelligent agent .   Ambiguity effect   The tendency to avoid options for which the probability of a favorable outcome is unknown. [11]    Anchoring or focalism  The tendency to rely too heavily, or ""anchor"", on one trait or piece of information when making decisions (usually the first piece of information acquired on that subject). [12] [13]    Anthropocentric thinking   The tendency to use human analogies as a basis for reasoning about other, less familiar, biological phenomena. [14]    Anthropomorphism or personification  The tendency to characterize animals, objects, and abstract concepts as possessing human-like traits, emotions, and intentions. [15] The opposite bias, of not attributing feelings or thoughts to another person, is dehumanised perception , [16] a type of objectification .   Attentional bias   The tendency of perception to be affected by recurring thoughts. [17]    Attribute substitution   Occurs when a judgment has to be made (of a target attribute) that is computationally complex, and instead a more easily calculated heuristic attribute is substituted. This substitution is thought of as taking place in the automatic intuitive judgment system, rather than the more self-aware reflective system.   Automation bias   The tendency to depend excessively on automated systems which can lead to erroneous automated information overriding correct decisions. [18]    Availability heuristic   The tendency to overestimate the likelihood of events with greater ""availability"" in memory, which can be influenced by how recent the memories are or how unusual or emotionally charged they may be. [19]    Availability cascade   A self-reinforcing process in which a collective belief gains more and more plausibility through its increasing repetition in public discourse (or ""repeat something long enough and it will become true""). [20]    Backfire effect   The reaction to disconfirming evidence by strengthening one's previous beliefs. [21] cf. Continued influence effect .   Bandwagon effect   The tendency to do (or believe) things because many other people do (or believe) the same. Related to groupthink and herd behavior . [22]    Base rate fallacy or Base rate neglect  The tendency to ignore general information and focus on information only pertaining to the specific case, even when the general information is more important. [23]    Belief bias   An effect where someone's evaluation of the logical strength of an argument is biased by the believability of the conclusion. [24]    Ben Franklin effect   A person who has performed a favor for someone is more likely to do another favor for that person than they would be if they had received a favor from that person. [25]    Berkson's paradox   The tendency to misinterpret statistical experiments involving conditional probabilities. [26]    Bias blind spot   The tendency to see oneself as less biased than other people, or to be able to identify more cognitive biases in others than in oneself. [27]    Choice-supportive bias   The tendency to remember one's choices as better than they actually were. [28]    Clustering illusion   The tendency to overestimate the importance of small runs, streaks, or clusters in large samples of random data (that is, seeing phantom patterns). [13]    Compassion fade   The predisposition to behave more compassionately towards a small number of identifiable victims than to a large number of anonymous ones. [29]    Confirmation bias   The tendency to search for, interpret, focus on and remember information in a way that confirms one's preconceptions. [30]    Congruence bias   The tendency to test hypotheses exclusively through direct testing, instead of testing possible alternative hypotheses. [13]    Conjunction fallacy   The tendency to assume that specific conditions are more probable than a more general version of those same conditions. For example, subjects in one experiment perceived the probability of a woman being both a bank teller and a feminist as more likely than the probability of her being a bank teller. [31]    Conservatism (belief revision)   The tendency to revise one's belief insufficiently when presented with new evidence. [6] [32] [33]    Continued influence effect   The tendency to believe previously learned misinformation even after it has been corrected. Misinformation can still influence inferences one generates after a correction has occurred. [34] cf. Backfire effect    Contrast effect   The enhancement or reduction of a certain stimulus' perception when compared with a recently observed, contrasting object. [35]    Courtesy bias  The tendency to give an opinion that is more socially correct than one's true opinion, so as to avoid offending anyone. [36]    Curse of knowledge   When better-informed people find it extremely difficult to think about problems from the perspective of lesser-informed people. [37]    Declinism   The predisposition to view the past favorably ( rosy retrospection ) and future negatively. [38]    Decoy effect   Preferences for either option A or B change in favor of option B when option C is presented, which is completely dominated by option B (inferior in all respects) and partially dominated by option A. [39]    Default effect   When given a choice between several options, the tendency to favor the default one. [40]    Denomination effect   The tendency to spend more money when it is denominated in small amounts (e.g., coins) rather than large amounts (e.g., bills). [41]    Disposition effect   The tendency to sell an asset that has accumulated in value and resist selling an asset that has declined in value.   Distinction bias   The tendency to view two options as more dissimilar when evaluating them simultaneously than when evaluating them separately. [42]    Dread aversion  Just as losses yield double the emotional impact of gains, dread yields double the emotional  impact of savouring. [43]    Dunning–Kruger effect   The tendency for unskilled individuals to overestimate their own ability and the tendency for experts to underestimate their own ability. [44]    Duration neglect   The neglect of the duration of an episode in determining its value. [45]    Empathy gap   The tendency to underestimate the influence or strength of feelings, in either oneself or others. [46]    End-of-history illusion   The age-independent belief that one will change less in the future than one has in the past. [47]    Endowment effect   The tendency for people to demand much more to give up an object than they would be willing to pay to acquire it. [48]    Exaggerated expectation   The tendency to expect or predict more extreme outcomes than those outcomes that actually happen. [6]    Experimenter's or expectation bias   The tendency for experimenters to believe, certify, and publish data that agree with their expectations for the outcome of an experiment, and to disbelieve, discard, or downgrade the corresponding weightings for data that appear to conflict with those expectations. [49]    Focusing effect   The tendency to place too much importance on one aspect of an event. [50]    Forer effect or Barnum effect   The observation that individuals will give high accuracy ratings to descriptions of their personality that supposedly are tailored specifically for them, but are in fact vague and general enough to apply to a wide range of people. This effect can provide a partial explanation for the widespread acceptance of some beliefs and practices, such as astrology, fortune telling, graphology, and some types of personality tests. [51]    Form function attribution bias  In human–robot interaction , the tendency of people to make systematic errors when interacting with a robot. People may base their expectations and perceptions of a robot on its appearance (form) and attribute functions which do not necessarily mirror the true functions of the robot. [52]    Framing effect   Drawing different conclusions from the same information, depending on how that information is presented.   Frequency illusion or Baader–Meinhof phenomenon  The frequency illusion is that once something has been noticed then every instance of that thing is noticed, leading to the belief it has a high frequency of occurrence (a form of selection bias ). [53] The Baader–Meinhof phenomenon is the illusion where something that has recently come to one's attention suddenly seems to appear with improbable frequency shortly afterwards. [54] The Baader–Meinhof phenomenon is sometimes conflated with frequency illusion and the recency illusion . [55] It was named after an incidence of frequency illusion in which the Baader–Meinhof Group was mentioned. [56]    Functional fixedness   Limits a person to using an object only in the way it is traditionally used. [57]    Gambler's fallacy   The tendency to think that future probabilities are altered by past events, when in reality they are unchanged. The fallacy arises from an erroneous conceptualization of the law of large numbers . For example, ""I've flipped heads with this coin five times consecutively, so the chance of tails coming out on the sixth flip is much greater than heads."" [58]    Gender bias   A widely held [59] set of implicit biases that discriminate against a gender (typically women [60] ). For example, the assumption that women are less suited to jobs requiring high intellectual ability [61] . Or the assumption that people or animals are male in the absence of any indicators of gender. [62]    Groupthink   The psychological phenomenon that occurs within a group of people in which the desire for harmony or conformity in the group results in an irrational or dysfunctional decision-making outcome. Group members try to minimize conflict and reach a consensus decision without critical evaluation of alternative viewpoints by actively suppressing dissenting viewpoints, and by isolating themselves from outside influences.   Hard–easy effect   The tendency to overestimate one's ability to accomplish hard tasks, and underestimate one's ability to accomplish easy tasks [6] [63] [64] [65]    Hindsight bias   Sometimes called the ""I-knew-it-all-along"" effect, the tendency to see past events as being predictable [66] at the time those events happened.   Hostile attribution bias   The ""hostile attribution bias"" is the tendency to interpret others' behaviors as having hostile intent, even when the behavior is ambiguous or benign. [67]    Hot-hand fallacy   The ""hot-hand fallacy"" (also known as the ""hot hand phenomenon"" or ""hot hand"") is the belief that a person who has experienced success with a random event has a greater chance of further success in additional attempts.   Hyperbolic discounting   Discounting is the tendency for people to have a stronger preference for more immediate payoffs relative to later payoffs. Hyperbolic discounting leads to choices that are inconsistent over time – people make choices today that their future selves would prefer not to have made, despite using the same reasoning. [68] Also known as current moment bias, present-bias, and related to Dynamic inconsistency . A good example of this: a study showed that when making food choices for the coming week, 74% of participants chose fruit, whereas when the food choice was for the current day, 70% chose chocolate.   IKEA effect   The tendency for people to place a disproportionately high value on objects that they partially assembled themselves, such as furniture from IKEA , regardless of the quality of the end product. [69]    Illicit transference   Occurs when a term in the distributive (referring to every member of a class) and collective (referring to the class itself as a whole) sense are treated as equivalent. The two variants of this fallacy are the fallacy of composition and the fallacy of division .   Illusion of control   The tendency to overestimate one's degree of influence over other external events. [70]    Illusion of validity   Believing that one's judgments are accurate, especially when available information is consistent or inter-correlated. [71]    Illusory correlation   Inaccurately perceiving a relationship between two unrelated events. [72] [73]    Illusory truth effect   A tendency to believe that a statement is true if it is easier to process , or if it has been stated multiple times , regardless of its actual veracity. These are specific cases of truthiness .   Impact bias   The tendency to overestimate the length or the intensity of the impact of future feeling states. [74]    Implicit association   The speed with which people can match words depends on how closely they are associated.   Information bias   The tendency to seek information even when it cannot affect action. [75]    Insensitivity to sample size   The tendency to under-expect variation in small samples.   Interoceptive bias  The tendency for sensory input about the body itself to affect one's judgement about external, unrelated circumstances. (As for example, in parole judges who are more lenient when fed and rested.) [76] [77] [78] [79]    Irrational escalation or Escalation of commitment   The phenomenon where people justify increased investment in a decision, based on the cumulative prior investment, despite new evidence suggesting that the decision was probably wrong. Also known as the sunk cost fallacy.   Law of the instrument   An over-reliance on a familiar tool or methods, ignoring or under-valuing alternative approaches. ""If all you have is a hammer, everything looks like a nail.""   Less-is-better effect   The tendency to prefer a smaller set to a larger set judged separately, but not jointly.   Look-elsewhere effect   An apparently statistically significant observation may have actually arisen by chance because of the size of the parameter space to be searched.   Loss aversion   The perceived disutility of giving up an object is greater than the utility associated with acquiring it. [80] (see also Sunk cost effects and endowment effect).   Mere exposure effect   The tendency to express undue liking for things merely because of familiarity with them. [81]    Money illusion   The tendency to concentrate on the nominal value (face value) of money rather than its value in terms of purchasing power. [82]    Moral credential effect   Occurs when someone who does something good gives themselves permission to be less good in the future.   Negativity bias or Negativity effect  Psychological phenomenon by which humans have a greater recall of unpleasant memories compared with positive memories. [83] [84] (see also actor-observer bias, group attribution error , positivity effect, and negativity effect ). [85]    Neglect of probability   The tendency to completely disregard probability when making a decision under uncertainty. [86]    Normalcy bias   The refusal to plan for, or react to, a disaster which has never happened before.   Not invented here   Aversion to contact with or use of products, research, standards, or knowledge developed outside a group. Related to IKEA effect .   Observer-expectancy effect   When a researcher expects a given result and therefore unconsciously manipulates an experiment or misinterprets data in order to find it (see also subject-expectancy effect ).   Omission bias   The tendency to judge harmful actions (commissions) as worse, or less moral, than equally harmful inactions (omissions). [87]    Optimism bias   The tendency to be over-optimistic, underestimating greatly the probability of undesirable outcomes and overestimating favorable and pleasing outcomes (see also wishful thinking , valence effect , positive outcome bias ). [88] [89]    Ostrich effect   Ignoring an obvious (negative) situation.   Outcome bias   The tendency to judge a decision by its eventual outcome instead of based on the quality of the decision at the time it was made.   Overconfidence effect   Excessive confidence in one's own answers to questions. For example, for certain types of questions, answers that people rate as ""99% certain"" turn out to be wrong 40% of the time. [6] [90] [91] [92]    Pareidolia   A vague and random stimulus (often an image or sound) is perceived as significant, e.g., seeing images of animals or faces in clouds, the man in the moon , and hearing non-existent hidden messages on records played in reverse .   Pygmalion effect   The phenomenon whereby others' expectations of a target person affect the target person's performance.   Pessimism bias   The tendency for some people, especially those suffering from depression , to overestimate the likelihood of negative things happening to them.   Plan continuation bias   Failure to recognize that the original plan of action is no longer appropriate for a changing situation or for a situation that is different than anticipated. [93]    Planning fallacy   The tendency to underestimate task-completion times. [74]    Present bias   The tendency of people to give stronger weight to payoffs that are closer to the present time when considering trade-offs between two future moments. [94]    Plant blindness   The tendency to ignore plants in their environment and a failure to recognize and appreciate the utility of plants to life on earth. [95]    Pro-innovation bias   The tendency to have an excessive optimism towards an invention or innovation's usefulness throughout society, while often failing to identify its limitations and weaknesses.   Projection bias   The tendency to overestimate how much our future selves share one's current preferences, thoughts and values, thus leading to sub-optimal choices. [96] [97] [84]    Pseudocertainty effect   The tendency to make risk-averse choices if the expected outcome is positive, but make risk-seeking choices to avoid negative outcomes. [98]    Reactance   The urge to do the opposite of what someone wants you to do out of a need to resist a perceived attempt to constrain your freedom of choice (see also Reverse psychology ).   Reactive devaluation   Devaluing proposals only because they purportedly originated with an adversary.   Recency illusion   The illusion that a phenomenon one has noticed only recently is itself recent. Often used to refer to linguistic phenomena; the illusion that a word or language usage that one has noticed only recently is an innovation when it is in fact long-established (see also frequency illusion ).   Systematic Bias  Judgement that arises when targets of differentiating judgement become subject to effects of regression that are not equivalent. [99]    Restraint bias   The tendency to overestimate one's ability to show restraint in the face of temptation.   Rhyme as reason effect   Rhyming statements are perceived as more truthful. A famous example being used in the O.J Simpson trial with the defense's use of the phrase ""If the gloves don't fit, then you must acquit.""   Risk compensation / Peltzman effect   The tendency to take greater risks when perceived safety increases.   Salience bias   The tendency to focus on items that are more prominent or emotionally striking and ignore those that are unremarkable, even though this difference is often irrelevant by objective standards.   Selection bias   The tendency to notice something more when something causes us to be more aware of it, such as when we buy a car, we tend to notice similar cars more often than we did before. They are not suddenly more common – we just are noticing them more. Also called the Observational Selection Bias.   Selective perception   The tendency for expectations to affect perception.   Semmelweis reflex   The tendency to reject new evidence that contradicts a paradigm. [33]    Sexual overperception bias / Sexual underperception bias   The tendency to over-/underestimate sexual interest of another person in oneself.   Social comparison bias   The tendency, when making decisions, to favour potential candidates who don't compete with one's own particular strengths. [100]    Social desirability bias   The tendency to over-report socially desirable characteristics or behaviours in oneself and under-report socially undesirable characteristics or behaviours. [101] See also: § Courtesy bias .   Status quo bias   The tendency to like things to stay relatively the same (see also loss aversion , endowment effect , and system justification ). [102] [103]    Stereotyping   Expecting a member of a group to have certain characteristics without having actual information about that individual.   Subadditivity effect   The tendency to judge probability of the whole to be less than the probabilities of the parts. [104]    Subjective validation   Perception that something is true if a subject's belief demands it to be true. Also assigns perceived connections between coincidences.   Surrogation   Losing sight of the strategic construct that a measure is intended to represent, and subsequently acting as though the measure is the construct of interest.   Survivorship bias   Concentrating on the people or things that ""survived"" some process and inadvertently overlooking those that didn't because of their lack of visibility.   Time-saving bias   Underestimations of the time that could be saved (or lost) when increasing (or decreasing) from a relatively low speed and overestimations of the time that could be saved (or lost) when increasing (or decreasing) from a relatively high speed.   Third-person effect   A hypothesized tendency to believe that mass communicated media messages have a greater effect on others than on themselves. As of 2020, the third-person effect has yet to be reliably demonstrated in a scientific context.   Parkinson's law of triviality   The tendency to give disproportionate weight to trivial issues. Also known as bikeshedding, this bias explains why an organization may avoid specialized or complex subjects, such as the design of a nuclear reactor, and instead focus on something easy to grasp or rewarding to the average participant, such as the design of an adjacent bike shed. [105]    Unit bias   The standard suggested amount of consumption (e.g., food serving size) is perceived to be appropriate, and a person would consume it all even if it is too much for this particular person. [106]    Weber–Fechner law   Difficulty in comparing small differences in large quantities.   Well travelled road effect   Underestimation of the duration taken to traverse oft-traveled routes and overestimation of the duration taken to traverse less familiar routes.   Women are wonderful effect   A tendency to associate more positive attributes with women than with men.   Zero-risk bias   Preference for reducing a small risk to zero over a greater reduction in a larger risk.   Zero-sum bias   A bias whereby a situation is incorrectly perceived to be like a zero-sum game (i.e., one person gains at the expense of another).  Social biases [ edit ]  Most of these biases are labeled as attributional biases .    Name  Description   Actor-observer bias   The tendency for explanations of other individuals' behaviors to overemphasize the influence of their personality and underemphasize the influence of their situation (see also Fundamental attribution error ), and for explanations of one's own behaviors to do the opposite (that is, to overemphasize the influence of our situation and underemphasize the influence of our own personality).   Authority bias   The tendency to attribute greater accuracy to the opinion of an authority figure (unrelated to its content) and be more influenced by that opinion. [107]    Cheerleader effect   The tendency for people to appear more attractive in a group than in isolation. [108]    Defensive attribution hypothesis   Attributing more blame to a harm-doer as the outcome becomes more severe or as personal or situational similarity to the victim increases.   Egocentric bias   Occurs when people claim more responsibility for themselves for the results of a joint action than an outside observer would credit them with.   Extrinsic incentives bias   An exception to the fundamental attribution error , when people view others as having (situational) extrinsic motivations and (dispositional) intrinsic motivations for oneself   False consensus effect   The tendency for people to overestimate the degree to which others agree with them. [109]    False uniqueness bias   The tendency of people to see their projects and themselves as more singular than they actually are. [110]    Fundamental attribution error   The tendency for people to over-emphasize personality-based explanations for behaviors observed in others while under-emphasizing the role and power of situational influences on the same behavior [84] (see also actor-observer bias, group attribution error , positivity effect, and negativity effect ). [85]    Group attribution error   The biased belief that the characteristics of an individual group member are reflective of the group as a whole or the tendency to assume that group decision outcomes reflect the preferences of group members, even when information is available that clearly suggests otherwise.   Halo effect   The tendency for a person's positive or negative traits to ""spill over"" from one personality area to another in others' perceptions of them (see also physical attractiveness stereotype ). [111]    Illusion of asymmetric insight   People perceive their knowledge of their peers to surpass their peers' knowledge of them. [112]    Illusion of external agency   When people view self-generated preferences as instead being caused by insightful, effective and benevolent agents.   Illusion of transparency   People overestimate others' ability to know themselves, and they also overestimate their ability to know others.   Illusory superiority   Overestimating one's desirable qualities, and underestimating undesirable qualities, relative to other people. (Also known as ""Lake Wobegon effect"", ""better-than-average effect"", or ""superiority bias"".) [113]    Ingroup bias   The tendency for people to give preferential treatment to others they perceive to be members of their own groups.   Intentionality bias  Tendency to judge human action to intentional rather than accidental. [114]    Just-world hypothesis   The tendency for people to want to believe that the world is fundamentally just, causing them to rationalize an otherwise inexplicable injustice as deserved by the victim(s).   Moral luck   The tendency for people to ascribe greater or lesser moral standing based on the outcome of an event.   Naïve cynicism   Expecting more egocentric bias in others than in oneself.   Naïve realism   The belief that we see reality as it really is – objectively and without bias; that the facts are plain for all to see; that rational people will agree with us; and that those who don't are either uninformed, lazy, irrational, or biased.   Outgroup homogeneity bias   Individuals see members of their own group as being relatively more varied than members of other groups. [115]    Puritanical bias   Refers to the tendency to attribute cause of an undesirable outcome or wrongdoing by an individual to a moral deficiency or lack of self control rather than taking into account the impact of broader societal determinants . [116]    Self-serving bias   The tendency to claim more responsibility for successes than failures. It may also manifest itself as a tendency for people to evaluate ambiguous information in a way beneficial to their interests (see also group-serving bias ). [117]    Shared information bias   Known as the tendency for group members to spend more time and energy discussing information that all members are already familiar with (i.e., shared information), and less time and energy discussing information that only some members are aware of (i.e., unshared information). [118]    System justification   The tendency to defend and bolster the status quo. Existing social, economic, and political arrangements tend to be preferred, and alternatives disparaged, sometimes even at the expense of individual and collective self-interest. (See also status quo bias.)   Trait ascription bias   The tendency for people to view themselves as relatively variable in terms of personality, behavior, and mood while viewing others as much more predictable.   Ultimate attribution error   Similar to the fundamental attribution error, in this error a person is likely to make an internal attribution to an entire group instead of the individuals within the group.   Worse-than-average effect   A tendency to believe ourselves to be worse than others at tasks which are difficult. [119]   Memory errors and biases [ edit ]  Main article: List of memory biases  In psychology  and  cognitive science , a memory bias is a cognitive bias that either enhances or impairs the recall of a memory (either the chances that the memory will be recalled at all, or the amount of time it takes for it to be recalled, or both), or that alters the content of a reported memory. There are many types of memory bias, including:    Name  Description   Bizarreness effect   Bizarre material is better remembered than common material.   Conservatism or Regressive bias  Tendency to remember high values and high likelihoods/probabilities/frequencies as lower than they actually were and low ones as higher than they actually were. Based on the evidence, memories are not extreme enough. [120] [121]    Consistency bias   Incorrectly remembering one's past attitudes and behaviour as resembling present attitudes and behaviour. [122]    Context effect   That cognition and memory are dependent on context, such that out-of-context memories are more difficult to retrieve than in-context memories (e.g., recall time and accuracy for a work-related memory will be lower at home, and vice versa).   Cross-race effect   The tendency for people of one race to have difficulty identifying members of a race other than their own.   Cryptomnesia   A form of misattribution where a memory is mistaken for imagination, because there is no subjective experience of it being a memory. [123]    Egocentric bias   Recalling the past in a self-serving manner, e.g., remembering one's exam grades as being better than they were, or remembering a caught fish as bigger than it really was.   Fading affect bias   A bias in which the emotion associated with unpleasant memories fades more quickly than the emotion associated with positive events. [124]    False memory   A form of misattribution where imagination is mistaken for a memory.   Generation effect (Self-generation effect)  That self-generated information is remembered best. For instance, people are better able to recall memories of statements that they have generated than similar statements generated by others.   Google effect   The tendency to forget information that can be found readily online by using Internet search engines.   Humor effect   That humorous items are more easily remembered than non-humorous ones, which might be explained by the distinctiveness of humor, the increased cognitive processing time to understand the humor, or the emotional arousal caused by the humor. [125]    Lag effect  The phenomenon whereby learning is greater when studying is spread out over time, as opposed to studying the same amount of time in a single session. See also spacing effect .   Leveling and sharpening   Memory distortions introduced by the loss of details in a recollection over time, often concurrent with sharpening or selective recollection of certain details that take on exaggerated significance in relation to the details or aspects of the experience lost through leveling. Both biases may be reinforced over time, and by repeated recollection or re-telling of a memory. [126]    Levels-of-processing effect   That different methods of encoding information into memory have different levels of effectiveness. [127]    List-length effect   A smaller percentage of items are remembered in a longer list, but as the length of the list increases, the absolute number of items remembered increases as well. For example, consider a list of 30 items (""L30"") and a list of 100 items (""L100""). An individual may remember 15 items from L30, or 50%, whereas the individual may remember 40 items from L100, or 40%. Although the percent of L30 items remembered (50%) is greater than the percent of L100 (40%), more L100 items (40) are remembered than L30 items (15). [128] [ further explanation needed ]    Misinformation effect   Memory becoming less accurate because of interference from post-event information . [129]    Modality effect   That memory recall is higher for the last items of a list when the list items were received via speech than when they were received through writing.   Mood-congruent memory bias   The improved recall of information congruent with one's current mood.   Next-in-line effect   When taking turns speaking in a group using a predetermined order (e.g. going clockwise around a room, taking numbers, etc.) people tend to have diminished recall for the words of the person who spoke immediately before them. [130]    Part-list cueing effect   That being shown some items from a list and later retrieving one item causes it to become harder to retrieve the other items. [131]    Peak-end rule   That people seem to perceive not the sum of an experience but the average of how it was at its peak (e.g., pleasant or unpleasant) and how it ended.   Picture superiority effect   The notion that concepts that are learned by viewing pictures are more easily and frequently recalled than are concepts that are learned by viewing their written word form counterparts. [132] [133] [134] [135] [136] [137]    Positivity effect ( Socioemotional selectivity theory )  That older adults favor positive over negative information in their memories.   Serial position effect   That items near the end of a sequence are the easiest to recall, followed by the items at the beginning of a sequence; items in the middle are the least likely to be remembered. [138]    Processing difficulty effect   That information that takes longer to read and is thought about more (processed with more difficulty) is more easily remembered. [139]    Reminiscence bump   The recalling of more personal events from adolescence and early adulthood than personal events from other lifetime periods. [140]    Self-relevance effect   That memories relating to the self are better recalled than similar information relating to others.   Source confusion   Confusing episodic memories with other information, creating distorted memories. [141]    Spacing effect   That information is better recalled if exposure to it is repeated over a long span of time rather than a short one.   Spotlight effect   The tendency to overestimate the amount that other people notice your appearance or behavior.   Stereotypical bias   Memory distorted towards stereotypes (e.g., racial or gender).   Suffix effect   Diminishment of the recency effect because a sound item is appended to the list that the subject is not required to recall. [142] [143]    Suggestibility   A form of misattribution where ideas suggested by a questioner are mistaken for memory.   Tachypsychia   When time perceived by the individual either lengthens, making events appear to slow down, or contracts. [144]    Telescoping effect   The tendency to displace recent events backward in time and remote events forward in time, so that recent events appear more remote, and remote events, more recent.   Testing effect   The fact that you more easily remember information you have read by rewriting it instead of rereading it. [145]    Tip of the tongue phenomenon  When a subject is able to recall parts of an item, or related information, but is frustratingly unable to recall the whole item. This is thought to be an instance of ""blocking"" where multiple similar memories are being recalled and interfere with each other. [123]    Travis Syndrome   Overestimating the significance of the present. [146] It is related to chronological snobbery with possibly an appeal to novelty  logical fallacy being part of the bias.   Verbatim effect   That the ""gist"" of what someone has said is better remembered than the verbatim wording. [147] This is because memories are representations, not exact copies.   von Restorff effect   That an item that sticks out is more likely to be remembered than other items. [148]    Zeigarnik effect   That uncompleted or interrupted tasks are remembered better than completed ones.  See also [ edit ]    Psychology portal  Society portal  Philosophy portal   Affective forecasting – Predicting someone's future emotions (affect)  Anecdotal evidence – Evidence relying on personal testimony  Apophenia – Tendency to perceive connections between unrelated things  Attribution (psychology) – The process by which individuals explain the causes of behavior and events  Black swan theory – Theory of response to surprise events  Chronostasis – Distortion in the perception of time  Cognitive distortion – An exaggerated or irrational thought pattern involved in the onset and perpetuation of psychopathological states  Defence mechanism – Unconscious psychological mechanism that reduces anxiety arising from unacceptable or potentially harmful stimuli  Dysrationalia – Inability to think and behave rationally despite adequate intelligence  Fear, uncertainty, and doubt – Tactic used to influence opinion by disseminating negative, dubious, or false information  Feedback – Process where information about current status is used to influence future status  Impostor syndrome – Psychological pattern of doubting one's accomplishments and fearing being exposed as a ""fraud""  List of common misconceptions – Wikipedia list article  List of fallacies – Types of reasoning that are logically incorrect  List of maladaptive schemas  List of memory biases  List of psychological effects – Wikipedia list article  iSheep#Behavioural patterns – Derogatory marketing term (references multiple cognitive biasses)  Media bias – Bias or perceived bias of journalists and news producers within the mass media in the selection of events and stories that are reported and how they are covered  Mind projection fallacy – An informal fallacy that the way one sees the world reflects the way the world really is  Motivated reasoning – Using emotionally-biased reasoning to produce justifications or make decisions  Observational error , also known as Systematic bias  Outline of public relations – Overview of and topical guide to public relations  Outline of thought – Overview of and topical guide to thought  Pollyanna principle – The tendency of people to remember pleasant events more than unpleasant ones  Positive feedback – Destabilising process that occurs in a feedback loop  Prevalence effect  Propaganda – Form of communication intended to sway the audience through presenting only one side of the argument  Publication bias – Higher probability of publishing results showing a significant finding  Recall bias – Systematic error caused by differences in the accuracy or completeness of the recollections retrieved  Self-handicapping – Cognitive strategy by which people avoid effort in the hopes of keeping potential failure from hurting self-esteem   Footnotes [ edit ]    ^  Haselton MG, Nettle D, Andrews PW (2005). ""The evolution of cognitive bias.""  (PDF) .  In Buss DM (ed.). The Handbook of Evolutionary Psychology . Hoboken, NJ, US: John Wiley & Sons Inc. pp. 724–746.   ^  ""Cognitive Bias – Association for Psychological Science"" . www.psychologicalscience.org . Retrieved 2018-10-10 .   ^  Thomas O (2018-01-19). ""Two decades of cognitive bias research in entrepreneurship: What do we know and where do we go from here?"". Management Review Quarterly . 68 (2): 107–143. doi : 10.1007/s11301-018-0135-9 . ISSN  2198-1620 .   ^  Dougherty MR, Gettys CF, Ogden EE (1999). ""MINERVA-DM: A memory processes model for judgments of likelihood""  (PDF) . Psychological Review . 106 (1): 180–209. doi : 10.1037/0033-295x.106.1.180 .   ^  Gigerenzer G (2006). ""Bounded and Rational"".  In Stainton RJ (ed.). Contemporary Debates in Cognitive Science . Blackwell. p. 129. ISBN  978-1-4051-1304-5 .   ^ a  b  c  d  e  Fielder, Klaus (October 2014). ""Regressive Judgment: Implications of a Universal Property of the Empirical World""  (PDF) . Sage Journals – via Google Scholar. Lay summary .   ^  MacCoun RJ (1998). ""Biases in the interpretation and use of research results""  (PDF) . Annual Review of Psychology . 49 (1): 259–87. doi : 10.1146/annurev.psych.49.1.259 . PMID  15012470 .   ^  Nickerson RS (1998). ""Confirmation Bias: A Ubiquitous Phenomenon in Many Guises""  (PDF) . Review of General Psychology . 2 (2): 175–220 [198]. doi : 10.1037/1089-2680.2.2.175 .   ^  Dardenne B, Leyens JP (1995). ""Confirmation Bias as a Social Skill"" . Personality and Social Psychology Bulletin . 21 (11): 1229–1239. doi : 10.1177/01461672952111011 .   ^  Alexander WH, Brown JW (June 2010). ""Hyperbolically discounted temporal difference learning"" . Neural Computation . 22 (6): 1511–27. doi : 10.1162/neco.2010.08-09-1080 . PMC  3005720 . PMID  20100071 .   ^  Baron 1994 , p. 372   ^  Zhang Y, Lewis M, Pellon M, Coleman P (2007). ""A Preliminary Research on Modeling Cognitive Agents for Social Environments in Multi-Agent Systems""  (PDF) : 116–123.  Cite journal requires |journal= ( help )   ^ a  b  c  Iverson GL, Brooks BL, Holdnack JA (2008). ""Misdiagnosis of Cognitive Impairment in Forensic Neuropsychology"".  In Heilbronner RL (ed.). Neuropsychology in the Courtroom: Expert Analysis of Reports and Testimony . New York: Guilford Press. p. 248. ISBN  9781593856342 . CS1 maint: ref=harv ( link )   ^  Coley JD, Tanner KD (2012). ""Common origins of diverse misconceptions: cognitive principles and the development of biology thinking"" . CBE Life Sciences Education . 11 (3): 209–15. doi : 10.1187/cbe.12-06-0074 . PMC  3433289 . PMID  22949417 .   ^  ""The Real Reason We Dress Pets Like People"" . LiveScience.com . Retrieved 2015-11-16 .   ^  Harris LT, Fiske ST (January 2011). ""Dehumanized Perception: A Psychological Means to Facilitate Atrocities, Torture, and Genocide?"" . Zeitschrift Fur Psychologie . 219 (3): 175–181. doi : 10.1027/2151-2604/a000065 . PMC  3915417 . PMID  24511459 .   ^  Bar-Haim Y, Lamy D, Pergamin L, Bakermans-Kranenburg MJ, van IJzendoorn MH (January 2007). ""Threat-related attentional bias in anxious and nonanxious individuals: a meta-analytic study"" . Psychological Bulletin . 133 (1): 1–24. doi : 10.1037/0033-2909.133.1.1 . PMID  17201568 .   ^  Goddard K, Roudsari A, Wyatt JC (2011). ""Automation Bias – A Hidden Issue for Clinical Decision Support System Use"" . International Perspectives in Health Informatics . Studies in Health Technology and Informatics. 164 . IOS Press. doi : 10.3233/978-1-60750-709-3-17 .   ^  Schwarz N, Bless H, Strack F, Klumpp G, Rittenauer-Schatka H, Simons A (1991). ""Ease of Retrieval as Information: Another Look at the Availability Heuristic""  (PDF) . Journal of Personality and Social Psychology . 61 (2): 195–202. doi : 10.1037/0022-3514.61.2.195 . Archived from the original  (PDF) on 9 February 2014 . Retrieved 19 Oct 2014 .   ^  Kuran T, Sunstein CR (1998). ""Availability Cascades and Risk Regulation"" . Stanford Law Review . 51 (4): 683–768. doi : 10.2307/1229439 . JSTOR  1229439 .   ^  Sanna LJ, Schwarz N, Stocker SL (2002). ""When debiasing backfires: Accessible content and accessibility experiences in debiasing hindsight""  (PDF) . Journal of Experimental Psychology: Learning, Memory, and Cognition . 28 (3): 497–502. CiteSeerX  10.1.1.387.5964 . doi : 10.1037/0278-7393.28.3.497 . ISSN  0278-7393 .   ^  Colman A (2003). Oxford Dictionary of Psychology . New York: Oxford University Press. p. 77 . ISBN  978-0-19-280632-1 .   ^  Baron 1994 , pp. 224–228   ^  Klauer KC, Musch J, Naumer B (October 2000). ""On belief bias in syllogistic reasoning"". Psychological Review . 107 (4): 852–84. doi : 10.1037/0033-295X.107.4.852 . PMID  11089409 .   ^  ""Harness the power of the 'Ben Franklin Effect' to get someone to like you"" . Business Insider . Retrieved 2018-10-10 .   ^  ""Berkson's Paradox | Brilliant Math & Science Wiki"" . brilliant.org . Retrieved 2018-10-10 .   ^  Pronin E, Kugler MB (July 2007). ""Valuing thoughts, ignoring behavior: The introspection illusion as a source of the bias blind spot"". Journal of Experimental Social Psychology . 43 (4): 565–578. doi : 10.1016/j.jesp.2006.05.011 . ISSN  0022-1031 .   ^  Mather M, Shafir E, Johnson MK (March 2000). ""Misremembrance of options past: source monitoring and choice""  (PDF) . Psychological Science . 11 (2): 132–8. doi : 10.1111/1467-9280.00228 . PMID  11273420 . Archived  (PDF) from the original on 2009-01-17.   ^  Västfjäll D, Slovic P, Mayorga M, Peters E (18 June 2014). ""Compassion fade: affect and charity are greatest for a single child in need"" . PLOS ONE . 9 (6): e100115. Bibcode : 2014PLoSO...9j0115V . doi : 10.1371/journal.pone.0100115 . PMC  4062481 . PMID  24940738 .   ^  Oswald ME, Grosjean S (2004). ""Confirmation Bias"" .  In Pohl RF (ed.). Cognitive Illusions: A Handbook on Fallacies and Biases in Thinking, Judgement and Memory . Hove, UK: Psychology Press. pp. 79–96 . ISBN  978-1-84169-351-4 . OCLC  55124398 .   ^  Fisk JE (2004). ""Conjunction fallacy"" .  In Pohl RF (ed.). Cognitive Illusions: A Handbook on Fallacies and Biases in Thinking, Judgement and Memory . Hove, UK: Psychology Press. pp. 23–42 . ISBN  978-1-84169-351-4 . OCLC  55124398 .   ^  DuCharme WW (1970). ""Response bias explanation of conservative human inference"". Journal of Experimental Psychology . 85 (1): 66–74. doi : 10.1037/h0029546 . hdl : 2060/19700009379 .   ^ a  b  Edwards W (1968). ""Conservatism in human information processing"".  In Kleinmuntz B (ed.). Formal representation of human judgment . New York: Wiley. pp. 17–52.   ^  Johnson HM, Seifert CM (November 1994). ""Sources of the continued influence effect: When misinformation in memory affects later inferences"". Journal of Experimental Psychology: Learning, Memory, and Cognition . 20 (6): 1420–1436. doi : 10.1037/0278-7393.20.6.1420 .   ^  Plous 1993 , pp. 38–41   ^  Ciccarelli S, White J (2014). Psychology (4th ed.). Pearson Education, Inc. p. 62. ISBN  978-0205973354 .   ^  Ackerman MS, ed. (2003). Sharing expertise beyond knowledge management (online ed.). Cambridge, Massachusetts: MIT Press. p. 7 . ISBN  9780262011952 .   ^  Quartz SR, The State Of The World Isn't Nearly As Bad As You Think , Edge Foundation, Inc. , retrieved 2016-02-17   ^  ""Evolution and cognitive biases: the decoy effect"" . FutureLearn . Retrieved 2018-10-10 .   ^  ""The Default Effect: How to Leverage Bias and Influence Behavior"" . Influence at Work. 2012-01-11 . Retrieved 2018-10-10 .   ^  Why We Spend Coins Faster Than Bills by Chana Joffe-Walt. All Things Considered , 12 May 2009.   ^  Hsee CK, Zhang J (May 2004). ""Distinction bias: misprediction and mischoice due to joint evaluation"". Journal of Personality and Social Psychology . 86 (5): 680–95. CiteSeerX  10.1.1.484.9171 . doi : 10.1037/0022-3514.86.5.680 . PMID  15161394 .   ^  de Meza D, Dawson C (January 24, 2018). ""Wishful Thinking, Prudent Behavior: The Evolutionary Origin of Optimism, Loss Aversion and Disappointment Aversion"". SSRN  3108432 .  Cite journal requires |journal= ( help )   ^  Kruger J, Dunning D (December 1999). ""Unskilled and unaware of it: how difficulties in recognizing one's own incompetence lead to inflated self-assessments"". Journal of Personality and Social Psychology . 77 (6): 1121–34. CiteSeerX  10.1.1.64.2655 . doi : 10.1037/0022-3514.77.6.1121 . PMID  10626367 .   ^  Duration Neglect in Retrospective Evaluations of Affective Episodes  Archived 2017-08-08 at the Wayback Machine | Journal of Personality and Social Psychology   ^  ""Understanding and Mastering the Empathy Gap"" . Psychology Today .   ^  Quoidbach J, Gilbert DT , Wilson TD (January 2013). ""The end of history illusion""  (PDF) . Science . 339 (6115): 96–8. Bibcode : 2013Sci...339...96Q . doi : 10.1126/science.1229294 . PMID  23288539 . Archived from the original  (PDF) on 2013-01-13. Young people, middle-aged people, and older people all believed they had changed a lot in the past but would change relatively little in the future.   ^  ( Kahneman, Knetsch & Thaler 1991 , p. 193) Richard Thaler coined the term ""endowment effect.""   ^  Jeng M (2006). ""A selected history of expectation bias in physics"". American Journal of Physics . 74 (7): 578–583. arXiv : physics/0508199 . Bibcode : 2006AmJPh..74..578J . doi : 10.1119/1.2186333 .   ^  Kahneman D, Krueger AB, Schkade D, Schwarz N, Stone AA (June 2006). ""Would you be happier if you were richer? A focusing illusion""  (PDF) . Science . 312 (5782): 1908–10. Bibcode : 2006Sci...312.1908K . CiteSeerX  10.1.1.373.2683 . doi : 10.1126/science.1129688 . PMID  16809528 .   ^  ""The Barnum Demonstration"" . psych.fullerton.edu . Retrieved 2018-10-10 .   ^  Haring KS, Watanabe K, Velonaki M, Tossell CC, Finomore V (2018). ""FFAB-The Form Function Attribution Bias in Human Robot Interaction"". IEEE Transactions on Cognitive and Developmental Systems . 10 (4): 843–851. doi : 10.1109/TCDS.2018.2851569 .   ^  Zwicky A (2005-08-07). ""Just Between Dr. Language and I"" . Language Log .   ^  ""The Baader-Meinhof Phenomenon"" . Damn Interesting . Retrieved 2020-02-16 .   ^  ""What's the Baader-Meinhof phenomenon?"" . howstuffworks.com . 20 March 2015 . Retrieved 15 April 2018 .   ^  ""What's in a name?"" . twincities.com . St. Paul Pioneer Press . Retrieved June 5, 2020 . As you might guess, the phenomenon is named after an incident in which I was talking to a friend about the Baader-Meinhof gang (and this was many years after they were in the news). The next day, my friend phoned me and referred me to an article in that day’s newspaper in which the Baader-Meinhof gang was mentioned.   ^  ""The Psychology Guide: What Does Functional Fixedness Mean?"" . PsycholoGenie . Retrieved 2018-10-10 .   ^  Investopedia Staff (2006-10-29). ""Gambler's Fallacy/Monte Carlo Fallacy"" . Investopedia . Retrieved 2018-10-10 .   ^  ""GSNI | Human Development Reports"" . hdr.undp.org . Retrieved 2020-06-10 .   ^  Abel, Martin (September 2019). ""Do Workers Discriminate against Female Bosses?""  (PDF) . IZA Institute of Labor Economics .   ^  Bian, Lin; Leslie, Sarah-Jane; Cimpian, Andrei (November 2018). ""Evidence of bias against girls and women in contexts that emphasize intellectual ability"" . American Psychologist . 73 (9): 1139–1153. doi : 10.1037/amp0000427 . ISSN  1935-990X .   ^  Hamilton, Mykol C. (1991). ""Masculine Bias in the Attribution of Personhood: People = Male, Male = People"" . Psychology of Women Quarterly . 15 (3): 393–402. doi : 10.1111/j.1471-6402.1991.tb00415.x . ISSN  0361-6843 .   ^  Lichtenstein S, Fischhoff B (1977). ""Do those who know more also know more about how much they know?"". Organizational Behavior and Human Performance . 20 (2): 159–183. doi : 10.1016/0030-5073(77)90001-0 .   ^  Merkle EC (February 2009). ""The disutility of the hard-easy effect in choice confidence"". Psychonomic Bulletin & Review . 16 (1): 204–13. doi : 10.3758/PBR.16.1.204 . PMID  19145033 .   ^  Juslin P, Winman A, Olsson H (April 2000). ""Naive empiricism and dogmatism in confidence research: a critical examination of the hard-easy effect"". Psychological Review . 107 (2): 384–96. doi : 10.1037/0033-295x.107.2.384 . PMID  10789203 .   ^  Pohl RF (2004). ""Hindsight Bias"" .  In Pohl RF (ed.). Cognitive Illusions: A Handbook on Fallacies and Biases in Thinking, Judgement and Memory . Hove, UK: Psychology Press. pp. 363–378 . ISBN  978-1-84169-351-4 . OCLC  55124398 .   ^  Anderson KB, Graham LM (2007). Hostile Attribution Bias . Encyclopedia of Social Psychology . SAGE Publications, Inc. pp. 446–447. doi : 10.4135/9781412956253 . ISBN  9781412916707 .   ^  Laibson D (1997). ""Golden Eggs and Hyperbolic Discounting"". Quarterly Journal of Economics . 112 (2): 443–477. CiteSeerX  10.1.1.337.3544 . doi : 10.1162/003355397555253 .   ^  The “IKEA Effect”: When Labor Leads to Love | Harvard Business School   ^  Thompson SC (1999). ""Illusions of Control: How We Overestimate Our Personal Influence"". Current Directions in Psychological Science . 8 (6): 187–190. doi : 10.1111/1467-8721.00044 . ISSN  0963-7214 . JSTOR  20182602 . CS1 maint: ref=harv ( link )   ^  Dierkes M, Antal AB, Child J, Ikujiro Nonaka (2003). Handbook of Organizational Learning and Knowledge . Oxford University Press. p. 22. ISBN  978-0-19-829582-2 . Retrieved 9 September 2013 .   ^  Tversky A, Kahneman D (September 1974). ""Judgment under Uncertainty: Heuristics and Biases"". Science . 185 (4157): 1124–31. Bibcode : 1974Sci...185.1124T . doi : 10.1126/science.185.4157.1124 . PMID  17835457 .   ^  Fiedler K (1991). ""The tricky nature of skewed frequency tables: An information loss account of distinctiveness-based illusory correlations"". Journal of Personality and Social Psychology . 60 (1): 24–36. doi : 10.1037/0022-3514.60.1.24 .   ^ a  b  Sanna LJ, Schwarz N (July 2004). ""Integrating temporal biases: the interplay of focal thoughts and accessibility experiences"" . Psychological Science . 15 (7): 474–81. doi : 10.1111/j.0956-7976.2004.00704.x . PMID  15200632 .   ^  Baron 1994 , pp. 258–259   ^  Danziger S, Levav J, Avnaim-Pesso L (April 2011). ""Extraneous factors in judicial decisions"" . Proceedings of the National Academy of Sciences of the United States of America . 108 (17): 6889–92. Bibcode : 2011PNAS..108.6889D . doi : 10.1073/pnas.1018033108 . PMC  3084045 . PMID  21482790 .   ^  Zaman J, De Peuter S, Van Diest I, Van den Bergh O, Vlaeyen JW (November 2016). ""Interoceptive cues predicting exteroceptive events"". International Journal of Psychophysiology . 109 : 100–106. doi : 10.1016/j.ijpsycho.2016.09.003 . PMID  27616473 .   ^  Barrett LF, Simmons WK (July 2015). ""Interoceptive predictions in the brain"" . Nature Reviews. Neuroscience . 16 (7): 419–29. doi : 10.1038/nrn3950 . PMC  4731102 . PMID  26016744 .   ^  Damasio AR (October 1996). ""The somatic marker hypothesis and the possible functions of the prefrontal cortex"" . Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences . 351 (1346): 1413–20. doi : 10.1098/rstb.1996.0125 . PMID  8941953 .   ^  ( Kahneman, Knetsch & Thaler 1991 , p. 193) Daniel Kahneman, together with Amos Tversky, coined the term ""loss aversion.""   ^  Bornstein RF, Crave-Lemley C (2004). ""Mere exposure effect"" .  In Pohl RF (ed.). Cognitive Illusions: A Handbook on Fallacies and Biases in Thinking, Judgement and Memory . Hove, UK: Psychology Press. pp. 215–234 . ISBN  978-1-84169-351-4 . OCLC  55124398 .   ^  Shafir E, Diamond P, Tversky A (2000). ""Money Illusion"".  In Kahneman D, Tversky A (eds.). Choices, values, and frames . Cambridge University Press. pp. 335–355. ISBN  978-0-521-62749-8 .   ^  Haizlip J, May N, Schorling J, Williams A, Plews-Ogan M (September 2012). ""Perspective: the negativity bias, medical education, and the culture of academic medicine: why culture change is hard"". Academic Medicine . 87 (9): 1205–9. doi : 10.1097/ACM.0b013e3182628f03 . PMID  22836850 .   ^ a  b  c  Trofimova I (2014). ""Observer bias: an interaction of temperament traits with biases in the semantic perception of lexical material"" . PLOS ONE . 9 (1): e85677. Bibcode : 2014PLoSO...985677T . doi : 10.1371/journal.pone.0085677 . PMC  3903487 . PMID  24475048 .   ^ a  b  Sutherland 2007 , pp. 138–139   ^  Baron 1994 , p. 353   ^  Baron 1994 , p. 386   ^  Baron 1994 , p. 44   ^  Hardman 2009 , p. 104   ^  Adams PA, Adams JK (December 1960). ""Confidence in the recognition and reproduction of words difficult to spell"". The American Journal of Psychology . 73 (4): 544–52. doi : 10.2307/1419942 . JSTOR  1419942 . PMID  13681411 .   ^  Hoffrage U (2004). ""Overconfidence"" .  In Rüdiger Pohl (ed.). Cognitive Illusions: a handbook on fallacies and biases in thinking, judgement and memory . Psychology Press. ISBN  978-1-84169-351-4 .   ^  Sutherland 2007 , pp. 172–178   ^  Tuccio, William (2011-01-01). ""Heuristics to Improve Human Factors Performance in Aviation"". Journal of Aviation/Aerospace Education & Research . 20 (3). doi : 10.15394/jaaer.2011.1640 . ISSN  2329-258X .   ^  O'Donoghue T, Rabin M (1999). ""Doing it now or later"" . American Economic Review . 89 (1): 103–124. doi : 10.1257/aer.89.1.103 .   ^  Balas B, Momsen JL (September 2014).  Holt EA (ed.). ""Attention ""blinks"" differently for plants and animals"" . CBE Life Sciences Education . 13 (3): 437–43. doi : 10.1187/cbe.14-05-0080 . PMC  4152205 . PMID  25185227 .   ^  Hsee CK, Hastie R (January 2006). ""Decision and experience: why don't we choose what makes us happy?""  (PDF) . Trends in Cognitive Sciences . 10 (1): 31–7. CiteSeerX  10.1.1.178.7054 . doi : 10.1016/j.tics.2005.11.007 . PMID  16318925 . Archived  (PDF) from the original on 2015-04-20.   ^  Trofimova I (October 1999). ""An investigation of how people of different age, sex, and temperament estimate the world"". Psychological Reports . 85 (2): 533–52. doi : 10.2466/pr0.1999.85.2.533 . PMID  10611787 .   ^  Hardman 2009 , p. 137   ^  Fiedler, Klaus; Unkelbach, Christian (2014-10-01). ""Regressive Judgment: Implications of a Universal Property of the Empirical World"" . Current Directions in Psychological Science . 23 (5): 361–367. doi : 10.1177/0963721414546330 . ISSN  0963-7214 .   ^  Garcia SM, Song H, Tesser A (November 2010). ""Tainted recommendations: The social comparison bias"". Organizational Behavior and Human Decision Processes . 113 (2): 97–101. doi : 10.1016/j.obhdp.2010.06.002 . ISSN  0749-5978 . Lay summary – BPS Research Digest (2010-10-30).   ^  Dalton D, Ortegren M (2011). ""Gender differences in ethics research: The importance of controlling for the social desirability response bias"". Journal of Business Ethics . 103 (1): 73–93. doi : 10.1007/s10551-011-0843-8 .   ^  Kahneman, Knetsch & Thaler 1991 , p. 193   ^  Baron 1994 , p. 382   ^  Baron, J. (in preparation). Thinking and Deciding , 4th edition. New York: Cambridge University Press.   ^  Forsyth DR (2009). Group Dynamics (5th ed.). Cengage Learning. p. 317. ISBN  978-0-495-59952-4 .   ^  ""Penn Psychologists Believe 'Unit Bias' Determines The Acceptable Amount To Eat"" . ScienceDaily (November 21, 2005)   ^  Milgram S (October 1963). ""Behavioral Study of Obedience"". Journal of Abnormal Psychology . 67 (4): 371–8. doi : 10.1037/h0040525 . PMID  14049516 .   ^  Walker D, Vul E (January 2014). ""Hierarchical encoding makes individuals in a group seem more attractive"". Psychological Science . 25 (1): 230–5. doi : 10.1177/0956797613497969 . PMID  24163333 .   ^  Marks G, Miller N (1987). ""Ten years of research on the false-consensus effect: An empirical and theoretical review"". Psychological Bulletin . 102 (1): 72–90. doi : 10.1037/0033-2909.102.1.72 .   ^  ""False Uniqueness Bias (SOCIAL PSYCHOLOGY) – IResearchNet"" . 2016-01-13.   ^  Baron 1994 , p. 275   ^  Pronin E, Kruger J, Savitsky K, Ross L (October 2001). ""You don't know me, but I know you: the illusion of asymmetric insight"". Journal of Personality and Social Psychology . 81 (4): 639–56. doi : 10.1037/0022-3514.81.4.639 . PMID  11642351 .   ^  Hoorens V (1993). ""Self-enhancement and Superiority Biases in Social Comparison"". European Review of Social Psychology . 4 (1): 113–139. doi : 10.1080/14792779343000040 .   ^  Rosset, Evelyn (2008-09-01). ""It's no accident: Our bias for intentional explanations"". Cognition . 108 (3): 771–780. doi : 10.1016/j.cognition.2008.07.001 . ISSN  0010-0277 . PMID  18692779 .   ^  Plous 2006 , p. 206 harvnb error: no target: CITEREFPlous2006 ( help )   ^  Kokkoris, Michail (2020-01-16). ""The Dark Side of Self-Control"" . Harvard Business Review . Retrieved 17 January 2020 .   ^  Plous 2006 , p. 185 harvnb error: no target: CITEREFPlous2006 ( help )   ^  Forsyth DR (2009). Group Dynamics (5th ed.). Pacific Grove, CA: Brooks/Cole.   ^  Kruger J (August 1999). ""Lake Wobegon be gone! The ""below-average effect"" and the egocentric nature of comparative ability judgments"". Journal of Personality and Social Psychology . 77 (2): 221–32. doi : 10.1037/0022-3514.77.2.221 . PMID  10474208 .   ^  Attneave F (August 1953). ""Psychological probability as a function of experienced frequency"". Journal of Experimental Psychology . 46 (2): 81–6. doi : 10.1037/h0057955 . PMID  13084849 .   ^  Fischhoff B, Slovic P, Lichtenstein S (1977). ""Knowing with certainty: The appropriateness of extreme confidence"" . Journal of Experimental Psychology: Human Perception and Performance . 3 (4): 552–564. doi : 10.1037/0096-1523.3.4.552 .   ^  Cacioppo J (2002). Foundations in social neuroscience . Cambridge, Mass: MIT Press. pp. 130–132. ISBN  978-0262531955 .   ^ a  b  Schacter DL (March 1999). ""The seven sins of memory. Insights from psychology and cognitive neuroscience"" . The American Psychologist . 54 (3): 182–203. doi : 10.1037/0003-066X.54.3.182 . PMID  10199218 .   ^  Schmidt SR (July 1994). ""Effects of humor on sentence memory""  (PDF) . Journal of Experimental Psychology. Learning, Memory, and Cognition . 20 (4): 953–67. doi : 10.1037/0278-7393.20.4.953 . PMID  8064254 . Archived from the original  (PDF) on 2016-03-15 . Retrieved 2015-04-19 .   ^  Schmidt SR (2003). ""Life Is Pleasant—and Memory Helps to Keep It That Way!""  (PDF) . Review of General Psychology . 7 (2): 203–210. doi : 10.1037/1089-2680.7.2.203 .   ^  Koriat A, Goldsmith M, Pansky A (2000). ""Toward a psychology of memory accuracy"". Annual Review of Psychology . 51 (1): 481–537. doi : 10.1146/annurev.psych.51.1.481 . PMID  10751979 .   ^  Craik & Lockhart, 1972   ^  Kinnell A, Dennis S (February 2011). ""The list length effect in recognition memory: an analysis of potential confounds"". Memory & Cognition . 39 (2): 348–63. doi : 10.3758/s13421-010-0007-6 . PMID  21264573 .   ^  Wayne Weiten (2010). Psychology: Themes and Variations . Cengage Learning. p. 338. ISBN  978-0-495-60197-5 .   ^  Wayne Weiten (2007). Psychology: Themes and Variations . Cengage Learning. p. 260. ISBN  978-0-495-09303-9 .   ^  Slamecka NJ (April 1968). ""An examination of trace storage in free recall"". Journal of Experimental Psychology . 76 (4): 504–13. doi : 10.1037/h0025695 . PMID  5650563 .   ^  Shepard RN (1967). ""Recognition memory for words, sentences, and pictures"". Journal of Learning and Verbal Behavior . 6 : 156–163. doi : 10.1016/s0022-5371(67)80067-7 .   ^  McBride DM, Dosher BA (2002). ""A comparison of conscious and automatic memory processes for picture and word stimuli: a process dissociation analysis"". Consciousness and Cognition . 11 (3): 423–460. doi : 10.1016/s1053-8100(02)00007-7 . PMID  12435377 .   ^  Defetyer MA, Russo R, McPartlin PL (2009). ""The picture superiority effect in recognition memory: a developmental study using the response signal procedure"". Cognitive Development . 24 (3): 265–273. doi : 10.1016/j.cogdev.2009.05.002 .   ^  Whitehouse AJ, Maybery MT, Durkin K (2006). ""The development of the picture-superiority effect"". British Journal of Developmental Psychology . 24 (4): 767–773. doi : 10.1348/026151005X74153 .   ^  Ally BA, Gold CA, Budson AE (January 2009). ""The picture superiority effect in patients with Alzheimer's disease and mild cognitive impairment"" . Neuropsychologia . 47 (2): 595–8. doi : 10.1016/j.neuropsychologia.2008.10.010 . PMC  2763351 . PMID  18992266 .   ^  Curran T, Doyle J (May 2011). ""Picture superiority doubly dissociates the ERP correlates of recollection and familiarity"". Journal of Cognitive Neuroscience . 23 (5): 1247–62. doi : 10.1162/jocn.2010.21464 . PMID  20350169 .   ^  Martin GN, Carlson NR, Buskist W (2007). Psychology (3rd ed.). Pearson Education. pp. 309–310. ISBN  978-0-273-71086-8 .   ^  O'Brien EJ, Myers JL (1985). ""When comprehension difficulty improves memory for text"". Journal of Experimental Psychology: Learning, Memory, and Cognition . 11 (1): 12–21. doi : 10.1037/0278-7393.11.1.12 .   ^  Rubin, Wetzler & Nebes, 1986; Rubin, Rahhal & Poon, 1998   ^  Lieberman DA (8 December 2011). Human Learning and Memory . Cambridge University Press. p. 432. ISBN  978-1-139-50253-5 .   ^  Morton, Crowder & Prussin, 1971   ^  Pitt I, Edwards AD (2003). Design of Speech-Based Devices: A Practical Guide . Springer. p. 26. ISBN  978-1-85233-436-9 .   ^  Stetson C, Fiesta MP, Eagleman DM (December 2007). ""Does time really slow down during a frightening event?"" . PLOS ONE . 2 (12): e1295. Bibcode : 2007PLoSO...2.1295S . doi : 10.1371/journal.pone.0001295 . PMC  2110887 . PMID  18074019 .   ^  Goldstein EB (2010-06-21). Cognitive Psychology: Connecting Mind, Research and Everyday Experience . Cengage Learning. p. 231. ISBN  978-1-133-00912-2 .   ^  ""Not everyone is in such awe of the internet"" . Evening Standard . Evening Standard. 2011-03-23 . Retrieved 28 October 2015 .   ^  Poppenk, Walia, Joanisse, Danckert, & Köhler, 2006   ^  Von Restorff, H (1933). ""Über die Wirkung von Bereichsbildungen im Spurenfeld (The effects of field formation in the trace field) "" "". Psychological Research . 18 (1): 299–342. doi : 10.1007/bf02409636 .    References [ edit ]   Baron J (1994). Thinking and deciding (2nd ed.). Cambridge University Press. ISBN  978-0-521-43732-5 . CS1 maint: ref=harv ( link )  Baron J (2000). Thinking and deciding (3rd ed.). New York: Cambridge University Press. ISBN  978-0-521-65030-4 . CS1 maint: ref=harv ( link )  Bishop MA, Trout JD (2004). Epistemology and the Psychology of Human Judgment . New York: Oxford University Press . ISBN  978-0-19-516229-5 . CS1 maint: ref=harv ( link )  Gilovich T (1993). How We Know What Isn't So: The Fallibility of Human Reason in Everyday Life . New York: The Free Press. ISBN  978-0-02-911706-4 . CS1 maint: ref=harv ( link )  Gilovich T, Griffin D, Kahneman D (2002). Heuristics and biases: The psychology of intuitive judgment . Cambridge, UK: Cambridge University Press. ISBN  978-0-521-79679-8 . CS1 maint: ref=harv ( link )  Greenwald AG (1980). ""The Totalitarian Ego: Fabrication and Revision of Personal History""  (PDF) . American Psychologist . 35 (7): 603–618. doi : 10.1037/0003-066x.35.7.603 . ISSN  0003-066X . CS1 maint: ref=harv ( link )  Hardman D (2009). Judgment and decision making: psychological perspectives . Wiley-Blackwell. ISBN  978-1-4051-2398-3 . CS1 maint: ref=harv ( link )  Kahneman D, Slovic P, Tversky A (1982). Judgment under Uncertainty: Heuristics and Biases . Science . 185 . Cambridge, UK: Cambridge University Press. pp. 1124–31. doi : 10.1126/science.185.4157.1124 . ISBN  978-0-521-28414-1 . PMID  17835457 . CS1 maint: ref=harv ( link )  Kahneman D, Knetsch JL, Thaler RH (1991). ""Anomalies: The Endowment Effect, Loss Aversion, and Status Quo Bias""  (PDF) . The Journal of Economic Perspectives . 5 (1): 193–206. doi : 10.1257/jep.5.1.193 . Archived from the original  (PDF) on November 24, 2012. CS1 maint: ref=harv ( link )  Plous S (1993). The Psychology of Judgment and Decision Making . New York: McGraw-Hill. ISBN  978-0-07-050477-6 . CS1 maint: ref=harv ( link )  Pohl RF (2017). Cognitive illusions: Intriguing phenomena in thinking, judgment and memory (2nd ed.). London and New York: Routledge. ISBN  978-1-138-90341-8 .  Schacter DL (March 1999). ""The seven sins of memory. Insights from psychology and cognitive neuroscience""  (PDF) . The American Psychologist . 54 (3): 182–203. doi : 10.1037/0003-066X.54.3.182 . PMID  10199218 . Archived from the original  (PDF) on May 13, 2013. CS1 maint: ref=harv ( link )  Sutherland S (2007). Irrationality . Pinter & Martin. ISBN  978-1-905177-07-3 . CS1 maint: ref=harv ( link )  Tetlock PE (2005). Expert Political Judgment: how good is it? how can we know? . Princeton: Princeton University Press. ISBN  978-0-691-12302-8 . CS1 maint: ref=harv ( link )  Virine L, Trumper M (2007). Project Decisions: The Art and Science . Vienna, VA: Management Concepts. ISBN  978-1-56726-217-9 . CS1 maint: ref=harv ( link )   v t e Biases Cognitive biases  Actor–observer  Acquiescence  Ambiguity  Anchoring  Attentional  Attribution  Authority  Automation  Belief  Blind spot  Choice-supportive  Confirmation  Congruence  Cultural  Distinction  Dunning–Kruger  Egocentric  Emotional  Extrinsic incentives  Fading affect  Framing  Correspondence  Halo effect  Hindsight  Horn effect  Hostile attribution  Impact  Implicit  In-group  Mere-exposure effect  Negativity  Normalcy  Omission  Optimism  Out-group homogeneity  Outcome  Overton window  Precision  Present  Pro-innovation  Response  Restraint  Self-serving  Social comparison  Status quo  Time-saving  Trait ascription  von Restorff effect  Zero-risk  In animals  Statistical biases  Estimator  Forecast  Healthy user  Information  Psychological  Lead time  Length time  Non-response  Observer  Omitted-variable  Participation  Recall  Sampling  Selection  Self-selection  Social desirability  Spectrum  Survivorship  Systematic error  Systemic  Verification  Wet  Other biases  Academic  Funding  FUTON  Inductive  Infrastructure  Inherent  In education  Media  False balance  Vietnam War  Norway  South Asia  Sweden  United States  Arab–Israeli conflict  Ukraine  Net  Political bias  Publication  Reporting  White hat  Bias reduction  Cognitive bias mitigation  Debiasing  Heuristics in judgment and decision-making  Lists: General  ·  Memory      Retrieved from "" https://en.wikipedia.org/w/index.php?title=List_of_cognitive_biases&oldid=965490136 ""  Categories : Cognitive biases Psychology lists Behavioral finance Cognitive science lists Hidden categories: CS1 errors: missing periodical CS1 maint: ref=harv Webarchive template wayback links Harv and Sfn no-target errors Articles with short description Wikipedia articles needing clarification from November 2013         Navigation menu      Personal tools      Not logged in Talk Contributions Create account Log in        Namespaces      Article Talk        Variants              Views      Read Edit View history        More           Search                     Navigation      Main page Contents Current events Random article About Wikipedia Contact us Donate Wikipedia store       Contribute      Help Community portal Recent changes Upload file       Tools      What links here Related changes Upload file Special pages Permanent link Page information Wikidata item Cite this page       Print/export      Download as PDF Printable version       Languages      العربية Azərbaycanca Català Deutsch Español فارسی Polski Português Русский ไทย Українська 中文  Edit links        This page was last edited on 1 July 2020, at 16:47 (UTC) .  Text is available under the Creative Commons Attribution-ShareAlike License ;
additional terms may apply.  By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc. , a non-profit organization.    Privacy policy  About Wikipedia  Disclaimers  Contact Wikipedia  Developers  Statistics  Cookie statement  Mobile view          "
14,undesirability of a negative event or consequence(fallacy OR bias OR heuristic) (coronavirus OR covid OR pandemic),2,https://www.sciencedirect.com/science/article/abs/pii/S0749597814000272,"We present a new event-level predictor of comparative optimism: comparative optimism is larger for more socially undesirable events. A meta-analysis shows that event social undesirability predicts comparative optimism effect sizes reported in the literature, over and above the effects of other known predictors. Four experiments corroborate this finding and demonstrate the key role played by respondents’ impression management motives. The effect of social undesirability decreases with stronger than usual anonymity assurances, increases with greater impression management tendencies, and reverses when people want to make a negative impression. Because social undesirability is correlated to other known predictors of comparative optimism (e.g., controllability, severity), it is important to take its effects into account when assessing the effect of other event characteristics. The current research adds to, and bridges, the literatures on event-level predictors and impression management in comparative optimism.","          List of cognitive biases   From Wikipedia, the free encyclopedia    Jump to navigation  Jump to search  Systematic patterns of deviation from norm or rationality in judgment   The loss aversion cognitive bias has been shown in monkeys  Cognitive biases are systematic patterns of deviation from norm and\or rationality in judgment. They are often studied in psychology and behavioral economics . [1]  Although the reality of most of these biases is confirmed by reproducible research, [2] [3] there are often controversies about how to classify these biases or how to explain them. [4]  Gerd Gigerenzer has criticized the framing of cognitive biases as errors in judgment, and favors interpreting them as arising from rational deviations from logical thought. [5]  Explanations include information-processing rules (i.e., mental shortcuts), called heuristics , that the brain uses to produce decisions or judgments. Biases have a variety of forms and appear as cognitive (""cold"") bias, such as mental noise, [6] or motivational (""hot"") bias, such as when beliefs are distorted by wishful thinking . Both effects can be present at the same time. [7] [8]  There are also controversies over some of these biases as to whether they count as useless or irrational , or whether they result in useful attitudes or behavior. For example, when getting to know others, people tend to ask leading questions which seem biased towards confirming their assumptions about the person. However, this kind of confirmation bias has also been argued to be an example of social skill ; a way to establish a connection with the other person. [9]  Although this research overwhelmingly involves human subjects, some findings that demonstrate bias have been found in non-human animals as well. For example, loss aversion has been shown in monkeys and hyperbolic discounting has been observed in rats, pigeons, and monkeys. [10]   Contents   1  Decision-making, belief, and behavioral biases  2  Social biases  3  Memory errors and biases  4  See also  5  Footnotes  6  References    Decision-making, belief, and behavioral biases [ edit ]  Many of these biases affect belief formation, business and economic decisions, and human behavior in general.    Name  Description   Agent detection   The inclination to presume the purposeful intervention of a sentient or intelligent agent .   Ambiguity effect   The tendency to avoid options for which the probability of a favorable outcome is unknown. [11]    Anchoring or focalism  The tendency to rely too heavily, or ""anchor"", on one trait or piece of information when making decisions (usually the first piece of information acquired on that subject). [12] [13]    Anthropocentric thinking   The tendency to use human analogies as a basis for reasoning about other, less familiar, biological phenomena. [14]    Anthropomorphism or personification  The tendency to characterize animals, objects, and abstract concepts as possessing human-like traits, emotions, and intentions. [15] The opposite bias, of not attributing feelings or thoughts to another person, is dehumanised perception , [16] a type of objectification .   Attentional bias   The tendency of perception to be affected by recurring thoughts. [17]    Attribute substitution   Occurs when a judgment has to be made (of a target attribute) that is computationally complex, and instead a more easily calculated heuristic attribute is substituted. This substitution is thought of as taking place in the automatic intuitive judgment system, rather than the more self-aware reflective system.   Automation bias   The tendency to depend excessively on automated systems which can lead to erroneous automated information overriding correct decisions. [18]    Availability heuristic   The tendency to overestimate the likelihood of events with greater ""availability"" in memory, which can be influenced by how recent the memories are or how unusual or emotionally charged they may be. [19]    Availability cascade   A self-reinforcing process in which a collective belief gains more and more plausibility through its increasing repetition in public discourse (or ""repeat something long enough and it will become true""). [20]    Backfire effect   The reaction to disconfirming evidence by strengthening one's previous beliefs. [21] cf. Continued influence effect .   Bandwagon effect   The tendency to do (or believe) things because many other people do (or believe) the same. Related to groupthink and herd behavior . [22]    Base rate fallacy or Base rate neglect  The tendency to ignore general information and focus on information only pertaining to the specific case, even when the general information is more important. [23]    Belief bias   An effect where someone's evaluation of the logical strength of an argument is biased by the believability of the conclusion. [24]    Ben Franklin effect   A person who has performed a favor for someone is more likely to do another favor for that person than they would be if they had received a favor from that person. [25]    Berkson's paradox   The tendency to misinterpret statistical experiments involving conditional probabilities. [26]    Bias blind spot   The tendency to see oneself as less biased than other people, or to be able to identify more cognitive biases in others than in oneself. [27]    Choice-supportive bias   The tendency to remember one's choices as better than they actually were. [28]    Clustering illusion   The tendency to overestimate the importance of small runs, streaks, or clusters in large samples of random data (that is, seeing phantom patterns). [13]    Compassion fade   The predisposition to behave more compassionately towards a small number of identifiable victims than to a large number of anonymous ones. [29]    Confirmation bias   The tendency to search for, interpret, focus on and remember information in a way that confirms one's preconceptions. [30]    Congruence bias   The tendency to test hypotheses exclusively through direct testing, instead of testing possible alternative hypotheses. [13]    Conjunction fallacy   The tendency to assume that specific conditions are more probable than a more general version of those same conditions. For example, subjects in one experiment perceived the probability of a woman being both a bank teller and a feminist as more likely than the probability of her being a bank teller. [31]    Conservatism (belief revision)   The tendency to revise one's belief insufficiently when presented with new evidence. [6] [32] [33]    Continued influence effect   The tendency to believe previously learned misinformation even after it has been corrected. Misinformation can still influence inferences one generates after a correction has occurred. [34] cf. Backfire effect    Contrast effect   The enhancement or reduction of a certain stimulus' perception when compared with a recently observed, contrasting object. [35]    Courtesy bias  The tendency to give an opinion that is more socially correct than one's true opinion, so as to avoid offending anyone. [36]    Curse of knowledge   When better-informed people find it extremely difficult to think about problems from the perspective of lesser-informed people. [37]    Declinism   The predisposition to view the past favorably ( rosy retrospection ) and future negatively. [38]    Decoy effect   Preferences for either option A or B change in favor of option B when option C is presented, which is completely dominated by option B (inferior in all respects) and partially dominated by option A. [39]    Default effect   When given a choice between several options, the tendency to favor the default one. [40]    Denomination effect   The tendency to spend more money when it is denominated in small amounts (e.g., coins) rather than large amounts (e.g., bills). [41]    Disposition effect   The tendency to sell an asset that has accumulated in value and resist selling an asset that has declined in value.   Distinction bias   The tendency to view two options as more dissimilar when evaluating them simultaneously than when evaluating them separately. [42]    Dread aversion  Just as losses yield double the emotional impact of gains, dread yields double the emotional  impact of savouring. [43]    Dunning–Kruger effect   The tendency for unskilled individuals to overestimate their own ability and the tendency for experts to underestimate their own ability. [44]    Duration neglect   The neglect of the duration of an episode in determining its value. [45]    Empathy gap   The tendency to underestimate the influence or strength of feelings, in either oneself or others. [46]    End-of-history illusion   The age-independent belief that one will change less in the future than one has in the past. [47]    Endowment effect   The tendency for people to demand much more to give up an object than they would be willing to pay to acquire it. [48]    Exaggerated expectation   The tendency to expect or predict more extreme outcomes than those outcomes that actually happen. [6]    Experimenter's or expectation bias   The tendency for experimenters to believe, certify, and publish data that agree with their expectations for the outcome of an experiment, and to disbelieve, discard, or downgrade the corresponding weightings for data that appear to conflict with those expectations. [49]    Focusing effect   The tendency to place too much importance on one aspect of an event. [50]    Forer effect or Barnum effect   The observation that individuals will give high accuracy ratings to descriptions of their personality that supposedly are tailored specifically for them, but are in fact vague and general enough to apply to a wide range of people. This effect can provide a partial explanation for the widespread acceptance of some beliefs and practices, such as astrology, fortune telling, graphology, and some types of personality tests. [51]    Form function attribution bias  In human–robot interaction , the tendency of people to make systematic errors when interacting with a robot. People may base their expectations and perceptions of a robot on its appearance (form) and attribute functions which do not necessarily mirror the true functions of the robot. [52]    Framing effect   Drawing different conclusions from the same information, depending on how that information is presented.   Frequency illusion or Baader–Meinhof phenomenon  The frequency illusion is that once something has been noticed then every instance of that thing is noticed, leading to the belief it has a high frequency of occurrence (a form of selection bias ). [53] The Baader–Meinhof phenomenon is the illusion where something that has recently come to one's attention suddenly seems to appear with improbable frequency shortly afterwards. [54] The Baader–Meinhof phenomenon is sometimes conflated with frequency illusion and the recency illusion . [55] It was named after an incidence of frequency illusion in which the Baader–Meinhof Group was mentioned. [56]    Functional fixedness   Limits a person to using an object only in the way it is traditionally used. [57]    Gambler's fallacy   The tendency to think that future probabilities are altered by past events, when in reality they are unchanged. The fallacy arises from an erroneous conceptualization of the law of large numbers . For example, ""I've flipped heads with this coin five times consecutively, so the chance of tails coming out on the sixth flip is much greater than heads."" [58]    Gender bias   A widely held [59] set of implicit biases that discriminate against a gender (typically women [60] ). For example, the assumption that women are less suited to jobs requiring high intellectual ability [61] . Or the assumption that people or animals are male in the absence of any indicators of gender. [62]    Groupthink   The psychological phenomenon that occurs within a group of people in which the desire for harmony or conformity in the group results in an irrational or dysfunctional decision-making outcome. Group members try to minimize conflict and reach a consensus decision without critical evaluation of alternative viewpoints by actively suppressing dissenting viewpoints, and by isolating themselves from outside influences.   Hard–easy effect   The tendency to overestimate one's ability to accomplish hard tasks, and underestimate one's ability to accomplish easy tasks [6] [63] [64] [65]    Hindsight bias   Sometimes called the ""I-knew-it-all-along"" effect, the tendency to see past events as being predictable [66] at the time those events happened.   Hostile attribution bias   The ""hostile attribution bias"" is the tendency to interpret others' behaviors as having hostile intent, even when the behavior is ambiguous or benign. [67]    Hot-hand fallacy   The ""hot-hand fallacy"" (also known as the ""hot hand phenomenon"" or ""hot hand"") is the belief that a person who has experienced success with a random event has a greater chance of further success in additional attempts.   Hyperbolic discounting   Discounting is the tendency for people to have a stronger preference for more immediate payoffs relative to later payoffs. Hyperbolic discounting leads to choices that are inconsistent over time – people make choices today that their future selves would prefer not to have made, despite using the same reasoning. [68] Also known as current moment bias, present-bias, and related to Dynamic inconsistency . A good example of this: a study showed that when making food choices for the coming week, 74% of participants chose fruit, whereas when the food choice was for the current day, 70% chose chocolate.   IKEA effect   The tendency for people to place a disproportionately high value on objects that they partially assembled themselves, such as furniture from IKEA , regardless of the quality of the end product. [69]    Illicit transference   Occurs when a term in the distributive (referring to every member of a class) and collective (referring to the class itself as a whole) sense are treated as equivalent. The two variants of this fallacy are the fallacy of composition and the fallacy of division .   Illusion of control   The tendency to overestimate one's degree of influence over other external events. [70]    Illusion of validity   Believing that one's judgments are accurate, especially when available information is consistent or inter-correlated. [71]    Illusory correlation   Inaccurately perceiving a relationship between two unrelated events. [72] [73]    Illusory truth effect   A tendency to believe that a statement is true if it is easier to process , or if it has been stated multiple times , regardless of its actual veracity. These are specific cases of truthiness .   Impact bias   The tendency to overestimate the length or the intensity of the impact of future feeling states. [74]    Implicit association   The speed with which people can match words depends on how closely they are associated.   Information bias   The tendency to seek information even when it cannot affect action. [75]    Insensitivity to sample size   The tendency to under-expect variation in small samples.   Interoceptive bias  The tendency for sensory input about the body itself to affect one's judgement about external, unrelated circumstances. (As for example, in parole judges who are more lenient when fed and rested.) [76] [77] [78] [79]    Irrational escalation or Escalation of commitment   The phenomenon where people justify increased investment in a decision, based on the cumulative prior investment, despite new evidence suggesting that the decision was probably wrong. Also known as the sunk cost fallacy.   Law of the instrument   An over-reliance on a familiar tool or methods, ignoring or under-valuing alternative approaches. ""If all you have is a hammer, everything looks like a nail.""   Less-is-better effect   The tendency to prefer a smaller set to a larger set judged separately, but not jointly.   Look-elsewhere effect   An apparently statistically significant observation may have actually arisen by chance because of the size of the parameter space to be searched.   Loss aversion   The perceived disutility of giving up an object is greater than the utility associated with acquiring it. [80] (see also Sunk cost effects and endowment effect).   Mere exposure effect   The tendency to express undue liking for things merely because of familiarity with them. [81]    Money illusion   The tendency to concentrate on the nominal value (face value) of money rather than its value in terms of purchasing power. [82]    Moral credential effect   Occurs when someone who does something good gives themselves permission to be less good in the future.   Negativity bias or Negativity effect  Psychological phenomenon by which humans have a greater recall of unpleasant memories compared with positive memories. [83] [84] (see also actor-observer bias, group attribution error , positivity effect, and negativity effect ). [85]    Neglect of probability   The tendency to completely disregard probability when making a decision under uncertainty. [86]    Normalcy bias   The refusal to plan for, or react to, a disaster which has never happened before.   Not invented here   Aversion to contact with or use of products, research, standards, or knowledge developed outside a group. Related to IKEA effect .   Observer-expectancy effect   When a researcher expects a given result and therefore unconsciously manipulates an experiment or misinterprets data in order to find it (see also subject-expectancy effect ).   Omission bias   The tendency to judge harmful actions (commissions) as worse, or less moral, than equally harmful inactions (omissions). [87]    Optimism bias   The tendency to be over-optimistic, underestimating greatly the probability of undesirable outcomes and overestimating favorable and pleasing outcomes (see also wishful thinking , valence effect , positive outcome bias ). [88] [89]    Ostrich effect   Ignoring an obvious (negative) situation.   Outcome bias   The tendency to judge a decision by its eventual outcome instead of based on the quality of the decision at the time it was made.   Overconfidence effect   Excessive confidence in one's own answers to questions. For example, for certain types of questions, answers that people rate as ""99% certain"" turn out to be wrong 40% of the time. [6] [90] [91] [92]    Pareidolia   A vague and random stimulus (often an image or sound) is perceived as significant, e.g., seeing images of animals or faces in clouds, the man in the moon , and hearing non-existent hidden messages on records played in reverse .   Pygmalion effect   The phenomenon whereby others' expectations of a target person affect the target person's performance.   Pessimism bias   The tendency for some people, especially those suffering from depression , to overestimate the likelihood of negative things happening to them.   Plan continuation bias   Failure to recognize that the original plan of action is no longer appropriate for a changing situation or for a situation that is different than anticipated. [93]    Planning fallacy   The tendency to underestimate task-completion times. [74]    Present bias   The tendency of people to give stronger weight to payoffs that are closer to the present time when considering trade-offs between two future moments. [94]    Plant blindness   The tendency to ignore plants in their environment and a failure to recognize and appreciate the utility of plants to life on earth. [95]    Pro-innovation bias   The tendency to have an excessive optimism towards an invention or innovation's usefulness throughout society, while often failing to identify its limitations and weaknesses.   Projection bias   The tendency to overestimate how much our future selves share one's current preferences, thoughts and values, thus leading to sub-optimal choices. [96] [97] [84]    Pseudocertainty effect   The tendency to make risk-averse choices if the expected outcome is positive, but make risk-seeking choices to avoid negative outcomes. [98]    Reactance   The urge to do the opposite of what someone wants you to do out of a need to resist a perceived attempt to constrain your freedom of choice (see also Reverse psychology ).   Reactive devaluation   Devaluing proposals only because they purportedly originated with an adversary.   Recency illusion   The illusion that a phenomenon one has noticed only recently is itself recent. Often used to refer to linguistic phenomena; the illusion that a word or language usage that one has noticed only recently is an innovation when it is in fact long-established (see also frequency illusion ).   Systematic Bias  Judgement that arises when targets of differentiating judgement become subject to effects of regression that are not equivalent. [99]    Restraint bias   The tendency to overestimate one's ability to show restraint in the face of temptation.   Rhyme as reason effect   Rhyming statements are perceived as more truthful. A famous example being used in the O.J Simpson trial with the defense's use of the phrase ""If the gloves don't fit, then you must acquit.""   Risk compensation / Peltzman effect   The tendency to take greater risks when perceived safety increases.   Salience bias   The tendency to focus on items that are more prominent or emotionally striking and ignore those that are unremarkable, even though this difference is often irrelevant by objective standards.   Selection bias   The tendency to notice something more when something causes us to be more aware of it, such as when we buy a car, we tend to notice similar cars more often than we did before. They are not suddenly more common – we just are noticing them more. Also called the Observational Selection Bias.   Selective perception   The tendency for expectations to affect perception.   Semmelweis reflex   The tendency to reject new evidence that contradicts a paradigm. [33]    Sexual overperception bias / Sexual underperception bias   The tendency to over-/underestimate sexual interest of another person in oneself.   Social comparison bias   The tendency, when making decisions, to favour potential candidates who don't compete with one's own particular strengths. [100]    Social desirability bias   The tendency to over-report socially desirable characteristics or behaviours in oneself and under-report socially undesirable characteristics or behaviours. [101] See also: § Courtesy bias .   Status quo bias   The tendency to like things to stay relatively the same (see also loss aversion , endowment effect , and system justification ). [102] [103]    Stereotyping   Expecting a member of a group to have certain characteristics without having actual information about that individual.   Subadditivity effect   The tendency to judge probability of the whole to be less than the probabilities of the parts. [104]    Subjective validation   Perception that something is true if a subject's belief demands it to be true. Also assigns perceived connections between coincidences.   Surrogation   Losing sight of the strategic construct that a measure is intended to represent, and subsequently acting as though the measure is the construct of interest.   Survivorship bias   Concentrating on the people or things that ""survived"" some process and inadvertently overlooking those that didn't because of their lack of visibility.   Time-saving bias   Underestimations of the time that could be saved (or lost) when increasing (or decreasing) from a relatively low speed and overestimations of the time that could be saved (or lost) when increasing (or decreasing) from a relatively high speed.   Third-person effect   A hypothesized tendency to believe that mass communicated media messages have a greater effect on others than on themselves. As of 2020, the third-person effect has yet to be reliably demonstrated in a scientific context.   Parkinson's law of triviality   The tendency to give disproportionate weight to trivial issues. Also known as bikeshedding, this bias explains why an organization may avoid specialized or complex subjects, such as the design of a nuclear reactor, and instead focus on something easy to grasp or rewarding to the average participant, such as the design of an adjacent bike shed. [105]    Unit bias   The standard suggested amount of consumption (e.g., food serving size) is perceived to be appropriate, and a person would consume it all even if it is too much for this particular person. [106]    Weber–Fechner law   Difficulty in comparing small differences in large quantities.   Well travelled road effect   Underestimation of the duration taken to traverse oft-traveled routes and overestimation of the duration taken to traverse less familiar routes.   Women are wonderful effect   A tendency to associate more positive attributes with women than with men.   Zero-risk bias   Preference for reducing a small risk to zero over a greater reduction in a larger risk.   Zero-sum bias   A bias whereby a situation is incorrectly perceived to be like a zero-sum game (i.e., one person gains at the expense of another).  Social biases [ edit ]  Most of these biases are labeled as attributional biases .    Name  Description   Actor-observer bias   The tendency for explanations of other individuals' behaviors to overemphasize the influence of their personality and underemphasize the influence of their situation (see also Fundamental attribution error ), and for explanations of one's own behaviors to do the opposite (that is, to overemphasize the influence of our situation and underemphasize the influence of our own personality).   Authority bias   The tendency to attribute greater accuracy to the opinion of an authority figure (unrelated to its content) and be more influenced by that opinion. [107]    Cheerleader effect   The tendency for people to appear more attractive in a group than in isolation. [108]    Defensive attribution hypothesis   Attributing more blame to a harm-doer as the outcome becomes more severe or as personal or situational similarity to the victim increases.   Egocentric bias   Occurs when people claim more responsibility for themselves for the results of a joint action than an outside observer would credit them with.   Extrinsic incentives bias   An exception to the fundamental attribution error , when people view others as having (situational) extrinsic motivations and (dispositional) intrinsic motivations for oneself   False consensus effect   The tendency for people to overestimate the degree to which others agree with them. [109]    False uniqueness bias   The tendency of people to see their projects and themselves as more singular than they actually are. [110]    Fundamental attribution error   The tendency for people to over-emphasize personality-based explanations for behaviors observed in others while under-emphasizing the role and power of situational influences on the same behavior [84] (see also actor-observer bias, group attribution error , positivity effect, and negativity effect ). [85]    Group attribution error   The biased belief that the characteristics of an individual group member are reflective of the group as a whole or the tendency to assume that group decision outcomes reflect the preferences of group members, even when information is available that clearly suggests otherwise.   Halo effect   The tendency for a person's positive or negative traits to ""spill over"" from one personality area to another in others' perceptions of them (see also physical attractiveness stereotype ). [111]    Illusion of asymmetric insight   People perceive their knowledge of their peers to surpass their peers' knowledge of them. [112]    Illusion of external agency   When people view self-generated preferences as instead being caused by insightful, effective and benevolent agents.   Illusion of transparency   People overestimate others' ability to know themselves, and they also overestimate their ability to know others.   Illusory superiority   Overestimating one's desirable qualities, and underestimating undesirable qualities, relative to other people. (Also known as ""Lake Wobegon effect"", ""better-than-average effect"", or ""superiority bias"".) [113]    Ingroup bias   The tendency for people to give preferential treatment to others they perceive to be members of their own groups.   Intentionality bias  Tendency to judge human action to intentional rather than accidental. [114]    Just-world hypothesis   The tendency for people to want to believe that the world is fundamentally just, causing them to rationalize an otherwise inexplicable injustice as deserved by the victim(s).   Moral luck   The tendency for people to ascribe greater or lesser moral standing based on the outcome of an event.   Naïve cynicism   Expecting more egocentric bias in others than in oneself.   Naïve realism   The belief that we see reality as it really is – objectively and without bias; that the facts are plain for all to see; that rational people will agree with us; and that those who don't are either uninformed, lazy, irrational, or biased.   Outgroup homogeneity bias   Individuals see members of their own group as being relatively more varied than members of other groups. [115]    Puritanical bias   Refers to the tendency to attribute cause of an undesirable outcome or wrongdoing by an individual to a moral deficiency or lack of self control rather than taking into account the impact of broader societal determinants . [116]    Self-serving bias   The tendency to claim more responsibility for successes than failures. It may also manifest itself as a tendency for people to evaluate ambiguous information in a way beneficial to their interests (see also group-serving bias ). [117]    Shared information bias   Known as the tendency for group members to spend more time and energy discussing information that all members are already familiar with (i.e., shared information), and less time and energy discussing information that only some members are aware of (i.e., unshared information). [118]    System justification   The tendency to defend and bolster the status quo. Existing social, economic, and political arrangements tend to be preferred, and alternatives disparaged, sometimes even at the expense of individual and collective self-interest. (See also status quo bias.)   Trait ascription bias   The tendency for people to view themselves as relatively variable in terms of personality, behavior, and mood while viewing others as much more predictable.   Ultimate attribution error   Similar to the fundamental attribution error, in this error a person is likely to make an internal attribution to an entire group instead of the individuals within the group.   Worse-than-average effect   A tendency to believe ourselves to be worse than others at tasks which are difficult. [119]   Memory errors and biases [ edit ]  Main article: List of memory biases  In psychology  and  cognitive science , a memory bias is a cognitive bias that either enhances or impairs the recall of a memory (either the chances that the memory will be recalled at all, or the amount of time it takes for it to be recalled, or both), or that alters the content of a reported memory. There are many types of memory bias, including:    Name  Description   Bizarreness effect   Bizarre material is better remembered than common material.   Conservatism or Regressive bias  Tendency to remember high values and high likelihoods/probabilities/frequencies as lower than they actually were and low ones as higher than they actually were. Based on the evidence, memories are not extreme enough. [120] [121]    Consistency bias   Incorrectly remembering one's past attitudes and behaviour as resembling present attitudes and behaviour. [122]    Context effect   That cognition and memory are dependent on context, such that out-of-context memories are more difficult to retrieve than in-context memories (e.g., recall time and accuracy for a work-related memory will be lower at home, and vice versa).   Cross-race effect   The tendency for people of one race to have difficulty identifying members of a race other than their own.   Cryptomnesia   A form of misattribution where a memory is mistaken for imagination, because there is no subjective experience of it being a memory. [123]    Egocentric bias   Recalling the past in a self-serving manner, e.g., remembering one's exam grades as being better than they were, or remembering a caught fish as bigger than it really was.   Fading affect bias   A bias in which the emotion associated with unpleasant memories fades more quickly than the emotion associated with positive events. [124]    False memory   A form of misattribution where imagination is mistaken for a memory.   Generation effect (Self-generation effect)  That self-generated information is remembered best. For instance, people are better able to recall memories of statements that they have generated than similar statements generated by others.   Google effect   The tendency to forget information that can be found readily online by using Internet search engines.   Humor effect   That humorous items are more easily remembered than non-humorous ones, which might be explained by the distinctiveness of humor, the increased cognitive processing time to understand the humor, or the emotional arousal caused by the humor. [125]    Lag effect  The phenomenon whereby learning is greater when studying is spread out over time, as opposed to studying the same amount of time in a single session. See also spacing effect .   Leveling and sharpening   Memory distortions introduced by the loss of details in a recollection over time, often concurrent with sharpening or selective recollection of certain details that take on exaggerated significance in relation to the details or aspects of the experience lost through leveling. Both biases may be reinforced over time, and by repeated recollection or re-telling of a memory. [126]    Levels-of-processing effect   That different methods of encoding information into memory have different levels of effectiveness. [127]    List-length effect   A smaller percentage of items are remembered in a longer list, but as the length of the list increases, the absolute number of items remembered increases as well. For example, consider a list of 30 items (""L30"") and a list of 100 items (""L100""). An individual may remember 15 items from L30, or 50%, whereas the individual may remember 40 items from L100, or 40%. Although the percent of L30 items remembered (50%) is greater than the percent of L100 (40%), more L100 items (40) are remembered than L30 items (15). [128] [ further explanation needed ]    Misinformation effect   Memory becoming less accurate because of interference from post-event information . [129]    Modality effect   That memory recall is higher for the last items of a list when the list items were received via speech than when they were received through writing.   Mood-congruent memory bias   The improved recall of information congruent with one's current mood.   Next-in-line effect   When taking turns speaking in a group using a predetermined order (e.g. going clockwise around a room, taking numbers, etc.) people tend to have diminished recall for the words of the person who spoke immediately before them. [130]    Part-list cueing effect   That being shown some items from a list and later retrieving one item causes it to become harder to retrieve the other items. [131]    Peak-end rule   That people seem to perceive not the sum of an experience but the average of how it was at its peak (e.g., pleasant or unpleasant) and how it ended.   Picture superiority effect   The notion that concepts that are learned by viewing pictures are more easily and frequently recalled than are concepts that are learned by viewing their written word form counterparts. [132] [133] [134] [135] [136] [137]    Positivity effect ( Socioemotional selectivity theory )  That older adults favor positive over negative information in their memories.   Serial position effect   That items near the end of a sequence are the easiest to recall, followed by the items at the beginning of a sequence; items in the middle are the least likely to be remembered. [138]    Processing difficulty effect   That information that takes longer to read and is thought about more (processed with more difficulty) is more easily remembered. [139]    Reminiscence bump   The recalling of more personal events from adolescence and early adulthood than personal events from other lifetime periods. [140]    Self-relevance effect   That memories relating to the self are better recalled than similar information relating to others.   Source confusion   Confusing episodic memories with other information, creating distorted memories. [141]    Spacing effect   That information is better recalled if exposure to it is repeated over a long span of time rather than a short one.   Spotlight effect   The tendency to overestimate the amount that other people notice your appearance or behavior.   Stereotypical bias   Memory distorted towards stereotypes (e.g., racial or gender).   Suffix effect   Diminishment of the recency effect because a sound item is appended to the list that the subject is not required to recall. [142] [143]    Suggestibility   A form of misattribution where ideas suggested by a questioner are mistaken for memory.   Tachypsychia   When time perceived by the individual either lengthens, making events appear to slow down, or contracts. [144]    Telescoping effect   The tendency to displace recent events backward in time and remote events forward in time, so that recent events appear more remote, and remote events, more recent.   Testing effect   The fact that you more easily remember information you have read by rewriting it instead of rereading it. [145]    Tip of the tongue phenomenon  When a subject is able to recall parts of an item, or related information, but is frustratingly unable to recall the whole item. This is thought to be an instance of ""blocking"" where multiple similar memories are being recalled and interfere with each other. [123]    Travis Syndrome   Overestimating the significance of the present. [146] It is related to chronological snobbery with possibly an appeal to novelty  logical fallacy being part of the bias.   Verbatim effect   That the ""gist"" of what someone has said is better remembered than the verbatim wording. [147] This is because memories are representations, not exact copies.   von Restorff effect   That an item that sticks out is more likely to be remembered than other items. [148]    Zeigarnik effect   That uncompleted or interrupted tasks are remembered better than completed ones.  See also [ edit ]    Psychology portal  Society portal  Philosophy portal   Affective forecasting – Predicting someone's future emotions (affect)  Anecdotal evidence – Evidence relying on personal testimony  Apophenia – Tendency to perceive connections between unrelated things  Attribution (psychology) – The process by which individuals explain the causes of behavior and events  Black swan theory – Theory of response to surprise events  Chronostasis – Distortion in the perception of time  Cognitive distortion – An exaggerated or irrational thought pattern involved in the onset and perpetuation of psychopathological states  Defence mechanism – Unconscious psychological mechanism that reduces anxiety arising from unacceptable or potentially harmful stimuli  Dysrationalia – Inability to think and behave rationally despite adequate intelligence  Fear, uncertainty, and doubt – Tactic used to influence opinion by disseminating negative, dubious, or false information  Feedback – Process where information about current status is used to influence future status  Impostor syndrome – Psychological pattern of doubting one's accomplishments and fearing being exposed as a ""fraud""  List of common misconceptions – Wikipedia list article  List of fallacies – Types of reasoning that are logically incorrect  List of maladaptive schemas  List of memory biases  List of psychological effects – Wikipedia list article  iSheep#Behavioural patterns – Derogatory marketing term (references multiple cognitive biasses)  Media bias – Bias or perceived bias of journalists and news producers within the mass media in the selection of events and stories that are reported and how they are covered  Mind projection fallacy – An informal fallacy that the way one sees the world reflects the way the world really is  Motivated reasoning – Using emotionally-biased reasoning to produce justifications or make decisions  Observational error , also known as Systematic bias  Outline of public relations – Overview of and topical guide to public relations  Outline of thought – Overview of and topical guide to thought  Pollyanna principle – The tendency of people to remember pleasant events more than unpleasant ones  Positive feedback – Destabilising process that occurs in a feedback loop  Prevalence effect  Propaganda – Form of communication intended to sway the audience through presenting only one side of the argument  Publication bias – Higher probability of publishing results showing a significant finding  Recall bias – Systematic error caused by differences in the accuracy or completeness of the recollections retrieved  Self-handicapping – Cognitive strategy by which people avoid effort in the hopes of keeping potential failure from hurting self-esteem   Footnotes [ edit ]    ^  Haselton MG, Nettle D, Andrews PW (2005). ""The evolution of cognitive bias.""  (PDF) .  In Buss DM (ed.). The Handbook of Evolutionary Psychology . Hoboken, NJ, US: John Wiley & Sons Inc. pp. 724–746.   ^  ""Cognitive Bias – Association for Psychological Science"" . www.psychologicalscience.org . Retrieved 2018-10-10 .   ^  Thomas O (2018-01-19). ""Two decades of cognitive bias research in entrepreneurship: What do we know and where do we go from here?"". Management Review Quarterly . 68 (2): 107–143. doi : 10.1007/s11301-018-0135-9 . ISSN  2198-1620 .   ^  Dougherty MR, Gettys CF, Ogden EE (1999). ""MINERVA-DM: A memory processes model for judgments of likelihood""  (PDF) . Psychological Review . 106 (1): 180–209. doi : 10.1037/0033-295x.106.1.180 .   ^  Gigerenzer G (2006). ""Bounded and Rational"".  In Stainton RJ (ed.). Contemporary Debates in Cognitive Science . Blackwell. p. 129. ISBN  978-1-4051-1304-5 .   ^ a  b  c  d  e  Fielder, Klaus (October 2014). ""Regressive Judgment: Implications of a Universal Property of the Empirical World""  (PDF) . Sage Journals – via Google Scholar. Lay summary .   ^  MacCoun RJ (1998). ""Biases in the interpretation and use of research results""  (PDF) . Annual Review of Psychology . 49 (1): 259–87. doi : 10.1146/annurev.psych.49.1.259 . PMID  15012470 .   ^  Nickerson RS (1998). ""Confirmation Bias: A Ubiquitous Phenomenon in Many Guises""  (PDF) . Review of General Psychology . 2 (2): 175–220 [198]. doi : 10.1037/1089-2680.2.2.175 .   ^  Dardenne B, Leyens JP (1995). ""Confirmation Bias as a Social Skill"" . Personality and Social Psychology Bulletin . 21 (11): 1229–1239. doi : 10.1177/01461672952111011 .   ^  Alexander WH, Brown JW (June 2010). ""Hyperbolically discounted temporal difference learning"" . Neural Computation . 22 (6): 1511–27. doi : 10.1162/neco.2010.08-09-1080 . PMC  3005720 . PMID  20100071 .   ^  Baron 1994 , p. 372   ^  Zhang Y, Lewis M, Pellon M, Coleman P (2007). ""A Preliminary Research on Modeling Cognitive Agents for Social Environments in Multi-Agent Systems""  (PDF) : 116–123.  Cite journal requires |journal= ( help )   ^ a  b  c  Iverson GL, Brooks BL, Holdnack JA (2008). ""Misdiagnosis of Cognitive Impairment in Forensic Neuropsychology"".  In Heilbronner RL (ed.). Neuropsychology in the Courtroom: Expert Analysis of Reports and Testimony . New York: Guilford Press. p. 248. ISBN  9781593856342 . CS1 maint: ref=harv ( link )   ^  Coley JD, Tanner KD (2012). ""Common origins of diverse misconceptions: cognitive principles and the development of biology thinking"" . CBE Life Sciences Education . 11 (3): 209–15. doi : 10.1187/cbe.12-06-0074 . PMC  3433289 . PMID  22949417 .   ^  ""The Real Reason We Dress Pets Like People"" . LiveScience.com . Retrieved 2015-11-16 .   ^  Harris LT, Fiske ST (January 2011). ""Dehumanized Perception: A Psychological Means to Facilitate Atrocities, Torture, and Genocide?"" . Zeitschrift Fur Psychologie . 219 (3): 175–181. doi : 10.1027/2151-2604/a000065 . PMC  3915417 . PMID  24511459 .   ^  Bar-Haim Y, Lamy D, Pergamin L, Bakermans-Kranenburg MJ, van IJzendoorn MH (January 2007). ""Threat-related attentional bias in anxious and nonanxious individuals: a meta-analytic study"" . Psychological Bulletin . 133 (1): 1–24. doi : 10.1037/0033-2909.133.1.1 . PMID  17201568 .   ^  Goddard K, Roudsari A, Wyatt JC (2011). ""Automation Bias – A Hidden Issue for Clinical Decision Support System Use"" . International Perspectives in Health Informatics . Studies in Health Technology and Informatics. 164 . IOS Press. doi : 10.3233/978-1-60750-709-3-17 .   ^  Schwarz N, Bless H, Strack F, Klumpp G, Rittenauer-Schatka H, Simons A (1991). ""Ease of Retrieval as Information: Another Look at the Availability Heuristic""  (PDF) . Journal of Personality and Social Psychology . 61 (2): 195–202. doi : 10.1037/0022-3514.61.2.195 . Archived from the original  (PDF) on 9 February 2014 . Retrieved 19 Oct 2014 .   ^  Kuran T, Sunstein CR (1998). ""Availability Cascades and Risk Regulation"" . Stanford Law Review . 51 (4): 683–768. doi : 10.2307/1229439 . JSTOR  1229439 .   ^  Sanna LJ, Schwarz N, Stocker SL (2002). ""When debiasing backfires: Accessible content and accessibility experiences in debiasing hindsight""  (PDF) . Journal of Experimental Psychology: Learning, Memory, and Cognition . 28 (3): 497–502. CiteSeerX  10.1.1.387.5964 . doi : 10.1037/0278-7393.28.3.497 . ISSN  0278-7393 .   ^  Colman A (2003). Oxford Dictionary of Psychology . New York: Oxford University Press. p. 77 . ISBN  978-0-19-280632-1 .   ^  Baron 1994 , pp. 224–228   ^  Klauer KC, Musch J, Naumer B (October 2000). ""On belief bias in syllogistic reasoning"". Psychological Review . 107 (4): 852–84. doi : 10.1037/0033-295X.107.4.852 . PMID  11089409 .   ^  ""Harness the power of the 'Ben Franklin Effect' to get someone to like you"" . Business Insider . Retrieved 2018-10-10 .   ^  ""Berkson's Paradox | Brilliant Math & Science Wiki"" . brilliant.org . Retrieved 2018-10-10 .   ^  Pronin E, Kugler MB (July 2007). ""Valuing thoughts, ignoring behavior: The introspection illusion as a source of the bias blind spot"". Journal of Experimental Social Psychology . 43 (4): 565–578. doi : 10.1016/j.jesp.2006.05.011 . ISSN  0022-1031 .   ^  Mather M, Shafir E, Johnson MK (March 2000). ""Misremembrance of options past: source monitoring and choice""  (PDF) . Psychological Science . 11 (2): 132–8. doi : 10.1111/1467-9280.00228 . PMID  11273420 . Archived  (PDF) from the original on 2009-01-17.   ^  Västfjäll D, Slovic P, Mayorga M, Peters E (18 June 2014). ""Compassion fade: affect and charity are greatest for a single child in need"" . PLOS ONE . 9 (6): e100115. Bibcode : 2014PLoSO...9j0115V . doi : 10.1371/journal.pone.0100115 . PMC  4062481 . PMID  24940738 .   ^  Oswald ME, Grosjean S (2004). ""Confirmation Bias"" .  In Pohl RF (ed.). Cognitive Illusions: A Handbook on Fallacies and Biases in Thinking, Judgement and Memory . Hove, UK: Psychology Press. pp. 79–96 . ISBN  978-1-84169-351-4 . OCLC  55124398 .   ^  Fisk JE (2004). ""Conjunction fallacy"" .  In Pohl RF (ed.). Cognitive Illusions: A Handbook on Fallacies and Biases in Thinking, Judgement and Memory . Hove, UK: Psychology Press. pp. 23–42 . ISBN  978-1-84169-351-4 . OCLC  55124398 .   ^  DuCharme WW (1970). ""Response bias explanation of conservative human inference"". Journal of Experimental Psychology . 85 (1): 66–74. doi : 10.1037/h0029546 . hdl : 2060/19700009379 .   ^ a  b  Edwards W (1968). ""Conservatism in human information processing"".  In Kleinmuntz B (ed.). Formal representation of human judgment . New York: Wiley. pp. 17–52.   ^  Johnson HM, Seifert CM (November 1994). ""Sources of the continued influence effect: When misinformation in memory affects later inferences"". Journal of Experimental Psychology: Learning, Memory, and Cognition . 20 (6): 1420–1436. doi : 10.1037/0278-7393.20.6.1420 .   ^  Plous 1993 , pp. 38–41   ^  Ciccarelli S, White J (2014). Psychology (4th ed.). Pearson Education, Inc. p. 62. ISBN  978-0205973354 .   ^  Ackerman MS, ed. (2003). Sharing expertise beyond knowledge management (online ed.). Cambridge, Massachusetts: MIT Press. p. 7 . ISBN  9780262011952 .   ^  Quartz SR, The State Of The World Isn't Nearly As Bad As You Think , Edge Foundation, Inc. , retrieved 2016-02-17   ^  ""Evolution and cognitive biases: the decoy effect"" . FutureLearn . Retrieved 2018-10-10 .   ^  ""The Default Effect: How to Leverage Bias and Influence Behavior"" . Influence at Work. 2012-01-11 . Retrieved 2018-10-10 .   ^  Why We Spend Coins Faster Than Bills by Chana Joffe-Walt. All Things Considered , 12 May 2009.   ^  Hsee CK, Zhang J (May 2004). ""Distinction bias: misprediction and mischoice due to joint evaluation"". Journal of Personality and Social Psychology . 86 (5): 680–95. CiteSeerX  10.1.1.484.9171 . doi : 10.1037/0022-3514.86.5.680 . PMID  15161394 .   ^  de Meza D, Dawson C (January 24, 2018). ""Wishful Thinking, Prudent Behavior: The Evolutionary Origin of Optimism, Loss Aversion and Disappointment Aversion"". SSRN  3108432 .  Cite journal requires |journal= ( help )   ^  Kruger J, Dunning D (December 1999). ""Unskilled and unaware of it: how difficulties in recognizing one's own incompetence lead to inflated self-assessments"". Journal of Personality and Social Psychology . 77 (6): 1121–34. CiteSeerX  10.1.1.64.2655 . doi : 10.1037/0022-3514.77.6.1121 . PMID  10626367 .   ^  Duration Neglect in Retrospective Evaluations of Affective Episodes  Archived 2017-08-08 at the Wayback Machine | Journal of Personality and Social Psychology   ^  ""Understanding and Mastering the Empathy Gap"" . Psychology Today .   ^  Quoidbach J, Gilbert DT , Wilson TD (January 2013). ""The end of history illusion""  (PDF) . Science . 339 (6115): 96–8. Bibcode : 2013Sci...339...96Q . doi : 10.1126/science.1229294 . PMID  23288539 . Archived from the original  (PDF) on 2013-01-13. Young people, middle-aged people, and older people all believed they had changed a lot in the past but would change relatively little in the future.   ^  ( Kahneman, Knetsch & Thaler 1991 , p. 193) Richard Thaler coined the term ""endowment effect.""   ^  Jeng M (2006). ""A selected history of expectation bias in physics"". American Journal of Physics . 74 (7): 578–583. arXiv : physics/0508199 . Bibcode : 2006AmJPh..74..578J . doi : 10.1119/1.2186333 .   ^  Kahneman D, Krueger AB, Schkade D, Schwarz N, Stone AA (June 2006). ""Would you be happier if you were richer? A focusing illusion""  (PDF) . Science . 312 (5782): 1908–10. Bibcode : 2006Sci...312.1908K . CiteSeerX  10.1.1.373.2683 . doi : 10.1126/science.1129688 . PMID  16809528 .   ^  ""The Barnum Demonstration"" . psych.fullerton.edu . Retrieved 2018-10-10 .   ^  Haring KS, Watanabe K, Velonaki M, Tossell CC, Finomore V (2018). ""FFAB-The Form Function Attribution Bias in Human Robot Interaction"". IEEE Transactions on Cognitive and Developmental Systems . 10 (4): 843–851. doi : 10.1109/TCDS.2018.2851569 .   ^  Zwicky A (2005-08-07). ""Just Between Dr. Language and I"" . Language Log .   ^  ""The Baader-Meinhof Phenomenon"" . Damn Interesting . Retrieved 2020-02-16 .   ^  ""What's the Baader-Meinhof phenomenon?"" . howstuffworks.com . 20 March 2015 . Retrieved 15 April 2018 .   ^  ""What's in a name?"" . twincities.com . St. Paul Pioneer Press . Retrieved June 5, 2020 . As you might guess, the phenomenon is named after an incident in which I was talking to a friend about the Baader-Meinhof gang (and this was many years after they were in the news). The next day, my friend phoned me and referred me to an article in that day’s newspaper in which the Baader-Meinhof gang was mentioned.   ^  ""The Psychology Guide: What Does Functional Fixedness Mean?"" . PsycholoGenie . Retrieved 2018-10-10 .   ^  Investopedia Staff (2006-10-29). ""Gambler's Fallacy/Monte Carlo Fallacy"" . Investopedia . Retrieved 2018-10-10 .   ^  ""GSNI | Human Development Reports"" . hdr.undp.org . Retrieved 2020-06-10 .   ^  Abel, Martin (September 2019). ""Do Workers Discriminate against Female Bosses?""  (PDF) . IZA Institute of Labor Economics .   ^  Bian, Lin; Leslie, Sarah-Jane; Cimpian, Andrei (November 2018). ""Evidence of bias against girls and women in contexts that emphasize intellectual ability"" . American Psychologist . 73 (9): 1139–1153. doi : 10.1037/amp0000427 . ISSN  1935-990X .   ^  Hamilton, Mykol C. (1991). ""Masculine Bias in the Attribution of Personhood: People = Male, Male = People"" . Psychology of Women Quarterly . 15 (3): 393–402. doi : 10.1111/j.1471-6402.1991.tb00415.x . ISSN  0361-6843 .   ^  Lichtenstein S, Fischhoff B (1977). ""Do those who know more also know more about how much they know?"". Organizational Behavior and Human Performance . 20 (2): 159–183. doi : 10.1016/0030-5073(77)90001-0 .   ^  Merkle EC (February 2009). ""The disutility of the hard-easy effect in choice confidence"". Psychonomic Bulletin & Review . 16 (1): 204–13. doi : 10.3758/PBR.16.1.204 . PMID  19145033 .   ^  Juslin P, Winman A, Olsson H (April 2000). ""Naive empiricism and dogmatism in confidence research: a critical examination of the hard-easy effect"". Psychological Review . 107 (2): 384–96. doi : 10.1037/0033-295x.107.2.384 . PMID  10789203 .   ^  Pohl RF (2004). ""Hindsight Bias"" .  In Pohl RF (ed.). Cognitive Illusions: A Handbook on Fallacies and Biases in Thinking, Judgement and Memory . Hove, UK: Psychology Press. pp. 363–378 . ISBN  978-1-84169-351-4 . OCLC  55124398 .   ^  Anderson KB, Graham LM (2007). Hostile Attribution Bias . Encyclopedia of Social Psychology . SAGE Publications, Inc. pp. 446–447. doi : 10.4135/9781412956253 . ISBN  9781412916707 .   ^  Laibson D (1997). ""Golden Eggs and Hyperbolic Discounting"". Quarterly Journal of Economics . 112 (2): 443–477. CiteSeerX  10.1.1.337.3544 . doi : 10.1162/003355397555253 .   ^  The “IKEA Effect”: When Labor Leads to Love | Harvard Business School   ^  Thompson SC (1999). ""Illusions of Control: How We Overestimate Our Personal Influence"". Current Directions in Psychological Science . 8 (6): 187–190. doi : 10.1111/1467-8721.00044 . ISSN  0963-7214 . JSTOR  20182602 . CS1 maint: ref=harv ( link )   ^  Dierkes M, Antal AB, Child J, Ikujiro Nonaka (2003). Handbook of Organizational Learning and Knowledge . Oxford University Press. p. 22. ISBN  978-0-19-829582-2 . Retrieved 9 September 2013 .   ^  Tversky A, Kahneman D (September 1974). ""Judgment under Uncertainty: Heuristics and Biases"". Science . 185 (4157): 1124–31. Bibcode : 1974Sci...185.1124T . doi : 10.1126/science.185.4157.1124 . PMID  17835457 .   ^  Fiedler K (1991). ""The tricky nature of skewed frequency tables: An information loss account of distinctiveness-based illusory correlations"". Journal of Personality and Social Psychology . 60 (1): 24–36. doi : 10.1037/0022-3514.60.1.24 .   ^ a  b  Sanna LJ, Schwarz N (July 2004). ""Integrating temporal biases: the interplay of focal thoughts and accessibility experiences"" . Psychological Science . 15 (7): 474–81. doi : 10.1111/j.0956-7976.2004.00704.x . PMID  15200632 .   ^  Baron 1994 , pp. 258–259   ^  Danziger S, Levav J, Avnaim-Pesso L (April 2011). ""Extraneous factors in judicial decisions"" . Proceedings of the National Academy of Sciences of the United States of America . 108 (17): 6889–92. Bibcode : 2011PNAS..108.6889D . doi : 10.1073/pnas.1018033108 . PMC  3084045 . PMID  21482790 .   ^  Zaman J, De Peuter S, Van Diest I, Van den Bergh O, Vlaeyen JW (November 2016). ""Interoceptive cues predicting exteroceptive events"". International Journal of Psychophysiology . 109 : 100–106. doi : 10.1016/j.ijpsycho.2016.09.003 . PMID  27616473 .   ^  Barrett LF, Simmons WK (July 2015). ""Interoceptive predictions in the brain"" . Nature Reviews. Neuroscience . 16 (7): 419–29. doi : 10.1038/nrn3950 . PMC  4731102 . PMID  26016744 .   ^  Damasio AR (October 1996). ""The somatic marker hypothesis and the possible functions of the prefrontal cortex"" . Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences . 351 (1346): 1413–20. doi : 10.1098/rstb.1996.0125 . PMID  8941953 .   ^  ( Kahneman, Knetsch & Thaler 1991 , p. 193) Daniel Kahneman, together with Amos Tversky, coined the term ""loss aversion.""   ^  Bornstein RF, Crave-Lemley C (2004). ""Mere exposure effect"" .  In Pohl RF (ed.). Cognitive Illusions: A Handbook on Fallacies and Biases in Thinking, Judgement and Memory . Hove, UK: Psychology Press. pp. 215–234 . ISBN  978-1-84169-351-4 . OCLC  55124398 .   ^  Shafir E, Diamond P, Tversky A (2000). ""Money Illusion"".  In Kahneman D, Tversky A (eds.). Choices, values, and frames . Cambridge University Press. pp. 335–355. ISBN  978-0-521-62749-8 .   ^  Haizlip J, May N, Schorling J, Williams A, Plews-Ogan M (September 2012). ""Perspective: the negativity bias, medical education, and the culture of academic medicine: why culture change is hard"". Academic Medicine . 87 (9): 1205–9. doi : 10.1097/ACM.0b013e3182628f03 . PMID  22836850 .   ^ a  b  c  Trofimova I (2014). ""Observer bias: an interaction of temperament traits with biases in the semantic perception of lexical material"" . PLOS ONE . 9 (1): e85677. Bibcode : 2014PLoSO...985677T . doi : 10.1371/journal.pone.0085677 . PMC  3903487 . PMID  24475048 .   ^ a  b  Sutherland 2007 , pp. 138–139   ^  Baron 1994 , p. 353   ^  Baron 1994 , p. 386   ^  Baron 1994 , p. 44   ^  Hardman 2009 , p. 104   ^  Adams PA, Adams JK (December 1960). ""Confidence in the recognition and reproduction of words difficult to spell"". The American Journal of Psychology . 73 (4): 544–52. doi : 10.2307/1419942 . JSTOR  1419942 . PMID  13681411 .   ^  Hoffrage U (2004). ""Overconfidence"" .  In Rüdiger Pohl (ed.). Cognitive Illusions: a handbook on fallacies and biases in thinking, judgement and memory . Psychology Press. ISBN  978-1-84169-351-4 .   ^  Sutherland 2007 , pp. 172–178   ^  Tuccio, William (2011-01-01). ""Heuristics to Improve Human Factors Performance in Aviation"". Journal of Aviation/Aerospace Education & Research . 20 (3). doi : 10.15394/jaaer.2011.1640 . ISSN  2329-258X .   ^  O'Donoghue T, Rabin M (1999). ""Doing it now or later"" . American Economic Review . 89 (1): 103–124. doi : 10.1257/aer.89.1.103 .   ^  Balas B, Momsen JL (September 2014).  Holt EA (ed.). ""Attention ""blinks"" differently for plants and animals"" . CBE Life Sciences Education . 13 (3): 437–43. doi : 10.1187/cbe.14-05-0080 . PMC  4152205 . PMID  25185227 .   ^  Hsee CK, Hastie R (January 2006). ""Decision and experience: why don't we choose what makes us happy?""  (PDF) . Trends in Cognitive Sciences . 10 (1): 31–7. CiteSeerX  10.1.1.178.7054 . doi : 10.1016/j.tics.2005.11.007 . PMID  16318925 . Archived  (PDF) from the original on 2015-04-20.   ^  Trofimova I (October 1999). ""An investigation of how people of different age, sex, and temperament estimate the world"". Psychological Reports . 85 (2): 533–52. doi : 10.2466/pr0.1999.85.2.533 . PMID  10611787 .   ^  Hardman 2009 , p. 137   ^  Fiedler, Klaus; Unkelbach, Christian (2014-10-01). ""Regressive Judgment: Implications of a Universal Property of the Empirical World"" . Current Directions in Psychological Science . 23 (5): 361–367. doi : 10.1177/0963721414546330 . ISSN  0963-7214 .   ^  Garcia SM, Song H, Tesser A (November 2010). ""Tainted recommendations: The social comparison bias"". Organizational Behavior and Human Decision Processes . 113 (2): 97–101. doi : 10.1016/j.obhdp.2010.06.002 . ISSN  0749-5978 . Lay summary – BPS Research Digest (2010-10-30).   ^  Dalton D, Ortegren M (2011). ""Gender differences in ethics research: The importance of controlling for the social desirability response bias"". Journal of Business Ethics . 103 (1): 73–93. doi : 10.1007/s10551-011-0843-8 .   ^  Kahneman, Knetsch & Thaler 1991 , p. 193   ^  Baron 1994 , p. 382   ^  Baron, J. (in preparation). Thinking and Deciding , 4th edition. New York: Cambridge University Press.   ^  Forsyth DR (2009). Group Dynamics (5th ed.). Cengage Learning. p. 317. ISBN  978-0-495-59952-4 .   ^  ""Penn Psychologists Believe 'Unit Bias' Determines The Acceptable Amount To Eat"" . ScienceDaily (November 21, 2005)   ^  Milgram S (October 1963). ""Behavioral Study of Obedience"". Journal of Abnormal Psychology . 67 (4): 371–8. doi : 10.1037/h0040525 . PMID  14049516 .   ^  Walker D, Vul E (January 2014). ""Hierarchical encoding makes individuals in a group seem more attractive"". Psychological Science . 25 (1): 230–5. doi : 10.1177/0956797613497969 . PMID  24163333 .   ^  Marks G, Miller N (1987). ""Ten years of research on the false-consensus effect: An empirical and theoretical review"". Psychological Bulletin . 102 (1): 72–90. doi : 10.1037/0033-2909.102.1.72 .   ^  ""False Uniqueness Bias (SOCIAL PSYCHOLOGY) – IResearchNet"" . 2016-01-13.   ^  Baron 1994 , p. 275   ^  Pronin E, Kruger J, Savitsky K, Ross L (October 2001). ""You don't know me, but I know you: the illusion of asymmetric insight"". Journal of Personality and Social Psychology . 81 (4): 639–56. doi : 10.1037/0022-3514.81.4.639 . PMID  11642351 .   ^  Hoorens V (1993). ""Self-enhancement and Superiority Biases in Social Comparison"". European Review of Social Psychology . 4 (1): 113–139. doi : 10.1080/14792779343000040 .   ^  Rosset, Evelyn (2008-09-01). ""It's no accident: Our bias for intentional explanations"". Cognition . 108 (3): 771–780. doi : 10.1016/j.cognition.2008.07.001 . ISSN  0010-0277 . PMID  18692779 .   ^  Plous 2006 , p. 206 harvnb error: no target: CITEREFPlous2006 ( help )   ^  Kokkoris, Michail (2020-01-16). ""The Dark Side of Self-Control"" . Harvard Business Review . Retrieved 17 January 2020 .   ^  Plous 2006 , p. 185 harvnb error: no target: CITEREFPlous2006 ( help )   ^  Forsyth DR (2009). Group Dynamics (5th ed.). Pacific Grove, CA: Brooks/Cole.   ^  Kruger J (August 1999). ""Lake Wobegon be gone! The ""below-average effect"" and the egocentric nature of comparative ability judgments"". Journal of Personality and Social Psychology . 77 (2): 221–32. doi : 10.1037/0022-3514.77.2.221 . PMID  10474208 .   ^  Attneave F (August 1953). ""Psychological probability as a function of experienced frequency"". Journal of Experimental Psychology . 46 (2): 81–6. doi : 10.1037/h0057955 . PMID  13084849 .   ^  Fischhoff B, Slovic P, Lichtenstein S (1977). ""Knowing with certainty: The appropriateness of extreme confidence"" . Journal of Experimental Psychology: Human Perception and Performance . 3 (4): 552–564. doi : 10.1037/0096-1523.3.4.552 .   ^  Cacioppo J (2002). Foundations in social neuroscience . Cambridge, Mass: MIT Press. pp. 130–132. ISBN  978-0262531955 .   ^ a  b  Schacter DL (March 1999). ""The seven sins of memory. Insights from psychology and cognitive neuroscience"" . The American Psychologist . 54 (3): 182–203. doi : 10.1037/0003-066X.54.3.182 . PMID  10199218 .   ^  Schmidt SR (July 1994). ""Effects of humor on sentence memory""  (PDF) . Journal of Experimental Psychology. Learning, Memory, and Cognition . 20 (4): 953–67. doi : 10.1037/0278-7393.20.4.953 . PMID  8064254 . Archived from the original  (PDF) on 2016-03-15 . Retrieved 2015-04-19 .   ^  Schmidt SR (2003). ""Life Is Pleasant—and Memory Helps to Keep It That Way!""  (PDF) . Review of General Psychology . 7 (2): 203–210. doi : 10.1037/1089-2680.7.2.203 .   ^  Koriat A, Goldsmith M, Pansky A (2000). ""Toward a psychology of memory accuracy"". Annual Review of Psychology . 51 (1): 481–537. doi : 10.1146/annurev.psych.51.1.481 . PMID  10751979 .   ^  Craik & Lockhart, 1972   ^  Kinnell A, Dennis S (February 2011). ""The list length effect in recognition memory: an analysis of potential confounds"". Memory & Cognition . 39 (2): 348–63. doi : 10.3758/s13421-010-0007-6 . PMID  21264573 .   ^  Wayne Weiten (2010). Psychology: Themes and Variations . Cengage Learning. p. 338. ISBN  978-0-495-60197-5 .   ^  Wayne Weiten (2007). Psychology: Themes and Variations . Cengage Learning. p. 260. ISBN  978-0-495-09303-9 .   ^  Slamecka NJ (April 1968). ""An examination of trace storage in free recall"". Journal of Experimental Psychology . 76 (4): 504–13. doi : 10.1037/h0025695 . PMID  5650563 .   ^  Shepard RN (1967). ""Recognition memory for words, sentences, and pictures"". Journal of Learning and Verbal Behavior . 6 : 156–163. doi : 10.1016/s0022-5371(67)80067-7 .   ^  McBride DM, Dosher BA (2002). ""A comparison of conscious and automatic memory processes for picture and word stimuli: a process dissociation analysis"". Consciousness and Cognition . 11 (3): 423–460. doi : 10.1016/s1053-8100(02)00007-7 . PMID  12435377 .   ^  Defetyer MA, Russo R, McPartlin PL (2009). ""The picture superiority effect in recognition memory: a developmental study using the response signal procedure"". Cognitive Development . 24 (3): 265–273. doi : 10.1016/j.cogdev.2009.05.002 .   ^  Whitehouse AJ, Maybery MT, Durkin K (2006). ""The development of the picture-superiority effect"". British Journal of Developmental Psychology . 24 (4): 767–773. doi : 10.1348/026151005X74153 .   ^  Ally BA, Gold CA, Budson AE (January 2009). ""The picture superiority effect in patients with Alzheimer's disease and mild cognitive impairment"" . Neuropsychologia . 47 (2): 595–8. doi : 10.1016/j.neuropsychologia.2008.10.010 . PMC  2763351 . PMID  18992266 .   ^  Curran T, Doyle J (May 2011). ""Picture superiority doubly dissociates the ERP correlates of recollection and familiarity"". Journal of Cognitive Neuroscience . 23 (5): 1247–62. doi : 10.1162/jocn.2010.21464 . PMID  20350169 .   ^  Martin GN, Carlson NR, Buskist W (2007). Psychology (3rd ed.). Pearson Education. pp. 309–310. ISBN  978-0-273-71086-8 .   ^  O'Brien EJ, Myers JL (1985). ""When comprehension difficulty improves memory for text"". Journal of Experimental Psychology: Learning, Memory, and Cognition . 11 (1): 12–21. doi : 10.1037/0278-7393.11.1.12 .   ^  Rubin, Wetzler & Nebes, 1986; Rubin, Rahhal & Poon, 1998   ^  Lieberman DA (8 December 2011). Human Learning and Memory . Cambridge University Press. p. 432. ISBN  978-1-139-50253-5 .   ^  Morton, Crowder & Prussin, 1971   ^  Pitt I, Edwards AD (2003). Design of Speech-Based Devices: A Practical Guide . Springer. p. 26. ISBN  978-1-85233-436-9 .   ^  Stetson C, Fiesta MP, Eagleman DM (December 2007). ""Does time really slow down during a frightening event?"" . PLOS ONE . 2 (12): e1295. Bibcode : 2007PLoSO...2.1295S . doi : 10.1371/journal.pone.0001295 . PMC  2110887 . PMID  18074019 .   ^  Goldstein EB (2010-06-21). Cognitive Psychology: Connecting Mind, Research and Everyday Experience . Cengage Learning. p. 231. ISBN  978-1-133-00912-2 .   ^  ""Not everyone is in such awe of the internet"" . Evening Standard . Evening Standard. 2011-03-23 . Retrieved 28 October 2015 .   ^  Poppenk, Walia, Joanisse, Danckert, & Köhler, 2006   ^  Von Restorff, H (1933). ""Über die Wirkung von Bereichsbildungen im Spurenfeld (The effects of field formation in the trace field) "" "". Psychological Research . 18 (1): 299–342. doi : 10.1007/bf02409636 .    References [ edit ]   Baron J (1994). Thinking and deciding (2nd ed.). Cambridge University Press. ISBN  978-0-521-43732-5 . CS1 maint: ref=harv ( link )  Baron J (2000). Thinking and deciding (3rd ed.). New York: Cambridge University Press. ISBN  978-0-521-65030-4 . CS1 maint: ref=harv ( link )  Bishop MA, Trout JD (2004). Epistemology and the Psychology of Human Judgment . New York: Oxford University Press . ISBN  978-0-19-516229-5 . CS1 maint: ref=harv ( link )  Gilovich T (1993). How We Know What Isn't So: The Fallibility of Human Reason in Everyday Life . New York: The Free Press. ISBN  978-0-02-911706-4 . CS1 maint: ref=harv ( link )  Gilovich T, Griffin D, Kahneman D (2002). Heuristics and biases: The psychology of intuitive judgment . Cambridge, UK: Cambridge University Press. ISBN  978-0-521-79679-8 . CS1 maint: ref=harv ( link )  Greenwald AG (1980). ""The Totalitarian Ego: Fabrication and Revision of Personal History""  (PDF) . American Psychologist . 35 (7): 603–618. doi : 10.1037/0003-066x.35.7.603 . ISSN  0003-066X . CS1 maint: ref=harv ( link )  Hardman D (2009). Judgment and decision making: psychological perspectives . Wiley-Blackwell. ISBN  978-1-4051-2398-3 . CS1 maint: ref=harv ( link )  Kahneman D, Slovic P, Tversky A (1982). Judgment under Uncertainty: Heuristics and Biases . Science . 185 . Cambridge, UK: Cambridge University Press. pp. 1124–31. doi : 10.1126/science.185.4157.1124 . ISBN  978-0-521-28414-1 . PMID  17835457 . CS1 maint: ref=harv ( link )  Kahneman D, Knetsch JL, Thaler RH (1991). ""Anomalies: The Endowment Effect, Loss Aversion, and Status Quo Bias""  (PDF) . The Journal of Economic Perspectives . 5 (1): 193–206. doi : 10.1257/jep.5.1.193 . Archived from the original  (PDF) on November 24, 2012. CS1 maint: ref=harv ( link )  Plous S (1993). The Psychology of Judgment and Decision Making . New York: McGraw-Hill. ISBN  978-0-07-050477-6 . CS1 maint: ref=harv ( link )  Pohl RF (2017). Cognitive illusions: Intriguing phenomena in thinking, judgment and memory (2nd ed.). London and New York: Routledge. ISBN  978-1-138-90341-8 .  Schacter DL (March 1999). ""The seven sins of memory. Insights from psychology and cognitive neuroscience""  (PDF) . The American Psychologist . 54 (3): 182–203. doi : 10.1037/0003-066X.54.3.182 . PMID  10199218 . Archived from the original  (PDF) on May 13, 2013. CS1 maint: ref=harv ( link )  Sutherland S (2007). Irrationality . Pinter & Martin. ISBN  978-1-905177-07-3 . CS1 maint: ref=harv ( link )  Tetlock PE (2005). Expert Political Judgment: how good is it? how can we know? . Princeton: Princeton University Press. ISBN  978-0-691-12302-8 . CS1 maint: ref=harv ( link )  Virine L, Trumper M (2007). Project Decisions: The Art and Science . Vienna, VA: Management Concepts. ISBN  978-1-56726-217-9 . CS1 maint: ref=harv ( link )   v t e Biases Cognitive biases  Actor–observer  Acquiescence  Ambiguity  Anchoring  Attentional  Attribution  Authority  Automation  Belief  Blind spot  Choice-supportive  Confirmation  Congruence  Cultural  Distinction  Dunning–Kruger  Egocentric  Emotional  Extrinsic incentives  Fading affect  Framing  Correspondence  Halo effect  Hindsight  Horn effect  Hostile attribution  Impact  Implicit  In-group  Mere-exposure effect  Negativity  Normalcy  Omission  Optimism  Out-group homogeneity  Outcome  Overton window  Precision  Present  Pro-innovation  Response  Restraint  Self-serving  Social comparison  Status quo  Time-saving  Trait ascription  von Restorff effect  Zero-risk  In animals  Statistical biases  Estimator  Forecast  Healthy user  Information  Psychological  Lead time  Length time  Non-response  Observer  Omitted-variable  Participation  Recall  Sampling  Selection  Self-selection  Social desirability  Spectrum  Survivorship  Systematic error  Systemic  Verification  Wet  Other biases  Academic  Funding  FUTON  Inductive  Infrastructure  Inherent  In education  Media  False balance  Vietnam War  Norway  South Asia  Sweden  United States  Arab–Israeli conflict  Ukraine  Net  Political bias  Publication  Reporting  White hat  Bias reduction  Cognitive bias mitigation  Debiasing  Heuristics in judgment and decision-making  Lists: General  ·  Memory      Retrieved from "" https://en.wikipedia.org/w/index.php?title=List_of_cognitive_biases&oldid=965490136 ""  Categories : Cognitive biases Psychology lists Behavioral finance Cognitive science lists Hidden categories: CS1 errors: missing periodical CS1 maint: ref=harv Webarchive template wayback links Harv and Sfn no-target errors Articles with short description Wikipedia articles needing clarification from November 2013         Navigation menu      Personal tools      Not logged in Talk Contributions Create account Log in        Namespaces      Article Talk        Variants              Views      Read Edit View history        More           Search                     Navigation      Main page Contents Current events Random article About Wikipedia Contact us Donate Wikipedia store       Contribute      Help Community portal Recent changes Upload file       Tools      What links here Related changes Upload file Special pages Permanent link Page information Wikidata item Cite this page       Print/export      Download as PDF Printable version       Languages      العربية Azərbaycanca Català Deutsch Español فارسی Polski Português Русский ไทย Українська 中文  Edit links        This page was last edited on 1 July 2020, at 16:47 (UTC) .  Text is available under the Creative Commons Attribution-ShareAlike License ;
additional terms may apply.  By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc. , a non-profit organization.    Privacy policy  About Wikipedia  Disclaimers  Contact Wikipedia  Developers  Statistics  Cookie statement  Mobile view          "
15,desirability of options/choice(fallacy OR bias OR heuristic) (coronavirus OR covid OR pandemic),1,https://www.outlookindia.com/website/story/opinion-normalcy-bias-dulled-our-response-towards-mitigating-covid-19-spread/351987,"Pew Research, in its 2014 Global Attitude survey, asked over 45,000 people from 44 countries to name which of the following five global threats posed the greatest danger to civilization: religious hatred, inequality, pollution, nuclear weapons, and infectious diseases. And there were only 5 countries among the 44 where people named infectious disease as the top threat. Not surprisingly, all these 5 African nations were in the throes of a raging Ebola outbreak which was wreaking havoc across their society then. In 25 countries – more than half of the countries surveyed – people considered infectious diseases to be the most innocuous and least threatening to civilization. Why?

The answer is relatively straightforward. It’s difficult for people to fathom the potential magnitude of the threat without a first-hand perspective. Local threats almost always supersede global threats, being more eminent and visible. That’s why religious and ethnic hatred is perceived as the top danger by those in the Middle East, Europeans worry more about inequality and Japanese about nuclear weapons, and so on. However, if a similar survey were conducted today, we could certainly say that people from across the globe would name infectious disease as the top threat. But why is it so despite the probability of a contagion remaining unchanged, in all likelihoods, since 2014? What could explain such a phenomenon leading to wide variations in our threat perception?

The answer lies in an often-used term by psychologists and behavioural economists i.e. hindsight bias – the tendency to exaggerate one's ability to have foreseen how something turned out, after learning an outcome. In other words, the tendency to view events as more likely to have occurred, after they occur than before. While we would like to believe that we already knew the destructive nature of the virus, its infectivity, virulence, or potential for chaos and far-reaching consequences of containment measures such as the great Indian migration, the truth is we probably didn’t till we read or witnessed it. Despite being the most advanced species to tread the planet, most of us don’t have a sound mental model in place to deal with such pandemics.

We all have inherent psychological biases and prior beliefs that make it hard to know whether we are responding appropriately to any given situation, especially in fairly unprecedented situations involving viruses that we know relatively little about. It is akin to writing on a scribbled upon slate, the scribbles being our priors and biases, with a chalk that keeps breaking, to finally arrive at a coherent picture. Add to that the self-reinforcement mechanisms, complex and opaque algorithms adopted by search engines and social media platforms, which tend to perpetuate our priors by feeding us content that we have the greatest affinity for. Placing the dots where one would rather have them. All the while compelling us to connect the dots to arrive at the big picture, which though distorted is the most acceptable to our minds.

That explains why many of us fell prey to normalcy bias when the situation started to worsen in the first quarter of 2020. We kept on expecting that things will continue to occur in the future the way they have typically occurred in the past – i.e. things will remain ‘normal’ eventually. That is why despite WHO declaring the outbreak to be a Public Health Emergency of International Concern and eventually recognizing it as a pandemic, many of us continued to underestimate the likelihood of a crisis occurring and the potential impact of the crisis. This thought process is harmless in most scenarios, as true disasters are relatively rare events. Thus, having a strong sense that things will remain more or less ‘normal’ is helpful when there are small deviations from the daily grind. In those scenarios, normalcy bias stops us from overreacting and thus making the situation worse.

Unfortunately, this pandemic is one of those ‘true disasters’ or ‘extremely low probability events’ to strike us where normalcy bias dulled our response towards mitigating and suppressing the spread. While President Trump’s initial statements of the virus disappearing, into nowhere presumably, and winning the war by Easter were reflective of normalcy bias, the evidence soon got too overwhelming to refute. Prolonged delay shown by heads of leading nations in responding adequately to the pandemic also indicated that confirmation bias was at play. Confirmation bias results in a tendency to search for and weigh the information that confirms one's preconceptions (priors) more strongly than information that challenges them. Another self-reinforcing mechanism hard wired into human psychology which needs to be acknowledged to be corrected. No wonder the final picture looked like an embellished version of the scribbles one had started with.* While, all this time, what was probably needed was wiping the slate clean.

Eventually, that did happen with an increasing number of leaders heeding the advice of the scientific community and taking bold steps to contain the virus. With significant, albeit nascent, research into the origins, genealogy, and traits of the virus and varying projections of infections and casualties, the world seems to have transitioned into a phase of information overload where the premium on sifting through heaps of (mis)information and getting the facts right is enormous. The catch is that not only does one have to get the facts right, but they also have to do it in record time, which seems antithetical to the rigorous, time consuming, and foolproof nature of science to arrive at any conclusion.

Each day throws up new often confounding correlations, which unless filtered through a scientific lens, are more likely to result in distortion of resource allocation and side effects (hydroxychloroquine?) than desirable effects. Luckily, consensus on measures to tackle the spread – if not the extent to which these measures should be enforced – offers some respite.

Here’s also a special mention for the heads of states who initiated effective measures early on; their priors were likely to be more robust due to training in a particular or associated discipline (Angela Merkel in Germany) or previous experience with an epidemic (Moon Jae-in in South Korea and Tsai Ing-wen in Taiwan) or foresightedness (Jacinda Ardern in New Zealand) or pure coincidence.

Nevertheless, world leaders face the unenviable task of chalking out a future path during these uncertain times with most of the options resembling a choice between the devil and the deep sea. It doesn’t seem that the choices will become any easier. Nor will the biases disappear anytime soon!

(Anusree Raha is an Indian Economic Service officer and TEDx speaker. Bodhisattwa Biswas is a doctor turned management professional and TEDx speaker. Views expressed are personal)","                                                July 05, 2020               Login | Register            the fully loaded magazine                        Magazine   Current Issue  Cover Story  Opinion  Interviews  India  World  Sports  Business  Entertainment  Books  Poliglot  Previous Issues  Subscription    India  Cricket  Opinion  Videos  Photos  World  Entertainment  Business  Archives  Subscription    hindi  business  money  traveller  responsible tourism  Poshan  Tandarust Punjab   others   CSR  Agriculture  Speakout  Automobiles  Saluting the bravest                    Home » Website » National » Opinion » 
	Normalcy Bias Dulled Our Response Towards Mitigating Covid-19 Spread   Opinion  Normalcy Bias Dulled Our Response Towards Mitigating Covid-19 Spread  We all have inherent psychological biases and prior beliefs that make it hard to know whether we are responding appropriately to any given situation.          Anusree Raha ,   Bodhisattwa Biswas    03 May 2020      Facebook  Twitter  Google +  Linkedin  Whatsapp       Follow Outlook India On  News           Migrant workers wait to board a train, during the ongoing COVID-19 nationwide lockdown, in Kozhikode.  PTI Photo          Anusree Raha, Bodhisattwa Biswas  May 03, 2020 00:00 IST  Normalcy Bias Dulled Our Response Towards Mitigating Covid-19 Spread            outlookindia.com   2020-05-03T12:41:09+0530      Pew Research, in its 2014 Global Attitude survey, asked over 45,000 people from 44 countries to name which of the following five global threats posed the greatest danger to civilization: religious hatred, inequality, pollution, nuclear weapons, and infectious diseases. And there were only 5 countries among the 44 where people named infectious disease as the top threat. Not surprisingly, all these 5 African nations were in the throes of a raging Ebola outbreak which was wreaking havoc across their society then. In 25 countries – more than half of the countries surveyed – people considered infectious diseases to be the most innocuous and least threatening to civilization. Why?  The answer is relatively straightforward. It’s difficult for people to fathom the potential magnitude of the threat without a first-hand perspective. Local threats almost always supersede global threats, being more eminent and visible. That’s why religious and ethnic hatred is perceived as the top danger by those in the Middle East, Europeans worry more about inequality and Japanese about nuclear weapons, and so on. However, if a similar survey were conducted today, we could certainly say that people from across the globe would name infectious disease as the top threat. But why is it so despite the probability of a contagion remaining unchanged, in all likelihoods, since 2014? What could explain such a phenomenon leading to wide variations in our threat perception?  The answer lies in an often-used term by psychologists and behavioural economists i.e. hindsight bias – the tendency to exaggerate one's ability to have foreseen how something turned out, after learning an outcome. In other words, the tendency to view events as more likely to have occurred, after they occur than before. While we would like to believe that we already knew the destructive nature of the virus, its infectivity, virulence, or potential for chaos and far-reaching consequences of containment measures such as the great Indian migration, the truth is we probably didn’t till we read or witnessed it. Despite being the most advanced species to tread the planet, most of us don’t have a sound mental model in place to deal with such pandemics.  We all have inherent psychological biases and prior beliefs that make it hard to know whether we are responding appropriately to any given situation, especially in fairly unprecedented situations involving viruses that we know relatively little about. It is akin to writing on a scribbled upon slate, the scribbles being our priors and biases, with a chalk that keeps breaking, to finally arrive at a coherent picture. Add to that the self-reinforcement mechanisms, complex and opaque algorithms adopted by search engines and social media platforms, which tend to perpetuate our priors by feeding us content that we have the greatest affinity for. Placing the dots where one would rather have them. All the while compelling us to connect the dots to arrive at the big picture, which though distorted is the most acceptable to our minds.  That explains why many of us fell prey to normalcy bias when the situation started to worsen in the first quarter of 2020. We kept on expecting that things will continue to occur in the future the way they have typically occurred in the past – i.e. things will remain ‘normal’ eventually. That is why despite WHO declaring the outbreak to be a Public Health Emergency of International Concern and eventually recognizing it as a pandemic, many of us continued to underestimate the likelihood of a crisis occurring and the potential impact of the crisis. This thought process is harmless in most scenarios, as true disasters are relatively rare events. Thus, having a strong sense that things will remain more or less ‘normal’ is helpful when there are small deviations from the daily grind. In those scenarios, normalcy bias stops us from overreacting and thus making the situation worse.  Unfortunately, this pandemic is one of those ‘true disasters’ or ‘extremely low probability events’ to strike us where normalcy bias dulled our response towards mitigating and suppressing the spread. While President Trump’s initial statements of the virus disappearing, into nowhere presumably, and winning the war by Easter were reflective of normalcy bias, the evidence soon got too overwhelming to refute. Prolonged delay shown by heads of leading nations in responding adequately to the pandemic also indicated that confirmation bias was at play. Confirmation bias results in a tendency to search for and weigh the information that confirms one's preconceptions (priors) more strongly than information that challenges them. Another self-reinforcing mechanism hard wired into human psychology which needs to be acknowledged to be corrected. No wonder the final picture looked like an embellished version of the scribbles one had started with.* While, all this time, what was probably needed was wiping the slate clean.  Eventually, that did happen with an increasing number of leaders heeding the advice of the scientific community and taking bold steps to contain the virus. With significant, albeit nascent, research into the origins, genealogy, and traits of the virus and varying projections of infections and casualties, the world seems to have transitioned into a phase of information overload where the premium on sifting through heaps of (mis)information and getting the facts right is enormous. The catch is that not only does one have to get the facts right, but they also have to do it in record time, which seems antithetical to the rigorous, time consuming, and foolproof nature of science to arrive at any conclusion.  Each day throws up new often confounding correlations, which unless filtered through a scientific lens, are more likely to result in distortion of resource allocation and side effects (hydroxychloroquine?) than desirable effects. Luckily, consensus on measures to tackle the spread – if not the extent to which these measures should be enforced – offers some respite. Here’s also a special mention for the heads of states who initiated effective measures early on; their priors were likely to be more robust due to training in a particular or associated discipline (Angela Merkel in Germany) or previous experience with an epidemic (Moon Jae-in in South Korea and Tsai Ing-wen in Taiwan) or foresightedness (Jacinda Ardern in New Zealand) or pure coincidence.  Nevertheless, world leaders face the unenviable task of chalking out a future path during these uncertain times with most of the options resembling a choice between the devil and the deep sea. It doesn’t seem that the choices will become any easier. Nor will the biases disappear anytime soon!  (Anusree Raha is an Indian Economic Service officer and TEDx speaker. Bodhisattwa Biswas is a doctor turned management professional and TEDx speaker. Views expressed are personal)      Subscribe to Outlook’s Newsletter            Next Story >>         Does The World Need To Worry If The US Quits WHO? Not Really        Facebook  Twitter  Google +  Linkedin  Whatsapp        Read More in:   Anusree Raha  Bodhisattwa Biswas  India  Novel Coronavirus Outbreak  National  Opinion             More from India          More COVID Patients In Delhi Now Getting Cured At Home, Less Need Hospitalisation: Kejriwal           Govt Proposal To Promote Migrant Housing A Welcome Step, But It Needs To Do More           Chennai Man Spends 10 Days In Corona Ward For Physically Challenged, Covid-Positive Son           Delhi Govt To Set Up 'COVID-19 War Room' To Keep Eye On City's Fight Against Virus         More From Outlook Magazine          Money Is In The Journey: Tourism Sector Is Key To Revival of Indian Economy           How A Delhi Engineer Dug Out Dara Shikoh's Grave, The Brother Aurangzeb Beheaded!           Will Congress Ploy To Play Nehru Card In Indo-China Standoff Resurrect Their Political Impact?           To Travel Or Not? Privacy, Security, Health Will Be Key As Exotic Locales Beckon         More from Website          More COVID Patients In Delhi Now Getting Cured At Home, Less Need Hospitalisation: Kejriwal           Gangster Vikas Dubey's Main Aide Arrested, Was Present When 8 UP Cops Were Killed           Has PM Modi Helped Congress Bury Ghost Of 1962 Nehruvian Blunder?           Setting Deadline For Covid-19 Vaccine A Futile Exercise: Ex-Health Secretary Sujatha Rao         More from Blog          What A Monastery Can Teach Us About Isolation During The Pandemic  Priyadarshini Sen           Govt Proposal To Promote Migrant Housing A Welcome Step, But It Needs To Do More  Gautam Bhatia           Locked In And Dreaming Of Sky  Sathya Saran                   The Latest Issue      Most Viewed   Most Commented                How Much Would You Pay For A Kidney?  Shailesh Chitnis         PM Modi Reaches Ladakh To Review Security Situation Amid Border Standoff With China  Outlook Web Bureau         Modi Not First PM To Visit Battlefront; Real Test Is Reclaiming Territory: Ex-Defence Minister A K Antony  Preetha Nair         PM Modi's Surprise Ladakh Visit Has Silenced His Critics And Sent Out Strong Message To China  Bhavna Vij-Aurora        Advertisement*      Latest   Photos   News   Blogs      Delhi To Set Up 'COVID-19 War Room' To Keep Eye On City's Fight    Hyderabad Jeweller Dies Of Covid After Throwing B'day Party With 100 Attendees    The Outlier Ep-5  NITI Aayog CEO Amitabh Kant Speaks On Economy, 'Boycott China'    Record 24,850 People Test Positive For Covid-19 In Single Day In India    Sushant Singh Rajput’s 'Dil Bechara' Co-star Pens Emotional Note    Despite Falling Interest Rates, PPF Still A Sound Investment Option    Sri Lanka Cricketer Mendis Arrested For Causing Fatal Road Accident    Clubs Bargain Salary With Women Players Like Buying Veg: Ashalata                                              More COVID Patients In Delhi Now Getting Cured At Home, Less Need Hospitalisation: Kejriwal     Gangster Vikas Dubey's Main Aide Arrested, Was Present When 8 UP Cops Were Killed     Has PM Modi Helped Congress Bury Ghost Of 1962 Nehruvian Blunder?     Setting Deadline For Covid-19 Vaccine A Futile Exercise: Ex-Health Secretary Sujatha Rao     Chennai Man Spends 10 Days In Corona Ward For Physically Challenged, Covid+ve Son           Cricket Diaries | When Gary ‘Glittering’ Gilmour Left Lillee, Thomson In His Wake     OBITUARY | Sir Everton Weekes, Butcher And A Gentleman     Guilty Conscience And Sheer Neglect Of Legacy: Congress Has Lost Narasimha Rao To BJP     Sanjay Jha Is Right About Congress But He Also Symbolises What Ails The Party     PM CARES To Ladakh Standoff: The Govt's Use Of Opaque Filters     Big Punch Of The Little Magazine        Outlook Videos               The Outlier Ep5 - Amitabh Kant, CEO, NITI Aayog, in conversation with Satish Padmanabhan                 Bollywood TALKies with Outlook Episode 9 : Anupam Kher                 Delhi Sub-Inspector Hits Elderly Woman With Car, Then Runs Her Over                 Planet Outlook Ep 4 : India's Wild Places: Then and Now           Advertisement*        Advertisement*                   Magazine  CURRENT ISSUE  COVER STORY  NATIONAL  INTERNATIONAL  BUSINESS  BOOK REVIEWS  DOCUMENTS  ESSAYS  EXTRACTS  INTERVIEWS  OPINIONS  PORTRAITS  PROFILES     Traveller  TRAVELOGUES  WEEKEND BREAKS  HOLIDAYS WITH OT  PHOTO FEATURES  HOTELS  GUIDEBOOKS     Money  Mutual Funds  Insurance  Equity  Fixed Assets  Banking  ASK     Business  THE BIG STORY  SPECIALS  PERSPECTIVE  PIXSTORY  ENTERPRISE  STRATEGY  MARKETS  C'EST LA VIE     Social Media  Facebook  Twitter  Instagram  Youtube  RSS      about us  contact us  subscribe  advertising rates  copyright & disclaimer  osm awards  compliance  newsscroll  poshan    Outlook topics A  B  C  D  E  F  G  H  I  J  K  L  M  N  O  P  Q  R  S  T  U  V  W  X  Y  Z  0  1  2  3  4  5  6  7  8  9 or just type initial letters                  "
15,desirability of options/choice(fallacy OR bias OR heuristic) (coronavirus OR covid OR pandemic),2,https://en.wikipedia.org/wiki/List_of_cognitive_biases,"Name Description

Agent detection The inclination to presume the purposeful intervention of a sentient or intelligent agent.

Ambiguity effect The tendency to avoid options for which the probability of a favorable outcome is unknown.[11]

Anchoring or focalism The tendency to rely too heavily, or ""anchor"", on one trait or piece of information when making decisions (usually the first piece of information acquired on that subject).[12][13]

Anthropocentric thinking The tendency to use human analogies as a basis for reasoning about other, less familiar, biological phenomena.[14]

Anthropomorphism or personification The tendency to characterize animals, objects, and abstract concepts as possessing human-like traits, emotions, and intentions.[15] The opposite bias, of not attributing feelings or thoughts to another person, is dehumanised perception,[16] a type of objectification.

Attentional bias The tendency of perception to be affected by recurring thoughts.[17]

Attribute substitution Occurs when a judgment has to be made (of a target attribute) that is computationally complex, and instead a more easily calculated heuristic attribute is substituted. This substitution is thought of as taking place in the automatic intuitive judgment system, rather than the more self-aware reflective system.

Automation bias The tendency to depend excessively on automated systems which can lead to erroneous automated information overriding correct decisions.[18]

Availability heuristic The tendency to overestimate the likelihood of events with greater ""availability"" in memory, which can be influenced by how recent the memories are or how unusual or emotionally charged they may be.[19]

Availability cascade A self-reinforcing process in which a collective belief gains more and more plausibility through its increasing repetition in public discourse (or ""repeat something long enough and it will become true"").[20]

Backfire effect The reaction to disconfirming evidence by strengthening one's previous beliefs.[21] cf. Continued influence effect.

Bandwagon effect The tendency to do (or believe) things because many other people do (or believe) the same. Related to groupthink and herd behavior.[22]

Base rate fallacy or Base rate neglect The tendency to ignore general information and focus on information only pertaining to the specific case, even when the general information is more important.[23]

Belief bias An effect where someone's evaluation of the logical strength of an argument is biased by the believability of the conclusion.[24]

Ben Franklin effect A person who has performed a favor for someone is more likely to do another favor for that person than they would be if they had received a favor from that person.[25]

Berkson's paradox The tendency to misinterpret statistical experiments involving conditional probabilities.[26]

Bias blind spot The tendency to see oneself as less biased than other people, or to be able to identify more cognitive biases in others than in oneself.[27]

Choice-supportive bias The tendency to remember one's choices as better than they actually were.[28]

Clustering illusion The tendency to overestimate the importance of small runs, streaks, or clusters in large samples of random data (that is, seeing phantom patterns).[13]

Compassion fade The predisposition to behave more compassionately towards a small number of identifiable victims than to a large number of anonymous ones.[29]

Confirmation bias The tendency to search for, interpret, focus on and remember information in a way that confirms one's preconceptions.[30]

Congruence bias The tendency to test hypotheses exclusively through direct testing, instead of testing possible alternative hypotheses.[13]

Conjunction fallacy The tendency to assume that specific conditions are more probable than a more general version of those same conditions. For example, subjects in one experiment perceived the probability of a woman being both a bank teller and a feminist as more likely than the probability of her being a bank teller.[31]

Continued influence effect The tendency to believe previously learned misinformation even after it has been corrected. Misinformation can still influence inferences one generates after a correction has occurred.[34] cf. Backfire effect

Contrast effect The enhancement or reduction of a certain stimulus' perception when compared with a recently observed, contrasting object.[35]

Courtesy bias The tendency to give an opinion that is more socially correct than one's true opinion, so as to avoid offending anyone.[36]

Curse of knowledge When better-informed people find it extremely difficult to think about problems from the perspective of lesser-informed people.[37]

Declinism The predisposition to view the past favorably (rosy retrospection) and future negatively.[38]

Decoy effect Preferences for either option A or B change in favor of option B when option C is presented, which is completely dominated by option B (inferior in all respects) and partially dominated by option A.[39]

Default effect When given a choice between several options, the tendency to favor the default one.[40]

Denomination effect The tendency to spend more money when it is denominated in small amounts (e.g., coins) rather than large amounts (e.g., bills).[41]

Disposition effect The tendency to sell an asset that has accumulated in value and resist selling an asset that has declined in value.

Distinction bias The tendency to view two options as more dissimilar when evaluating them simultaneously than when evaluating them separately.[42]

Dread aversion Just as losses yield double the emotional impact of gains, dread yields double the emotional impact of savouring.[43]

Dunning–Kruger effect The tendency for unskilled individuals to overestimate their own ability and the tendency for experts to underestimate their own ability.[44]

Duration neglect The neglect of the duration of an episode in determining its value.[45]

Empathy gap The tendency to underestimate the influence or strength of feelings, in either oneself or others.[46]

End-of-history illusion The age-independent belief that one will change less in the future than one has in the past.[47]

Endowment effect The tendency for people to demand much more to give up an object than they would be willing to pay to acquire it.[48]

Exaggerated expectation The tendency to expect or predict more extreme outcomes than those outcomes that actually happen.[6]

Experimenter's or expectation bias The tendency for experimenters to believe, certify, and publish data that agree with their expectations for the outcome of an experiment, and to disbelieve, discard, or downgrade the corresponding weightings for data that appear to conflict with those expectations.[49]

Focusing effect The tendency to place too much importance on one aspect of an event.[50]

Forer effect or Barnum effect The observation that individuals will give high accuracy ratings to descriptions of their personality that supposedly are tailored specifically for them, but are in fact vague and general enough to apply to a wide range of people. This effect can provide a partial explanation for the widespread acceptance of some beliefs and practices, such as astrology, fortune telling, graphology, and some types of personality tests.[51]

Form function attribution bias In human–robot interaction, the tendency of people to make systematic errors when interacting with a robot. People may base their expectations and perceptions of a robot on its appearance (form) and attribute functions which do not necessarily mirror the true functions of the robot.[52]

Framing effect Drawing different conclusions from the same information, depending on how that information is presented.

Frequency illusion or Baader–Meinhof phenomenon The frequency illusion is that once something has been noticed then every instance of that thing is noticed, leading to the belief it has a high frequency of occurrence (a form of selection bias).[53] The Baader–Meinhof phenomenon is the illusion where something that has recently come to one's attention suddenly seems to appear with improbable frequency shortly afterwards.[54] The Baader–Meinhof phenomenon is sometimes conflated with frequency illusion and the recency illusion.[55] It was named after an incidence of frequency illusion in which the Baader–Meinhof Group was mentioned.[56]

Functional fixedness Limits a person to using an object only in the way it is traditionally used.[57]

Gambler's fallacy The tendency to think that future probabilities are altered by past events, when in reality they are unchanged. The fallacy arises from an erroneous conceptualization of the law of large numbers. For example, ""I've flipped heads with this coin five times consecutively, so the chance of tails coming out on the sixth flip is much greater than heads.""[58]

Gender bias A widely held[59] set of implicit biases that discriminate against a gender (typically women[60]). For example, the assumption that women are less suited to jobs requiring high intellectual ability[61]. Or the assumption that people or animals are male in the absence of any indicators of gender.[62]

Groupthink The psychological phenomenon that occurs within a group of people in which the desire for harmony or conformity in the group results in an irrational or dysfunctional decision-making outcome. Group members try to minimize conflict and reach a consensus decision without critical evaluation of alternative viewpoints by actively suppressing dissenting viewpoints, and by isolating themselves from outside influences.

Hard–easy effect The tendency to overestimate one's ability to accomplish hard tasks, and underestimate one's ability to accomplish easy tasks[6][63][64][65]

Hindsight bias Sometimes called the ""I-knew-it-all-along"" effect, the tendency to see past events as being predictable[66] at the time those events happened.

Hostile attribution bias The ""hostile attribution bias"" is the tendency to interpret others' behaviors as having hostile intent, even when the behavior is ambiguous or benign.[67]

Hot-hand fallacy The ""hot-hand fallacy"" (also known as the ""hot hand phenomenon"" or ""hot hand"") is the belief that a person who has experienced success with a random event has a greater chance of further success in additional attempts.

Hyperbolic discounting Discounting is the tendency for people to have a stronger preference for more immediate payoffs relative to later payoffs. Hyperbolic discounting leads to choices that are inconsistent over time – people make choices today that their future selves would prefer not to have made, despite using the same reasoning.[68] Also known as current moment bias, present-bias, and related to Dynamic inconsistency. A good example of this: a study showed that when making food choices for the coming week, 74% of participants chose fruit, whereas when the food choice was for the current day, 70% chose chocolate.

IKEA effect The tendency for people to place a disproportionately high value on objects that they partially assembled themselves, such as furniture from IKEA, regardless of the quality of the end product.[69]

Illicit transference Occurs when a term in the distributive (referring to every member of a class) and collective (referring to the class itself as a whole) sense are treated as equivalent. The two variants of this fallacy are the fallacy of composition and the fallacy of division.

Illusion of control The tendency to overestimate one's degree of influence over other external events.[70]

Illusion of validity Believing that one's judgments are accurate, especially when available information is consistent or inter-correlated.[71]

Illusory correlation Inaccurately perceiving a relationship between two unrelated events.[72][73]

Illusory truth effect A tendency to believe that a statement is true if it is easier to process, or if it has been stated multiple times, regardless of its actual veracity. These are specific cases of truthiness.

Impact bias The tendency to overestimate the length or the intensity of the impact of future feeling states.[74]

Implicit association The speed with which people can match words depends on how closely they are associated.

Information bias The tendency to seek information even when it cannot affect action.[75]

Insensitivity to sample size The tendency to under-expect variation in small samples.

Interoceptive bias The tendency for sensory input about the body itself to affect one's judgement about external, unrelated circumstances. (As for example, in parole judges who are more lenient when fed and rested.) [76][77][78][79]

Irrational escalation or Escalation of commitment The phenomenon where people justify increased investment in a decision, based on the cumulative prior investment, despite new evidence suggesting that the decision was probably wrong. Also known as the sunk cost fallacy.

Law of the instrument An over-reliance on a familiar tool or methods, ignoring or under-valuing alternative approaches. ""If all you have is a hammer, everything looks like a nail.""

Less-is-better effect The tendency to prefer a smaller set to a larger set judged separately, but not jointly.

Look-elsewhere effect An apparently statistically significant observation may have actually arisen by chance because of the size of the parameter space to be searched.

Loss aversion The perceived disutility of giving up an object is greater than the utility associated with acquiring it.[80] (see also Sunk cost effects and endowment effect).

Mere exposure effect The tendency to express undue liking for things merely because of familiarity with them.[81]

Money illusion The tendency to concentrate on the nominal value (face value) of money rather than its value in terms of purchasing power.[82]

Moral credential effect Occurs when someone who does something good gives themselves permission to be less good in the future.

Neglect of probability The tendency to completely disregard probability when making a decision under uncertainty.[86]

Normalcy bias The refusal to plan for, or react to, a disaster which has never happened before.

Not invented here Aversion to contact with or use of products, research, standards, or knowledge developed outside a group. Related to IKEA effect.

Observer-expectancy effect When a researcher expects a given result and therefore unconsciously manipulates an experiment or misinterprets data in order to find it (see also subject-expectancy effect).

Omission bias The tendency to judge harmful actions (commissions) as worse, or less moral, than equally harmful inactions (omissions).[87]

Ostrich effect Ignoring an obvious (negative) situation.

Outcome bias The tendency to judge a decision by its eventual outcome instead of based on the quality of the decision at the time it was made.

Overconfidence effect Excessive confidence in one's own answers to questions. For example, for certain types of questions, answers that people rate as ""99% certain"" turn out to be wrong 40% of the time.[6][90][91][92]

Pareidolia A vague and random stimulus (often an image or sound) is perceived as significant, e.g., seeing images of animals or faces in clouds, the man in the moon, and hearing non-existent hidden messages on records played in reverse.

Pygmalion effect The phenomenon whereby others' expectations of a target person affect the target person's performance.

Pessimism bias The tendency for some people, especially those suffering from depression, to overestimate the likelihood of negative things happening to them.

Plan continuation bias Failure to recognize that the original plan of action is no longer appropriate for a changing situation or for a situation that is different than anticipated.[93]

Planning fallacy The tendency to underestimate task-completion times.[74]

Present bias The tendency of people to give stronger weight to payoffs that are closer to the present time when considering trade-offs between two future moments.[94]

Plant blindness The tendency to ignore plants in their environment and a failure to recognize and appreciate the utility of plants to life on earth.[95]

Pro-innovation bias The tendency to have an excessive optimism towards an invention or innovation's usefulness throughout society, while often failing to identify its limitations and weaknesses.

Projection bias The tendency to overestimate how much our future selves share one's current preferences, thoughts and values, thus leading to sub-optimal choices.[96][97][84]

Pseudocertainty effect The tendency to make risk-averse choices if the expected outcome is positive, but make risk-seeking choices to avoid negative outcomes.[98]

Reactance The urge to do the opposite of what someone wants you to do out of a need to resist a perceived attempt to constrain your freedom of choice (see also Reverse psychology).

Reactive devaluation Devaluing proposals only because they purportedly originated with an adversary.

Recency illusion The illusion that a phenomenon one has noticed only recently is itself recent. Often used to refer to linguistic phenomena; the illusion that a word or language usage that one has noticed only recently is an innovation when it is in fact long-established (see also frequency illusion).

Systematic Bias Judgement that arises when targets of differentiating judgement become subject to effects of regression that are not equivalent. [99]

Restraint bias The tendency to overestimate one's ability to show restraint in the face of temptation.

Rhyme as reason effect Rhyming statements are perceived as more truthful. A famous example being used in the O.J Simpson trial with the defense's use of the phrase ""If the gloves don't fit, then you must acquit.""

Risk compensation / Peltzman effect The tendency to take greater risks when perceived safety increases.

Salience bias The tendency to focus on items that are more prominent or emotionally striking and ignore those that are unremarkable, even though this difference is often irrelevant by objective standards.

Selection bias The tendency to notice something more when something causes us to be more aware of it, such as when we buy a car, we tend to notice similar cars more often than we did before. They are not suddenly more common – we just are noticing them more. Also called the Observational Selection Bias.

Selective perception The tendency for expectations to affect perception.

Semmelweis reflex The tendency to reject new evidence that contradicts a paradigm.[33]

Sexual overperception bias / Sexual underperception bias The tendency to over-/underestimate sexual interest of another person in oneself.

Social comparison bias The tendency, when making decisions, to favour potential candidates who don't compete with one's own particular strengths.[100]

Social desirability bias The tendency to over-report socially desirable characteristics or behaviours in oneself and under-report socially undesirable characteristics or behaviours.[101] See also: § Courtesy bias.

Stereotyping Expecting a member of a group to have certain characteristics without having actual information about that individual.

Subadditivity effect The tendency to judge probability of the whole to be less than the probabilities of the parts.[104]

Subjective validation Perception that something is true if a subject's belief demands it to be true. Also assigns perceived connections between coincidences.

Surrogation Losing sight of the strategic construct that a measure is intended to represent, and subsequently acting as though the measure is the construct of interest.

Survivorship bias Concentrating on the people or things that ""survived"" some process and inadvertently overlooking those that didn't because of their lack of visibility.

Time-saving bias Underestimations of the time that could be saved (or lost) when increasing (or decreasing) from a relatively low speed and overestimations of the time that could be saved (or lost) when increasing (or decreasing) from a relatively high speed.

Third-person effect A hypothesized tendency to believe that mass communicated media messages have a greater effect on others than on themselves. As of 2020, the third-person effect has yet to be reliably demonstrated in a scientific context.

Parkinson's law of triviality The tendency to give disproportionate weight to trivial issues. Also known as bikeshedding, this bias explains why an organization may avoid specialized or complex subjects, such as the design of a nuclear reactor, and instead focus on something easy to grasp or rewarding to the average participant, such as the design of an adjacent bike shed.[105]

Unit bias The standard suggested amount of consumption (e.g., food serving size) is perceived to be appropriate, and a person would consume it all even if it is too much for this particular person.[106]

Weber–Fechner law Difficulty in comparing small differences in large quantities.

Well travelled road effect Underestimation of the duration taken to traverse oft-traveled routes and overestimation of the duration taken to traverse less familiar routes.

Women are wonderful effect A tendency to associate more positive attributes with women than with men.

Zero-risk bias Preference for reducing a small risk to zero over a greater reduction in a larger risk.","          List of cognitive biases   From Wikipedia, the free encyclopedia    Jump to navigation  Jump to search  Systematic patterns of deviation from norm or rationality in judgment   The loss aversion cognitive bias has been shown in monkeys  Cognitive biases are systematic patterns of deviation from norm and\or rationality in judgment. They are often studied in psychology and behavioral economics . [1]  Although the reality of most of these biases is confirmed by reproducible research, [2] [3] there are often controversies about how to classify these biases or how to explain them. [4]  Gerd Gigerenzer has criticized the framing of cognitive biases as errors in judgment, and favors interpreting them as arising from rational deviations from logical thought. [5]  Explanations include information-processing rules (i.e., mental shortcuts), called heuristics , that the brain uses to produce decisions or judgments. Biases have a variety of forms and appear as cognitive (""cold"") bias, such as mental noise, [6] or motivational (""hot"") bias, such as when beliefs are distorted by wishful thinking . Both effects can be present at the same time. [7] [8]  There are also controversies over some of these biases as to whether they count as useless or irrational , or whether they result in useful attitudes or behavior. For example, when getting to know others, people tend to ask leading questions which seem biased towards confirming their assumptions about the person. However, this kind of confirmation bias has also been argued to be an example of social skill ; a way to establish a connection with the other person. [9]  Although this research overwhelmingly involves human subjects, some findings that demonstrate bias have been found in non-human animals as well. For example, loss aversion has been shown in monkeys and hyperbolic discounting has been observed in rats, pigeons, and monkeys. [10]   Contents   1  Decision-making, belief, and behavioral biases  2  Social biases  3  Memory errors and biases  4  See also  5  Footnotes  6  References    Decision-making, belief, and behavioral biases [ edit ]  Many of these biases affect belief formation, business and economic decisions, and human behavior in general.    Name  Description   Agent detection   The inclination to presume the purposeful intervention of a sentient or intelligent agent .   Ambiguity effect   The tendency to avoid options for which the probability of a favorable outcome is unknown. [11]    Anchoring or focalism  The tendency to rely too heavily, or ""anchor"", on one trait or piece of information when making decisions (usually the first piece of information acquired on that subject). [12] [13]    Anthropocentric thinking   The tendency to use human analogies as a basis for reasoning about other, less familiar, biological phenomena. [14]    Anthropomorphism or personification  The tendency to characterize animals, objects, and abstract concepts as possessing human-like traits, emotions, and intentions. [15] The opposite bias, of not attributing feelings or thoughts to another person, is dehumanised perception , [16] a type of objectification .   Attentional bias   The tendency of perception to be affected by recurring thoughts. [17]    Attribute substitution   Occurs when a judgment has to be made (of a target attribute) that is computationally complex, and instead a more easily calculated heuristic attribute is substituted. This substitution is thought of as taking place in the automatic intuitive judgment system, rather than the more self-aware reflective system.   Automation bias   The tendency to depend excessively on automated systems which can lead to erroneous automated information overriding correct decisions. [18]    Availability heuristic   The tendency to overestimate the likelihood of events with greater ""availability"" in memory, which can be influenced by how recent the memories are or how unusual or emotionally charged they may be. [19]    Availability cascade   A self-reinforcing process in which a collective belief gains more and more plausibility through its increasing repetition in public discourse (or ""repeat something long enough and it will become true""). [20]    Backfire effect   The reaction to disconfirming evidence by strengthening one's previous beliefs. [21] cf. Continued influence effect .   Bandwagon effect   The tendency to do (or believe) things because many other people do (or believe) the same. Related to groupthink and herd behavior . [22]    Base rate fallacy or Base rate neglect  The tendency to ignore general information and focus on information only pertaining to the specific case, even when the general information is more important. [23]    Belief bias   An effect where someone's evaluation of the logical strength of an argument is biased by the believability of the conclusion. [24]    Ben Franklin effect   A person who has performed a favor for someone is more likely to do another favor for that person than they would be if they had received a favor from that person. [25]    Berkson's paradox   The tendency to misinterpret statistical experiments involving conditional probabilities. [26]    Bias blind spot   The tendency to see oneself as less biased than other people, or to be able to identify more cognitive biases in others than in oneself. [27]    Choice-supportive bias   The tendency to remember one's choices as better than they actually were. [28]    Clustering illusion   The tendency to overestimate the importance of small runs, streaks, or clusters in large samples of random data (that is, seeing phantom patterns). [13]    Compassion fade   The predisposition to behave more compassionately towards a small number of identifiable victims than to a large number of anonymous ones. [29]    Confirmation bias   The tendency to search for, interpret, focus on and remember information in a way that confirms one's preconceptions. [30]    Congruence bias   The tendency to test hypotheses exclusively through direct testing, instead of testing possible alternative hypotheses. [13]    Conjunction fallacy   The tendency to assume that specific conditions are more probable than a more general version of those same conditions. For example, subjects in one experiment perceived the probability of a woman being both a bank teller and a feminist as more likely than the probability of her being a bank teller. [31]    Conservatism (belief revision)   The tendency to revise one's belief insufficiently when presented with new evidence. [6] [32] [33]    Continued influence effect   The tendency to believe previously learned misinformation even after it has been corrected. Misinformation can still influence inferences one generates after a correction has occurred. [34] cf. Backfire effect    Contrast effect   The enhancement or reduction of a certain stimulus' perception when compared with a recently observed, contrasting object. [35]    Courtesy bias  The tendency to give an opinion that is more socially correct than one's true opinion, so as to avoid offending anyone. [36]    Curse of knowledge   When better-informed people find it extremely difficult to think about problems from the perspective of lesser-informed people. [37]    Declinism   The predisposition to view the past favorably ( rosy retrospection ) and future negatively. [38]    Decoy effect   Preferences for either option A or B change in favor of option B when option C is presented, which is completely dominated by option B (inferior in all respects) and partially dominated by option A. [39]    Default effect   When given a choice between several options, the tendency to favor the default one. [40]    Denomination effect   The tendency to spend more money when it is denominated in small amounts (e.g., coins) rather than large amounts (e.g., bills). [41]    Disposition effect   The tendency to sell an asset that has accumulated in value and resist selling an asset that has declined in value.   Distinction bias   The tendency to view two options as more dissimilar when evaluating them simultaneously than when evaluating them separately. [42]    Dread aversion  Just as losses yield double the emotional impact of gains, dread yields double the emotional  impact of savouring. [43]    Dunning–Kruger effect   The tendency for unskilled individuals to overestimate their own ability and the tendency for experts to underestimate their own ability. [44]    Duration neglect   The neglect of the duration of an episode in determining its value. [45]    Empathy gap   The tendency to underestimate the influence or strength of feelings, in either oneself or others. [46]    End-of-history illusion   The age-independent belief that one will change less in the future than one has in the past. [47]    Endowment effect   The tendency for people to demand much more to give up an object than they would be willing to pay to acquire it. [48]    Exaggerated expectation   The tendency to expect or predict more extreme outcomes than those outcomes that actually happen. [6]    Experimenter's or expectation bias   The tendency for experimenters to believe, certify, and publish data that agree with their expectations for the outcome of an experiment, and to disbelieve, discard, or downgrade the corresponding weightings for data that appear to conflict with those expectations. [49]    Focusing effect   The tendency to place too much importance on one aspect of an event. [50]    Forer effect or Barnum effect   The observation that individuals will give high accuracy ratings to descriptions of their personality that supposedly are tailored specifically for them, but are in fact vague and general enough to apply to a wide range of people. This effect can provide a partial explanation for the widespread acceptance of some beliefs and practices, such as astrology, fortune telling, graphology, and some types of personality tests. [51]    Form function attribution bias  In human–robot interaction , the tendency of people to make systematic errors when interacting with a robot. People may base their expectations and perceptions of a robot on its appearance (form) and attribute functions which do not necessarily mirror the true functions of the robot. [52]    Framing effect   Drawing different conclusions from the same information, depending on how that information is presented.   Frequency illusion or Baader–Meinhof phenomenon  The frequency illusion is that once something has been noticed then every instance of that thing is noticed, leading to the belief it has a high frequency of occurrence (a form of selection bias ). [53] The Baader–Meinhof phenomenon is the illusion where something that has recently come to one's attention suddenly seems to appear with improbable frequency shortly afterwards. [54] The Baader–Meinhof phenomenon is sometimes conflated with frequency illusion and the recency illusion . [55] It was named after an incidence of frequency illusion in which the Baader–Meinhof Group was mentioned. [56]    Functional fixedness   Limits a person to using an object only in the way it is traditionally used. [57]    Gambler's fallacy   The tendency to think that future probabilities are altered by past events, when in reality they are unchanged. The fallacy arises from an erroneous conceptualization of the law of large numbers . For example, ""I've flipped heads with this coin five times consecutively, so the chance of tails coming out on the sixth flip is much greater than heads."" [58]    Gender bias   A widely held [59] set of implicit biases that discriminate against a gender (typically women [60] ). For example, the assumption that women are less suited to jobs requiring high intellectual ability [61] . Or the assumption that people or animals are male in the absence of any indicators of gender. [62]    Groupthink   The psychological phenomenon that occurs within a group of people in which the desire for harmony or conformity in the group results in an irrational or dysfunctional decision-making outcome. Group members try to minimize conflict and reach a consensus decision without critical evaluation of alternative viewpoints by actively suppressing dissenting viewpoints, and by isolating themselves from outside influences.   Hard–easy effect   The tendency to overestimate one's ability to accomplish hard tasks, and underestimate one's ability to accomplish easy tasks [6] [63] [64] [65]    Hindsight bias   Sometimes called the ""I-knew-it-all-along"" effect, the tendency to see past events as being predictable [66] at the time those events happened.   Hostile attribution bias   The ""hostile attribution bias"" is the tendency to interpret others' behaviors as having hostile intent, even when the behavior is ambiguous or benign. [67]    Hot-hand fallacy   The ""hot-hand fallacy"" (also known as the ""hot hand phenomenon"" or ""hot hand"") is the belief that a person who has experienced success with a random event has a greater chance of further success in additional attempts.   Hyperbolic discounting   Discounting is the tendency for people to have a stronger preference for more immediate payoffs relative to later payoffs. Hyperbolic discounting leads to choices that are inconsistent over time – people make choices today that their future selves would prefer not to have made, despite using the same reasoning. [68] Also known as current moment bias, present-bias, and related to Dynamic inconsistency . A good example of this: a study showed that when making food choices for the coming week, 74% of participants chose fruit, whereas when the food choice was for the current day, 70% chose chocolate.   IKEA effect   The tendency for people to place a disproportionately high value on objects that they partially assembled themselves, such as furniture from IKEA , regardless of the quality of the end product. [69]    Illicit transference   Occurs when a term in the distributive (referring to every member of a class) and collective (referring to the class itself as a whole) sense are treated as equivalent. The two variants of this fallacy are the fallacy of composition and the fallacy of division .   Illusion of control   The tendency to overestimate one's degree of influence over other external events. [70]    Illusion of validity   Believing that one's judgments are accurate, especially when available information is consistent or inter-correlated. [71]    Illusory correlation   Inaccurately perceiving a relationship between two unrelated events. [72] [73]    Illusory truth effect   A tendency to believe that a statement is true if it is easier to process , or if it has been stated multiple times , regardless of its actual veracity. These are specific cases of truthiness .   Impact bias   The tendency to overestimate the length or the intensity of the impact of future feeling states. [74]    Implicit association   The speed with which people can match words depends on how closely they are associated.   Information bias   The tendency to seek information even when it cannot affect action. [75]    Insensitivity to sample size   The tendency to under-expect variation in small samples.   Interoceptive bias  The tendency for sensory input about the body itself to affect one's judgement about external, unrelated circumstances. (As for example, in parole judges who are more lenient when fed and rested.) [76] [77] [78] [79]    Irrational escalation or Escalation of commitment   The phenomenon where people justify increased investment in a decision, based on the cumulative prior investment, despite new evidence suggesting that the decision was probably wrong. Also known as the sunk cost fallacy.   Law of the instrument   An over-reliance on a familiar tool or methods, ignoring or under-valuing alternative approaches. ""If all you have is a hammer, everything looks like a nail.""   Less-is-better effect   The tendency to prefer a smaller set to a larger set judged separately, but not jointly.   Look-elsewhere effect   An apparently statistically significant observation may have actually arisen by chance because of the size of the parameter space to be searched.   Loss aversion   The perceived disutility of giving up an object is greater than the utility associated with acquiring it. [80] (see also Sunk cost effects and endowment effect).   Mere exposure effect   The tendency to express undue liking for things merely because of familiarity with them. [81]    Money illusion   The tendency to concentrate on the nominal value (face value) of money rather than its value in terms of purchasing power. [82]    Moral credential effect   Occurs when someone who does something good gives themselves permission to be less good in the future.   Negativity bias or Negativity effect  Psychological phenomenon by which humans have a greater recall of unpleasant memories compared with positive memories. [83] [84] (see also actor-observer bias, group attribution error , positivity effect, and negativity effect ). [85]    Neglect of probability   The tendency to completely disregard probability when making a decision under uncertainty. [86]    Normalcy bias   The refusal to plan for, or react to, a disaster which has never happened before.   Not invented here   Aversion to contact with or use of products, research, standards, or knowledge developed outside a group. Related to IKEA effect .   Observer-expectancy effect   When a researcher expects a given result and therefore unconsciously manipulates an experiment or misinterprets data in order to find it (see also subject-expectancy effect ).   Omission bias   The tendency to judge harmful actions (commissions) as worse, or less moral, than equally harmful inactions (omissions). [87]    Optimism bias   The tendency to be over-optimistic, underestimating greatly the probability of undesirable outcomes and overestimating favorable and pleasing outcomes (see also wishful thinking , valence effect , positive outcome bias ). [88] [89]    Ostrich effect   Ignoring an obvious (negative) situation.   Outcome bias   The tendency to judge a decision by its eventual outcome instead of based on the quality of the decision at the time it was made.   Overconfidence effect   Excessive confidence in one's own answers to questions. For example, for certain types of questions, answers that people rate as ""99% certain"" turn out to be wrong 40% of the time. [6] [90] [91] [92]    Pareidolia   A vague and random stimulus (often an image or sound) is perceived as significant, e.g., seeing images of animals or faces in clouds, the man in the moon , and hearing non-existent hidden messages on records played in reverse .   Pygmalion effect   The phenomenon whereby others' expectations of a target person affect the target person's performance.   Pessimism bias   The tendency for some people, especially those suffering from depression , to overestimate the likelihood of negative things happening to them.   Plan continuation bias   Failure to recognize that the original plan of action is no longer appropriate for a changing situation or for a situation that is different than anticipated. [93]    Planning fallacy   The tendency to underestimate task-completion times. [74]    Present bias   The tendency of people to give stronger weight to payoffs that are closer to the present time when considering trade-offs between two future moments. [94]    Plant blindness   The tendency to ignore plants in their environment and a failure to recognize and appreciate the utility of plants to life on earth. [95]    Pro-innovation bias   The tendency to have an excessive optimism towards an invention or innovation's usefulness throughout society, while often failing to identify its limitations and weaknesses.   Projection bias   The tendency to overestimate how much our future selves share one's current preferences, thoughts and values, thus leading to sub-optimal choices. [96] [97] [84]    Pseudocertainty effect   The tendency to make risk-averse choices if the expected outcome is positive, but make risk-seeking choices to avoid negative outcomes. [98]    Reactance   The urge to do the opposite of what someone wants you to do out of a need to resist a perceived attempt to constrain your freedom of choice (see also Reverse psychology ).   Reactive devaluation   Devaluing proposals only because they purportedly originated with an adversary.   Recency illusion   The illusion that a phenomenon one has noticed only recently is itself recent. Often used to refer to linguistic phenomena; the illusion that a word or language usage that one has noticed only recently is an innovation when it is in fact long-established (see also frequency illusion ).   Systematic Bias  Judgement that arises when targets of differentiating judgement become subject to effects of regression that are not equivalent. [99]    Restraint bias   The tendency to overestimate one's ability to show restraint in the face of temptation.   Rhyme as reason effect   Rhyming statements are perceived as more truthful. A famous example being used in the O.J Simpson trial with the defense's use of the phrase ""If the gloves don't fit, then you must acquit.""   Risk compensation / Peltzman effect   The tendency to take greater risks when perceived safety increases.   Salience bias   The tendency to focus on items that are more prominent or emotionally striking and ignore those that are unremarkable, even though this difference is often irrelevant by objective standards.   Selection bias   The tendency to notice something more when something causes us to be more aware of it, such as when we buy a car, we tend to notice similar cars more often than we did before. They are not suddenly more common – we just are noticing them more. Also called the Observational Selection Bias.   Selective perception   The tendency for expectations to affect perception.   Semmelweis reflex   The tendency to reject new evidence that contradicts a paradigm. [33]    Sexual overperception bias / Sexual underperception bias   The tendency to over-/underestimate sexual interest of another person in oneself.   Social comparison bias   The tendency, when making decisions, to favour potential candidates who don't compete with one's own particular strengths. [100]    Social desirability bias   The tendency to over-report socially desirable characteristics or behaviours in oneself and under-report socially undesirable characteristics or behaviours. [101] See also: § Courtesy bias .   Status quo bias   The tendency to like things to stay relatively the same (see also loss aversion , endowment effect , and system justification ). [102] [103]    Stereotyping   Expecting a member of a group to have certain characteristics without having actual information about that individual.   Subadditivity effect   The tendency to judge probability of the whole to be less than the probabilities of the parts. [104]    Subjective validation   Perception that something is true if a subject's belief demands it to be true. Also assigns perceived connections between coincidences.   Surrogation   Losing sight of the strategic construct that a measure is intended to represent, and subsequently acting as though the measure is the construct of interest.   Survivorship bias   Concentrating on the people or things that ""survived"" some process and inadvertently overlooking those that didn't because of their lack of visibility.   Time-saving bias   Underestimations of the time that could be saved (or lost) when increasing (or decreasing) from a relatively low speed and overestimations of the time that could be saved (or lost) when increasing (or decreasing) from a relatively high speed.   Third-person effect   A hypothesized tendency to believe that mass communicated media messages have a greater effect on others than on themselves. As of 2020, the third-person effect has yet to be reliably demonstrated in a scientific context.   Parkinson's law of triviality   The tendency to give disproportionate weight to trivial issues. Also known as bikeshedding, this bias explains why an organization may avoid specialized or complex subjects, such as the design of a nuclear reactor, and instead focus on something easy to grasp or rewarding to the average participant, such as the design of an adjacent bike shed. [105]    Unit bias   The standard suggested amount of consumption (e.g., food serving size) is perceived to be appropriate, and a person would consume it all even if it is too much for this particular person. [106]    Weber–Fechner law   Difficulty in comparing small differences in large quantities.   Well travelled road effect   Underestimation of the duration taken to traverse oft-traveled routes and overestimation of the duration taken to traverse less familiar routes.   Women are wonderful effect   A tendency to associate more positive attributes with women than with men.   Zero-risk bias   Preference for reducing a small risk to zero over a greater reduction in a larger risk.   Zero-sum bias   A bias whereby a situation is incorrectly perceived to be like a zero-sum game (i.e., one person gains at the expense of another).  Social biases [ edit ]  Most of these biases are labeled as attributional biases .    Name  Description   Actor-observer bias   The tendency for explanations of other individuals' behaviors to overemphasize the influence of their personality and underemphasize the influence of their situation (see also Fundamental attribution error ), and for explanations of one's own behaviors to do the opposite (that is, to overemphasize the influence of our situation and underemphasize the influence of our own personality).   Authority bias   The tendency to attribute greater accuracy to the opinion of an authority figure (unrelated to its content) and be more influenced by that opinion. [107]    Cheerleader effect   The tendency for people to appear more attractive in a group than in isolation. [108]    Defensive attribution hypothesis   Attributing more blame to a harm-doer as the outcome becomes more severe or as personal or situational similarity to the victim increases.   Egocentric bias   Occurs when people claim more responsibility for themselves for the results of a joint action than an outside observer would credit them with.   Extrinsic incentives bias   An exception to the fundamental attribution error , when people view others as having (situational) extrinsic motivations and (dispositional) intrinsic motivations for oneself   False consensus effect   The tendency for people to overestimate the degree to which others agree with them. [109]    False uniqueness bias   The tendency of people to see their projects and themselves as more singular than they actually are. [110]    Fundamental attribution error   The tendency for people to over-emphasize personality-based explanations for behaviors observed in others while under-emphasizing the role and power of situational influences on the same behavior [84] (see also actor-observer bias, group attribution error , positivity effect, and negativity effect ). [85]    Group attribution error   The biased belief that the characteristics of an individual group member are reflective of the group as a whole or the tendency to assume that group decision outcomes reflect the preferences of group members, even when information is available that clearly suggests otherwise.   Halo effect   The tendency for a person's positive or negative traits to ""spill over"" from one personality area to another in others' perceptions of them (see also physical attractiveness stereotype ). [111]    Illusion of asymmetric insight   People perceive their knowledge of their peers to surpass their peers' knowledge of them. [112]    Illusion of external agency   When people view self-generated preferences as instead being caused by insightful, effective and benevolent agents.   Illusion of transparency   People overestimate others' ability to know themselves, and they also overestimate their ability to know others.   Illusory superiority   Overestimating one's desirable qualities, and underestimating undesirable qualities, relative to other people. (Also known as ""Lake Wobegon effect"", ""better-than-average effect"", or ""superiority bias"".) [113]    Ingroup bias   The tendency for people to give preferential treatment to others they perceive to be members of their own groups.   Intentionality bias  Tendency to judge human action to intentional rather than accidental. [114]    Just-world hypothesis   The tendency for people to want to believe that the world is fundamentally just, causing them to rationalize an otherwise inexplicable injustice as deserved by the victim(s).   Moral luck   The tendency for people to ascribe greater or lesser moral standing based on the outcome of an event.   Naïve cynicism   Expecting more egocentric bias in others than in oneself.   Naïve realism   The belief that we see reality as it really is – objectively and without bias; that the facts are plain for all to see; that rational people will agree with us; and that those who don't are either uninformed, lazy, irrational, or biased.   Outgroup homogeneity bias   Individuals see members of their own group as being relatively more varied than members of other groups. [115]    Puritanical bias   Refers to the tendency to attribute cause of an undesirable outcome or wrongdoing by an individual to a moral deficiency or lack of self control rather than taking into account the impact of broader societal determinants . [116]    Self-serving bias   The tendency to claim more responsibility for successes than failures. It may also manifest itself as a tendency for people to evaluate ambiguous information in a way beneficial to their interests (see also group-serving bias ). [117]    Shared information bias   Known as the tendency for group members to spend more time and energy discussing information that all members are already familiar with (i.e., shared information), and less time and energy discussing information that only some members are aware of (i.e., unshared information). [118]    System justification   The tendency to defend and bolster the status quo. Existing social, economic, and political arrangements tend to be preferred, and alternatives disparaged, sometimes even at the expense of individual and collective self-interest. (See also status quo bias.)   Trait ascription bias   The tendency for people to view themselves as relatively variable in terms of personality, behavior, and mood while viewing others as much more predictable.   Ultimate attribution error   Similar to the fundamental attribution error, in this error a person is likely to make an internal attribution to an entire group instead of the individuals within the group.   Worse-than-average effect   A tendency to believe ourselves to be worse than others at tasks which are difficult. [119]   Memory errors and biases [ edit ]  Main article: List of memory biases  In psychology  and  cognitive science , a memory bias is a cognitive bias that either enhances or impairs the recall of a memory (either the chances that the memory will be recalled at all, or the amount of time it takes for it to be recalled, or both), or that alters the content of a reported memory. There are many types of memory bias, including:    Name  Description   Bizarreness effect   Bizarre material is better remembered than common material.   Conservatism or Regressive bias  Tendency to remember high values and high likelihoods/probabilities/frequencies as lower than they actually were and low ones as higher than they actually were. Based on the evidence, memories are not extreme enough. [120] [121]    Consistency bias   Incorrectly remembering one's past attitudes and behaviour as resembling present attitudes and behaviour. [122]    Context effect   That cognition and memory are dependent on context, such that out-of-context memories are more difficult to retrieve than in-context memories (e.g., recall time and accuracy for a work-related memory will be lower at home, and vice versa).   Cross-race effect   The tendency for people of one race to have difficulty identifying members of a race other than their own.   Cryptomnesia   A form of misattribution where a memory is mistaken for imagination, because there is no subjective experience of it being a memory. [123]    Egocentric bias   Recalling the past in a self-serving manner, e.g., remembering one's exam grades as being better than they were, or remembering a caught fish as bigger than it really was.   Fading affect bias   A bias in which the emotion associated with unpleasant memories fades more quickly than the emotion associated with positive events. [124]    False memory   A form of misattribution where imagination is mistaken for a memory.   Generation effect (Self-generation effect)  That self-generated information is remembered best. For instance, people are better able to recall memories of statements that they have generated than similar statements generated by others.   Google effect   The tendency to forget information that can be found readily online by using Internet search engines.   Humor effect   That humorous items are more easily remembered than non-humorous ones, which might be explained by the distinctiveness of humor, the increased cognitive processing time to understand the humor, or the emotional arousal caused by the humor. [125]    Lag effect  The phenomenon whereby learning is greater when studying is spread out over time, as opposed to studying the same amount of time in a single session. See also spacing effect .   Leveling and sharpening   Memory distortions introduced by the loss of details in a recollection over time, often concurrent with sharpening or selective recollection of certain details that take on exaggerated significance in relation to the details or aspects of the experience lost through leveling. Both biases may be reinforced over time, and by repeated recollection or re-telling of a memory. [126]    Levels-of-processing effect   That different methods of encoding information into memory have different levels of effectiveness. [127]    List-length effect   A smaller percentage of items are remembered in a longer list, but as the length of the list increases, the absolute number of items remembered increases as well. For example, consider a list of 30 items (""L30"") and a list of 100 items (""L100""). An individual may remember 15 items from L30, or 50%, whereas the individual may remember 40 items from L100, or 40%. Although the percent of L30 items remembered (50%) is greater than the percent of L100 (40%), more L100 items (40) are remembered than L30 items (15). [128] [ further explanation needed ]    Misinformation effect   Memory becoming less accurate because of interference from post-event information . [129]    Modality effect   That memory recall is higher for the last items of a list when the list items were received via speech than when they were received through writing.   Mood-congruent memory bias   The improved recall of information congruent with one's current mood.   Next-in-line effect   When taking turns speaking in a group using a predetermined order (e.g. going clockwise around a room, taking numbers, etc.) people tend to have diminished recall for the words of the person who spoke immediately before them. [130]    Part-list cueing effect   That being shown some items from a list and later retrieving one item causes it to become harder to retrieve the other items. [131]    Peak-end rule   That people seem to perceive not the sum of an experience but the average of how it was at its peak (e.g., pleasant or unpleasant) and how it ended.   Picture superiority effect   The notion that concepts that are learned by viewing pictures are more easily and frequently recalled than are concepts that are learned by viewing their written word form counterparts. [132] [133] [134] [135] [136] [137]    Positivity effect ( Socioemotional selectivity theory )  That older adults favor positive over negative information in their memories.   Serial position effect   That items near the end of a sequence are the easiest to recall, followed by the items at the beginning of a sequence; items in the middle are the least likely to be remembered. [138]    Processing difficulty effect   That information that takes longer to read and is thought about more (processed with more difficulty) is more easily remembered. [139]    Reminiscence bump   The recalling of more personal events from adolescence and early adulthood than personal events from other lifetime periods. [140]    Self-relevance effect   That memories relating to the self are better recalled than similar information relating to others.   Source confusion   Confusing episodic memories with other information, creating distorted memories. [141]    Spacing effect   That information is better recalled if exposure to it is repeated over a long span of time rather than a short one.   Spotlight effect   The tendency to overestimate the amount that other people notice your appearance or behavior.   Stereotypical bias   Memory distorted towards stereotypes (e.g., racial or gender).   Suffix effect   Diminishment of the recency effect because a sound item is appended to the list that the subject is not required to recall. [142] [143]    Suggestibility   A form of misattribution where ideas suggested by a questioner are mistaken for memory.   Tachypsychia   When time perceived by the individual either lengthens, making events appear to slow down, or contracts. [144]    Telescoping effect   The tendency to displace recent events backward in time and remote events forward in time, so that recent events appear more remote, and remote events, more recent.   Testing effect   The fact that you more easily remember information you have read by rewriting it instead of rereading it. [145]    Tip of the tongue phenomenon  When a subject is able to recall parts of an item, or related information, but is frustratingly unable to recall the whole item. This is thought to be an instance of ""blocking"" where multiple similar memories are being recalled and interfere with each other. [123]    Travis Syndrome   Overestimating the significance of the present. [146] It is related to chronological snobbery with possibly an appeal to novelty  logical fallacy being part of the bias.   Verbatim effect   That the ""gist"" of what someone has said is better remembered than the verbatim wording. [147] This is because memories are representations, not exact copies.   von Restorff effect   That an item that sticks out is more likely to be remembered than other items. [148]    Zeigarnik effect   That uncompleted or interrupted tasks are remembered better than completed ones.  See also [ edit ]    Psychology portal  Society portal  Philosophy portal   Affective forecasting – Predicting someone's future emotions (affect)  Anecdotal evidence – Evidence relying on personal testimony  Apophenia – Tendency to perceive connections between unrelated things  Attribution (psychology) – The process by which individuals explain the causes of behavior and events  Black swan theory – Theory of response to surprise events  Chronostasis – Distortion in the perception of time  Cognitive distortion – An exaggerated or irrational thought pattern involved in the onset and perpetuation of psychopathological states  Defence mechanism – Unconscious psychological mechanism that reduces anxiety arising from unacceptable or potentially harmful stimuli  Dysrationalia – Inability to think and behave rationally despite adequate intelligence  Fear, uncertainty, and doubt – Tactic used to influence opinion by disseminating negative, dubious, or false information  Feedback – Process where information about current status is used to influence future status  Impostor syndrome – Psychological pattern of doubting one's accomplishments and fearing being exposed as a ""fraud""  List of common misconceptions – Wikipedia list article  List of fallacies – Types of reasoning that are logically incorrect  List of maladaptive schemas  List of memory biases  List of psychological effects – Wikipedia list article  iSheep#Behavioural patterns – Derogatory marketing term (references multiple cognitive biasses)  Media bias – Bias or perceived bias of journalists and news producers within the mass media in the selection of events and stories that are reported and how they are covered  Mind projection fallacy – An informal fallacy that the way one sees the world reflects the way the world really is  Motivated reasoning – Using emotionally-biased reasoning to produce justifications or make decisions  Observational error , also known as Systematic bias  Outline of public relations – Overview of and topical guide to public relations  Outline of thought – Overview of and topical guide to thought  Pollyanna principle – The tendency of people to remember pleasant events more than unpleasant ones  Positive feedback – Destabilising process that occurs in a feedback loop  Prevalence effect  Propaganda – Form of communication intended to sway the audience through presenting only one side of the argument  Publication bias – Higher probability of publishing results showing a significant finding  Recall bias – Systematic error caused by differences in the accuracy or completeness of the recollections retrieved  Self-handicapping – Cognitive strategy by which people avoid effort in the hopes of keeping potential failure from hurting self-esteem   Footnotes [ edit ]    ^  Haselton MG, Nettle D, Andrews PW (2005). ""The evolution of cognitive bias.""  (PDF) .  In Buss DM (ed.). The Handbook of Evolutionary Psychology . Hoboken, NJ, US: John Wiley & Sons Inc. pp. 724–746.   ^  ""Cognitive Bias – Association for Psychological Science"" . www.psychologicalscience.org . Retrieved 2018-10-10 .   ^  Thomas O (2018-01-19). ""Two decades of cognitive bias research in entrepreneurship: What do we know and where do we go from here?"". Management Review Quarterly . 68 (2): 107–143. doi : 10.1007/s11301-018-0135-9 . ISSN  2198-1620 .   ^  Dougherty MR, Gettys CF, Ogden EE (1999). ""MINERVA-DM: A memory processes model for judgments of likelihood""  (PDF) . Psychological Review . 106 (1): 180–209. doi : 10.1037/0033-295x.106.1.180 .   ^  Gigerenzer G (2006). ""Bounded and Rational"".  In Stainton RJ (ed.). Contemporary Debates in Cognitive Science . Blackwell. p. 129. ISBN  978-1-4051-1304-5 .   ^ a  b  c  d  e  Fielder, Klaus (October 2014). ""Regressive Judgment: Implications of a Universal Property of the Empirical World""  (PDF) . Sage Journals – via Google Scholar. Lay summary .   ^  MacCoun RJ (1998). ""Biases in the interpretation and use of research results""  (PDF) . Annual Review of Psychology . 49 (1): 259–87. doi : 10.1146/annurev.psych.49.1.259 . PMID  15012470 .   ^  Nickerson RS (1998). ""Confirmation Bias: A Ubiquitous Phenomenon in Many Guises""  (PDF) . Review of General Psychology . 2 (2): 175–220 [198]. doi : 10.1037/1089-2680.2.2.175 .   ^  Dardenne B, Leyens JP (1995). ""Confirmation Bias as a Social Skill"" . Personality and Social Psychology Bulletin . 21 (11): 1229–1239. doi : 10.1177/01461672952111011 .   ^  Alexander WH, Brown JW (June 2010). ""Hyperbolically discounted temporal difference learning"" . Neural Computation . 22 (6): 1511–27. doi : 10.1162/neco.2010.08-09-1080 . PMC  3005720 . PMID  20100071 .   ^  Baron 1994 , p. 372   ^  Zhang Y, Lewis M, Pellon M, Coleman P (2007). ""A Preliminary Research on Modeling Cognitive Agents for Social Environments in Multi-Agent Systems""  (PDF) : 116–123.  Cite journal requires |journal= ( help )   ^ a  b  c  Iverson GL, Brooks BL, Holdnack JA (2008). ""Misdiagnosis of Cognitive Impairment in Forensic Neuropsychology"".  In Heilbronner RL (ed.). Neuropsychology in the Courtroom: Expert Analysis of Reports and Testimony . New York: Guilford Press. p. 248. ISBN  9781593856342 . CS1 maint: ref=harv ( link )   ^  Coley JD, Tanner KD (2012). ""Common origins of diverse misconceptions: cognitive principles and the development of biology thinking"" . CBE Life Sciences Education . 11 (3): 209–15. doi : 10.1187/cbe.12-06-0074 . PMC  3433289 . PMID  22949417 .   ^  ""The Real Reason We Dress Pets Like People"" . LiveScience.com . Retrieved 2015-11-16 .   ^  Harris LT, Fiske ST (January 2011). ""Dehumanized Perception: A Psychological Means to Facilitate Atrocities, Torture, and Genocide?"" . Zeitschrift Fur Psychologie . 219 (3): 175–181. doi : 10.1027/2151-2604/a000065 . PMC  3915417 . PMID  24511459 .   ^  Bar-Haim Y, Lamy D, Pergamin L, Bakermans-Kranenburg MJ, van IJzendoorn MH (January 2007). ""Threat-related attentional bias in anxious and nonanxious individuals: a meta-analytic study"" . Psychological Bulletin . 133 (1): 1–24. doi : 10.1037/0033-2909.133.1.1 . PMID  17201568 .   ^  Goddard K, Roudsari A, Wyatt JC (2011). ""Automation Bias – A Hidden Issue for Clinical Decision Support System Use"" . International Perspectives in Health Informatics . Studies in Health Technology and Informatics. 164 . IOS Press. doi : 10.3233/978-1-60750-709-3-17 .   ^  Schwarz N, Bless H, Strack F, Klumpp G, Rittenauer-Schatka H, Simons A (1991). ""Ease of Retrieval as Information: Another Look at the Availability Heuristic""  (PDF) . Journal of Personality and Social Psychology . 61 (2): 195–202. doi : 10.1037/0022-3514.61.2.195 . Archived from the original  (PDF) on 9 February 2014 . Retrieved 19 Oct 2014 .   ^  Kuran T, Sunstein CR (1998). ""Availability Cascades and Risk Regulation"" . Stanford Law Review . 51 (4): 683–768. doi : 10.2307/1229439 . JSTOR  1229439 .   ^  Sanna LJ, Schwarz N, Stocker SL (2002). ""When debiasing backfires: Accessible content and accessibility experiences in debiasing hindsight""  (PDF) . Journal of Experimental Psychology: Learning, Memory, and Cognition . 28 (3): 497–502. CiteSeerX  10.1.1.387.5964 . doi : 10.1037/0278-7393.28.3.497 . ISSN  0278-7393 .   ^  Colman A (2003). Oxford Dictionary of Psychology . New York: Oxford University Press. p. 77 . ISBN  978-0-19-280632-1 .   ^  Baron 1994 , pp. 224–228   ^  Klauer KC, Musch J, Naumer B (October 2000). ""On belief bias in syllogistic reasoning"". Psychological Review . 107 (4): 852–84. doi : 10.1037/0033-295X.107.4.852 . PMID  11089409 .   ^  ""Harness the power of the 'Ben Franklin Effect' to get someone to like you"" . Business Insider . Retrieved 2018-10-10 .   ^  ""Berkson's Paradox | Brilliant Math & Science Wiki"" . brilliant.org . Retrieved 2018-10-10 .   ^  Pronin E, Kugler MB (July 2007). ""Valuing thoughts, ignoring behavior: The introspection illusion as a source of the bias blind spot"". Journal of Experimental Social Psychology . 43 (4): 565–578. doi : 10.1016/j.jesp.2006.05.011 . ISSN  0022-1031 .   ^  Mather M, Shafir E, Johnson MK (March 2000). ""Misremembrance of options past: source monitoring and choice""  (PDF) . Psychological Science . 11 (2): 132–8. doi : 10.1111/1467-9280.00228 . PMID  11273420 . Archived  (PDF) from the original on 2009-01-17.   ^  Västfjäll D, Slovic P, Mayorga M, Peters E (18 June 2014). ""Compassion fade: affect and charity are greatest for a single child in need"" . PLOS ONE . 9 (6): e100115. Bibcode : 2014PLoSO...9j0115V . doi : 10.1371/journal.pone.0100115 . PMC  4062481 . PMID  24940738 .   ^  Oswald ME, Grosjean S (2004). ""Confirmation Bias"" .  In Pohl RF (ed.). Cognitive Illusions: A Handbook on Fallacies and Biases in Thinking, Judgement and Memory . Hove, UK: Psychology Press. pp. 79–96 . ISBN  978-1-84169-351-4 . OCLC  55124398 .   ^  Fisk JE (2004). ""Conjunction fallacy"" .  In Pohl RF (ed.). Cognitive Illusions: A Handbook on Fallacies and Biases in Thinking, Judgement and Memory . Hove, UK: Psychology Press. pp. 23–42 . ISBN  978-1-84169-351-4 . OCLC  55124398 .   ^  DuCharme WW (1970). ""Response bias explanation of conservative human inference"". Journal of Experimental Psychology . 85 (1): 66–74. doi : 10.1037/h0029546 . hdl : 2060/19700009379 .   ^ a  b  Edwards W (1968). ""Conservatism in human information processing"".  In Kleinmuntz B (ed.). Formal representation of human judgment . New York: Wiley. pp. 17–52.   ^  Johnson HM, Seifert CM (November 1994). ""Sources of the continued influence effect: When misinformation in memory affects later inferences"". Journal of Experimental Psychology: Learning, Memory, and Cognition . 20 (6): 1420–1436. doi : 10.1037/0278-7393.20.6.1420 .   ^  Plous 1993 , pp. 38–41   ^  Ciccarelli S, White J (2014). Psychology (4th ed.). Pearson Education, Inc. p. 62. ISBN  978-0205973354 .   ^  Ackerman MS, ed. (2003). Sharing expertise beyond knowledge management (online ed.). Cambridge, Massachusetts: MIT Press. p. 7 . ISBN  9780262011952 .   ^  Quartz SR, The State Of The World Isn't Nearly As Bad As You Think , Edge Foundation, Inc. , retrieved 2016-02-17   ^  ""Evolution and cognitive biases: the decoy effect"" . FutureLearn . Retrieved 2018-10-10 .   ^  ""The Default Effect: How to Leverage Bias and Influence Behavior"" . Influence at Work. 2012-01-11 . Retrieved 2018-10-10 .   ^  Why We Spend Coins Faster Than Bills by Chana Joffe-Walt. All Things Considered , 12 May 2009.   ^  Hsee CK, Zhang J (May 2004). ""Distinction bias: misprediction and mischoice due to joint evaluation"". Journal of Personality and Social Psychology . 86 (5): 680–95. CiteSeerX  10.1.1.484.9171 . doi : 10.1037/0022-3514.86.5.680 . PMID  15161394 .   ^  de Meza D, Dawson C (January 24, 2018). ""Wishful Thinking, Prudent Behavior: The Evolutionary Origin of Optimism, Loss Aversion and Disappointment Aversion"". SSRN  3108432 .  Cite journal requires |journal= ( help )   ^  Kruger J, Dunning D (December 1999). ""Unskilled and unaware of it: how difficulties in recognizing one's own incompetence lead to inflated self-assessments"". Journal of Personality and Social Psychology . 77 (6): 1121–34. CiteSeerX  10.1.1.64.2655 . doi : 10.1037/0022-3514.77.6.1121 . PMID  10626367 .   ^  Duration Neglect in Retrospective Evaluations of Affective Episodes  Archived 2017-08-08 at the Wayback Machine | Journal of Personality and Social Psychology   ^  ""Understanding and Mastering the Empathy Gap"" . Psychology Today .   ^  Quoidbach J, Gilbert DT , Wilson TD (January 2013). ""The end of history illusion""  (PDF) . Science . 339 (6115): 96–8. Bibcode : 2013Sci...339...96Q . doi : 10.1126/science.1229294 . PMID  23288539 . Archived from the original  (PDF) on 2013-01-13. Young people, middle-aged people, and older people all believed they had changed a lot in the past but would change relatively little in the future.   ^  ( Kahneman, Knetsch & Thaler 1991 , p. 193) Richard Thaler coined the term ""endowment effect.""   ^  Jeng M (2006). ""A selected history of expectation bias in physics"". American Journal of Physics . 74 (7): 578–583. arXiv : physics/0508199 . Bibcode : 2006AmJPh..74..578J . doi : 10.1119/1.2186333 .   ^  Kahneman D, Krueger AB, Schkade D, Schwarz N, Stone AA (June 2006). ""Would you be happier if you were richer? A focusing illusion""  (PDF) . Science . 312 (5782): 1908–10. Bibcode : 2006Sci...312.1908K . CiteSeerX  10.1.1.373.2683 . doi : 10.1126/science.1129688 . PMID  16809528 .   ^  ""The Barnum Demonstration"" . psych.fullerton.edu . Retrieved 2018-10-10 .   ^  Haring KS, Watanabe K, Velonaki M, Tossell CC, Finomore V (2018). ""FFAB-The Form Function Attribution Bias in Human Robot Interaction"". IEEE Transactions on Cognitive and Developmental Systems . 10 (4): 843–851. doi : 10.1109/TCDS.2018.2851569 .   ^  Zwicky A (2005-08-07). ""Just Between Dr. Language and I"" . Language Log .   ^  ""The Baader-Meinhof Phenomenon"" . Damn Interesting . Retrieved 2020-02-16 .   ^  ""What's the Baader-Meinhof phenomenon?"" . howstuffworks.com . 20 March 2015 . Retrieved 15 April 2018 .   ^  ""What's in a name?"" . twincities.com . St. Paul Pioneer Press . Retrieved June 5, 2020 . As you might guess, the phenomenon is named after an incident in which I was talking to a friend about the Baader-Meinhof gang (and this was many years after they were in the news). The next day, my friend phoned me and referred me to an article in that day’s newspaper in which the Baader-Meinhof gang was mentioned.   ^  ""The Psychology Guide: What Does Functional Fixedness Mean?"" . PsycholoGenie . Retrieved 2018-10-10 .   ^  Investopedia Staff (2006-10-29). ""Gambler's Fallacy/Monte Carlo Fallacy"" . Investopedia . Retrieved 2018-10-10 .   ^  ""GSNI | Human Development Reports"" . hdr.undp.org . Retrieved 2020-06-10 .   ^  Abel, Martin (September 2019). ""Do Workers Discriminate against Female Bosses?""  (PDF) . IZA Institute of Labor Economics .   ^  Bian, Lin; Leslie, Sarah-Jane; Cimpian, Andrei (November 2018). ""Evidence of bias against girls and women in contexts that emphasize intellectual ability"" . American Psychologist . 73 (9): 1139–1153. doi : 10.1037/amp0000427 . ISSN  1935-990X .   ^  Hamilton, Mykol C. (1991). ""Masculine Bias in the Attribution of Personhood: People = Male, Male = People"" . Psychology of Women Quarterly . 15 (3): 393–402. doi : 10.1111/j.1471-6402.1991.tb00415.x . ISSN  0361-6843 .   ^  Lichtenstein S, Fischhoff B (1977). ""Do those who know more also know more about how much they know?"". Organizational Behavior and Human Performance . 20 (2): 159–183. doi : 10.1016/0030-5073(77)90001-0 .   ^  Merkle EC (February 2009). ""The disutility of the hard-easy effect in choice confidence"". Psychonomic Bulletin & Review . 16 (1): 204–13. doi : 10.3758/PBR.16.1.204 . PMID  19145033 .   ^  Juslin P, Winman A, Olsson H (April 2000). ""Naive empiricism and dogmatism in confidence research: a critical examination of the hard-easy effect"". Psychological Review . 107 (2): 384–96. doi : 10.1037/0033-295x.107.2.384 . PMID  10789203 .   ^  Pohl RF (2004). ""Hindsight Bias"" .  In Pohl RF (ed.). Cognitive Illusions: A Handbook on Fallacies and Biases in Thinking, Judgement and Memory . Hove, UK: Psychology Press. pp. 363–378 . ISBN  978-1-84169-351-4 . OCLC  55124398 .   ^  Anderson KB, Graham LM (2007). Hostile Attribution Bias . Encyclopedia of Social Psychology . SAGE Publications, Inc. pp. 446–447. doi : 10.4135/9781412956253 . ISBN  9781412916707 .   ^  Laibson D (1997). ""Golden Eggs and Hyperbolic Discounting"". Quarterly Journal of Economics . 112 (2): 443–477. CiteSeerX  10.1.1.337.3544 . doi : 10.1162/003355397555253 .   ^  The “IKEA Effect”: When Labor Leads to Love | Harvard Business School   ^  Thompson SC (1999). ""Illusions of Control: How We Overestimate Our Personal Influence"". Current Directions in Psychological Science . 8 (6): 187–190. doi : 10.1111/1467-8721.00044 . ISSN  0963-7214 . JSTOR  20182602 . CS1 maint: ref=harv ( link )   ^  Dierkes M, Antal AB, Child J, Ikujiro Nonaka (2003). Handbook of Organizational Learning and Knowledge . Oxford University Press. p. 22. ISBN  978-0-19-829582-2 . Retrieved 9 September 2013 .   ^  Tversky A, Kahneman D (September 1974). ""Judgment under Uncertainty: Heuristics and Biases"". Science . 185 (4157): 1124–31. Bibcode : 1974Sci...185.1124T . doi : 10.1126/science.185.4157.1124 . PMID  17835457 .   ^  Fiedler K (1991). ""The tricky nature of skewed frequency tables: An information loss account of distinctiveness-based illusory correlations"". Journal of Personality and Social Psychology . 60 (1): 24–36. doi : 10.1037/0022-3514.60.1.24 .   ^ a  b  Sanna LJ, Schwarz N (July 2004). ""Integrating temporal biases: the interplay of focal thoughts and accessibility experiences"" . Psychological Science . 15 (7): 474–81. doi : 10.1111/j.0956-7976.2004.00704.x . PMID  15200632 .   ^  Baron 1994 , pp. 258–259   ^  Danziger S, Levav J, Avnaim-Pesso L (April 2011). ""Extraneous factors in judicial decisions"" . Proceedings of the National Academy of Sciences of the United States of America . 108 (17): 6889–92. Bibcode : 2011PNAS..108.6889D . doi : 10.1073/pnas.1018033108 . PMC  3084045 . PMID  21482790 .   ^  Zaman J, De Peuter S, Van Diest I, Van den Bergh O, Vlaeyen JW (November 2016). ""Interoceptive cues predicting exteroceptive events"". International Journal of Psychophysiology . 109 : 100–106. doi : 10.1016/j.ijpsycho.2016.09.003 . PMID  27616473 .   ^  Barrett LF, Simmons WK (July 2015). ""Interoceptive predictions in the brain"" . Nature Reviews. Neuroscience . 16 (7): 419–29. doi : 10.1038/nrn3950 . PMC  4731102 . PMID  26016744 .   ^  Damasio AR (October 1996). ""The somatic marker hypothesis and the possible functions of the prefrontal cortex"" . Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences . 351 (1346): 1413–20. doi : 10.1098/rstb.1996.0125 . PMID  8941953 .   ^  ( Kahneman, Knetsch & Thaler 1991 , p. 193) Daniel Kahneman, together with Amos Tversky, coined the term ""loss aversion.""   ^  Bornstein RF, Crave-Lemley C (2004). ""Mere exposure effect"" .  In Pohl RF (ed.). Cognitive Illusions: A Handbook on Fallacies and Biases in Thinking, Judgement and Memory . Hove, UK: Psychology Press. pp. 215–234 . ISBN  978-1-84169-351-4 . OCLC  55124398 .   ^  Shafir E, Diamond P, Tversky A (2000). ""Money Illusion"".  In Kahneman D, Tversky A (eds.). Choices, values, and frames . Cambridge University Press. pp. 335–355. ISBN  978-0-521-62749-8 .   ^  Haizlip J, May N, Schorling J, Williams A, Plews-Ogan M (September 2012). ""Perspective: the negativity bias, medical education, and the culture of academic medicine: why culture change is hard"". Academic Medicine . 87 (9): 1205–9. doi : 10.1097/ACM.0b013e3182628f03 . PMID  22836850 .   ^ a  b  c  Trofimova I (2014). ""Observer bias: an interaction of temperament traits with biases in the semantic perception of lexical material"" . PLOS ONE . 9 (1): e85677. Bibcode : 2014PLoSO...985677T . doi : 10.1371/journal.pone.0085677 . PMC  3903487 . PMID  24475048 .   ^ a  b  Sutherland 2007 , pp. 138–139   ^  Baron 1994 , p. 353   ^  Baron 1994 , p. 386   ^  Baron 1994 , p. 44   ^  Hardman 2009 , p. 104   ^  Adams PA, Adams JK (December 1960). ""Confidence in the recognition and reproduction of words difficult to spell"". The American Journal of Psychology . 73 (4): 544–52. doi : 10.2307/1419942 . JSTOR  1419942 . PMID  13681411 .   ^  Hoffrage U (2004). ""Overconfidence"" .  In Rüdiger Pohl (ed.). Cognitive Illusions: a handbook on fallacies and biases in thinking, judgement and memory . Psychology Press. ISBN  978-1-84169-351-4 .   ^  Sutherland 2007 , pp. 172–178   ^  Tuccio, William (2011-01-01). ""Heuristics to Improve Human Factors Performance in Aviation"". Journal of Aviation/Aerospace Education & Research . 20 (3). doi : 10.15394/jaaer.2011.1640 . ISSN  2329-258X .   ^  O'Donoghue T, Rabin M (1999). ""Doing it now or later"" . American Economic Review . 89 (1): 103–124. doi : 10.1257/aer.89.1.103 .   ^  Balas B, Momsen JL (September 2014).  Holt EA (ed.). ""Attention ""blinks"" differently for plants and animals"" . CBE Life Sciences Education . 13 (3): 437–43. doi : 10.1187/cbe.14-05-0080 . PMC  4152205 . PMID  25185227 .   ^  Hsee CK, Hastie R (January 2006). ""Decision and experience: why don't we choose what makes us happy?""  (PDF) . Trends in Cognitive Sciences . 10 (1): 31–7. CiteSeerX  10.1.1.178.7054 . doi : 10.1016/j.tics.2005.11.007 . PMID  16318925 . Archived  (PDF) from the original on 2015-04-20.   ^  Trofimova I (October 1999). ""An investigation of how people of different age, sex, and temperament estimate the world"". Psychological Reports . 85 (2): 533–52. doi : 10.2466/pr0.1999.85.2.533 . PMID  10611787 .   ^  Hardman 2009 , p. 137   ^  Fiedler, Klaus; Unkelbach, Christian (2014-10-01). ""Regressive Judgment: Implications of a Universal Property of the Empirical World"" . Current Directions in Psychological Science . 23 (5): 361–367. doi : 10.1177/0963721414546330 . ISSN  0963-7214 .   ^  Garcia SM, Song H, Tesser A (November 2010). ""Tainted recommendations: The social comparison bias"". Organizational Behavior and Human Decision Processes . 113 (2): 97–101. doi : 10.1016/j.obhdp.2010.06.002 . ISSN  0749-5978 . Lay summary – BPS Research Digest (2010-10-30).   ^  Dalton D, Ortegren M (2011). ""Gender differences in ethics research: The importance of controlling for the social desirability response bias"". Journal of Business Ethics . 103 (1): 73–93. doi : 10.1007/s10551-011-0843-8 .   ^  Kahneman, Knetsch & Thaler 1991 , p. 193   ^  Baron 1994 , p. 382   ^  Baron, J. (in preparation). Thinking and Deciding , 4th edition. New York: Cambridge University Press.   ^  Forsyth DR (2009). Group Dynamics (5th ed.). Cengage Learning. p. 317. ISBN  978-0-495-59952-4 .   ^  ""Penn Psychologists Believe 'Unit Bias' Determines The Acceptable Amount To Eat"" . ScienceDaily (November 21, 2005)   ^  Milgram S (October 1963). ""Behavioral Study of Obedience"". Journal of Abnormal Psychology . 67 (4): 371–8. doi : 10.1037/h0040525 . PMID  14049516 .   ^  Walker D, Vul E (January 2014). ""Hierarchical encoding makes individuals in a group seem more attractive"". Psychological Science . 25 (1): 230–5. doi : 10.1177/0956797613497969 . PMID  24163333 .   ^  Marks G, Miller N (1987). ""Ten years of research on the false-consensus effect: An empirical and theoretical review"". Psychological Bulletin . 102 (1): 72–90. doi : 10.1037/0033-2909.102.1.72 .   ^  ""False Uniqueness Bias (SOCIAL PSYCHOLOGY) – IResearchNet"" . 2016-01-13.   ^  Baron 1994 , p. 275   ^  Pronin E, Kruger J, Savitsky K, Ross L (October 2001). ""You don't know me, but I know you: the illusion of asymmetric insight"". Journal of Personality and Social Psychology . 81 (4): 639–56. doi : 10.1037/0022-3514.81.4.639 . PMID  11642351 .   ^  Hoorens V (1993). ""Self-enhancement and Superiority Biases in Social Comparison"". European Review of Social Psychology . 4 (1): 113–139. doi : 10.1080/14792779343000040 .   ^  Rosset, Evelyn (2008-09-01). ""It's no accident: Our bias for intentional explanations"". Cognition . 108 (3): 771–780. doi : 10.1016/j.cognition.2008.07.001 . ISSN  0010-0277 . PMID  18692779 .   ^  Plous 2006 , p. 206 harvnb error: no target: CITEREFPlous2006 ( help )   ^  Kokkoris, Michail (2020-01-16). ""The Dark Side of Self-Control"" . Harvard Business Review . Retrieved 17 January 2020 .   ^  Plous 2006 , p. 185 harvnb error: no target: CITEREFPlous2006 ( help )   ^  Forsyth DR (2009). Group Dynamics (5th ed.). Pacific Grove, CA: Brooks/Cole.   ^  Kruger J (August 1999). ""Lake Wobegon be gone! The ""below-average effect"" and the egocentric nature of comparative ability judgments"". Journal of Personality and Social Psychology . 77 (2): 221–32. doi : 10.1037/0022-3514.77.2.221 . PMID  10474208 .   ^  Attneave F (August 1953). ""Psychological probability as a function of experienced frequency"". Journal of Experimental Psychology . 46 (2): 81–6. doi : 10.1037/h0057955 . PMID  13084849 .   ^  Fischhoff B, Slovic P, Lichtenstein S (1977). ""Knowing with certainty: The appropriateness of extreme confidence"" . Journal of Experimental Psychology: Human Perception and Performance . 3 (4): 552–564. doi : 10.1037/0096-1523.3.4.552 .   ^  Cacioppo J (2002). Foundations in social neuroscience . Cambridge, Mass: MIT Press. pp. 130–132. ISBN  978-0262531955 .   ^ a  b  Schacter DL (March 1999). ""The seven sins of memory. Insights from psychology and cognitive neuroscience"" . The American Psychologist . 54 (3): 182–203. doi : 10.1037/0003-066X.54.3.182 . PMID  10199218 .   ^  Schmidt SR (July 1994). ""Effects of humor on sentence memory""  (PDF) . Journal of Experimental Psychology. Learning, Memory, and Cognition . 20 (4): 953–67. doi : 10.1037/0278-7393.20.4.953 . PMID  8064254 . Archived from the original  (PDF) on 2016-03-15 . Retrieved 2015-04-19 .   ^  Schmidt SR (2003). ""Life Is Pleasant—and Memory Helps to Keep It That Way!""  (PDF) . Review of General Psychology . 7 (2): 203–210. doi : 10.1037/1089-2680.7.2.203 .   ^  Koriat A, Goldsmith M, Pansky A (2000). ""Toward a psychology of memory accuracy"". Annual Review of Psychology . 51 (1): 481–537. doi : 10.1146/annurev.psych.51.1.481 . PMID  10751979 .   ^  Craik & Lockhart, 1972   ^  Kinnell A, Dennis S (February 2011). ""The list length effect in recognition memory: an analysis of potential confounds"". Memory & Cognition . 39 (2): 348–63. doi : 10.3758/s13421-010-0007-6 . PMID  21264573 .   ^  Wayne Weiten (2010). Psychology: Themes and Variations . Cengage Learning. p. 338. ISBN  978-0-495-60197-5 .   ^  Wayne Weiten (2007). Psychology: Themes and Variations . Cengage Learning. p. 260. ISBN  978-0-495-09303-9 .   ^  Slamecka NJ (April 1968). ""An examination of trace storage in free recall"". Journal of Experimental Psychology . 76 (4): 504–13. doi : 10.1037/h0025695 . PMID  5650563 .   ^  Shepard RN (1967). ""Recognition memory for words, sentences, and pictures"". Journal of Learning and Verbal Behavior . 6 : 156–163. doi : 10.1016/s0022-5371(67)80067-7 .   ^  McBride DM, Dosher BA (2002). ""A comparison of conscious and automatic memory processes for picture and word stimuli: a process dissociation analysis"". Consciousness and Cognition . 11 (3): 423–460. doi : 10.1016/s1053-8100(02)00007-7 . PMID  12435377 .   ^  Defetyer MA, Russo R, McPartlin PL (2009). ""The picture superiority effect in recognition memory: a developmental study using the response signal procedure"". Cognitive Development . 24 (3): 265–273. doi : 10.1016/j.cogdev.2009.05.002 .   ^  Whitehouse AJ, Maybery MT, Durkin K (2006). ""The development of the picture-superiority effect"". British Journal of Developmental Psychology . 24 (4): 767–773. doi : 10.1348/026151005X74153 .   ^  Ally BA, Gold CA, Budson AE (January 2009). ""The picture superiority effect in patients with Alzheimer's disease and mild cognitive impairment"" . Neuropsychologia . 47 (2): 595–8. doi : 10.1016/j.neuropsychologia.2008.10.010 . PMC  2763351 . PMID  18992266 .   ^  Curran T, Doyle J (May 2011). ""Picture superiority doubly dissociates the ERP correlates of recollection and familiarity"". Journal of Cognitive Neuroscience . 23 (5): 1247–62. doi : 10.1162/jocn.2010.21464 . PMID  20350169 .   ^  Martin GN, Carlson NR, Buskist W (2007). Psychology (3rd ed.). Pearson Education. pp. 309–310. ISBN  978-0-273-71086-8 .   ^  O'Brien EJ, Myers JL (1985). ""When comprehension difficulty improves memory for text"". Journal of Experimental Psychology: Learning, Memory, and Cognition . 11 (1): 12–21. doi : 10.1037/0278-7393.11.1.12 .   ^  Rubin, Wetzler & Nebes, 1986; Rubin, Rahhal & Poon, 1998   ^  Lieberman DA (8 December 2011). Human Learning and Memory . Cambridge University Press. p. 432. ISBN  978-1-139-50253-5 .   ^  Morton, Crowder & Prussin, 1971   ^  Pitt I, Edwards AD (2003). Design of Speech-Based Devices: A Practical Guide . Springer. p. 26. ISBN  978-1-85233-436-9 .   ^  Stetson C, Fiesta MP, Eagleman DM (December 2007). ""Does time really slow down during a frightening event?"" . PLOS ONE . 2 (12): e1295. Bibcode : 2007PLoSO...2.1295S . doi : 10.1371/journal.pone.0001295 . PMC  2110887 . PMID  18074019 .   ^  Goldstein EB (2010-06-21). Cognitive Psychology: Connecting Mind, Research and Everyday Experience . Cengage Learning. p. 231. ISBN  978-1-133-00912-2 .   ^  ""Not everyone is in such awe of the internet"" . Evening Standard . Evening Standard. 2011-03-23 . Retrieved 28 October 2015 .   ^  Poppenk, Walia, Joanisse, Danckert, & Köhler, 2006   ^  Von Restorff, H (1933). ""Über die Wirkung von Bereichsbildungen im Spurenfeld (The effects of field formation in the trace field) "" "". Psychological Research . 18 (1): 299–342. doi : 10.1007/bf02409636 .    References [ edit ]   Baron J (1994). Thinking and deciding (2nd ed.). Cambridge University Press. ISBN  978-0-521-43732-5 . CS1 maint: ref=harv ( link )  Baron J (2000). Thinking and deciding (3rd ed.). New York: Cambridge University Press. ISBN  978-0-521-65030-4 . CS1 maint: ref=harv ( link )  Bishop MA, Trout JD (2004). Epistemology and the Psychology of Human Judgment . New York: Oxford University Press . ISBN  978-0-19-516229-5 . CS1 maint: ref=harv ( link )  Gilovich T (1993). How We Know What Isn't So: The Fallibility of Human Reason in Everyday Life . New York: The Free Press. ISBN  978-0-02-911706-4 . CS1 maint: ref=harv ( link )  Gilovich T, Griffin D, Kahneman D (2002). Heuristics and biases: The psychology of intuitive judgment . Cambridge, UK: Cambridge University Press. ISBN  978-0-521-79679-8 . CS1 maint: ref=harv ( link )  Greenwald AG (1980). ""The Totalitarian Ego: Fabrication and Revision of Personal History""  (PDF) . American Psychologist . 35 (7): 603–618. doi : 10.1037/0003-066x.35.7.603 . ISSN  0003-066X . CS1 maint: ref=harv ( link )  Hardman D (2009). Judgment and decision making: psychological perspectives . Wiley-Blackwell. ISBN  978-1-4051-2398-3 . CS1 maint: ref=harv ( link )  Kahneman D, Slovic P, Tversky A (1982). Judgment under Uncertainty: Heuristics and Biases . Science . 185 . Cambridge, UK: Cambridge University Press. pp. 1124–31. doi : 10.1126/science.185.4157.1124 . ISBN  978-0-521-28414-1 . PMID  17835457 . CS1 maint: ref=harv ( link )  Kahneman D, Knetsch JL, Thaler RH (1991). ""Anomalies: The Endowment Effect, Loss Aversion, and Status Quo Bias""  (PDF) . The Journal of Economic Perspectives . 5 (1): 193–206. doi : 10.1257/jep.5.1.193 . Archived from the original  (PDF) on November 24, 2012. CS1 maint: ref=harv ( link )  Plous S (1993). The Psychology of Judgment and Decision Making . New York: McGraw-Hill. ISBN  978-0-07-050477-6 . CS1 maint: ref=harv ( link )  Pohl RF (2017). Cognitive illusions: Intriguing phenomena in thinking, judgment and memory (2nd ed.). London and New York: Routledge. ISBN  978-1-138-90341-8 .  Schacter DL (March 1999). ""The seven sins of memory. Insights from psychology and cognitive neuroscience""  (PDF) . The American Psychologist . 54 (3): 182–203. doi : 10.1037/0003-066X.54.3.182 . PMID  10199218 . Archived from the original  (PDF) on May 13, 2013. CS1 maint: ref=harv ( link )  Sutherland S (2007). Irrationality . Pinter & Martin. ISBN  978-1-905177-07-3 . CS1 maint: ref=harv ( link )  Tetlock PE (2005). Expert Political Judgment: how good is it? how can we know? . Princeton: Princeton University Press. ISBN  978-0-691-12302-8 . CS1 maint: ref=harv ( link )  Virine L, Trumper M (2007). Project Decisions: The Art and Science . Vienna, VA: Management Concepts. ISBN  978-1-56726-217-9 . CS1 maint: ref=harv ( link )   v t e Biases Cognitive biases  Actor–observer  Acquiescence  Ambiguity  Anchoring  Attentional  Attribution  Authority  Automation  Belief  Blind spot  Choice-supportive  Confirmation  Congruence  Cultural  Distinction  Dunning–Kruger  Egocentric  Emotional  Extrinsic incentives  Fading affect  Framing  Correspondence  Halo effect  Hindsight  Horn effect  Hostile attribution  Impact  Implicit  In-group  Mere-exposure effect  Negativity  Normalcy  Omission  Optimism  Out-group homogeneity  Outcome  Overton window  Precision  Present  Pro-innovation  Response  Restraint  Self-serving  Social comparison  Status quo  Time-saving  Trait ascription  von Restorff effect  Zero-risk  In animals  Statistical biases  Estimator  Forecast  Healthy user  Information  Psychological  Lead time  Length time  Non-response  Observer  Omitted-variable  Participation  Recall  Sampling  Selection  Self-selection  Social desirability  Spectrum  Survivorship  Systematic error  Systemic  Verification  Wet  Other biases  Academic  Funding  FUTON  Inductive  Infrastructure  Inherent  In education  Media  False balance  Vietnam War  Norway  South Asia  Sweden  United States  Arab–Israeli conflict  Ukraine  Net  Political bias  Publication  Reporting  White hat  Bias reduction  Cognitive bias mitigation  Debiasing  Heuristics in judgment and decision-making  Lists: General  ·  Memory      Retrieved from "" https://en.wikipedia.org/w/index.php?title=List_of_cognitive_biases&oldid=965490136 ""  Categories : Cognitive biases Psychology lists Behavioral finance Cognitive science lists Hidden categories: CS1 errors: missing periodical CS1 maint: ref=harv Webarchive template wayback links Harv and Sfn no-target errors Articles with short description Wikipedia articles needing clarification from November 2013         Navigation menu      Personal tools      Not logged in Talk Contributions Create account Log in        Namespaces      Article Talk        Variants              Views      Read Edit View history        More           Search                     Navigation      Main page Contents Current events Random article About Wikipedia Contact us Donate Wikipedia store       Contribute      Help Community portal Recent changes Upload file       Tools      What links here Related changes Upload file Special pages Permanent link Page information Wikidata item Cite this page       Print/export      Download as PDF Printable version       Languages      العربية Azərbaycanca Català Deutsch Español فارسی Polski Português Русский ไทย Українська 中文  Edit links        This page was last edited on 1 July 2020, at 16:47 (UTC) .  Text is available under the Creative Commons Attribution-ShareAlike License ;
additional terms may apply.  By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc. , a non-profit organization.    Privacy policy  About Wikipedia  Disclaimers  Contact Wikipedia  Developers  Statistics  Cookie statement  Mobile view          "
16,ambiguity aversion(fallacy OR bias OR heuristic) (coronavirus OR covid OR pandemic),1,https://link.springer.com/article/10.1007/s11166-011-9134-0,"Appendix I. Genotyping method

The polymorphism for the SLC6A4 44 bp deletion/insertion (5-HTTLPR) in the promoter region was characterized using PCR amplification procedure with the following primers: F5′-GGCGTTGCCGCTCTGAATTGC-3′, R5′-GAGGGACTGAGCTGGACAACC-3′. PCR reactions were performed using 5 μl Master Mix (Thermo scientific), 2 μl primers (0.5 μM), 0.6 μl Mg/Cl2 (2.5 mM), 0.4 μl DMSO 5% and 1 μl of water to total of 9 μl total volume and an additional 1 μl of genomic DNA was added to the mixture. All PCR reactions were employed on a Biometra T1 Thermocycler (Biometra, Güttingem, Germany). PCR reaction conditions were as follows: preheating step at 94.0°C for 5 min, 34 cycles of denaturation at 94.0°C for 30 s, reannealing at 55°C for 30 s and extension at 72°C for 90 s. The reaction proceeded to a hold at 72°C for 5 min. All reaction mixtures were electrophoresed on a 3% agarose gel (AMRESCO) with ethidium bromide to screen for genotype.

Amplification of the DRD5, ESR1 and ESR2 microsatellites was achieved using the following primers: DRD5: forward: 5′- CGTGTATGATCCCTGCAG -3′; reverse: 5′- GCTCATGAGAAGAATGGAGTG -3′; ESR1 (corresponds to the TA dinucleotide repeat in the 5′ promoter region): forward 5′- AGACGCATGATATACTTCACC -3′; reverse 5′- GTTCACTTGGGCTAGGATAT -3′. ESR2 (corresponds to the CA dinucleotide repeat in intron 5): forward (fluorescent) 5′- GGTAAACCATGGTCTGTACC -3′; reverse 5′- AACAAAATGTTGAATGAGTGGG -3′. PCR reactions were performed using 5 μl Master Mix (Thermo scientific), 0.5 μl primers (0.5 μM), 0.4 μl Mg/Cl2 (2.5 mM) and 3.1 μl of water to total of 9 μl total volume and an additional 1 μl of genomic DNA was added to the mixture. All PCR reactions were employed on a Biometra T1 Thermocycler (Biometra, Güttingem, Germany). PCR reaction conditions were as follows: preheating step at 95.0°C for 5 min, 30 cycles of denaturation at 95.0°C for 30 s, reannealing at 55°C for 30 s and extension at 72°C for 40 s. The reaction proceeded to a hold at 72°C for 10 min. The PCR product was analyzed on an ABI 310 DNA Analyzer.","           Advertisement                  Search         Log in              Search SpringerLink     Search                     Published: 07 December 2011   Ambiguity aversion and familiarity bias: Evidence from behavioral and gene association studies  Soo Hong Chew 1 , 2 , Richard P. Ebstein 3 , 4 & Songfa Zhong 5    Journal of Risk and Uncertainty  volume 44 , pages 1 – 18 ( 2012 ) Cite this article        1268 Accesses    25 Citations    Metrics details          Abstract It is increasingly recognized that decision making under uncertainty depends not only on probabilities, but also on psychological factors such as ambiguity and familiarity. Using 325 Beijing subjects, we conduct a neurogenetic study of ambiguity aversion and familiarity bias in an incentivized laboratory setting. For ambiguity aversion, 49.4% of the subjects choose to bet on the 50–50 deck despite the unknown deck paying 20% more. For familiarity bias, 39.6% choose the bet on Beijing’s temperature rather than the corresponding bet with Tokyo even though the latter pays 20% more. We genotype subjects for anxiety-related candidate genes and find a serotonin transporter polymorphism being associated with familiarity bias, but not ambiguity aversion, while the dopamine D5 receptor gene and estrogen receptor beta gene are associated with ambiguity aversion only among female subjects. Our findings contribute to understanding of decision making under uncertainty beyond revealed preference.   This is a preview of subscription content, log in to check access.     Access options   Buy single article    Instant access to the full article PDF.    42,99 €  Price includes VAT for Poland                  Subscribe to journal    Immediate online access to all issues from 2019. Subscription will auto renew annually.    71,04 €  Price includes VAT for Poland                Rent this article via DeepDyve.         Learn more about Institutional subscriptions         Fig 1 Fig 2 Fig. 3   References Abdellaoui, M., Baillon, A., Placido, L., & Wakker, P. P. (2011). The rich domain of uncertainty: Source functions and their experimental implementation. American Economic Review, 101 (2), 695–723. Article  Google Scholar  Benjamin, D. J., Chabris, C. F., Glaeser, E., Gudnason, V., Harris, T. B., Laibson, D., Lenore, J. L., & Purcell, S. (2007). Genoeconomics. In J. Milner, B. Elaine, C. Trujillo, M. Kaefer, & S. Ross (Eds.), Biosocial surveys . Washington: The National Academies. Google Scholar  Canli, T., & Lesch, K. P. (2007). Long story short: The serotonin transporter in emotion regulation and social cognition. Nature Neuroscience, 10 (9), 1103–1109. Article  Google Scholar  Carpenter, J., Garcia, J., & Lum, J. (2011). Dopamine receptor genes predict risk preferences, time preferences, and related economic choices. Journal of Risk and Uncertainty, 42 (3), 233–261. Article  Google Scholar  Cesarini, D., Dawes, C. T., Johannesson, M., Lichtenstein, P., & Wallace, B. (2009). Genetic variation in preferences for giving and risk taking. Quarterly Journal of Economics, 124 (2), 809–842. Article  Google Scholar  Chew, S. H. (1983). A generalization of the quasilinear mean with applications to the measurement of income inequality and decision theory resolving the Allais paradox. Econometrica, 51 (4), 1065–1092. Article  Google Scholar  Chew, S. H. (1989). Axiomatic utility theories with the betweenness property. Annals of Operations Research 19 (1), 273–298. Google Scholar  Chew, S. H., & Sagi, J. S. (2006). Event exchangeability: Probabilistic sophistication without continuity or monotonicity. Econometrica, 74 (3), 771–786. Article  Google Scholar  Chew, S. H., & Sagi, J. S. (2008). Small worlds: Modeling attitudes toward sources of uncertainty. Journal of Economic Theory, 139 (1), 1–24. Article  Google Scholar  Chew, S. H., Li, K. K., Chark, R., & Zhong, S. (2008). Source preference and ambiguity aversion: Models and evidence from behavioral and neuroimaging experiments. In D. Houser, K. McCabe (Eds.), Neuroeconomics . Emerald. Chew, S. H., Li, K. K., Chark, R., & Zhong, S. (2010). Familiarity bias: Evidence from laboratory and field experiments. Working paper. Cloninger, C. R. (1986). A unified biosocial theory of personality and its role in the development of anxiety states. Psychiatric Developments, 4 (3), 167–226. Google Scholar  Comings, D. E., Muhleman, D., Johnson, P., & MacMurray, J. P. (1999). Potential role of the estrogen receptor gene(ESR 1) in anxiety. Molecular Psychiatry, 4 (4), 374–377. Article  Google Scholar  Crisan, L. G., Pana, S., Vulturar, R., Heilman, R. M., Szekely, R., Druga, B., Dragos, N., & Miu, A. C. (2009). Genetic contributions of the serotonin transporter to social learning of fear and economic decision making. Social Cognitive and Affective Neuroscience, 4 (4), 399–408. Article  Google Scholar  Croson, R., & Gneezy, U. (2009). Gender differences in preferences. Journal of Economic Literature, 47 (2), 1–27. Article  Google Scholar  Dekel, E. (1986). An axiomatic characterization of preferences under uncertainty: Weakening the independence axiom. Journal of Economic Theory, 40 (2), 304–318. Article  Google Scholar  Ding, Y. C., Chi, H. C. et al. (2002). Evidence of positive selection acting at the human dopamine receptor D4 gene locus. Proceedings of the National Academy of Sciences of the United States of America 99 (1), 309–314. Google Scholar  Dreber, A., Apicella, C. L., Eisenberg, D. T. A., Garcia, J. R., Zamore, R. S., Lum, J. K., & Campbell, B. (2009). The 7R polymorphism in the dopamine receptor D4 gene (DRD4) is associated with financial risk taking in men. Evolution and Human Behavior, 30 (30), 85–92. Article  Google Scholar  Dreber, A., Rand, D. G., Garcia, J. R., Wernerfelt, N., Lum, J. K., & Zeckhauser, R. (2011). Dopamine and risk preferences in different domains. Journal of Risk and Uncertainty, 43 (1), 19–38. Article  Google Scholar  Ellsberg, D. (1961). Risk, ambiguity, and the savage axioms. Quarterly Journal of Economics, 75 (4), 643–669. Article  Google Scholar  Ergin, H., & Gul, F. (2009). A subjective theory of compound lotteries. Journal of Economic Theory, 144 , 899–929. Article  Google Scholar  Fox, C. R., & Tversky, A. (1995). Ambiguity aversion and comparative ignorance. Quarterly Journal of Economics, 110 (3), 585–603. Article  Google Scholar  Frydman, C., Camerer, C., Bossaerts, P., & Rangel, A. (2011). MAOA-L carriers are better at making optimal financial decisions under risk. Proceedings of the Royal Society B: Biological Sciences, 278 (1714), 2053–2059. Article  Google Scholar  Geng, Y. G., Su, Q. R., Su, L. Y., Chen, Q., & Ren, G. Y. (2007). Comparison of the polymorphisms of androgen receptor gene and estrogen alpha and beta gene between adolescent females with first-onset major depressive disorder and control. International Journal of Neuroscience, 117 (4), 539–547. Article  Google Scholar  Gilboa, I., & Schmeidler, D. (1989). Maximin expected utility with a non-unique prior. Journal of Mathematical Economics, 18 , 141–153. Article  Google Scholar  Green, J. R., & Jullien, B. (1988). Ordinal independence in nonlinear utility theory. Journal of Risk and Uncertainty, 1 (4), 355–387. Article  Google Scholar  Hariri, A. R., Mattay, V. S., Tessitore, A., Kolachana, B., Fera, F., Goldman, D., Egan, M. F., & Weinberger, D. R. (2002). Serotonin transporter genetic variation and the response of the human amygdala. Science, 297 (5580), 400–403. Article  Google Scholar  Heath, C., & Tversky, A. (1991). Preference and belief: Ambiguity and competence in choice under uncertainty. Journal of Risk and Uncertainty, 4 (1), 5–28. Article  Google Scholar  Hsu, M., Bhatt, M., Adolphs, R., Tranel, D., & Camerer, C. F. (2005). Neural systems responding to degrees of uncertainty in human decision-making. Science, 310 , 1680–1683. Article  Google Scholar  Huberman, G. (2001). Familiarity breeds investment. The Review of Financial Studies, 14 (3), 659–680. Article  Google Scholar  Huettel, S. A., Stowe, C. J., Gordon, E. M., Warner, B. T., & Platt, M. L. (2006). Neural signatures of economic preferences for risk and ambiguity. Neuron, 49 (5), 765–775. Article  Google Scholar  Imwalle, D. B., Gustafsson, J. A., & Rissman, E. F. (2005). Lack of functional estrogen receptor influences anxiety behavior and serotonin content in female mice. Physiology and Behavior, 84 (1), 157–163. Article  Google Scholar  Jakobsdottir, J., Gorin, M. B., Conley, Y. P., Ferrell, R. E., & Weeks, D. E. (2009). Interpretation of genetic association studies: Markers with replicated highly significant odds ratios may be poor classifiers. PLoS Genetics, 5 (2), e1000337. Article  Google Scholar  Keynes, J. M. (1921). A treatise on probability . New York: Macmillan and Co., limited. Google Scholar  Kuhnen, C. M., & Chiao, J. Y. (2009). Genetic determinants of financial risk taking. PLoS One, 4 (2), e4362. Article  Google Scholar  Lesch, K. P., Bengel, D., Heils, A., Sabol, S. Z., Greenberg, B. D., Petri, S., Benjamin, J., Muller, C. R., Hamer, D. H., & Murphy, D. L. (1996). Association of anxiety-related traits with a polymorphism in the serotonin transporter gene regulatory region. Science, 274 (5292), 1527–1531. Article  Google Scholar  Li, D., Sham, P. C., Owen, M. J., & He, L. (2006). Meta-analysis shows significant association between dopamine system genes and attention deficit hyperactivity disorder (ADHD). Human Molecular Genetics, 15 (14), 2276–2284. Article  Google Scholar  Lowe, N., Kirley, A., Hawi, Z., Sham, P., Wickham, H., Kratochvil, C. J., Smith, S. D., Lee, S. Y., Levy, F., & Kent, L. (2004). Joint analysis of the DRD5 marker concludes association with attention-deficit/hyperactivity disorder confined to the predominantly inattentive and combined subtypes. The American Journal of Human Genetics, 74 (2), 348–356. Article  Google Scholar  Lund, T. D., Rovis, T., Chung, W. C. J., & Handa, R. J. (2005). Novel actions of estrogen receptor-{beta} on anxiety-related behaviors. Endocrinology, 146 (2), 797–807. Article  Google Scholar  Machina, M. J. (2004). Almost-objective uncertainty. Economic Theory, 24 (1), 1–54. Article  Google Scholar  Machina, M. J., & Schmeidler, D. (1992). A more robust definition of subjective probability. Econometrica, 60 (4), 745–780. Article  Google Scholar  McGough, J. J. (2005). Attention-deficit/hyperactivity disorder pharmacogenomics. Biological Psychiatry, 57 (11), 1367–1373. Article  Google Scholar  McIntyre, M. H., Kantoff, P. W., Stampfer, M. J., Mucci, L. A., Parslow, D., Li, H., Gaziano, J. M., Abe, M., & Ma, J. (2007). Prostate cancer risk and ESR1 TA, ESR2 CA repeat polymorphisms. Cancer Epidemiology, Biomarkers & Prevention, 16 (11), 2233. Article  Google Scholar  Munaf, M. R., Yalcin, B., Willis-Owen, S. A., & Flint, J. (2008). Association of the dopamine D4 receptor (DRD4) gene and approach-related personality traits: Meta-analysis and new data. Biological Psychiatry, 63 , 197–206. Article  Google Scholar  Neale, M. C., & Cardon, L. R. (1992). Methodology for genetic studies of twins and families . Netherlands: Kluwer Academic Publishers. Google Scholar  Quiggin, J. (1982). A theory of anticipated utility. Journal of Economic Behavior and Organization, 3 (4), 323–343. Article  Google Scholar  Ramsey, F. P. (1926). Truth and probability. Studies in Subjective Probability , 61–92. Roe, B. E., Tilley, M. R., Gu, H. H., Beversdorf, D. Q., Sadee, W., Haab, T. C., Papp, A. C. (2009). Financial and psychological risk attitudes associated with two single nucleotide polymorphisms in the nicotine receptor (CHRNA4) gene. PLoS ONE , e6704. Roiser, J. P., de Martino, B., Tan, G. C. Y., Kumaran, D., Seymour, B., Wood, N. W., & Dolan, R. J. (2009). A genetically mediated bias in decision making driven by failure of amygdala control. Journal of Neuroscience, 29 (18), 5985–5991. Article  Google Scholar  Savage, L. J. (1954). The foundations of statistics . New York: Wiley. Google Scholar  Schmeidler, D. (1989). Subjective probability and expected utility without additivity. Econometrica, 57 (3), 571–587. Article  Google Scholar  Takeo, C., Negishi, E., Nakajima, A., Ueno, K., Tatsuno, I., Saito, Y., Amano, K., & Hirai, A. (2005). Association of cytosine-adenine repeat polymorphism of the estrogen receptor-gene with menopausal symptoms. Gender Medicine, 2 (2), 96–105. Article  Google Scholar  Tversky, A., & Kahneman, D. (1992). Advances in prospect theory: Cumulative representation of uncertainty. Journal of Risk and Uncertainty, 5 (4), 297–323. Article  Google Scholar  Vanyukov, M. M., Moss, H. B., Kaplan, B. B., Kirillova, G. P., & Tarter, R. E. (2000). Brief research communication antisociality, substance dependence, and the DRD5 gene: A preliminary study. American Journal of Medical Genetics (Neuropsychiatric Genetics), 96 , 654–658. Article  Google Scholar  von Neumann, J., & Morgenstern, O. (1944). Theory of games and economic behavior . New York: Princeton University Press. Google Scholar  Westberg, L., Melke, J., Landen, M., Nilsson, S., Baghaei, F., Rosmond, R., Jansson, M., Holm, G., Bjntorp, P., & Eriksson, E. (2003). Association between a dinucleotide repeat polymorphism of the estrogen receptor alpha gene and personality traits in women. Molecular Psychiatry, 8 (1), 118–122. Article  Google Scholar  Zhong, S., Chew, S. H., Set, E., Zhang, J., Xue, H., Sham, P. C., Ebstein, R. P., & Israel, S. (2009a). The heritability of attitude toward economic risk. Twin Research and Human Genetics, 12 (1), 103–107. Article  Google Scholar  Zhong, S., Israel, S., Xue, H., Sham, P. C., Ebstein, R. P., & Chew, S. H. (2009b). A neurochemical approach to valuation sensitivity over gains and losses. Proceedings of the Royal Society B: Biological Sciences, 276 , 4181–4188. Article  Google Scholar  Zhong, S., Salomon, I., Xue, H., Ebstein, R. P., & Chew, S. H. (2009c). Monoamine oxidase A gene (MAOA) associated with attitude towards longshot risks. PLoS One, 4 (12), e8516. Article  Google Scholar  Zion, I. Z. B., Tessler, R., Cohen, L., Lerer, E., Raz, Y., Bachner-Melman, R., Gritsenko, I., Nemanov, L., Zohar, A. H., & Belmaker, R. H. (2006). Polymorphisms in the dopamine D4 receptor gene (DRD4) contribute to individual differences in human sexual behavior: Desire, arousal and sexual function. Molecular Psychiatry, 11 (8), 782–786. Article  Google Scholar  Download references Acknowledgements We thank Wang Rui, Wu Qingyu, and Ye Qiaofeng for assistance in conducting the behavioral experiments, and Idan Shalev for doing genotyping. Financial support from the Hong Kong University of Science and Technology, as well as National University of Singapore, is gratefully acknowledged. Author information Affiliations Department of Economics and Department of Finance, National University of Singapore, Singapore, Singapore Soo Hong Chew Center for Experimental Business Research and Department of Economics, Hong Kong University of Science and Technology, Hong Kong, Hong Kong Soo Hong Chew Department of Psychology, National University of Singapore, Singapore, Singapore Richard P. Ebstein Scheinfeld Center of Human Genetics for Social Sciences, Hebrew University, Jerusalem, Israel Richard P. Ebstein Department of Economics, National University of Singapore, Singapore, Singapore Songfa Zhong Authors Soo Hong Chew View author publications You can also search for this author in PubMed  Google Scholar Richard P. Ebstein View author publications You can also search for this author in PubMed  Google Scholar Songfa Zhong View author publications You can also search for this author in PubMed  Google Scholar Corresponding author Correspondence to Soo Hong Chew . Appendix I. Genotyping method Appendix I. Genotyping method The polymorphism for the SLC6A4 44 bp deletion/insertion (5-HTTLPR) in the promoter region was characterized using PCR amplification procedure with the following primers: F5′-GGCGTTGCCGCTCTGAATTGC-3′, R5′-GAGGGACTGAGCTGGACAACC-3′. PCR reactions were performed using 5 μl Master Mix (Thermo scientific), 2 μl primers (0.5 μM), 0.6 μl Mg/Cl2 (2.5 mM), 0.4 μl DMSO 5% and 1 μl of water to total of 9 μl total volume and an additional 1 μl of genomic DNA was added to the mixture. All PCR reactions were employed on a Biometra T1 Thermocycler (Biometra, Güttingem, Germany). PCR reaction conditions were as follows: preheating step at 94.0°C for 5 min, 34 cycles of denaturation at 94.0°C for 30 s, reannealing at 55°C for 30 s and extension at 72°C for 90 s. The reaction proceeded to a hold at 72°C for 5 min. All reaction mixtures were electrophoresed on a 3% agarose gel (AMRESCO) with ethidium bromide to screen for genotype. Amplification of the DRD5, ESR1 and ESR2 microsatellites was achieved using the following primers: DRD5: forward: 5′- CGTGTATGATCCCTGCAG -3′; reverse: 5′- GCTCATGAGAAGAATGGAGTG -3′; ESR1 (corresponds to the TA dinucleotide repeat in the 5′ promoter region): forward 5′- AGACGCATGATATACTTCACC -3′; reverse 5′- GTTCACTTGGGCTAGGATAT -3′. ESR2 (corresponds to the CA dinucleotide repeat in intron 5): forward (fluorescent) 5′- GGTAAACCATGGTCTGTACC -3′; reverse 5′- AACAAAATGTTGAATGAGTGGG -3′. PCR reactions were performed using 5 μl Master Mix (Thermo scientific), 0.5 μl primers (0.5 μM), 0.4 μl Mg/Cl2 (2.5 mM) and 3.1 μl of water to total of 9 μl total volume and an additional 1 μl of genomic DNA was added to the mixture. All PCR reactions were employed on a Biometra T1 Thermocycler (Biometra, Güttingem, Germany). PCR reaction conditions were as follows: preheating step at 95.0°C for 5 min, 30 cycles of denaturation at 95.0°C for 30 s, reannealing at 55°C for 30 s and extension at 72°C for 40 s. The reaction proceeded to a hold at 72°C for 10 min. The PCR product was analyzed on an ABI 310 DNA Analyzer. Rights and permissions Reprints and Permissions About this article Cite this article Chew, S.H., Ebstein, R.P. & Zhong, S. Ambiguity aversion and familiarity bias: Evidence from behavioral and gene association studies. J Risk Uncertain  44, 1–18 (2012). https://doi.org/10.1007/s11166-011-9134-0 Download citation Published : 07 December 2011 Issue Date : February 2012 DOI : https://doi.org/10.1007/s11166-011-9134-0 Keywords Ambiguity aversion Familiarity bias Source dependence Genetics Neuroeconomics JEL Classification C91 D14 D81 D87 G11                 Access options   Buy single article    Instant access to the full article PDF.    42,99 €  Price includes VAT for Poland                  Subscribe to journal    Immediate online access to all issues from 2019. Subscription will auto renew annually.    71,04 €  Price includes VAT for Poland                Rent this article via DeepDyve.         Learn more about Institutional subscriptions            Advertisement                 Over 10 million scientific documents at your fingertips   Switch Edition    Academic Edition    Corporate Edition         Home  Impressum  Legal information  Privacy statement  How we use cookies  Accessibility  Contact us     Not logged in  - 89.64.25.55   Not affiliated    Springer Nature       © 2020 Springer Nature Switzerland AG. Part of Springer Nature .                      "
16,ambiguity aversion(fallacy OR bias OR heuristic) (coronavirus OR covid OR pandemic),2,https://towardsdatascience.com/why-most-of-you-made-an-irrational-decision-aeeac532ef92,"What is Ambiguity Aversion?

Part 2 of 3 — The differences between risk, uncertainty, and ambiguity

Did you choose A&C, A&D, B&C, or B&D in the rationality quiz?

(No idea what I’m talking about? That’s probably because you got here without first reading Part 1 (a 3 min quiz), so please do that before going any further. You’re going to need to know what those X&X things mean for the rest to make sense.)

I put up a barrier of cute pets from the internet so you don’t accidentally see the spoilers below. Scroll past them when you’ve finished reading Part 1.

What do most people choose?

If you’re like most people, you chose the perennial favorite: A&D.

Alas, if that’s true, it means you didn’t choose rationally! Yup, most people don’t, so there’s no need to feel bad. You’re in good company. All kinds of irrational decision-making is wired into our species and takes a lot of training to overcome.

Results from the Twitter poll are in line with myriad lab studies looking at this example (and its variants) since the 1960s.

What’s your result?

A&C — Rational , especially if you like the color pink (or think I do).

— , especially if you like the color pink (or think I do). B&D — Rational , especially if you dislike the color pink (or think I do).

— , especially if you dislike the color pink (or think I do). Flipping a coin twice — Rational , especially if you’re a trained economist.

— , especially if you’re a trained economist. A&D — Irrational but popular. Science thinks it knows why people do this.

— but popular. Science thinks it knows why people do this. B&C — Irrational and science doesn’t understand you. :(

The majority of my readers are data science professionals and/or senior data leaders. While the quiz was running yesterday, I was amused to receive a bunch of comments like, “Too easy, give us something harder!” (Such bravado! My fingers itched to do a post on the overconfidence effect.)

If rational decision-making is so easy, why did three quarters of you mathy folks get it wrong?

Perhaps it’s not so easy after all and the data science curriculum isn’t the one that immunizes you from your human foibles. Maybe data scientists could stand to learn a thing or two from behavioral economists and psychologists…

Irrational decision-making

Someday I’ll write an article that zooms in on how economists define rationality (and introduce you to the four axioms of Von Neumann–Morgenstern rationality… mmmm, fancy) but for now, let’s keep it simple:

One way (among others) that a decision can be irrational is inconsistency, which is something the overwhelmingly popular A&D answer suffers from.

Get out your scalpels and prepare to dissect! We’ll do a little proof by rational cases.

Rational case #1: What the behavioral economist does

My undergraduate major in economics qualifies me for membership in that most coldblooded of groups — economists. Part of the training of a baby experimental/behavioral economist is to be beaten with a stick until we learn to take a rational approach to decisions like this one. So which answer do we prefer after our brainwashin- er, I mean, training has successfully taken root?

We. Don’t. Care.

The completely rational mind shouldn’t have any kind of preference here. We feel total, brazen, belligerent indifference when presented with these options. All four X&X combos are identical.

An economist is trained to look at the options and collapse the probabilities down. We ask, “While we know there are 30 white balls, what do we know about the distribution of yellow and pink among the remaining 60?”

Best answer: nothing.

Fact: I’ve given you no info about that in the setup and you’d have to know me pre-e-e-etty well to have info about my relative love of pink versus yellow. If you think you know which color I prefer, that’s you showing your biases.

So, as far as a trained economist is concerned, there’s:

No info to suggest there’s more pink than yellow. No info to suggest there’s more yellow than pink.

That means there’s no reason to treat yellow and pink differently when we run the numbers, so here’s what’s most consistent with the contents of the economist’s head at the moment:

30 white

30 pink

30 yellow

Try running the numbers yourself to get the solution if you like:

Gamble A: 30 good vs 60 bad balls

Gamble B: 30 good vs 60 bad balls

Gamble C: 60 good vs 30 bad balls

Gamble D: 60 good vs 30 bad balls

This makes all bundles (A&C, A&D, B&C, B&D) equally attractive to an economist, who is then forced pick arbitrarily (or with a coin toss) to ease the pain of such profound indifference.

Rational case #2: What the fan of pink does

If you can’t muster total apathy and you like the act of picking a racehorse just for the fun of it, let’s suppose you like the color pink and you want to bet on there being more pink than yellow. (You’ll get a similar answer if you’re afflicted with the bias that I’d put more pink than yellow in the bucket for some strange reason.)

We’ll imagine the most gentle case—instead of equal yellow-pink odds, let’s start there and make a single yellow ball switch allegiances instead. The balance is now tipped minimally in favor of pink.

In your head at the moment:

30 white

31 pink

29 yellow

Gamble A: 30 good vs 60 bad balls

Gamble B: 29 good vs 61 bad balls

Gamble C: 61 good vs 29 bad balls

Gamble D: 60 good vs 30 bad balls

Since you’ll be playing Game 1 (A vs B) OR Game 2 (C vs D) but not both, there’s no benefit to trying to hedge your bets. If you want the $100, you’d be after gambles more good balls than bad. That means your only reasonable combo would be A&C.

Now try the extreme all-pink case:

30 white

60 pink

0 yellow

Gamble A: 30 good vs 60 bad balls

Gamble B: 0 good vs 90 bad balls

Gamble C: 90 good vs 0 bad balls

Gamble D: 60 good vs 30 bad balls

Again, it’s got to be A&C. Anything else makes no sense!

Rational case #3: What the hater of pink does

If you want to bet on yellow or you’re convinced of my fondness for little duckies, the logic is the same, but now the only answer that makes sense is B&D.

Most people picked irrational option A&D… WAT?!

Despite all that, on the Twitter poll, we got nearly 65% of folks picking irrational option A&D. The choice of those two together makes no sense by the logic above.

This screenshot was taken before the one above, at the time before explanations and spoilers were available, so these are the results we got when you couldn’t sneak a peek at the answers before voting. I’m heartened to see that things didn’t become unglued when cheating became an option.

A&D are inconsistent options (if A is better than B, then C should be better than D… in fact, the two sets are almost the same gamble — all we did was move the same pink balls over the line to the other side):

Gamble A: W good, rest bad

Gamble B: Y good, rest bad

Gamble C: W+P good, rest bad

Gamble D: Y+P good, rest bad

That symmetry is gorgeous… If you bet on yellow in gamble B, rationality suggests you should still do it in gamble D. But people don’t. Despite the fact that A&D wouldn’t make sense together if you were working with the same bucket twice, despite the fact that the choice is irrational and inconsistent, people still like it best.

The Ellsberg Paradox

Yes, indeedy, it’s a paradox! In fact, it’s called the Ellsberg Paradox, named for the decision theorist Daniel Ellsberg who came up with the little game we just played while he was working on his 1962 PhD thesis at Harvard — a fact that his Wikipedia page barely mentions in its excitement over his other claim to fame: leaking the Pentagon Papers (yes, the ones you’ve heard of in the context of Nixon, the Watergate scandal, and all that jazz).

Daniel Ellsberg, our man of the Ellsberg Paradox. Allow me my little fantasy that these reporters are dying to learn more about utility theory and rational decision-making. Image: SOURCE.

But what does it mean?

Most economists’ answer to the puzzle involves the introduction of a new concept called “ambiguity aversion” to explain this weird choice pattern.

Let’s look at it like Ellsberg looked at it. What do options A&D have in common? What might have drawn you to them?

If you didn’t collapse the numbers of pink and yellow like we did and kept working with unknowns all the way through, here’s what’s in your head:

30 white

?? pink

?? yellow

Try running the numbers yourself to get the solution if you like:

Gamble A: 30 good vs 60 bad balls

Gamble B: ?? good vs ?? bad balls

Gamble C: ?? good vs ?? bad balls

Gamble D: 60 good vs 30 bad balls

Here be dragons! It’s the great unknown!! Oh no. Let’s stick with something safe and pick the known gambles over the ones where the state of the universe is ambiguous…

“Risk” means the probabilities are known. “Ambiguity” means they’re not.

What is ambiguity aversion?

To put the theory simply: ambiguity hurts the human animal. We don’t like it. We’re willing to make suboptimal choices to avoid it.

When things are risky, it’s not so bad. Ambiguity scares us much more.

Risk is something we can handle if we’re not too grumpy about making a probability calculation or two, but ambiguity really gets to us. Ambiguity causes a different discomfort than risk. A more primal one. (“What?!” I hear you asking, “Aren’t risk, uncertainty, and ambiguity the same thing? Aren’t they synonyms?” Mais non, not if you’re a behavioral economist.)

In basic terms:

“Risk” refers to decisions where you know the probabilities explicitly. (A and D)

refers to decisions where you know the probabilities explicitly. (A and D) “Ambiguity” refers to decisions where you have no idea and your decision-making gets stuck. (B and C)

refers to decisions where you have no idea and your decision-making gets stuck. (B and C) “Uncertainty” — could refer to either, depending on which field and decade you grew up in.*

When you plug in the concept of ambiguity aversion into the behavioral paradox, everything makes perfect sense. If you picked A&D, the popular theory proposed by Ellsberg in the 60s and refined by economists since then says that it was those question marks that stopped you in your tracks.

Psychologists think humans are hardwired to find ambiguity painful.

You didn’t want to deal with the pain of that ambiguity, so you picked the options that seemed easier to get a handle on… even if they were irrational and inconsistent. That’s a pretty good adaptation for a creature that lives on the savannah — be careful with that dark patch of tall grass… you never know what’s lurking there, so it might be best to avoid it. If possible, stick to the sunny patches where you can at least see what gambles you’re getting into.

Relevance to COVID-19

Perhaps you see where I’m going with this and why I was inspired to scribble about this specific topic at a time when all the headlines are pandemic-flavored… One of the emotional drains of the early days of COVID-19 is all the ambiguity we suddenly have to navigate. If you believe the economic theory about ambiguity aversion and you’re feeling down because so little is known about the new dangers the media warns you about daily, you have an explanation for at least one of the lurking stressors eating at you.

Watch this space for a link to an article I’m working on about what you can do to cope better with your ambiguity aversion, but for now, I hope you get a little bit of comfort from understanding that it’s quite normal to feel extra pain from ambiguity. At the very least, if here must be dragons, they now have a name.

For my decision-making guide to COVID-19, see this link.

If here must be dragons, at least now they have a name.

Part 3 is for the excessively interested

If you want to get into the academic nitty gritty of what the word uncertainty means, here’s the link.

[P.S. For those who *must* know which color I prefer after all, here’s the reveal… This cat is indifferent between pink and yellow. Ha! That’s why I picked them for the example. Ellsberg’s original used the colors black (my favorite), white, and red.]

About those comments

After this article was published, I received several grumpy comments from people doggedly defending their choice behavior. (Calling mathy folk irrational is akin to kicking a hornet’s nest.) You can find my responses here.","           Advertisement                  Search         Log in              Search SpringerLink     Search                     Published: 07 December 2011   Ambiguity aversion and familiarity bias: Evidence from behavioral and gene association studies  Soo Hong Chew 1 , 2 , Richard P. Ebstein 3 , 4 & Songfa Zhong 5    Journal of Risk and Uncertainty  volume 44 , pages 1 – 18 ( 2012 ) Cite this article        1268 Accesses    25 Citations    Metrics details          Abstract It is increasingly recognized that decision making under uncertainty depends not only on probabilities, but also on psychological factors such as ambiguity and familiarity. Using 325 Beijing subjects, we conduct a neurogenetic study of ambiguity aversion and familiarity bias in an incentivized laboratory setting. For ambiguity aversion, 49.4% of the subjects choose to bet on the 50–50 deck despite the unknown deck paying 20% more. For familiarity bias, 39.6% choose the bet on Beijing’s temperature rather than the corresponding bet with Tokyo even though the latter pays 20% more. We genotype subjects for anxiety-related candidate genes and find a serotonin transporter polymorphism being associated with familiarity bias, but not ambiguity aversion, while the dopamine D5 receptor gene and estrogen receptor beta gene are associated with ambiguity aversion only among female subjects. Our findings contribute to understanding of decision making under uncertainty beyond revealed preference.   This is a preview of subscription content, log in to check access.     Access options   Buy single article    Instant access to the full article PDF.    42,99 €  Price includes VAT for Poland                  Subscribe to journal    Immediate online access to all issues from 2019. Subscription will auto renew annually.    71,04 €  Price includes VAT for Poland                Rent this article via DeepDyve.         Learn more about Institutional subscriptions         Fig 1 Fig 2 Fig. 3   References Abdellaoui, M., Baillon, A., Placido, L., & Wakker, P. P. (2011). The rich domain of uncertainty: Source functions and their experimental implementation. American Economic Review, 101 (2), 695–723. Article  Google Scholar  Benjamin, D. J., Chabris, C. F., Glaeser, E., Gudnason, V., Harris, T. B., Laibson, D., Lenore, J. L., & Purcell, S. (2007). Genoeconomics. In J. Milner, B. Elaine, C. Trujillo, M. Kaefer, & S. Ross (Eds.), Biosocial surveys . Washington: The National Academies. Google Scholar  Canli, T., & Lesch, K. P. (2007). Long story short: The serotonin transporter in emotion regulation and social cognition. Nature Neuroscience, 10 (9), 1103–1109. Article  Google Scholar  Carpenter, J., Garcia, J., & Lum, J. (2011). Dopamine receptor genes predict risk preferences, time preferences, and related economic choices. Journal of Risk and Uncertainty, 42 (3), 233–261. Article  Google Scholar  Cesarini, D., Dawes, C. T., Johannesson, M., Lichtenstein, P., & Wallace, B. (2009). Genetic variation in preferences for giving and risk taking. Quarterly Journal of Economics, 124 (2), 809–842. Article  Google Scholar  Chew, S. H. (1983). A generalization of the quasilinear mean with applications to the measurement of income inequality and decision theory resolving the Allais paradox. Econometrica, 51 (4), 1065–1092. Article  Google Scholar  Chew, S. H. (1989). Axiomatic utility theories with the betweenness property. Annals of Operations Research 19 (1), 273–298. Google Scholar  Chew, S. H., & Sagi, J. S. (2006). Event exchangeability: Probabilistic sophistication without continuity or monotonicity. Econometrica, 74 (3), 771–786. Article  Google Scholar  Chew, S. H., & Sagi, J. S. (2008). Small worlds: Modeling attitudes toward sources of uncertainty. Journal of Economic Theory, 139 (1), 1–24. Article  Google Scholar  Chew, S. H., Li, K. K., Chark, R., & Zhong, S. (2008). Source preference and ambiguity aversion: Models and evidence from behavioral and neuroimaging experiments. In D. Houser, K. McCabe (Eds.), Neuroeconomics . Emerald. Chew, S. H., Li, K. K., Chark, R., & Zhong, S. (2010). Familiarity bias: Evidence from laboratory and field experiments. Working paper. Cloninger, C. R. (1986). A unified biosocial theory of personality and its role in the development of anxiety states. Psychiatric Developments, 4 (3), 167–226. Google Scholar  Comings, D. E., Muhleman, D., Johnson, P., & MacMurray, J. P. (1999). Potential role of the estrogen receptor gene(ESR 1) in anxiety. Molecular Psychiatry, 4 (4), 374–377. Article  Google Scholar  Crisan, L. G., Pana, S., Vulturar, R., Heilman, R. M., Szekely, R., Druga, B., Dragos, N., & Miu, A. C. (2009). Genetic contributions of the serotonin transporter to social learning of fear and economic decision making. Social Cognitive and Affective Neuroscience, 4 (4), 399–408. Article  Google Scholar  Croson, R., & Gneezy, U. (2009). Gender differences in preferences. Journal of Economic Literature, 47 (2), 1–27. Article  Google Scholar  Dekel, E. (1986). An axiomatic characterization of preferences under uncertainty: Weakening the independence axiom. Journal of Economic Theory, 40 (2), 304–318. Article  Google Scholar  Ding, Y. C., Chi, H. C. et al. (2002). Evidence of positive selection acting at the human dopamine receptor D4 gene locus. Proceedings of the National Academy of Sciences of the United States of America 99 (1), 309–314. Google Scholar  Dreber, A., Apicella, C. L., Eisenberg, D. T. A., Garcia, J. R., Zamore, R. S., Lum, J. K., & Campbell, B. (2009). The 7R polymorphism in the dopamine receptor D4 gene (DRD4) is associated with financial risk taking in men. Evolution and Human Behavior, 30 (30), 85–92. Article  Google Scholar  Dreber, A., Rand, D. G., Garcia, J. R., Wernerfelt, N., Lum, J. K., & Zeckhauser, R. (2011). Dopamine and risk preferences in different domains. Journal of Risk and Uncertainty, 43 (1), 19–38. Article  Google Scholar  Ellsberg, D. (1961). Risk, ambiguity, and the savage axioms. Quarterly Journal of Economics, 75 (4), 643–669. Article  Google Scholar  Ergin, H., & Gul, F. (2009). A subjective theory of compound lotteries. Journal of Economic Theory, 144 , 899–929. Article  Google Scholar  Fox, C. R., & Tversky, A. (1995). Ambiguity aversion and comparative ignorance. Quarterly Journal of Economics, 110 (3), 585–603. Article  Google Scholar  Frydman, C., Camerer, C., Bossaerts, P., & Rangel, A. (2011). MAOA-L carriers are better at making optimal financial decisions under risk. Proceedings of the Royal Society B: Biological Sciences, 278 (1714), 2053–2059. Article  Google Scholar  Geng, Y. G., Su, Q. R., Su, L. Y., Chen, Q., & Ren, G. Y. (2007). Comparison of the polymorphisms of androgen receptor gene and estrogen alpha and beta gene between adolescent females with first-onset major depressive disorder and control. International Journal of Neuroscience, 117 (4), 539–547. Article  Google Scholar  Gilboa, I., & Schmeidler, D. (1989). Maximin expected utility with a non-unique prior. Journal of Mathematical Economics, 18 , 141–153. Article  Google Scholar  Green, J. R., & Jullien, B. (1988). Ordinal independence in nonlinear utility theory. Journal of Risk and Uncertainty, 1 (4), 355–387. Article  Google Scholar  Hariri, A. R., Mattay, V. S., Tessitore, A., Kolachana, B., Fera, F., Goldman, D., Egan, M. F., & Weinberger, D. R. (2002). Serotonin transporter genetic variation and the response of the human amygdala. Science, 297 (5580), 400–403. Article  Google Scholar  Heath, C., & Tversky, A. (1991). Preference and belief: Ambiguity and competence in choice under uncertainty. Journal of Risk and Uncertainty, 4 (1), 5–28. Article  Google Scholar  Hsu, M., Bhatt, M., Adolphs, R., Tranel, D., & Camerer, C. F. (2005). Neural systems responding to degrees of uncertainty in human decision-making. Science, 310 , 1680–1683. Article  Google Scholar  Huberman, G. (2001). Familiarity breeds investment. The Review of Financial Studies, 14 (3), 659–680. Article  Google Scholar  Huettel, S. A., Stowe, C. J., Gordon, E. M., Warner, B. T., & Platt, M. L. (2006). Neural signatures of economic preferences for risk and ambiguity. Neuron, 49 (5), 765–775. Article  Google Scholar  Imwalle, D. B., Gustafsson, J. A., & Rissman, E. F. (2005). Lack of functional estrogen receptor influences anxiety behavior and serotonin content in female mice. Physiology and Behavior, 84 (1), 157–163. Article  Google Scholar  Jakobsdottir, J., Gorin, M. B., Conley, Y. P., Ferrell, R. E., & Weeks, D. E. (2009). Interpretation of genetic association studies: Markers with replicated highly significant odds ratios may be poor classifiers. PLoS Genetics, 5 (2), e1000337. Article  Google Scholar  Keynes, J. M. (1921). A treatise on probability . New York: Macmillan and Co., limited. Google Scholar  Kuhnen, C. M., & Chiao, J. Y. (2009). Genetic determinants of financial risk taking. PLoS One, 4 (2), e4362. Article  Google Scholar  Lesch, K. P., Bengel, D., Heils, A., Sabol, S. Z., Greenberg, B. D., Petri, S., Benjamin, J., Muller, C. R., Hamer, D. H., & Murphy, D. L. (1996). Association of anxiety-related traits with a polymorphism in the serotonin transporter gene regulatory region. Science, 274 (5292), 1527–1531. Article  Google Scholar  Li, D., Sham, P. C., Owen, M. J., & He, L. (2006). Meta-analysis shows significant association between dopamine system genes and attention deficit hyperactivity disorder (ADHD). Human Molecular Genetics, 15 (14), 2276–2284. Article  Google Scholar  Lowe, N., Kirley, A., Hawi, Z., Sham, P., Wickham, H., Kratochvil, C. J., Smith, S. D., Lee, S. Y., Levy, F., & Kent, L. (2004). Joint analysis of the DRD5 marker concludes association with attention-deficit/hyperactivity disorder confined to the predominantly inattentive and combined subtypes. The American Journal of Human Genetics, 74 (2), 348–356. Article  Google Scholar  Lund, T. D., Rovis, T., Chung, W. C. J., & Handa, R. J. (2005). Novel actions of estrogen receptor-{beta} on anxiety-related behaviors. Endocrinology, 146 (2), 797–807. Article  Google Scholar  Machina, M. J. (2004). Almost-objective uncertainty. Economic Theory, 24 (1), 1–54. Article  Google Scholar  Machina, M. J., & Schmeidler, D. (1992). A more robust definition of subjective probability. Econometrica, 60 (4), 745–780. Article  Google Scholar  McGough, J. J. (2005). Attention-deficit/hyperactivity disorder pharmacogenomics. Biological Psychiatry, 57 (11), 1367–1373. Article  Google Scholar  McIntyre, M. H., Kantoff, P. W., Stampfer, M. J., Mucci, L. A., Parslow, D., Li, H., Gaziano, J. M., Abe, M., & Ma, J. (2007). Prostate cancer risk and ESR1 TA, ESR2 CA repeat polymorphisms. Cancer Epidemiology, Biomarkers & Prevention, 16 (11), 2233. Article  Google Scholar  Munaf, M. R., Yalcin, B., Willis-Owen, S. A., & Flint, J. (2008). Association of the dopamine D4 receptor (DRD4) gene and approach-related personality traits: Meta-analysis and new data. Biological Psychiatry, 63 , 197–206. Article  Google Scholar  Neale, M. C., & Cardon, L. R. (1992). Methodology for genetic studies of twins and families . Netherlands: Kluwer Academic Publishers. Google Scholar  Quiggin, J. (1982). A theory of anticipated utility. Journal of Economic Behavior and Organization, 3 (4), 323–343. Article  Google Scholar  Ramsey, F. P. (1926). Truth and probability. Studies in Subjective Probability , 61–92. Roe, B. E., Tilley, M. R., Gu, H. H., Beversdorf, D. Q., Sadee, W., Haab, T. C., Papp, A. C. (2009). Financial and psychological risk attitudes associated with two single nucleotide polymorphisms in the nicotine receptor (CHRNA4) gene. PLoS ONE , e6704. Roiser, J. P., de Martino, B., Tan, G. C. Y., Kumaran, D., Seymour, B., Wood, N. W., & Dolan, R. J. (2009). A genetically mediated bias in decision making driven by failure of amygdala control. Journal of Neuroscience, 29 (18), 5985–5991. Article  Google Scholar  Savage, L. J. (1954). The foundations of statistics . New York: Wiley. Google Scholar  Schmeidler, D. (1989). Subjective probability and expected utility without additivity. Econometrica, 57 (3), 571–587. Article  Google Scholar  Takeo, C., Negishi, E., Nakajima, A., Ueno, K., Tatsuno, I., Saito, Y., Amano, K., & Hirai, A. (2005). Association of cytosine-adenine repeat polymorphism of the estrogen receptor-gene with menopausal symptoms. Gender Medicine, 2 (2), 96–105. Article  Google Scholar  Tversky, A., & Kahneman, D. (1992). Advances in prospect theory: Cumulative representation of uncertainty. Journal of Risk and Uncertainty, 5 (4), 297–323. Article  Google Scholar  Vanyukov, M. M., Moss, H. B., Kaplan, B. B., Kirillova, G. P., & Tarter, R. E. (2000). Brief research communication antisociality, substance dependence, and the DRD5 gene: A preliminary study. American Journal of Medical Genetics (Neuropsychiatric Genetics), 96 , 654–658. Article  Google Scholar  von Neumann, J., & Morgenstern, O. (1944). Theory of games and economic behavior . New York: Princeton University Press. Google Scholar  Westberg, L., Melke, J., Landen, M., Nilsson, S., Baghaei, F., Rosmond, R., Jansson, M., Holm, G., Bjntorp, P., & Eriksson, E. (2003). Association between a dinucleotide repeat polymorphism of the estrogen receptor alpha gene and personality traits in women. Molecular Psychiatry, 8 (1), 118–122. Article  Google Scholar  Zhong, S., Chew, S. H., Set, E., Zhang, J., Xue, H., Sham, P. C., Ebstein, R. P., & Israel, S. (2009a). The heritability of attitude toward economic risk. Twin Research and Human Genetics, 12 (1), 103–107. Article  Google Scholar  Zhong, S., Israel, S., Xue, H., Sham, P. C., Ebstein, R. P., & Chew, S. H. (2009b). A neurochemical approach to valuation sensitivity over gains and losses. Proceedings of the Royal Society B: Biological Sciences, 276 , 4181–4188. Article  Google Scholar  Zhong, S., Salomon, I., Xue, H., Ebstein, R. P., & Chew, S. H. (2009c). Monoamine oxidase A gene (MAOA) associated with attitude towards longshot risks. PLoS One, 4 (12), e8516. Article  Google Scholar  Zion, I. Z. B., Tessler, R., Cohen, L., Lerer, E., Raz, Y., Bachner-Melman, R., Gritsenko, I., Nemanov, L., Zohar, A. H., & Belmaker, R. H. (2006). Polymorphisms in the dopamine D4 receptor gene (DRD4) contribute to individual differences in human sexual behavior: Desire, arousal and sexual function. Molecular Psychiatry, 11 (8), 782–786. Article  Google Scholar  Download references Acknowledgements We thank Wang Rui, Wu Qingyu, and Ye Qiaofeng for assistance in conducting the behavioral experiments, and Idan Shalev for doing genotyping. Financial support from the Hong Kong University of Science and Technology, as well as National University of Singapore, is gratefully acknowledged. Author information Affiliations Department of Economics and Department of Finance, National University of Singapore, Singapore, Singapore Soo Hong Chew Center for Experimental Business Research and Department of Economics, Hong Kong University of Science and Technology, Hong Kong, Hong Kong Soo Hong Chew Department of Psychology, National University of Singapore, Singapore, Singapore Richard P. Ebstein Scheinfeld Center of Human Genetics for Social Sciences, Hebrew University, Jerusalem, Israel Richard P. Ebstein Department of Economics, National University of Singapore, Singapore, Singapore Songfa Zhong Authors Soo Hong Chew View author publications You can also search for this author in PubMed  Google Scholar Richard P. Ebstein View author publications You can also search for this author in PubMed  Google Scholar Songfa Zhong View author publications You can also search for this author in PubMed  Google Scholar Corresponding author Correspondence to Soo Hong Chew . Appendix I. Genotyping method Appendix I. Genotyping method The polymorphism for the SLC6A4 44 bp deletion/insertion (5-HTTLPR) in the promoter region was characterized using PCR amplification procedure with the following primers: F5′-GGCGTTGCCGCTCTGAATTGC-3′, R5′-GAGGGACTGAGCTGGACAACC-3′. PCR reactions were performed using 5 μl Master Mix (Thermo scientific), 2 μl primers (0.5 μM), 0.6 μl Mg/Cl2 (2.5 mM), 0.4 μl DMSO 5% and 1 μl of water to total of 9 μl total volume and an additional 1 μl of genomic DNA was added to the mixture. All PCR reactions were employed on a Biometra T1 Thermocycler (Biometra, Güttingem, Germany). PCR reaction conditions were as follows: preheating step at 94.0°C for 5 min, 34 cycles of denaturation at 94.0°C for 30 s, reannealing at 55°C for 30 s and extension at 72°C for 90 s. The reaction proceeded to a hold at 72°C for 5 min. All reaction mixtures were electrophoresed on a 3% agarose gel (AMRESCO) with ethidium bromide to screen for genotype. Amplification of the DRD5, ESR1 and ESR2 microsatellites was achieved using the following primers: DRD5: forward: 5′- CGTGTATGATCCCTGCAG -3′; reverse: 5′- GCTCATGAGAAGAATGGAGTG -3′; ESR1 (corresponds to the TA dinucleotide repeat in the 5′ promoter region): forward 5′- AGACGCATGATATACTTCACC -3′; reverse 5′- GTTCACTTGGGCTAGGATAT -3′. ESR2 (corresponds to the CA dinucleotide repeat in intron 5): forward (fluorescent) 5′- GGTAAACCATGGTCTGTACC -3′; reverse 5′- AACAAAATGTTGAATGAGTGGG -3′. PCR reactions were performed using 5 μl Master Mix (Thermo scientific), 0.5 μl primers (0.5 μM), 0.4 μl Mg/Cl2 (2.5 mM) and 3.1 μl of water to total of 9 μl total volume and an additional 1 μl of genomic DNA was added to the mixture. All PCR reactions were employed on a Biometra T1 Thermocycler (Biometra, Güttingem, Germany). PCR reaction conditions were as follows: preheating step at 95.0°C for 5 min, 30 cycles of denaturation at 95.0°C for 30 s, reannealing at 55°C for 30 s and extension at 72°C for 40 s. The reaction proceeded to a hold at 72°C for 10 min. The PCR product was analyzed on an ABI 310 DNA Analyzer. Rights and permissions Reprints and Permissions About this article Cite this article Chew, S.H., Ebstein, R.P. & Zhong, S. Ambiguity aversion and familiarity bias: Evidence from behavioral and gene association studies. J Risk Uncertain  44, 1–18 (2012). https://doi.org/10.1007/s11166-011-9134-0 Download citation Published : 07 December 2011 Issue Date : February 2012 DOI : https://doi.org/10.1007/s11166-011-9134-0 Keywords Ambiguity aversion Familiarity bias Source dependence Genetics Neuroeconomics JEL Classification C91 D14 D81 D87 G11                 Access options   Buy single article    Instant access to the full article PDF.    42,99 €  Price includes VAT for Poland                  Subscribe to journal    Immediate online access to all issues from 2019. Subscription will auto renew annually.    71,04 €  Price includes VAT for Poland                Rent this article via DeepDyve.         Learn more about Institutional subscriptions            Advertisement                 Over 10 million scientific documents at your fingertips   Switch Edition    Academic Edition    Corporate Edition         Home  Impressum  Legal information  Privacy statement  How we use cookies  Accessibility  Contact us     Not logged in  - 89.64.25.55   Not affiliated    Springer Nature       © 2020 Springer Nature Switzerland AG. Part of Springer Nature .                      "
17,(base rate fallacy or base rate neglect)(fallacy OR bias OR heuristic) (coronavirus OR covid OR pandemic),1,https://en.wikipedia.org/wiki/Base_rate_fallacy,"Statistical formal fallacy

The base rate fallacy, also called base rate neglect or base rate bias, is a fallacy. If presented with related base rate information (i.e. generic, general information) and specific information (information pertaining only to a certain case), the mind tends to ignore the former and focus on the latter.[1]

Base rate neglect is a specific form of the more general extension neglect.

False positive paradox [ edit ]

An example of the base rate fallacy is how surprised people are by the false positive paradox, situations where there are more false positive test results than true positives. For example, it might be that of 1,000 people tested for AIDS, 50 of them test positive for having it, but that is due to 10 truly having it and 40 mistaken test results, because only 10 people of those tested actually have AIDS but the test sometimes gives false results. The probability of a positive test result is determined not only by the accuracy of the test but by the characteristics of the sampled population.[2] When the prevalence, the proportion of those who have a given condition, is lower than the test's false positive rate, even tests that have a very low chance of giving a false positive in an individual case will give more false than true positives overall.[3] The paradox surprises most people.[4]

It is especially counter-intuitive when interpreting a positive result in a test on a low-prevalence population after having dealt with positive results drawn from a high-prevalence population.[3] If the false positive rate of the test is higher than the proportion of the new population with the condition, then a test administrator whose experience has been drawn from testing in a high-prevalence population may conclude from experience that a positive test result usually indicates a positive subject, when in fact a false positive is far more likely to have occurred.

Examples [ edit ]

Example 1: Disease [ edit ]

High-incidence population [ edit ]

Number

of people Infected Uninfected Total Test

positive 400

(true positive) 30

(false positive) 430 Test

negative 0

(false negative) 570

(true negative) 570 Total 400 600 1000

Imagine running an infectious disease test on a population A of 1000 persons, in which 40% are infected. The test has a false positive rate of 5% (0.05) and no false negative rate. The expected outcome of the 1000 tests on population A would be:

Infected and test indicates disease (true positive) 1000 × 40 / 100 = 400 people would receive a true positive Uninfected and test indicates disease (false positive) 1000 × 100 – 40 / 100 × 0.05 = 30 people would receive a false positive The remaining 570 tests are correctly negative.

So, in population A, a person receiving a positive test could be over 93% confident (400/30 + 400) that it correctly indicates infection.

Low-incidence population [ edit ]

Number

of people Infected Uninfected Total Test

positive 20

(true positive) 49

(false positive) 69 Test

negative 0

(false negative) 931

(true negative) 931 Total 20 980 1000

Now consider the same test applied to population B, in which only 2% is infected. The expected outcome of 1000 tests on population B would be:

Infected and test indicates disease (true positive) 1000 × 2 / 100 = 20 people would receive a true positive Uninfected and test indicates disease (false positive) 1000 × 100 – 2 / 100 × 0.05 = 49 people would receive a false positive The remaining 931 (= 1000 - (49 + 20)) tests are correctly negative.

In population B, only 20 of the 69 total people with a positive test result are actually infected. So, the probability of actually being infected after one is told that one is infected is only 29% (20/20 + 49) for a test that otherwise appears to be ""95% accurate"".

A tester with experience of group A might find it a paradox that in group B, a result that had usually correctly indicated infection is now usually a false positive. The confusion of the posterior probability of infection with the prior probability of receiving a false positive is a natural error after receiving a health-threatening test result.

Example 2: Drunk drivers [ edit ]

A group of police officers have breathalyzers displaying false drunkenness in 5% of the cases in which the driver is sober. However, the breathalyzers never fail to detect a truly drunk person. One in a thousand drivers is driving drunk. Suppose the police officers then stop a driver at random to administer a breathalyzer test. It indicates that the driver is drunk. We assume you don't know anything else about them. How high is the probability they really are drunk?

Many would answer as high as 95%, but the correct probability is about 2%.

An explanation for this is as follows: on average, for every 1,000 drivers tested,

1 driver is drunk, and it is 100% certain that for that driver there is a true positive test result, so there is 1 true positive test result

positive test result, so there is 1 positive test result 999 drivers are not drunk, and among those drivers there are 5% false positive test results, so there are 49.95 false positive test results

Therefore, the probability that one of the drivers among the 1 + 49.95 = 50.95 positive test results really is drunk is 1 / 50.95 ≈ 0.019627 {\displaystyle 1/50.95\approx 0.019627} .

The validity of this result does, however, hinge on the validity of the initial assumption that the police officer stopped the driver truly at random, and not because of bad driving. If that or another non-arbitrary reason for stopping the driver was present, then the calculation also involves the probability of a drunk driver driving competently and a non-drunk driver driving (in-)competently.

More formally, the same probability of roughly 0.02 can be established using Bayes's theorem. The goal is to find the probability that the driver is drunk given that the breathalyzer indicated they are drunk, which can be represented as

p ( d r u n k ∣ D ) {\displaystyle p(\mathrm {drunk} \mid D)}

where D means that the breathalyzer indicates that the driver is drunk. Bayes's theorem tells us that

p ( d r u n k ∣ D ) = p ( D ∣ d r u n k ) p ( d r u n k ) p ( D ) . {\displaystyle p(\mathrm {drunk} \mid D)={\frac {p(D\mid \mathrm {drunk} )\,p(\mathrm {drunk} )}{p(D)}}.}

We were told the following in the first paragraph:

p ( d r u n k ) = 0.001 , {\displaystyle p(\mathrm {drunk} )=0.001,} p ( s o b e r ) = 0.999 , {\displaystyle p(\mathrm {sober} )=0.999,} p ( D ∣ d r u n k ) = 1.00 , {\displaystyle p(D\mid \mathrm {drunk} )=1.00,} p ( D ∣ s o b e r ) = 0.05. {\displaystyle p(D\mid \mathrm {sober} )=0.05.}

As you can see from the formula, one needs p(D) for Bayes' theorem, which one can compute from the preceding values using the law of total probability:

p ( D ) = p ( D ∣ d r u n k ) p ( d r u n k ) + p ( D ∣ s o b e r ) p ( s o b e r ) {\displaystyle p(D)=p(D\mid \mathrm {drunk} )\,p(\mathrm {drunk} )+p(D\mid \mathrm {sober} )\,p(\mathrm {sober} )}

which gives

p ( D ) = ( 1.00 × 0.001 ) + ( 0.05 × 0.999 ) = 0.05095. {\displaystyle p(D)=(1.00\times 0.001)+(0.05\times 0.999)=0.05095.}

Plugging these numbers into Bayes' theorem, one finds that

p ( d r u n k ∣ D ) = 1.00 × 0.001 0.05095 = 0.019627. {\displaystyle p(\mathrm {drunk} \mid D)={\frac {1.00\times 0.001}{0.05095}}=0.019627.}

Example 3: Terrorist identification [ edit ]

In a city of 1 million inhabitants let there be 100 terrorists and 999,900 non-terrorists. To simplify the example, it is assumed that all people present in the city are inhabitants. Thus, the base rate probability of a randomly selected inhabitant of the city being a terrorist is 0.0001, and the base rate probability of that same inhabitant being a non-terrorist is 0.9999. In an attempt to catch the terrorists, the city installs an alarm system with a surveillance camera and automatic facial recognition software.

The software has two failure rates of 1%:

The false negative rate: If the camera scans a terrorist, a bell will ring 99% of the time, and it will fail to ring 1% of the time.

The false positive rate: If the camera scans a non-terrorist, a bell will not ring 99% of the time, but it will ring 1% of the time.

Suppose now that an inhabitant triggers the alarm. What is the chance that the person is a terrorist? In other words, what is P(T | B), the probability that a terrorist has been detected given the ringing of the bell? Someone making the 'base rate fallacy' would infer that there is a 99% chance that the detected person is a terrorist. Although the inference seems to make sense, it is actually bad reasoning, and a calculation below will show that the chances they are a terrorist are actually near 1%, not near 99%.

The fallacy arises from confusing the natures of two different failure rates. The 'number of non-bells per 100 terrorists' and the 'number of non-terrorists per 100 bells' are unrelated quantities. One does not necessarily equal the other, and they don't even have to be almost equal. To show this, consider what happens if an identical alarm system were set up in a second city with no terrorists at all. As in the first city, the alarm sounds for 1 out of every 100 non-terrorist inhabitants detected, but unlike in the first city, the alarm never sounds for a terrorist. Therefore, 100% of all occasions of the alarm sounding are for non-terrorists, but a false negative rate cannot even be calculated. The 'number of non-terrorists per 100 bells' in that city is 100, yet P(T | B) = 0%. There is zero chance that a terrorist has been detected given the ringing of the bell.

Imagine that the first city's entire population of one million people pass in front of the camera. About 99 of the 100 terrorists will trigger the alarm—and so will about 9,999 of the 999,900 non-terrorists. Therefore, about 10,098 people will trigger the alarm, among which about 99 will be terrorists. So, the probability that a person triggering the alarm actually is a terrorist, is only about 99 in 10,098, which is less than 1%, and very, very far below our initial guess of 99%.

The base rate fallacy is so misleading in this example because there are many more non-terrorists than terrorists, and the number of false positives (non-terrorists scanned as terrorists) is so much larger than the true positives (the real number of terrorists).

Findings in psychology [ edit ]

In experiments, people have been found to prefer individuating information over general information when the former is available.[5][6][7]

In some experiments, students were asked to estimate the grade point averages (GPAs) of hypothetical students. When given relevant statistics about GPA distribution, students tended to ignore them if given descriptive information about the particular student even if the new descriptive information was obviously of little or no relevance to school performance.[6] This finding has been used to argue that interviews are an unnecessary part of the college admissions process because interviewers are unable to pick successful candidates better than basic statistics.

Psychologists Daniel Kahneman and Amos Tversky attempted to explain this finding in terms of a simple rule or ""heuristic"" called representativeness. They argued that many judgments relating to likelihood, or to cause and effect, are based on how representative one thing is of another, or of a category.[6] Kahneman considers base rate neglect to be a specific form of extension neglect.[8] Richard Nisbett has argued that some attributional biases like the fundamental attribution error are instances of the base rate fallacy: people do not use the ""consensus information"" (the ""base rate"") about how others behaved in similar situations and instead prefer simpler dispositional attributions.[9]

There is considerable debate in psychology on the conditions under which people do or do not appreciate base rate information.[10][11] Researchers in the heuristics-and-biases program have stressed empirical findings showing that people tend to ignore base rates and make inferences that violate certain norms of probabilistic reasoning, such as Bayes' theorem. The conclusion drawn from this line of research was that human probabilistic thinking is fundamentally flawed and error-prone.[12] Other researchers have emphasized the link between cognitive processes and information formats, arguing that such conclusions are not generally warranted.[13][14]

Consider again Example 2 from above. The required inference is to estimate the (posterior) probability that a (randomly picked) driver is drunk, given that the breathalyzer test is positive. Formally, this probability can be calculated using Bayes' theorem, as shown above. However, there are different ways of presenting the relevant information. Consider the following, formally equivalent variant of the problem:

1 out of 1000 drivers are driving drunk. The breathalyzers never fail to detect a truly drunk person. For 50 out of the 999 drivers who are not drunk the breathalyzer falsely displays drunkenness. Suppose the policemen then stop a driver at random, and force them to take a breathalyzer test. It indicates that they are drunk. We assume you don't know anything else about them. How high is the probability they really are drunk?

In this case, the relevant numerical information—p(drunk), p(D | drunk), p(D | sober)—is presented in terms of natural frequencies with respect to a certain reference class (see reference class problem). Empirical studies show that people's inferences correspond more closely to Bayes' rule when information is presented this way, helping to overcome base-rate neglect in laypeople[14] and experts.[15] As a consequence, organizations like the Cochrane Collaboration recommend using this kind of format for communicating health statistics.[16] Teaching people to translate these kinds of Bayesian reasoning problems into natural frequency formats is more effective than merely teaching them to plug probabilities (or percentages) into Bayes' theorem.[17] It has also been shown that graphical representations of natural frequencies (e.g., icon arrays) help people to make better inferences.[17][18][19]

Why are natural frequency formats helpful? One important reason is that this information format facilitates the required inference because it simplifies the necessary calculations. This can be seen when using an alternative way of computing the required probability p(drunk|D):

p ( d r u n k ∣ D ) = N ( d r u n k ∩ D ) N ( D ) = 1 51 = 0.0196 {\displaystyle p(\mathrm {drunk} \mid D)={\frac {N(\mathrm {drunk} \cap D)}{N(D)}}={\frac {1}{51}}=0.0196}

where N(drunk ∩ D) denotes the number of drivers that are drunk and get a positive breathalyzer result, and N(D) denotes the total number of cases with a positive breathalyzer result. The equivalence of this equation to the above one follows from the axioms of probability theory, according to which N(drunk ∩ D) = N × p (D | drunk) × p (drunk). Importantly, although this equation is formally equivalent to Bayes' rule, it is not psychologically equivalent. Using natural frequencies simplifies the inference because the required mathematical operation can be performed on natural numbers, instead of normalized fractions (i.e., probabilities), because it makes the high number of false positives more transparent, and because natural frequencies exhibit a ""nested-set structure"".[20][21]

Not every frequency format facilitates Bayesian reasoning.[21][22] Natural frequencies refer to frequency information that results from natural sampling,[23] which preserves base rate information (e.g., number of drunken drivers when taking a random sample of drivers). This is different from systematic sampling, in which base rates are fixed a priori (e.g., in scientific experiments). In the latter case it is not possible to infer the posterior probability p (drunk | positive test) from comparing the number of drivers who are drunk and test positive compared to the total number of people who get a positive breathalyzer result, because base rate information is not preserved and must be explicitly re-introduced using Bayes' theorem.

See also [ edit ]

References [ edit ]","          Base rate fallacy   From Wikipedia, the free encyclopedia    Jump to navigation  Jump to search  Statistical formal fallacy  The base rate fallacy , also called base rate neglect or base rate bias , is a fallacy . If presented with related base rate information (i.e. generic, general information) and specific information (information pertaining only to a certain case), the mind tends to ignore the former and focus on the latter. [1]  Base rate neglect is a specific form of the more general extension neglect .  Contents   1  False positive paradox  2  Examples   2.1  Example 1: Disease   2.1.1  High-incidence population  2.1.2  Low-incidence population    2.2  Example 2: Drunk drivers  2.3  Example 3: Terrorist identification    3  Findings in psychology  4  See also  5  References  6  External links    False positive paradox [ edit ]  An example of the base rate fallacy is how surprised people are by the false positive paradox ,    situations where there are more false positive test results than true positives. For example, it might be that of 1,000 people tested for AIDS, 50 of them test positive for having it, but that is due to 10 truly having it and 40  mistaken test results, because only 10 people of those tested actually have AIDS but the test sometimes gives false results.   The probability of a positive test result is determined not only by the accuracy of the test but by the characteristics of the sampled population. [2] When the prevalence, the proportion of those who have a given condition, is lower than the test's false positive rate, even tests that have a very low chance of giving a false positive in an individual case will give more false than true positives overall . [3] The paradox  surprises  most people. [4]  It is especially counter-intuitive when interpreting a positive result in a test on a low-prevalence population after having dealt with positive results drawn from a high-prevalence population. [3] If the false positive rate of the test is higher than the proportion of the new population with the condition, then a test administrator whose experience has been drawn from testing in a high-prevalence population may conclude from experience that a positive test result usually indicates a positive subject, when in fact a false positive is far more likely to have occurred.  Examples [ edit ]  Example 1: Disease [ edit ]  High-incidence population [ edit ]    Number of people  Infected  Uninfected  Total   Test positive  400 (true positive)  30 (false positive)  430   Test negative  0 (false negative)  570 (true negative)  570   Total  400  600  1000  Imagine running an infectious disease test on a population A of 1000 persons, in which 40% are infected. The test has a false positive rate of 5% (0.05) and no false negative rate. The expected outcome of the 1000 tests on population A would be:  Infected and test indicates disease ( true positive ) 1000 × 40 / 100 = 400 people would receive a true positive  Uninfected and test indicates disease (false positive) 1000 × 100 – 40 / 100 × 0.05 = 30 people would receive a false positive  The remaining 570 tests are correctly negative.  So, in population A , a person receiving a positive test could be over 93% confident ( 400 / 30 + 400 ) that it correctly indicates infection.  Low-incidence population [ edit ]    Number of people  Infected  Uninfected  Total   Test positive  20 (true positive)  49 (false positive)  69   Test negative  0 (false negative)  931 (true negative)  931   Total  20  980  1000  Now consider the same test applied to population B , in which only 2% is infected. The expected outcome of 1000 tests on population B would be:  Infected and test indicates disease ( true positive ) 1000 × 2 / 100 = 20 people would receive a true positive  Uninfected and test indicates disease (false positive) 1000 × 100 – 2 / 100 × 0.05 = 49 people would receive a false positive  The remaining 931 (= 1000 - (49 + 20)) tests are correctly negative.  In population B , only 20 of the 69 total people with a positive test result are actually infected. So, the probability of actually being infected after one is told that one is infected is only 29% ( 20 / 20 + 49 ) for a test that otherwise appears to be ""95% accurate"". A tester with experience of group A might find it a paradox that in group B , a result that had usually correctly indicated infection is now usually a false positive . The confusion of the posterior probability of infection with the prior probability of receiving a false positive is a natural error after receiving a health-threatening test result.  Example 2: Drunk drivers [ edit ]  A group of police officers have breathalyzers displaying false drunkenness in 5% of the cases in which the driver is sober. However, the breathalyzers never fail to detect a truly drunk person. One in a thousand drivers is driving drunk. Suppose the police officers then stop a driver at random to administer a breathalyzer test. It indicates that the driver is drunk. We assume you don't know anything else about them. How high is the probability they really are drunk?  Many would answer as high as 95%, but the correct probability is about 2%. An explanation for this is as follows: on average, for every 1,000 drivers tested,  1 driver is drunk, and it is 100% certain that for that driver there is a true positive test result, so there is 1 true positive test result  999 drivers are not drunk, and among those drivers there are 5% false positive test results, so there are 49.95 false positive test results  Therefore, the probability that one of the drivers among the 1 + 49.95 = 50.95 positive test results really is drunk is     1   /   50.95  ≈  0.019627    {\displaystyle 1/50.95\approx 0.019627}   . The validity of this result does, however, hinge on the validity of the initial assumption that the police officer stopped the driver truly at random, and not because of bad driving. If that or another non-arbitrary reason for stopping the driver was present, then the calculation also involves the probability of a drunk driver driving competently and a non-drunk driver driving (in-)competently. More formally, the same probability of roughly 0.02 can be established using Bayes's theorem . The goal is to find the probability that the driver is drunk given that the breathalyzer indicated they are drunk, which can be represented as      p  (   d  r  u  n  k   ∣  D  )    {\displaystyle p(\mathrm {drunk} \mid D)}    where D means that the breathalyzer indicates that the driver is drunk. Bayes's theorem tells us that      p  (   d  r  u  n  k   ∣  D  )  =     p  (  D  ∣   d  r  u  n  k   )   p  (   d  r  u  n  k   )    p  (  D  )     .    {\displaystyle p(\mathrm {drunk} \mid D)={\frac {p(D\mid \mathrm {drunk} )\,p(\mathrm {drunk} )}{p(D)}}.}    We were told the following in the first paragraph:      p  (   d  r  u  n  k   )  =  0.001  ,    {\displaystyle p(\mathrm {drunk} )=0.001,}        p  (   s  o  b  e  r   )  =  0.999  ,    {\displaystyle p(\mathrm {sober} )=0.999,}        p  (  D  ∣   d  r  u  n  k   )  =  1.00  ,    {\displaystyle p(D\mid \mathrm {drunk} )=1.00,}   and      p  (  D  ∣   s  o  b  e  r   )  =  0.05.    {\displaystyle p(D\mid \mathrm {sober} )=0.05.}    As you can see from the formula, one needs p ( D ) for Bayes' theorem, which one can compute from the preceding values using the law of total probability :      p  (  D  )  =  p  (  D  ∣   d  r  u  n  k   )   p  (   d  r  u  n  k   )  +  p  (  D  ∣   s  o  b  e  r   )   p  (   s  o  b  e  r   )    {\displaystyle p(D)=p(D\mid \mathrm {drunk} )\,p(\mathrm {drunk} )+p(D\mid \mathrm {sober} )\,p(\mathrm {sober} )}    which gives      p  (  D  )  =  (  1.00  ×  0.001  )  +  (  0.05  ×  0.999  )  =  0.05095.    {\displaystyle p(D)=(1.00\times 0.001)+(0.05\times 0.999)=0.05095.}    Plugging these numbers into Bayes' theorem, one finds that      p  (   d  r  u  n  k   ∣  D  )  =     1.00  ×  0.001   0.05095    =  0.019627.    {\displaystyle p(\mathrm {drunk} \mid D)={\frac {1.00\times 0.001}{0.05095}}=0.019627.}    Example 3: Terrorist identification [ edit ]  In a city of 1 million inhabitants let there be 100 terrorists and 999,900 non-terrorists. To simplify the example, it is assumed that all people present in the city are inhabitants. Thus, the base rate probability of a randomly selected inhabitant of the city being a terrorist is 0.0001, and the base rate probability of that same inhabitant being a non-terrorist is 0.9999. In an attempt to catch the terrorists, the city installs an alarm system with a surveillance camera and automatic facial recognition software . The software has two failure rates of 1%:  The false negative rate: If the camera scans a terrorist, a bell will ring 99% of the time, and it will fail to ring 1% of the time.  The false positive rate: If the camera scans a non-terrorist, a bell will not ring 99% of the time, but it will ring 1% of the time.  Suppose now that an inhabitant triggers the alarm. What is the chance that the person is a terrorist? In other words, what is P(T | B), the probability that a terrorist has been detected given the ringing of the bell? Someone making the 'base rate fallacy' would infer that there is a 99% chance that the detected person is a terrorist. Although the inference seems to make sense, it is actually bad reasoning, and a calculation below will show that the chances they are a terrorist are actually near 1%, not near 99%. The fallacy arises from confusing the natures of two different failure rates. The 'number of non-bells per 100 terrorists' and the 'number of non-terrorists per 100 bells' are unrelated quantities. One does not necessarily equal the other, and they don't even have to be almost equal. To show this, consider what happens if an identical alarm system were set up in a second city with no terrorists at all. As in the first city, the alarm sounds for 1 out of every 100 non-terrorist inhabitants detected, but unlike in the first city, the alarm never sounds for a terrorist. Therefore, 100% of all occasions of the alarm sounding are for non-terrorists, but a false negative rate cannot even be calculated. The 'number of non-terrorists per 100 bells' in that city is 100, yet P(T | B) = 0%. There is zero chance that a terrorist has been detected given the ringing of the bell. Imagine that the first city's entire population of one million people pass in front of the camera. About 99 of the 100 terrorists will trigger the alarm—and so will about 9,999 of the 999,900 non-terrorists. Therefore, about 10,098 people will trigger the alarm, among which about 99 will be terrorists. So, the probability that a person triggering the alarm actually is a terrorist, is only about 99 in 10,098, which is less than 1%, and very, very far below our initial guess of 99%. The base rate fallacy is so misleading in this example because there are many more non-terrorists than terrorists, and the number of false positives (non-terrorists scanned as terrorists) is so much larger than the true positives (the real number of terrorists).  Findings in psychology [ edit ]  In experiments, people have been found to prefer individuating information over general information when the former is available. [5] [6] [7]  In some experiments, students were asked to estimate the grade point averages (GPAs) of hypothetical students. When given relevant statistics about GPA distribution, students tended to ignore them if given descriptive information about the particular student even if the new descriptive information was obviously of little or no relevance to school performance. [6] This finding has been used to argue that interviews are an unnecessary part of the college admissions process because interviewers are unable to pick successful candidates better than basic statistics. Psychologists  Daniel Kahneman and Amos Tversky attempted to explain this finding in terms of a simple rule or ""heuristic"" called representativeness . They argued that many judgments relating to likelihood, or to cause and effect, are based on how representative one thing is of another, or of a category. [6] Kahneman considers base rate neglect to be a specific form of extension neglect . [8]  Richard Nisbett has argued that some attributional biases like the fundamental attribution error are instances of the base rate fallacy: people do not use the ""consensus information"" (the ""base rate"") about how others behaved in similar situations and instead prefer simpler dispositional attributions . [9]  There is considerable debate in psychology on the conditions under which people do or do not appreciate base rate information. [10] [11] Researchers in the heuristics-and-biases program have stressed empirical findings showing that people tend to ignore base rates and make inferences that violate certain norms of probabilistic reasoning, such as Bayes' theorem . The conclusion drawn from this line of research was that human probabilistic thinking is fundamentally flawed and error-prone. [12] Other researchers have emphasized the link between cognitive processes and information formats, arguing that such conclusions are not generally warranted. [13] [14]  Consider again Example 2 from above. The required inference is to estimate the (posterior) probability that a (randomly picked) driver is drunk, given that the breathalyzer test is positive. Formally, this probability can be calculated using Bayes' theorem , as shown above. However, there are different ways of presenting the relevant information. Consider the following, formally equivalent variant of the problem:  1 out of 1000 drivers are driving drunk. The breathalyzers never fail to detect a truly drunk person. For 50 out of the 999 drivers who are not drunk the breathalyzer falsely displays drunkenness. Suppose the policemen then stop a driver at random, and force them to take a breathalyzer test. It indicates that they are drunk. We assume you don't know anything else about them. How high is the probability they really are drunk?  In this case, the relevant numerical information— p (drunk), p ( D | drunk), p ( D | sober)—is presented in terms of natural frequencies with respect to a certain reference class (see reference class problem ). Empirical studies show that people's inferences correspond more closely to Bayes' rule when information is presented this way, helping to overcome base-rate neglect in laypeople [14] and experts. [15] As a consequence, organizations like the Cochrane Collaboration recommend using this kind of format for communicating health statistics. [16] Teaching people to translate these kinds of Bayesian reasoning problems into natural frequency formats is more effective than merely teaching them to plug probabilities (or percentages) into Bayes' theorem. [17] It has also been shown that graphical representations of natural frequencies (e.g., icon arrays) help people to make better inferences. [17] [18] [19]  Why are natural frequency formats helpful? One important reason is that this information format facilitates the required inference because it simplifies the necessary calculations. This can be seen when using an alternative way of computing the required probability p (drunk| D ):      p  (   d  r  u  n  k   ∣  D  )  =     N  (   d  r  u  n  k   ∩  D  )    N  (  D  )     =    1  51    =  0.0196    {\displaystyle p(\mathrm {drunk} \mid D)={\frac {N(\mathrm {drunk} \cap D)}{N(D)}}={\frac {1}{51}}=0.0196}    where N (drunk ∩ D ) denotes the number of drivers that are drunk and get a positive breathalyzer result, and N ( D ) denotes the total number of cases with a positive breathalyzer result. The equivalence of this equation to the above one follows from the axioms of probability theory, according to which N (drunk ∩ D ) = N × p ( D | drunk) × p (drunk). Importantly, although this equation is formally equivalent to Bayes' rule, it is not psychologically equivalent. Using natural frequencies simplifies the inference because the required mathematical operation can be performed on natural numbers, instead of normalized fractions (i.e., probabilities), because it makes the high number of false positives more transparent, and because natural frequencies exhibit a ""nested-set structure"". [20] [21]  Not every frequency format facilitates Bayesian reasoning. [21] [22] Natural frequencies refer to frequency information that results from natural sampling , [23] which preserves base rate information (e.g., number of drunken drivers when taking a random sample of drivers). This is different from systematic sampling , in which base rates are fixed a priori (e.g., in scientific experiments). In the latter case it is not possible to infer the posterior probability p (drunk | positive test) from comparing the number of drivers who are drunk and test positive compared to the total number of people who get a positive breathalyzer result, because base rate information is not preserved and must be explicitly re-introduced using Bayes' theorem.  See also [ edit ]  Bayesian probability  Bayes' theorem  Data dredging  Inductive argument  List of cognitive biases  List of paradoxes  Misleading vividness  Prevention paradox  Prosecutor's fallacy , a mistake in reasoning that involves ignoring a low prior probability  Simpson's paradox , another error in statistical reasoning dealing with comparing groups  Stereotype  References [ edit ]    ^  ""Logical Fallacy: The Base Rate Fallacy"" . Fallacyfiles.org . Retrieved 2013-06-15 .   ^  Rheinfurth, M. H.; Howell, L. W. (March 1998). Probability and Statistics in Aerospace Engineering  (PDF) . NASA . p. 16. MESSAGE: False positive tests are more probable than true positive tests when the overall population has a low prevalence of the disease. This is called the false-positive paradox.   ^ a  b  Vacher, H. L. (May 2003). ""Quantitative literacy - drug testing, cancer screening, and the identification of igneous rocks"" . Journal of Geoscience Education : 2. At first glance, this seems perverse: the less the students as a whole use steroids , the more likely a student identified as a user will be a non-user. This has been called the False Positive Paradox - Citing: Gonick, L.; Smith, W. (1993). The cartoon guide to statistics . New York: Harper Collins. p. 49.   ^  Madison, B. L. (August 2007). ""Mathematical Proficiency for Citizenship"" .  In Schoenfeld, A. H. (ed.). Assessing Mathematical Proficiency . Mathematical Sciences Research Institute Publications (New ed.). Cambridge University Press. p. 122. ISBN  978-0-521-69766-8 . The correct [probability estimate...] is surprising to many; hence, the term paradox .   ^  Bar-Hillel, Maya (1980). ""The base-rate fallacy in probability judgments"". Acta Psychologica . 44 (3): 211–233. doi : 10.1016/0001-6918(80)90046-3 .   ^ a  b  c  Kahneman, Daniel; Amos Tversky (1973). ""On the psychology of prediction"". Psychological Review . 80 (4): 237–251. doi : 10.1037/h0034747 . S2CID  17786757 .   ^  Kahneman, Daniel; Amos Tversky (1985). ""Evidential impact of base rates"".  In Daniel Kahneman, Paul Slovic & Amos Tversky (ed.). Judgment under uncertainty: Heuristics and biases . pp. 153–160. PMID  17835457 .   ^  Kahneman, Daniel (2000). ""Evaluation by moments, past and future"".  In Daniel Kahneman and Amos Tversky (ed.). Choices, Values and Frames .   ^  Nisbett, Richard E.; E. Borgida; R. Crandall; H. Reed (1976). ""Popular induction: Information is not always informative"".  In J. S. Carroll & J. W. Payne (ed.). Cognition and social behavior . 2 . pp. 227–236.   ^  Koehler, J. J. (2010). ""The base rate fallacy reconsidered: Descriptive, normative, and methodological challenges"". Behavioral and Brain Sciences . 19 : 1–17. doi : 10.1017/S0140525X00041157 . S2CID  53343238 .   ^  Barbey, A. K.; Sloman, S. A. (2007). ""Base-rate respect: From ecological rationality to dual processes"". Behavioral and Brain Sciences . 30 (3): 241–254, discussion 255–297. doi : 10.1017/S0140525X07001653 . PMID  17963533 . S2CID  31741077 .   ^  Tversky, A.; Kahneman, D. (1974). ""Judgment under Uncertainty: Heuristics and Biases"". Science . 185 (4157): 1124–1131. Bibcode : 1974Sci...185.1124T . doi : 10.1126/science.185.4157.1124 . PMID  17835457 .   ^  Cosmides, Leda; John Tooby (1996). ""Are humans good intuitive statisticians after all? Rethinking some conclusions of the literature on judgment under uncertainty"". Cognition . 58 : 1–73. CiteSeerX  10.1.1.131.8290 . doi : 10.1016/0010-0277(95)00664-8 .   ^ a  b  Gigerenzer, G.; Hoffrage, U. (1995). ""How to improve Bayesian reasoning without instruction: Frequency formats"". Psychological Review . 102 (4): 684. CiteSeerX  10.1.1.128.3201 . doi : 10.1037/0033-295X.102.4.684 .   ^  Hoffrage, U.; Lindsey, S.; Hertwig, R.; Gigerenzer, G. (2000). ""Medicine: Communicating Statistical Information"". Science . 290 (5500): 2261–2262. doi : 10.1126/science.290.5500.2261 . PMID  11188724 . S2CID  33050943 .   ^  Akl, E. A.; Oxman, A. D.; Herrin, J.; Vist, G. E.; Terrenato, I.; Sperati, F.; Costiniuk, C.; Blank, D.; Schünemann, H. (2011).  Schünemann, Holger (ed.). ""Using alternative statistical formats for presenting risks and risk reductions"" . The Cochrane Library (3): CD006776. doi : 10.1002/14651858.CD006776.pub2 . PMC  6464912 . PMID  21412897 .   ^ a  b  Sedlmeier, P.; Gigerenzer, G. (2001). ""Teaching Bayesian reasoning in less than two hours"" . Journal of Experimental Psychology: General . 130 (3): 380. doi : 10.1037/0096-3445.130.3.380 . hdl : 11858/00-001M-0000-0025-9504-E .   ^  Brase, G. L. (2009). ""Pictorial representations in statistical reasoning"". Applied Cognitive Psychology . 23 (3): 369–381. doi : 10.1002/acp.1460 . S2CID  18817707 .   ^  Edwards, A.; Elwyn, G.; Mulley, A. (2002). ""Explaining risks: Turning numerical data into meaningful pictures"" . BMJ . 324 (7341): 827–830. doi : 10.1136/bmj.324.7341.827 . PMC  1122766 . PMID  11934777 .   ^  Girotto, V.; Gonzalez, M. (2001). ""Solving probabilistic and statistical problems: A matter of information structure and question form"". Cognition . 78 (3): 247–276. doi : 10.1016/S0010-0277(00)00133-5 . PMID  11124351 .   ^ a  b  Hoffrage, U.; Gigerenzer, G.; Krauss, S.; Martignon, L. (2002). ""Representation facilitates reasoning: What natural frequencies are and what they are not"". Cognition . 84 (3): 343–352. doi : 10.1016/S0010-0277(02)00050-1 . PMID  12044739 .   ^  Gigerenzer, G.; Hoffrage, U. (1999). ""Overcoming difficulties in Bayesian reasoning: A reply to Lewis and Keren (1999) and Mellers and McGraw (1999)"" . Psychological Review . 106 (2): 425. doi : 10.1037/0033-295X.106.2.425 . hdl : 11858/00-001M-0000-0025-9CB4-8 .   ^  Kleiter, G. D. (1994). ""Natural Sampling: Rationality without Base Rates"". Contributions to Mathematical Psychology, Psychometrics, and Methodology . Recent Research in Psychology. pp. 375–388. doi : 10.1007/978-1-4612-4308-3_27 . ISBN  978-0-387-94169-1 .    External links [ edit ]  The Base Rate Fallacy The Fallacy Files  v t e Biases Cognitive biases  Actor–observer  Acquiescence  Ambiguity  Anchoring  Attentional  Attribution  Authority  Automation  Belief  Blind spot  Choice-supportive  Confirmation  Congruence  Cultural  Distinction  Dunning–Kruger  Egocentric  Emotional  Extrinsic incentives  Fading affect  Framing  Correspondence  Halo effect  Hindsight  Horn effect  Hostile attribution  Impact  Implicit  In-group  Mere-exposure effect  Negativity  Normalcy  Omission  Optimism  Out-group homogeneity  Outcome  Overton window  Precision  Present  Pro-innovation  Response  Restraint  Self-serving  Social comparison  Status quo  Time-saving  Trait ascription  von Restorff effect  Zero-risk  In animals  Statistical biases  Estimator  Forecast  Healthy user  Information  Psychological  Lead time  Length time  Non-response  Observer  Omitted-variable  Participation  Recall  Sampling  Selection  Self-selection  Social desirability  Spectrum  Survivorship  Systematic error  Systemic  Verification  Wet  Other biases  Academic  Funding  FUTON  Inductive  Infrastructure  Inherent  In education  Media  False balance  Vietnam War  Norway  South Asia  Sweden  United States  Arab–Israeli conflict  Ukraine  Net  Political bias  Publication  Reporting  White hat  Bias reduction  Cognitive bias mitigation  Debiasing  Heuristics in judgment and decision-making  Lists: General  ·  Memory  v t e Fallacies ( list ) Formal In propositional logic  Affirming a disjunct  Affirming the consequent  Denying the antecedent  Argument from fallacy  In quantificational logic  Existential  Illicit conversion  Proof by example  Quantifier shift  Syllogistic fallacy  Affirmative conclusion from a negative premise  Exclusive premises  Existential  Necessity  Four terms  Illicit major  Illicit minor  Negative conclusion from affirmative premises  Undistributed middle   Masked man  Mathematical fallacy  Informal Equivocation  Equivocation  False equivalence  False attribution  Quoting out of context  Loki's Wager  No true Scotsman  Reification  Question-begging fallacies  Circular reasoning / Begging the question  Loaded language  Leading question  Compound question / Loaded question / Complex question  No true Scotsman  Correlative-based fallacies  False dilemma  Perfect solution  Denying the correlative  Suppressed correlative  Illicit transference  Composition  Division  Ecological  Secundum quid  Accident  Converse accident  Faulty generalization  Anecdotal evidence  Sampling bias  Cherry picking  McNamara  Base rate / Conjunction  Double counting  False analogy  Slothful induction  Overwhelming exception  Vagueness / Ambiguity  Accent  False precision  Moving the goalposts  Quoting out of context  Slippery slope  Sorites paradox  Syntactic ambiguity  Questionable cause  Animistic  Furtive  Correlation implies causation Cum hoc  Post hoc  Gambler's  Inverse  Regression  Single cause  Slippery slope  Texas sharpshooter  Fallacies of relevance Appeals to emotion  Fear  Flattery  Novelty  Pity  Ridicule  Think of the children  In-group favoritism  Invented here / Not invented here  Island mentality  Loyalty  Parade of horribles  Spite  Stirring symbols  Wisdom of repugnance  Genetic fallacies Ad hominem  Appeal to motive  Association  Reductio ad Hitlerum  Godwin's law  Reductio ad Stalinum  Bulverism  Poisoning the well  Tone  Tu quoque  Whataboutism   Authority  Accomplishment  Ipse dixit  Poverty / Wealth  Etymology  Nature  Tradition / Novelty  Chronological snobbery  Appeals to consequences  Argumentum ad baculum  Wishful thinking   Ad nauseam  Argument to moderation  Argumentum ad populum  Appeal to the stone / Proof by assertion  Ignoratio elenchi  Argument from silence  Invincible ignorance  Moralistic / Naturalistic  Motte-and-bailey fallacy  Rationalization  Red herring  Two wrongs make a right  Special pleading  Straw man  Cliché  I'm entitled to my opinion    Category       Retrieved from "" https://en.wikipedia.org/w/index.php?title=Base_rate_fallacy&oldid=963044273 ""  Categories : Relevance fallacies Cognitive biases Behavioral finance Probability fallacies Statistical paradoxes Hidden categories: Articles with short description         Navigation menu      Personal tools      Not logged in Talk Contributions Create account Log in        Namespaces      Article Talk        Variants              Views      Read Edit View history        More           Search                     Navigation      Main page Contents Current events Random article About Wikipedia Contact us Donate Wikipedia store       Contribute      Help Community portal Recent changes Upload file       Tools      What links here Related changes Upload file Special pages Permanent link Page information Wikidata item Cite this page       Print/export      Download as PDF Printable version       Languages      العربية Deutsch Español Français עברית Polski Русский ไทย Türkçe Українська 中文  Edit links        This page was last edited on 17 June 2020, at 13:34 (UTC) .  Text is available under the Creative Commons Attribution-ShareAlike License ;
additional terms may apply.  By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc. , a non-profit organization.    Privacy policy  About Wikipedia  Disclaimers  Contact Wikipedia  Developers  Statistics  Cookie statement  Mobile view          "
17,(base rate fallacy or base rate neglect)(fallacy OR bias OR heuristic) (coronavirus OR covid OR pandemic),2,https://en.wikipedia.org/wiki/Base_rate_fallacy#False_positive_paradox,"Statistical formal fallacy

The base rate fallacy, also called base rate neglect or base rate bias, is a fallacy. If presented with related base rate information (i.e. generic, general information) and specific information (information pertaining only to a certain case), the mind tends to ignore the former and focus on the latter.[1]

Base rate neglect is a specific form of the more general extension neglect.

False positive paradox [ edit ]

An example of the base rate fallacy is how surprised people are by the false positive paradox, situations where there are more false positive test results than true positives. For example, it might be that of 1,000 people tested for AIDS, 50 of them test positive for having it, but that is due to 10 truly having it and 40 mistaken test results, because only 10 people of those tested actually have AIDS but the test sometimes gives false results. The probability of a positive test result is determined not only by the accuracy of the test but by the characteristics of the sampled population.[2] When the prevalence, the proportion of those who have a given condition, is lower than the test's false positive rate, even tests that have a very low chance of giving a false positive in an individual case will give more false than true positives overall.[3] The paradox surprises most people.[4]

It is especially counter-intuitive when interpreting a positive result in a test on a low-prevalence population after having dealt with positive results drawn from a high-prevalence population.[3] If the false positive rate of the test is higher than the proportion of the new population with the condition, then a test administrator whose experience has been drawn from testing in a high-prevalence population may conclude from experience that a positive test result usually indicates a positive subject, when in fact a false positive is far more likely to have occurred.

Examples [ edit ]

Example 1: Disease [ edit ]

High-incidence population [ edit ]

Number

of people Infected Uninfected Total Test

positive 400

(true positive) 30

(false positive) 430 Test

negative 0

(false negative) 570

(true negative) 570 Total 400 600 1000

Imagine running an infectious disease test on a population A of 1000 persons, in which 40% are infected. The test has a false positive rate of 5% (0.05) and no false negative rate. The expected outcome of the 1000 tests on population A would be:

Infected and test indicates disease (true positive) 1000 × 40 / 100 = 400 people would receive a true positive Uninfected and test indicates disease (false positive) 1000 × 100 – 40 / 100 × 0.05 = 30 people would receive a false positive The remaining 570 tests are correctly negative.

So, in population A, a person receiving a positive test could be over 93% confident (400/30 + 400) that it correctly indicates infection.

Low-incidence population [ edit ]

Number

of people Infected Uninfected Total Test

positive 20

(true positive) 49

(false positive) 69 Test

negative 0

(false negative) 931

(true negative) 931 Total 20 980 1000

Now consider the same test applied to population B, in which only 2% is infected. The expected outcome of 1000 tests on population B would be:

Infected and test indicates disease (true positive) 1000 × 2 / 100 = 20 people would receive a true positive Uninfected and test indicates disease (false positive) 1000 × 100 – 2 / 100 × 0.05 = 49 people would receive a false positive The remaining 931 (= 1000 - (49 + 20)) tests are correctly negative.

In population B, only 20 of the 69 total people with a positive test result are actually infected. So, the probability of actually being infected after one is told that one is infected is only 29% (20/20 + 49) for a test that otherwise appears to be ""95% accurate"".

A tester with experience of group A might find it a paradox that in group B, a result that had usually correctly indicated infection is now usually a false positive. The confusion of the posterior probability of infection with the prior probability of receiving a false positive is a natural error after receiving a health-threatening test result.

Example 2: Drunk drivers [ edit ]

A group of police officers have breathalyzers displaying false drunkenness in 5% of the cases in which the driver is sober. However, the breathalyzers never fail to detect a truly drunk person. One in a thousand drivers is driving drunk. Suppose the police officers then stop a driver at random to administer a breathalyzer test. It indicates that the driver is drunk. We assume you don't know anything else about them. How high is the probability they really are drunk?

Many would answer as high as 95%, but the correct probability is about 2%.

An explanation for this is as follows: on average, for every 1,000 drivers tested,

1 driver is drunk, and it is 100% certain that for that driver there is a true positive test result, so there is 1 true positive test result

positive test result, so there is 1 positive test result 999 drivers are not drunk, and among those drivers there are 5% false positive test results, so there are 49.95 false positive test results

Therefore, the probability that one of the drivers among the 1 + 49.95 = 50.95 positive test results really is drunk is 1 / 50.95 ≈ 0.019627 {\displaystyle 1/50.95\approx 0.019627} .

The validity of this result does, however, hinge on the validity of the initial assumption that the police officer stopped the driver truly at random, and not because of bad driving. If that or another non-arbitrary reason for stopping the driver was present, then the calculation also involves the probability of a drunk driver driving competently and a non-drunk driver driving (in-)competently.

More formally, the same probability of roughly 0.02 can be established using Bayes's theorem. The goal is to find the probability that the driver is drunk given that the breathalyzer indicated they are drunk, which can be represented as

p ( d r u n k ∣ D ) {\displaystyle p(\mathrm {drunk} \mid D)}

where D means that the breathalyzer indicates that the driver is drunk. Bayes's theorem tells us that

p ( d r u n k ∣ D ) = p ( D ∣ d r u n k ) p ( d r u n k ) p ( D ) . {\displaystyle p(\mathrm {drunk} \mid D)={\frac {p(D\mid \mathrm {drunk} )\,p(\mathrm {drunk} )}{p(D)}}.}

We were told the following in the first paragraph:

p ( d r u n k ) = 0.001 , {\displaystyle p(\mathrm {drunk} )=0.001,} p ( s o b e r ) = 0.999 , {\displaystyle p(\mathrm {sober} )=0.999,} p ( D ∣ d r u n k ) = 1.00 , {\displaystyle p(D\mid \mathrm {drunk} )=1.00,} p ( D ∣ s o b e r ) = 0.05. {\displaystyle p(D\mid \mathrm {sober} )=0.05.}

As you can see from the formula, one needs p(D) for Bayes' theorem, which one can compute from the preceding values using the law of total probability:

p ( D ) = p ( D ∣ d r u n k ) p ( d r u n k ) + p ( D ∣ s o b e r ) p ( s o b e r ) {\displaystyle p(D)=p(D\mid \mathrm {drunk} )\,p(\mathrm {drunk} )+p(D\mid \mathrm {sober} )\,p(\mathrm {sober} )}

which gives

p ( D ) = ( 1.00 × 0.001 ) + ( 0.05 × 0.999 ) = 0.05095. {\displaystyle p(D)=(1.00\times 0.001)+(0.05\times 0.999)=0.05095.}

Plugging these numbers into Bayes' theorem, one finds that

p ( d r u n k ∣ D ) = 1.00 × 0.001 0.05095 = 0.019627. {\displaystyle p(\mathrm {drunk} \mid D)={\frac {1.00\times 0.001}{0.05095}}=0.019627.}

Example 3: Terrorist identification [ edit ]

In a city of 1 million inhabitants let there be 100 terrorists and 999,900 non-terrorists. To simplify the example, it is assumed that all people present in the city are inhabitants. Thus, the base rate probability of a randomly selected inhabitant of the city being a terrorist is 0.0001, and the base rate probability of that same inhabitant being a non-terrorist is 0.9999. In an attempt to catch the terrorists, the city installs an alarm system with a surveillance camera and automatic facial recognition software.

The software has two failure rates of 1%:

The false negative rate: If the camera scans a terrorist, a bell will ring 99% of the time, and it will fail to ring 1% of the time.

The false positive rate: If the camera scans a non-terrorist, a bell will not ring 99% of the time, but it will ring 1% of the time.

Suppose now that an inhabitant triggers the alarm. What is the chance that the person is a terrorist? In other words, what is P(T | B), the probability that a terrorist has been detected given the ringing of the bell? Someone making the 'base rate fallacy' would infer that there is a 99% chance that the detected person is a terrorist. Although the inference seems to make sense, it is actually bad reasoning, and a calculation below will show that the chances they are a terrorist are actually near 1%, not near 99%.

The fallacy arises from confusing the natures of two different failure rates. The 'number of non-bells per 100 terrorists' and the 'number of non-terrorists per 100 bells' are unrelated quantities. One does not necessarily equal the other, and they don't even have to be almost equal. To show this, consider what happens if an identical alarm system were set up in a second city with no terrorists at all. As in the first city, the alarm sounds for 1 out of every 100 non-terrorist inhabitants detected, but unlike in the first city, the alarm never sounds for a terrorist. Therefore, 100% of all occasions of the alarm sounding are for non-terrorists, but a false negative rate cannot even be calculated. The 'number of non-terrorists per 100 bells' in that city is 100, yet P(T | B) = 0%. There is zero chance that a terrorist has been detected given the ringing of the bell.

Imagine that the first city's entire population of one million people pass in front of the camera. About 99 of the 100 terrorists will trigger the alarm—and so will about 9,999 of the 999,900 non-terrorists. Therefore, about 10,098 people will trigger the alarm, among which about 99 will be terrorists. So, the probability that a person triggering the alarm actually is a terrorist, is only about 99 in 10,098, which is less than 1%, and very, very far below our initial guess of 99%.

The base rate fallacy is so misleading in this example because there are many more non-terrorists than terrorists, and the number of false positives (non-terrorists scanned as terrorists) is so much larger than the true positives (the real number of terrorists).

Findings in psychology [ edit ]

In experiments, people have been found to prefer individuating information over general information when the former is available.[5][6][7]

In some experiments, students were asked to estimate the grade point averages (GPAs) of hypothetical students. When given relevant statistics about GPA distribution, students tended to ignore them if given descriptive information about the particular student even if the new descriptive information was obviously of little or no relevance to school performance.[6] This finding has been used to argue that interviews are an unnecessary part of the college admissions process because interviewers are unable to pick successful candidates better than basic statistics.

Psychologists Daniel Kahneman and Amos Tversky attempted to explain this finding in terms of a simple rule or ""heuristic"" called representativeness. They argued that many judgments relating to likelihood, or to cause and effect, are based on how representative one thing is of another, or of a category.[6] Kahneman considers base rate neglect to be a specific form of extension neglect.[8] Richard Nisbett has argued that some attributional biases like the fundamental attribution error are instances of the base rate fallacy: people do not use the ""consensus information"" (the ""base rate"") about how others behaved in similar situations and instead prefer simpler dispositional attributions.[9]

There is considerable debate in psychology on the conditions under which people do or do not appreciate base rate information.[10][11] Researchers in the heuristics-and-biases program have stressed empirical findings showing that people tend to ignore base rates and make inferences that violate certain norms of probabilistic reasoning, such as Bayes' theorem. The conclusion drawn from this line of research was that human probabilistic thinking is fundamentally flawed and error-prone.[12] Other researchers have emphasized the link between cognitive processes and information formats, arguing that such conclusions are not generally warranted.[13][14]

Consider again Example 2 from above. The required inference is to estimate the (posterior) probability that a (randomly picked) driver is drunk, given that the breathalyzer test is positive. Formally, this probability can be calculated using Bayes' theorem, as shown above. However, there are different ways of presenting the relevant information. Consider the following, formally equivalent variant of the problem:

1 out of 1000 drivers are driving drunk. The breathalyzers never fail to detect a truly drunk person. For 50 out of the 999 drivers who are not drunk the breathalyzer falsely displays drunkenness. Suppose the policemen then stop a driver at random, and force them to take a breathalyzer test. It indicates that they are drunk. We assume you don't know anything else about them. How high is the probability they really are drunk?

In this case, the relevant numerical information—p(drunk), p(D | drunk), p(D | sober)—is presented in terms of natural frequencies with respect to a certain reference class (see reference class problem). Empirical studies show that people's inferences correspond more closely to Bayes' rule when information is presented this way, helping to overcome base-rate neglect in laypeople[14] and experts.[15] As a consequence, organizations like the Cochrane Collaboration recommend using this kind of format for communicating health statistics.[16] Teaching people to translate these kinds of Bayesian reasoning problems into natural frequency formats is more effective than merely teaching them to plug probabilities (or percentages) into Bayes' theorem.[17] It has also been shown that graphical representations of natural frequencies (e.g., icon arrays) help people to make better inferences.[17][18][19]

Why are natural frequency formats helpful? One important reason is that this information format facilitates the required inference because it simplifies the necessary calculations. This can be seen when using an alternative way of computing the required probability p(drunk|D):

p ( d r u n k ∣ D ) = N ( d r u n k ∩ D ) N ( D ) = 1 51 = 0.0196 {\displaystyle p(\mathrm {drunk} \mid D)={\frac {N(\mathrm {drunk} \cap D)}{N(D)}}={\frac {1}{51}}=0.0196}

where N(drunk ∩ D) denotes the number of drivers that are drunk and get a positive breathalyzer result, and N(D) denotes the total number of cases with a positive breathalyzer result. The equivalence of this equation to the above one follows from the axioms of probability theory, according to which N(drunk ∩ D) = N × p (D | drunk) × p (drunk). Importantly, although this equation is formally equivalent to Bayes' rule, it is not psychologically equivalent. Using natural frequencies simplifies the inference because the required mathematical operation can be performed on natural numbers, instead of normalized fractions (i.e., probabilities), because it makes the high number of false positives more transparent, and because natural frequencies exhibit a ""nested-set structure"".[20][21]

Not every frequency format facilitates Bayesian reasoning.[21][22] Natural frequencies refer to frequency information that results from natural sampling,[23] which preserves base rate information (e.g., number of drunken drivers when taking a random sample of drivers). This is different from systematic sampling, in which base rates are fixed a priori (e.g., in scientific experiments). In the latter case it is not possible to infer the posterior probability p (drunk | positive test) from comparing the number of drivers who are drunk and test positive compared to the total number of people who get a positive breathalyzer result, because base rate information is not preserved and must be explicitly re-introduced using Bayes' theorem.

See also [ edit ]

References [ edit ]","          Base rate fallacy   From Wikipedia, the free encyclopedia    Jump to navigation  Jump to search  Statistical formal fallacy  The base rate fallacy , also called base rate neglect or base rate bias , is a fallacy . If presented with related base rate information (i.e. generic, general information) and specific information (information pertaining only to a certain case), the mind tends to ignore the former and focus on the latter. [1]  Base rate neglect is a specific form of the more general extension neglect .  Contents   1  False positive paradox  2  Examples   2.1  Example 1: Disease   2.1.1  High-incidence population  2.1.2  Low-incidence population    2.2  Example 2: Drunk drivers  2.3  Example 3: Terrorist identification    3  Findings in psychology  4  See also  5  References  6  External links    False positive paradox [ edit ]  An example of the base rate fallacy is how surprised people are by the false positive paradox ,    situations where there are more false positive test results than true positives. For example, it might be that of 1,000 people tested for AIDS, 50 of them test positive for having it, but that is due to 10 truly having it and 40  mistaken test results, because only 10 people of those tested actually have AIDS but the test sometimes gives false results.   The probability of a positive test result is determined not only by the accuracy of the test but by the characteristics of the sampled population. [2] When the prevalence, the proportion of those who have a given condition, is lower than the test's false positive rate, even tests that have a very low chance of giving a false positive in an individual case will give more false than true positives overall . [3] The paradox  surprises  most people. [4]  It is especially counter-intuitive when interpreting a positive result in a test on a low-prevalence population after having dealt with positive results drawn from a high-prevalence population. [3] If the false positive rate of the test is higher than the proportion of the new population with the condition, then a test administrator whose experience has been drawn from testing in a high-prevalence population may conclude from experience that a positive test result usually indicates a positive subject, when in fact a false positive is far more likely to have occurred.  Examples [ edit ]  Example 1: Disease [ edit ]  High-incidence population [ edit ]    Number of people  Infected  Uninfected  Total   Test positive  400 (true positive)  30 (false positive)  430   Test negative  0 (false negative)  570 (true negative)  570   Total  400  600  1000  Imagine running an infectious disease test on a population A of 1000 persons, in which 40% are infected. The test has a false positive rate of 5% (0.05) and no false negative rate. The expected outcome of the 1000 tests on population A would be:  Infected and test indicates disease ( true positive ) 1000 × 40 / 100 = 400 people would receive a true positive  Uninfected and test indicates disease (false positive) 1000 × 100 – 40 / 100 × 0.05 = 30 people would receive a false positive  The remaining 570 tests are correctly negative.  So, in population A , a person receiving a positive test could be over 93% confident ( 400 / 30 + 400 ) that it correctly indicates infection.  Low-incidence population [ edit ]    Number of people  Infected  Uninfected  Total   Test positive  20 (true positive)  49 (false positive)  69   Test negative  0 (false negative)  931 (true negative)  931   Total  20  980  1000  Now consider the same test applied to population B , in which only 2% is infected. The expected outcome of 1000 tests on population B would be:  Infected and test indicates disease ( true positive ) 1000 × 2 / 100 = 20 people would receive a true positive  Uninfected and test indicates disease (false positive) 1000 × 100 – 2 / 100 × 0.05 = 49 people would receive a false positive  The remaining 931 (= 1000 - (49 + 20)) tests are correctly negative.  In population B , only 20 of the 69 total people with a positive test result are actually infected. So, the probability of actually being infected after one is told that one is infected is only 29% ( 20 / 20 + 49 ) for a test that otherwise appears to be ""95% accurate"". A tester with experience of group A might find it a paradox that in group B , a result that had usually correctly indicated infection is now usually a false positive . The confusion of the posterior probability of infection with the prior probability of receiving a false positive is a natural error after receiving a health-threatening test result.  Example 2: Drunk drivers [ edit ]  A group of police officers have breathalyzers displaying false drunkenness in 5% of the cases in which the driver is sober. However, the breathalyzers never fail to detect a truly drunk person. One in a thousand drivers is driving drunk. Suppose the police officers then stop a driver at random to administer a breathalyzer test. It indicates that the driver is drunk. We assume you don't know anything else about them. How high is the probability they really are drunk?  Many would answer as high as 95%, but the correct probability is about 2%. An explanation for this is as follows: on average, for every 1,000 drivers tested,  1 driver is drunk, and it is 100% certain that for that driver there is a true positive test result, so there is 1 true positive test result  999 drivers are not drunk, and among those drivers there are 5% false positive test results, so there are 49.95 false positive test results  Therefore, the probability that one of the drivers among the 1 + 49.95 = 50.95 positive test results really is drunk is     1   /   50.95  ≈  0.019627    {\displaystyle 1/50.95\approx 0.019627}   . The validity of this result does, however, hinge on the validity of the initial assumption that the police officer stopped the driver truly at random, and not because of bad driving. If that or another non-arbitrary reason for stopping the driver was present, then the calculation also involves the probability of a drunk driver driving competently and a non-drunk driver driving (in-)competently. More formally, the same probability of roughly 0.02 can be established using Bayes's theorem . The goal is to find the probability that the driver is drunk given that the breathalyzer indicated they are drunk, which can be represented as      p  (   d  r  u  n  k   ∣  D  )    {\displaystyle p(\mathrm {drunk} \mid D)}    where D means that the breathalyzer indicates that the driver is drunk. Bayes's theorem tells us that      p  (   d  r  u  n  k   ∣  D  )  =     p  (  D  ∣   d  r  u  n  k   )   p  (   d  r  u  n  k   )    p  (  D  )     .    {\displaystyle p(\mathrm {drunk} \mid D)={\frac {p(D\mid \mathrm {drunk} )\,p(\mathrm {drunk} )}{p(D)}}.}    We were told the following in the first paragraph:      p  (   d  r  u  n  k   )  =  0.001  ,    {\displaystyle p(\mathrm {drunk} )=0.001,}        p  (   s  o  b  e  r   )  =  0.999  ,    {\displaystyle p(\mathrm {sober} )=0.999,}        p  (  D  ∣   d  r  u  n  k   )  =  1.00  ,    {\displaystyle p(D\mid \mathrm {drunk} )=1.00,}   and      p  (  D  ∣   s  o  b  e  r   )  =  0.05.    {\displaystyle p(D\mid \mathrm {sober} )=0.05.}    As you can see from the formula, one needs p ( D ) for Bayes' theorem, which one can compute from the preceding values using the law of total probability :      p  (  D  )  =  p  (  D  ∣   d  r  u  n  k   )   p  (   d  r  u  n  k   )  +  p  (  D  ∣   s  o  b  e  r   )   p  (   s  o  b  e  r   )    {\displaystyle p(D)=p(D\mid \mathrm {drunk} )\,p(\mathrm {drunk} )+p(D\mid \mathrm {sober} )\,p(\mathrm {sober} )}    which gives      p  (  D  )  =  (  1.00  ×  0.001  )  +  (  0.05  ×  0.999  )  =  0.05095.    {\displaystyle p(D)=(1.00\times 0.001)+(0.05\times 0.999)=0.05095.}    Plugging these numbers into Bayes' theorem, one finds that      p  (   d  r  u  n  k   ∣  D  )  =     1.00  ×  0.001   0.05095    =  0.019627.    {\displaystyle p(\mathrm {drunk} \mid D)={\frac {1.00\times 0.001}{0.05095}}=0.019627.}    Example 3: Terrorist identification [ edit ]  In a city of 1 million inhabitants let there be 100 terrorists and 999,900 non-terrorists. To simplify the example, it is assumed that all people present in the city are inhabitants. Thus, the base rate probability of a randomly selected inhabitant of the city being a terrorist is 0.0001, and the base rate probability of that same inhabitant being a non-terrorist is 0.9999. In an attempt to catch the terrorists, the city installs an alarm system with a surveillance camera and automatic facial recognition software . The software has two failure rates of 1%:  The false negative rate: If the camera scans a terrorist, a bell will ring 99% of the time, and it will fail to ring 1% of the time.  The false positive rate: If the camera scans a non-terrorist, a bell will not ring 99% of the time, but it will ring 1% of the time.  Suppose now that an inhabitant triggers the alarm. What is the chance that the person is a terrorist? In other words, what is P(T | B), the probability that a terrorist has been detected given the ringing of the bell? Someone making the 'base rate fallacy' would infer that there is a 99% chance that the detected person is a terrorist. Although the inference seems to make sense, it is actually bad reasoning, and a calculation below will show that the chances they are a terrorist are actually near 1%, not near 99%. The fallacy arises from confusing the natures of two different failure rates. The 'number of non-bells per 100 terrorists' and the 'number of non-terrorists per 100 bells' are unrelated quantities. One does not necessarily equal the other, and they don't even have to be almost equal. To show this, consider what happens if an identical alarm system were set up in a second city with no terrorists at all. As in the first city, the alarm sounds for 1 out of every 100 non-terrorist inhabitants detected, but unlike in the first city, the alarm never sounds for a terrorist. Therefore, 100% of all occasions of the alarm sounding are for non-terrorists, but a false negative rate cannot even be calculated. The 'number of non-terrorists per 100 bells' in that city is 100, yet P(T | B) = 0%. There is zero chance that a terrorist has been detected given the ringing of the bell. Imagine that the first city's entire population of one million people pass in front of the camera. About 99 of the 100 terrorists will trigger the alarm—and so will about 9,999 of the 999,900 non-terrorists. Therefore, about 10,098 people will trigger the alarm, among which about 99 will be terrorists. So, the probability that a person triggering the alarm actually is a terrorist, is only about 99 in 10,098, which is less than 1%, and very, very far below our initial guess of 99%. The base rate fallacy is so misleading in this example because there are many more non-terrorists than terrorists, and the number of false positives (non-terrorists scanned as terrorists) is so much larger than the true positives (the real number of terrorists).  Findings in psychology [ edit ]  In experiments, people have been found to prefer individuating information over general information when the former is available. [5] [6] [7]  In some experiments, students were asked to estimate the grade point averages (GPAs) of hypothetical students. When given relevant statistics about GPA distribution, students tended to ignore them if given descriptive information about the particular student even if the new descriptive information was obviously of little or no relevance to school performance. [6] This finding has been used to argue that interviews are an unnecessary part of the college admissions process because interviewers are unable to pick successful candidates better than basic statistics. Psychologists  Daniel Kahneman and Amos Tversky attempted to explain this finding in terms of a simple rule or ""heuristic"" called representativeness . They argued that many judgments relating to likelihood, or to cause and effect, are based on how representative one thing is of another, or of a category. [6] Kahneman considers base rate neglect to be a specific form of extension neglect . [8]  Richard Nisbett has argued that some attributional biases like the fundamental attribution error are instances of the base rate fallacy: people do not use the ""consensus information"" (the ""base rate"") about how others behaved in similar situations and instead prefer simpler dispositional attributions . [9]  There is considerable debate in psychology on the conditions under which people do or do not appreciate base rate information. [10] [11] Researchers in the heuristics-and-biases program have stressed empirical findings showing that people tend to ignore base rates and make inferences that violate certain norms of probabilistic reasoning, such as Bayes' theorem . The conclusion drawn from this line of research was that human probabilistic thinking is fundamentally flawed and error-prone. [12] Other researchers have emphasized the link between cognitive processes and information formats, arguing that such conclusions are not generally warranted. [13] [14]  Consider again Example 2 from above. The required inference is to estimate the (posterior) probability that a (randomly picked) driver is drunk, given that the breathalyzer test is positive. Formally, this probability can be calculated using Bayes' theorem , as shown above. However, there are different ways of presenting the relevant information. Consider the following, formally equivalent variant of the problem:  1 out of 1000 drivers are driving drunk. The breathalyzers never fail to detect a truly drunk person. For 50 out of the 999 drivers who are not drunk the breathalyzer falsely displays drunkenness. Suppose the policemen then stop a driver at random, and force them to take a breathalyzer test. It indicates that they are drunk. We assume you don't know anything else about them. How high is the probability they really are drunk?  In this case, the relevant numerical information— p (drunk), p ( D | drunk), p ( D | sober)—is presented in terms of natural frequencies with respect to a certain reference class (see reference class problem ). Empirical studies show that people's inferences correspond more closely to Bayes' rule when information is presented this way, helping to overcome base-rate neglect in laypeople [14] and experts. [15] As a consequence, organizations like the Cochrane Collaboration recommend using this kind of format for communicating health statistics. [16] Teaching people to translate these kinds of Bayesian reasoning problems into natural frequency formats is more effective than merely teaching them to plug probabilities (or percentages) into Bayes' theorem. [17] It has also been shown that graphical representations of natural frequencies (e.g., icon arrays) help people to make better inferences. [17] [18] [19]  Why are natural frequency formats helpful? One important reason is that this information format facilitates the required inference because it simplifies the necessary calculations. This can be seen when using an alternative way of computing the required probability p (drunk| D ):      p  (   d  r  u  n  k   ∣  D  )  =     N  (   d  r  u  n  k   ∩  D  )    N  (  D  )     =    1  51    =  0.0196    {\displaystyle p(\mathrm {drunk} \mid D)={\frac {N(\mathrm {drunk} \cap D)}{N(D)}}={\frac {1}{51}}=0.0196}    where N (drunk ∩ D ) denotes the number of drivers that are drunk and get a positive breathalyzer result, and N ( D ) denotes the total number of cases with a positive breathalyzer result. The equivalence of this equation to the above one follows from the axioms of probability theory, according to which N (drunk ∩ D ) = N × p ( D | drunk) × p (drunk). Importantly, although this equation is formally equivalent to Bayes' rule, it is not psychologically equivalent. Using natural frequencies simplifies the inference because the required mathematical operation can be performed on natural numbers, instead of normalized fractions (i.e., probabilities), because it makes the high number of false positives more transparent, and because natural frequencies exhibit a ""nested-set structure"". [20] [21]  Not every frequency format facilitates Bayesian reasoning. [21] [22] Natural frequencies refer to frequency information that results from natural sampling , [23] which preserves base rate information (e.g., number of drunken drivers when taking a random sample of drivers). This is different from systematic sampling , in which base rates are fixed a priori (e.g., in scientific experiments). In the latter case it is not possible to infer the posterior probability p (drunk | positive test) from comparing the number of drivers who are drunk and test positive compared to the total number of people who get a positive breathalyzer result, because base rate information is not preserved and must be explicitly re-introduced using Bayes' theorem.  See also [ edit ]  Bayesian probability  Bayes' theorem  Data dredging  Inductive argument  List of cognitive biases  List of paradoxes  Misleading vividness  Prevention paradox  Prosecutor's fallacy , a mistake in reasoning that involves ignoring a low prior probability  Simpson's paradox , another error in statistical reasoning dealing with comparing groups  Stereotype  References [ edit ]    ^  ""Logical Fallacy: The Base Rate Fallacy"" . Fallacyfiles.org . Retrieved 2013-06-15 .   ^  Rheinfurth, M. H.; Howell, L. W. (March 1998). Probability and Statistics in Aerospace Engineering  (PDF) . NASA . p. 16. MESSAGE: False positive tests are more probable than true positive tests when the overall population has a low prevalence of the disease. This is called the false-positive paradox.   ^ a  b  Vacher, H. L. (May 2003). ""Quantitative literacy - drug testing, cancer screening, and the identification of igneous rocks"" . Journal of Geoscience Education : 2. At first glance, this seems perverse: the less the students as a whole use steroids , the more likely a student identified as a user will be a non-user. This has been called the False Positive Paradox - Citing: Gonick, L.; Smith, W. (1993). The cartoon guide to statistics . New York: Harper Collins. p. 49.   ^  Madison, B. L. (August 2007). ""Mathematical Proficiency for Citizenship"" .  In Schoenfeld, A. H. (ed.). Assessing Mathematical Proficiency . Mathematical Sciences Research Institute Publications (New ed.). Cambridge University Press. p. 122. ISBN  978-0-521-69766-8 . The correct [probability estimate...] is surprising to many; hence, the term paradox .   ^  Bar-Hillel, Maya (1980). ""The base-rate fallacy in probability judgments"". Acta Psychologica . 44 (3): 211–233. doi : 10.1016/0001-6918(80)90046-3 .   ^ a  b  c  Kahneman, Daniel; Amos Tversky (1973). ""On the psychology of prediction"". Psychological Review . 80 (4): 237–251. doi : 10.1037/h0034747 . S2CID  17786757 .   ^  Kahneman, Daniel; Amos Tversky (1985). ""Evidential impact of base rates"".  In Daniel Kahneman, Paul Slovic & Amos Tversky (ed.). Judgment under uncertainty: Heuristics and biases . pp. 153–160. PMID  17835457 .   ^  Kahneman, Daniel (2000). ""Evaluation by moments, past and future"".  In Daniel Kahneman and Amos Tversky (ed.). Choices, Values and Frames .   ^  Nisbett, Richard E.; E. Borgida; R. Crandall; H. Reed (1976). ""Popular induction: Information is not always informative"".  In J. S. Carroll & J. W. Payne (ed.). Cognition and social behavior . 2 . pp. 227–236.   ^  Koehler, J. J. (2010). ""The base rate fallacy reconsidered: Descriptive, normative, and methodological challenges"". Behavioral and Brain Sciences . 19 : 1–17. doi : 10.1017/S0140525X00041157 . S2CID  53343238 .   ^  Barbey, A. K.; Sloman, S. A. (2007). ""Base-rate respect: From ecological rationality to dual processes"". Behavioral and Brain Sciences . 30 (3): 241–254, discussion 255–297. doi : 10.1017/S0140525X07001653 . PMID  17963533 . S2CID  31741077 .   ^  Tversky, A.; Kahneman, D. (1974). ""Judgment under Uncertainty: Heuristics and Biases"". Science . 185 (4157): 1124–1131. Bibcode : 1974Sci...185.1124T . doi : 10.1126/science.185.4157.1124 . PMID  17835457 .   ^  Cosmides, Leda; John Tooby (1996). ""Are humans good intuitive statisticians after all? Rethinking some conclusions of the literature on judgment under uncertainty"". Cognition . 58 : 1–73. CiteSeerX  10.1.1.131.8290 . doi : 10.1016/0010-0277(95)00664-8 .   ^ a  b  Gigerenzer, G.; Hoffrage, U. (1995). ""How to improve Bayesian reasoning without instruction: Frequency formats"". Psychological Review . 102 (4): 684. CiteSeerX  10.1.1.128.3201 . doi : 10.1037/0033-295X.102.4.684 .   ^  Hoffrage, U.; Lindsey, S.; Hertwig, R.; Gigerenzer, G. (2000). ""Medicine: Communicating Statistical Information"". Science . 290 (5500): 2261–2262. doi : 10.1126/science.290.5500.2261 . PMID  11188724 . S2CID  33050943 .   ^  Akl, E. A.; Oxman, A. D.; Herrin, J.; Vist, G. E.; Terrenato, I.; Sperati, F.; Costiniuk, C.; Blank, D.; Schünemann, H. (2011).  Schünemann, Holger (ed.). ""Using alternative statistical formats for presenting risks and risk reductions"" . The Cochrane Library (3): CD006776. doi : 10.1002/14651858.CD006776.pub2 . PMC  6464912 . PMID  21412897 .   ^ a  b  Sedlmeier, P.; Gigerenzer, G. (2001). ""Teaching Bayesian reasoning in less than two hours"" . Journal of Experimental Psychology: General . 130 (3): 380. doi : 10.1037/0096-3445.130.3.380 . hdl : 11858/00-001M-0000-0025-9504-E .   ^  Brase, G. L. (2009). ""Pictorial representations in statistical reasoning"". Applied Cognitive Psychology . 23 (3): 369–381. doi : 10.1002/acp.1460 . S2CID  18817707 .   ^  Edwards, A.; Elwyn, G.; Mulley, A. (2002). ""Explaining risks: Turning numerical data into meaningful pictures"" . BMJ . 324 (7341): 827–830. doi : 10.1136/bmj.324.7341.827 . PMC  1122766 . PMID  11934777 .   ^  Girotto, V.; Gonzalez, M. (2001). ""Solving probabilistic and statistical problems: A matter of information structure and question form"". Cognition . 78 (3): 247–276. doi : 10.1016/S0010-0277(00)00133-5 . PMID  11124351 .   ^ a  b  Hoffrage, U.; Gigerenzer, G.; Krauss, S.; Martignon, L. (2002). ""Representation facilitates reasoning: What natural frequencies are and what they are not"". Cognition . 84 (3): 343–352. doi : 10.1016/S0010-0277(02)00050-1 . PMID  12044739 .   ^  Gigerenzer, G.; Hoffrage, U. (1999). ""Overcoming difficulties in Bayesian reasoning: A reply to Lewis and Keren (1999) and Mellers and McGraw (1999)"" . Psychological Review . 106 (2): 425. doi : 10.1037/0033-295X.106.2.425 . hdl : 11858/00-001M-0000-0025-9CB4-8 .   ^  Kleiter, G. D. (1994). ""Natural Sampling: Rationality without Base Rates"". Contributions to Mathematical Psychology, Psychometrics, and Methodology . Recent Research in Psychology. pp. 375–388. doi : 10.1007/978-1-4612-4308-3_27 . ISBN  978-0-387-94169-1 .    External links [ edit ]  The Base Rate Fallacy The Fallacy Files  v t e Biases Cognitive biases  Actor–observer  Acquiescence  Ambiguity  Anchoring  Attentional  Attribution  Authority  Automation  Belief  Blind spot  Choice-supportive  Confirmation  Congruence  Cultural  Distinction  Dunning–Kruger  Egocentric  Emotional  Extrinsic incentives  Fading affect  Framing  Correspondence  Halo effect  Hindsight  Horn effect  Hostile attribution  Impact  Implicit  In-group  Mere-exposure effect  Negativity  Normalcy  Omission  Optimism  Out-group homogeneity  Outcome  Overton window  Precision  Present  Pro-innovation  Response  Restraint  Self-serving  Social comparison  Status quo  Time-saving  Trait ascription  von Restorff effect  Zero-risk  In animals  Statistical biases  Estimator  Forecast  Healthy user  Information  Psychological  Lead time  Length time  Non-response  Observer  Omitted-variable  Participation  Recall  Sampling  Selection  Self-selection  Social desirability  Spectrum  Survivorship  Systematic error  Systemic  Verification  Wet  Other biases  Academic  Funding  FUTON  Inductive  Infrastructure  Inherent  In education  Media  False balance  Vietnam War  Norway  South Asia  Sweden  United States  Arab–Israeli conflict  Ukraine  Net  Political bias  Publication  Reporting  White hat  Bias reduction  Cognitive bias mitigation  Debiasing  Heuristics in judgment and decision-making  Lists: General  ·  Memory  v t e Fallacies ( list ) Formal In propositional logic  Affirming a disjunct  Affirming the consequent  Denying the antecedent  Argument from fallacy  In quantificational logic  Existential  Illicit conversion  Proof by example  Quantifier shift  Syllogistic fallacy  Affirmative conclusion from a negative premise  Exclusive premises  Existential  Necessity  Four terms  Illicit major  Illicit minor  Negative conclusion from affirmative premises  Undistributed middle   Masked man  Mathematical fallacy  Informal Equivocation  Equivocation  False equivalence  False attribution  Quoting out of context  Loki's Wager  No true Scotsman  Reification  Question-begging fallacies  Circular reasoning / Begging the question  Loaded language  Leading question  Compound question / Loaded question / Complex question  No true Scotsman  Correlative-based fallacies  False dilemma  Perfect solution  Denying the correlative  Suppressed correlative  Illicit transference  Composition  Division  Ecological  Secundum quid  Accident  Converse accident  Faulty generalization  Anecdotal evidence  Sampling bias  Cherry picking  McNamara  Base rate / Conjunction  Double counting  False analogy  Slothful induction  Overwhelming exception  Vagueness / Ambiguity  Accent  False precision  Moving the goalposts  Quoting out of context  Slippery slope  Sorites paradox  Syntactic ambiguity  Questionable cause  Animistic  Furtive  Correlation implies causation Cum hoc  Post hoc  Gambler's  Inverse  Regression  Single cause  Slippery slope  Texas sharpshooter  Fallacies of relevance Appeals to emotion  Fear  Flattery  Novelty  Pity  Ridicule  Think of the children  In-group favoritism  Invented here / Not invented here  Island mentality  Loyalty  Parade of horribles  Spite  Stirring symbols  Wisdom of repugnance  Genetic fallacies Ad hominem  Appeal to motive  Association  Reductio ad Hitlerum  Godwin's law  Reductio ad Stalinum  Bulverism  Poisoning the well  Tone  Tu quoque  Whataboutism   Authority  Accomplishment  Ipse dixit  Poverty / Wealth  Etymology  Nature  Tradition / Novelty  Chronological snobbery  Appeals to consequences  Argumentum ad baculum  Wishful thinking   Ad nauseam  Argument to moderation  Argumentum ad populum  Appeal to the stone / Proof by assertion  Ignoratio elenchi  Argument from silence  Invincible ignorance  Moralistic / Naturalistic  Motte-and-bailey fallacy  Rationalization  Red herring  Two wrongs make a right  Special pleading  Straw man  Cliché  I'm entitled to my opinion    Category       Retrieved from "" https://en.wikipedia.org/w/index.php?title=Base_rate_fallacy&oldid=963044273 ""  Categories : Relevance fallacies Cognitive biases Behavioral finance Probability fallacies Statistical paradoxes Hidden categories: Articles with short description         Navigation menu      Personal tools      Not logged in Talk Contributions Create account Log in        Namespaces      Article Talk        Variants              Views      Read Edit View history        More           Search                     Navigation      Main page Contents Current events Random article About Wikipedia Contact us Donate Wikipedia store       Contribute      Help Community portal Recent changes Upload file       Tools      What links here Related changes Upload file Special pages Permanent link Page information Wikidata item Cite this page       Print/export      Download as PDF Printable version       Languages      العربية Deutsch Español Français עברית Polski Русский ไทย Türkçe Українська 中文  Edit links        This page was last edited on 17 June 2020, at 13:34 (UTC) .  Text is available under the Creative Commons Attribution-ShareAlike License ;
additional terms may apply.  By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc. , a non-profit organization.    Privacy policy  About Wikipedia  Disclaimers  Contact Wikipedia  Developers  Statistics  Cookie statement  Mobile view          "
18,conjunction fallacy(fallacy OR bias OR heuristic) (coronavirus OR covid OR pandemic),1,https://fs.blog/2016/09/bias-conjunction-fallacy/,"Mental Model: Bias from Conjunction Fallacy

The bias from conjunction fallacy is a common reasoning error in which we believe that two events happening in conjunction is more probable than one of those events happening alone. Here’s why this happens and how we can overcome the fallacy.

***

Daniel Kahneman and Amos Tversky spent decades in psychology research to disentangle patterns in errors of human reasoning. The duo discovered a variety of logical fallacies that we tend to make when facing information that appears vaguely familiar. These fallacies lead to bias — irrational behavior based on beliefs that are not always grounded in reality.

In his book Thinking Fast and Slow, which summarizes his and Tversky’s life work, Kahneman introduces biases that stem from the conjunction fallacy – the false belief that a conjunction of two events is more probable than one of the events on its own.

What is Probability?

Probability can be a difficult concept. Most of us have an intuitive understanding of what probability is, but there is little consensus on what it actually means. It is just as vague and subjective a concept as democracy, beauty or freedom. However, this is not always troublesome – we can still easily discuss the notion with others. Kahneman reflects:

In all the years I spent asking questions about the probability of events, no one ever raised a hand to ask me, “Sir, what do you mean by probability?” as they would have done if I had asked them to assess a strange concept such as globability. Everyone acted as if they knew how to answer my questions, although we all understood that it would be unfair to ask them for an explanation of what the word means.

While logicians and statisticians might disagree, to most of us probability is simply a tool that describes our degree of belief. For instance, we know that the sun will rise tomorrow and we consider it near impossible that there will be two suns up in the sky instead of one. In addition to the extremes, there are also events which lie somewhere in the middle of the probability spectrum, such as the degree of belief that it will rain tomorrow.

Despite its vagueness, probability has its virtues. Assigning probabilities helps us make the degree of belief actionable and also communicable to others. If we believe that the probability it will rain tomorrow is 90%, we are likely to carry an umbrella and suggest our family do so as well.

Probability, Base Rates, and Representativeness

Most of us are already familiar with representativeness and base rates. Consider the classic example of x number of black and y number of white-colored marbles in a jar. It is a simple exercise to tell what the probabilities of drawing each color are if you know their base rates (proportion). Using base rates is the obvious approach for estimations when no other information is provided.

However, Kahneman managed to prove that we have a tendency to ignore base rates in light of specific descriptions. He calls this phenomenon the Representativeness Bias. To illustrate representativeness bias, consider the example of seeing a person reading The New York Times on the New York subway. Which do you think would be a better bet about the reading stranger?

1) She has a PhD.

2) She does not have a college degree.

Representativeness would tell you to bet on the PhD, but this is not necessarily a good idea. You should seriously consider the second alternative because many more non-graduates than PhDs ride in New York subways. While a more significant proportion of PhDs may read The New York Times, the total number of New York Times readers with only high school degrees is likely to be much larger, even if the proportion itself is very slim.

In a series of similar experiments, Kahneman’s subjects failed to recognize the base rates in light of individual information. This is unsurprising. Kahneman explains:

On most occasions, people who act friendly are in fact friendly. A professional athlete who is very tall and thin is much more likely to play basketball than football. People with a PhD are more likely to subscribe to The New York Times than people who ended their education after high school. Young men are more likely than elderly women to drive aggressively.

While following representativeness bias might improve your overall accuracy, it will not always be the statistically optimal approach.

In his bestseller Moneyball, Michael Lewis tells a story of Oakland A’s baseball team coach, Billy Beane, who recognized this fallacy and used it to his advantage.

When recruiting new players for the team, instead of relying on scouts he relied heavily on statistics of past performance. This approach allowed him to build a team of great players that were passed up by other teams because they did not look the part. Needless to say, the team achieved excellent results at a low cost.

Conjunction Fallacy

While representativeness bias occurs when we fail to account for low base rates, conjunction fallacy occurs when we assign a higher probability to an event of higher specificity. This violates the laws of probability.

Consider the following study:

Participants were asked to rank four possible outcomes of the next Wimbledon tournament from most to least probable. Björn Borg was the dominant tennis player of the day when the study was conducted. These were the outcomes:

A. Borg will win the match.

B. Borg will lose the first set.

C. Borg will lose the first set but win the match.

D. Borg will win the first set but lose the match.

How would you order them?

Kahneman was surprised to see that most subjects ordered the chances by directly contradicting the laws of logic and probability. He explains:

The critical items are B and C. B is the more inclusive event and its probability must be higher than that of an event it includes. Contrary to logic, but not to representativeness or plausibility, 72% assigned B a lower probability than C.

If you thought about the problem carefully, you drew the following diagram in your head. Losing the first set will always, by definition, be a more probable event than losing the first set and winning the match.



The Linda Problem

As discussed in our piece on the Narrative Fallacy, the best-known and most controversial of Kahneman and Tversky’s experiments involved a fictitious lady called Linda. The fictional character was created to illustrate the role heuristics play in our judgment and how it can be incompatible with logic. This is how they described Linda.

Linda is thirty-one years old, single, outspoken, and very bright. She majored in philosophy. As a student, she was deeply concerned with issues of discrimination and social justice, and also participated in antinuclear demonstrations.

Kahneman conducted a series of experiments, in which he showed that representativeness tends to cloud our judgments and that we ignore the base rates in light of stories. The Linda problem started off with the task to estimate the plausibility of 9 different scenarios that subjects were supposed to rank in order of likelihood.

Linda is a teacher in elementary school.

Linda works in a bookstore and takes yoga classes.

Linda is active in the feminist movement.

Linda is a psychiatric social worker.

Linda is a member of the League of Women Voters.

Linda is a bank teller.

Linda is an insurance salesperson.

Linda is a bank teller and is active in the feminist movement.

Kahneman was startled to see that his subjects judged the likelihood of Linda being a bank teller and a feminist more likely than her being just a bank teller. As explained earlier, doing so makes little sense. He went on to explore the phenomenon further:

In what we later described as “increasingly desperate” attempts to eliminate the error, we introduced large groups of people to Linda and asked them this simple question: Which alternative is more probable? Linda is a bank teller.

Linda is a bank teller and is active in the feminist movement. This stark version of the problem made Linda famous in some circles, and it earned us years of controversy. About 85% to 90% of undergraduates at several major universities chose the second option, contrary to logic. What is especially interesting about these results is that, even when aware of the biases in place, we do not discard them. When I asked my large undergraduate class in some indignation, “Do you realize that you have violated an elementary logical rule?” someone in the back row shouted, “So what?” and a graduate student who made the same error explained herself by saying, “I thought you just asked for my opinion.”

The issue is not constrained to students and but also affects professionals.

The naturalist Stephen Jay Gould described his own struggle with the Linda problem. He knew the correct answer, of course, and yet, he wrote, “a little homunculus in my head continues to jump up and down, shouting at me—‘but she can’t just be a bank teller; read the description.”

Our brains simply seem to prefer consistency over logic.

The Role of Plausibility

Representativeness and conjunction fallacy occurs because we make the mental shortcut from our perceived plausibility of a scenario to its probability.

The most coherent stories are not necessarily the most probable, but they are plausible, and the notions of coherence, plausibility, and probability are easily confused by the unwary. Representativeness belongs to a cluster of closely related assessments that are likely to be generated together. The most representative outcomes combine with the personality description to produce the most coherent stories.

Kahneman warns us about the effects of these biases on our perception of expert opinion and forecasting. He explains that we are more likely to believe scenarios that are illustrative rather than probable.

The uncritical substitution of plausibility for probability has pernicious effects on judgments when scenarios are used as tools of forecasting. Consider these two scenarios, which were presented to different groups, with a request to evaluate their probability:

A massive flood somewhere in North America next year, in which more than 1,000 people drown

An earthquake in California sometime next year, causing a flood in which more than 1,000 people drown

The California earthquake scenario is more plausible than the North America scenario, although its probability is certainly smaller. As expected, probability judgments were higher for the richer and more detailed scenario, contrary to logic. This is a trap for forecasters and their clients: adding detail to scenarios makes them more persuasive, but less likely to come true.

In order to appreciate the role of plausibility, he suggests we have a look at an example without an accompanying explanation.

Which alternative is more probable?

Jane is a teacher.

Jane is a teacher and walks to work.

In this case, when evaluating plausibility and coherence, there are no quick answers to the probability question, and we can easily conclude that the first one is more likely. The rule goes that in the absence of a competing intuition, logic prevails.

Taming our intuition

The first lesson to thinking clearly is to question how you think. We should not simply believe whatever comes to our mind – our beliefs must be constrained by logic. You don’t have to become an expert in probability to tame your intuition, but having a grasp of simple concepts will help. There are two main rules that are worth repeating in light of representativeness bias:

1) All probabilities add up to 100%.

This means that if you believe that there’s a 90% chance it will rain tomorrow, there’s a 10% chance that it will not rain tomorrow.

However, since you believe that there is only a 90% chance that it will rain tomorrow, you cannot be 95% certain that it will rain tomorrow morning.

We typically make this type of error, when we mean to say that, if it rains, there’s a 95% probability it will happen in the morning. That’s a different claim and the probability of raining tomorrow morning under such premises is 0.9*0.95=85.5%.

This also means the odds that, if it rains, it will not rain in the morning, are 90.0%-85.5% = 4.5%.

2) The second principle is called the Bayes rule.

It allows us to correctly adjust our beliefs with the diagnosticity of the evidence. Bayes rule follows the formula:

In essence, the formula states that the posterior odds are proportional to prior odds times the likelihood. Kahneman crystallizes two keys to disciplined Bayesian reasoning:

• Anchor your judgment of the probability of an outcome on a plausible base rate.

• Question the diagnosticity of your evidence.

Kahneman explains it with an example:

If you believe that 3% of graduate students are enrolled in computer science (the base rate), and you also believe that the description of Tom is 4 times more likely for a graduate student in computer science than in other fields, then Bayes’s rule says you must believe that the probability that Tom is a computer science student is now 11%.

Four times as likely means that we expect roughly 80% of all computer science students to resemble Tom. We use this proportion to obtain the adjusted odds. (The calculation goes as follows: 0.03*0.8/(0.03*0.8+((1-0.03)*(1-0.8)))=11%)

The easiest way to become better at making decisions is by making sure you question your assumptions and follow strong evidence. When evidence is anecdotal, adjust minimally, and trust the base rates. Odds are, you will be pleasantly surprised.

***

Want More? Check out our ever-growing collection of mental models and biases and get to work.","          Base rate fallacy   From Wikipedia, the free encyclopedia    Jump to navigation  Jump to search  Statistical formal fallacy  The base rate fallacy , also called base rate neglect or base rate bias , is a fallacy . If presented with related base rate information (i.e. generic, general information) and specific information (information pertaining only to a certain case), the mind tends to ignore the former and focus on the latter. [1]  Base rate neglect is a specific form of the more general extension neglect .  Contents   1  False positive paradox  2  Examples   2.1  Example 1: Disease   2.1.1  High-incidence population  2.1.2  Low-incidence population    2.2  Example 2: Drunk drivers  2.3  Example 3: Terrorist identification    3  Findings in psychology  4  See also  5  References  6  External links    False positive paradox [ edit ]  An example of the base rate fallacy is how surprised people are by the false positive paradox ,    situations where there are more false positive test results than true positives. For example, it might be that of 1,000 people tested for AIDS, 50 of them test positive for having it, but that is due to 10 truly having it and 40  mistaken test results, because only 10 people of those tested actually have AIDS but the test sometimes gives false results.   The probability of a positive test result is determined not only by the accuracy of the test but by the characteristics of the sampled population. [2] When the prevalence, the proportion of those who have a given condition, is lower than the test's false positive rate, even tests that have a very low chance of giving a false positive in an individual case will give more false than true positives overall . [3] The paradox  surprises  most people. [4]  It is especially counter-intuitive when interpreting a positive result in a test on a low-prevalence population after having dealt with positive results drawn from a high-prevalence population. [3] If the false positive rate of the test is higher than the proportion of the new population with the condition, then a test administrator whose experience has been drawn from testing in a high-prevalence population may conclude from experience that a positive test result usually indicates a positive subject, when in fact a false positive is far more likely to have occurred.  Examples [ edit ]  Example 1: Disease [ edit ]  High-incidence population [ edit ]    Number of people  Infected  Uninfected  Total   Test positive  400 (true positive)  30 (false positive)  430   Test negative  0 (false negative)  570 (true negative)  570   Total  400  600  1000  Imagine running an infectious disease test on a population A of 1000 persons, in which 40% are infected. The test has a false positive rate of 5% (0.05) and no false negative rate. The expected outcome of the 1000 tests on population A would be:  Infected and test indicates disease ( true positive ) 1000 × 40 / 100 = 400 people would receive a true positive  Uninfected and test indicates disease (false positive) 1000 × 100 – 40 / 100 × 0.05 = 30 people would receive a false positive  The remaining 570 tests are correctly negative.  So, in population A , a person receiving a positive test could be over 93% confident ( 400 / 30 + 400 ) that it correctly indicates infection.  Low-incidence population [ edit ]    Number of people  Infected  Uninfected  Total   Test positive  20 (true positive)  49 (false positive)  69   Test negative  0 (false negative)  931 (true negative)  931   Total  20  980  1000  Now consider the same test applied to population B , in which only 2% is infected. The expected outcome of 1000 tests on population B would be:  Infected and test indicates disease ( true positive ) 1000 × 2 / 100 = 20 people would receive a true positive  Uninfected and test indicates disease (false positive) 1000 × 100 – 2 / 100 × 0.05 = 49 people would receive a false positive  The remaining 931 (= 1000 - (49 + 20)) tests are correctly negative.  In population B , only 20 of the 69 total people with a positive test result are actually infected. So, the probability of actually being infected after one is told that one is infected is only 29% ( 20 / 20 + 49 ) for a test that otherwise appears to be ""95% accurate"". A tester with experience of group A might find it a paradox that in group B , a result that had usually correctly indicated infection is now usually a false positive . The confusion of the posterior probability of infection with the prior probability of receiving a false positive is a natural error after receiving a health-threatening test result.  Example 2: Drunk drivers [ edit ]  A group of police officers have breathalyzers displaying false drunkenness in 5% of the cases in which the driver is sober. However, the breathalyzers never fail to detect a truly drunk person. One in a thousand drivers is driving drunk. Suppose the police officers then stop a driver at random to administer a breathalyzer test. It indicates that the driver is drunk. We assume you don't know anything else about them. How high is the probability they really are drunk?  Many would answer as high as 95%, but the correct probability is about 2%. An explanation for this is as follows: on average, for every 1,000 drivers tested,  1 driver is drunk, and it is 100% certain that for that driver there is a true positive test result, so there is 1 true positive test result  999 drivers are not drunk, and among those drivers there are 5% false positive test results, so there are 49.95 false positive test results  Therefore, the probability that one of the drivers among the 1 + 49.95 = 50.95 positive test results really is drunk is     1   /   50.95  ≈  0.019627    {\displaystyle 1/50.95\approx 0.019627}   . The validity of this result does, however, hinge on the validity of the initial assumption that the police officer stopped the driver truly at random, and not because of bad driving. If that or another non-arbitrary reason for stopping the driver was present, then the calculation also involves the probability of a drunk driver driving competently and a non-drunk driver driving (in-)competently. More formally, the same probability of roughly 0.02 can be established using Bayes's theorem . The goal is to find the probability that the driver is drunk given that the breathalyzer indicated they are drunk, which can be represented as      p  (   d  r  u  n  k   ∣  D  )    {\displaystyle p(\mathrm {drunk} \mid D)}    where D means that the breathalyzer indicates that the driver is drunk. Bayes's theorem tells us that      p  (   d  r  u  n  k   ∣  D  )  =     p  (  D  ∣   d  r  u  n  k   )   p  (   d  r  u  n  k   )    p  (  D  )     .    {\displaystyle p(\mathrm {drunk} \mid D)={\frac {p(D\mid \mathrm {drunk} )\,p(\mathrm {drunk} )}{p(D)}}.}    We were told the following in the first paragraph:      p  (   d  r  u  n  k   )  =  0.001  ,    {\displaystyle p(\mathrm {drunk} )=0.001,}        p  (   s  o  b  e  r   )  =  0.999  ,    {\displaystyle p(\mathrm {sober} )=0.999,}        p  (  D  ∣   d  r  u  n  k   )  =  1.00  ,    {\displaystyle p(D\mid \mathrm {drunk} )=1.00,}   and      p  (  D  ∣   s  o  b  e  r   )  =  0.05.    {\displaystyle p(D\mid \mathrm {sober} )=0.05.}    As you can see from the formula, one needs p ( D ) for Bayes' theorem, which one can compute from the preceding values using the law of total probability :      p  (  D  )  =  p  (  D  ∣   d  r  u  n  k   )   p  (   d  r  u  n  k   )  +  p  (  D  ∣   s  o  b  e  r   )   p  (   s  o  b  e  r   )    {\displaystyle p(D)=p(D\mid \mathrm {drunk} )\,p(\mathrm {drunk} )+p(D\mid \mathrm {sober} )\,p(\mathrm {sober} )}    which gives      p  (  D  )  =  (  1.00  ×  0.001  )  +  (  0.05  ×  0.999  )  =  0.05095.    {\displaystyle p(D)=(1.00\times 0.001)+(0.05\times 0.999)=0.05095.}    Plugging these numbers into Bayes' theorem, one finds that      p  (   d  r  u  n  k   ∣  D  )  =     1.00  ×  0.001   0.05095    =  0.019627.    {\displaystyle p(\mathrm {drunk} \mid D)={\frac {1.00\times 0.001}{0.05095}}=0.019627.}    Example 3: Terrorist identification [ edit ]  In a city of 1 million inhabitants let there be 100 terrorists and 999,900 non-terrorists. To simplify the example, it is assumed that all people present in the city are inhabitants. Thus, the base rate probability of a randomly selected inhabitant of the city being a terrorist is 0.0001, and the base rate probability of that same inhabitant being a non-terrorist is 0.9999. In an attempt to catch the terrorists, the city installs an alarm system with a surveillance camera and automatic facial recognition software . The software has two failure rates of 1%:  The false negative rate: If the camera scans a terrorist, a bell will ring 99% of the time, and it will fail to ring 1% of the time.  The false positive rate: If the camera scans a non-terrorist, a bell will not ring 99% of the time, but it will ring 1% of the time.  Suppose now that an inhabitant triggers the alarm. What is the chance that the person is a terrorist? In other words, what is P(T | B), the probability that a terrorist has been detected given the ringing of the bell? Someone making the 'base rate fallacy' would infer that there is a 99% chance that the detected person is a terrorist. Although the inference seems to make sense, it is actually bad reasoning, and a calculation below will show that the chances they are a terrorist are actually near 1%, not near 99%. The fallacy arises from confusing the natures of two different failure rates. The 'number of non-bells per 100 terrorists' and the 'number of non-terrorists per 100 bells' are unrelated quantities. One does not necessarily equal the other, and they don't even have to be almost equal. To show this, consider what happens if an identical alarm system were set up in a second city with no terrorists at all. As in the first city, the alarm sounds for 1 out of every 100 non-terrorist inhabitants detected, but unlike in the first city, the alarm never sounds for a terrorist. Therefore, 100% of all occasions of the alarm sounding are for non-terrorists, but a false negative rate cannot even be calculated. The 'number of non-terrorists per 100 bells' in that city is 100, yet P(T | B) = 0%. There is zero chance that a terrorist has been detected given the ringing of the bell. Imagine that the first city's entire population of one million people pass in front of the camera. About 99 of the 100 terrorists will trigger the alarm—and so will about 9,999 of the 999,900 non-terrorists. Therefore, about 10,098 people will trigger the alarm, among which about 99 will be terrorists. So, the probability that a person triggering the alarm actually is a terrorist, is only about 99 in 10,098, which is less than 1%, and very, very far below our initial guess of 99%. The base rate fallacy is so misleading in this example because there are many more non-terrorists than terrorists, and the number of false positives (non-terrorists scanned as terrorists) is so much larger than the true positives (the real number of terrorists).  Findings in psychology [ edit ]  In experiments, people have been found to prefer individuating information over general information when the former is available. [5] [6] [7]  In some experiments, students were asked to estimate the grade point averages (GPAs) of hypothetical students. When given relevant statistics about GPA distribution, students tended to ignore them if given descriptive information about the particular student even if the new descriptive information was obviously of little or no relevance to school performance. [6] This finding has been used to argue that interviews are an unnecessary part of the college admissions process because interviewers are unable to pick successful candidates better than basic statistics. Psychologists  Daniel Kahneman and Amos Tversky attempted to explain this finding in terms of a simple rule or ""heuristic"" called representativeness . They argued that many judgments relating to likelihood, or to cause and effect, are based on how representative one thing is of another, or of a category. [6] Kahneman considers base rate neglect to be a specific form of extension neglect . [8]  Richard Nisbett has argued that some attributional biases like the fundamental attribution error are instances of the base rate fallacy: people do not use the ""consensus information"" (the ""base rate"") about how others behaved in similar situations and instead prefer simpler dispositional attributions . [9]  There is considerable debate in psychology on the conditions under which people do or do not appreciate base rate information. [10] [11] Researchers in the heuristics-and-biases program have stressed empirical findings showing that people tend to ignore base rates and make inferences that violate certain norms of probabilistic reasoning, such as Bayes' theorem . The conclusion drawn from this line of research was that human probabilistic thinking is fundamentally flawed and error-prone. [12] Other researchers have emphasized the link between cognitive processes and information formats, arguing that such conclusions are not generally warranted. [13] [14]  Consider again Example 2 from above. The required inference is to estimate the (posterior) probability that a (randomly picked) driver is drunk, given that the breathalyzer test is positive. Formally, this probability can be calculated using Bayes' theorem , as shown above. However, there are different ways of presenting the relevant information. Consider the following, formally equivalent variant of the problem:  1 out of 1000 drivers are driving drunk. The breathalyzers never fail to detect a truly drunk person. For 50 out of the 999 drivers who are not drunk the breathalyzer falsely displays drunkenness. Suppose the policemen then stop a driver at random, and force them to take a breathalyzer test. It indicates that they are drunk. We assume you don't know anything else about them. How high is the probability they really are drunk?  In this case, the relevant numerical information— p (drunk), p ( D | drunk), p ( D | sober)—is presented in terms of natural frequencies with respect to a certain reference class (see reference class problem ). Empirical studies show that people's inferences correspond more closely to Bayes' rule when information is presented this way, helping to overcome base-rate neglect in laypeople [14] and experts. [15] As a consequence, organizations like the Cochrane Collaboration recommend using this kind of format for communicating health statistics. [16] Teaching people to translate these kinds of Bayesian reasoning problems into natural frequency formats is more effective than merely teaching them to plug probabilities (or percentages) into Bayes' theorem. [17] It has also been shown that graphical representations of natural frequencies (e.g., icon arrays) help people to make better inferences. [17] [18] [19]  Why are natural frequency formats helpful? One important reason is that this information format facilitates the required inference because it simplifies the necessary calculations. This can be seen when using an alternative way of computing the required probability p (drunk| D ):      p  (   d  r  u  n  k   ∣  D  )  =     N  (   d  r  u  n  k   ∩  D  )    N  (  D  )     =    1  51    =  0.0196    {\displaystyle p(\mathrm {drunk} \mid D)={\frac {N(\mathrm {drunk} \cap D)}{N(D)}}={\frac {1}{51}}=0.0196}    where N (drunk ∩ D ) denotes the number of drivers that are drunk and get a positive breathalyzer result, and N ( D ) denotes the total number of cases with a positive breathalyzer result. The equivalence of this equation to the above one follows from the axioms of probability theory, according to which N (drunk ∩ D ) = N × p ( D | drunk) × p (drunk). Importantly, although this equation is formally equivalent to Bayes' rule, it is not psychologically equivalent. Using natural frequencies simplifies the inference because the required mathematical operation can be performed on natural numbers, instead of normalized fractions (i.e., probabilities), because it makes the high number of false positives more transparent, and because natural frequencies exhibit a ""nested-set structure"". [20] [21]  Not every frequency format facilitates Bayesian reasoning. [21] [22] Natural frequencies refer to frequency information that results from natural sampling , [23] which preserves base rate information (e.g., number of drunken drivers when taking a random sample of drivers). This is different from systematic sampling , in which base rates are fixed a priori (e.g., in scientific experiments). In the latter case it is not possible to infer the posterior probability p (drunk | positive test) from comparing the number of drivers who are drunk and test positive compared to the total number of people who get a positive breathalyzer result, because base rate information is not preserved and must be explicitly re-introduced using Bayes' theorem.  See also [ edit ]  Bayesian probability  Bayes' theorem  Data dredging  Inductive argument  List of cognitive biases  List of paradoxes  Misleading vividness  Prevention paradox  Prosecutor's fallacy , a mistake in reasoning that involves ignoring a low prior probability  Simpson's paradox , another error in statistical reasoning dealing with comparing groups  Stereotype  References [ edit ]    ^  ""Logical Fallacy: The Base Rate Fallacy"" . Fallacyfiles.org . Retrieved 2013-06-15 .   ^  Rheinfurth, M. H.; Howell, L. W. (March 1998). Probability and Statistics in Aerospace Engineering  (PDF) . NASA . p. 16. MESSAGE: False positive tests are more probable than true positive tests when the overall population has a low prevalence of the disease. This is called the false-positive paradox.   ^ a  b  Vacher, H. L. (May 2003). ""Quantitative literacy - drug testing, cancer screening, and the identification of igneous rocks"" . Journal of Geoscience Education : 2. At first glance, this seems perverse: the less the students as a whole use steroids , the more likely a student identified as a user will be a non-user. This has been called the False Positive Paradox - Citing: Gonick, L.; Smith, W. (1993). The cartoon guide to statistics . New York: Harper Collins. p. 49.   ^  Madison, B. L. (August 2007). ""Mathematical Proficiency for Citizenship"" .  In Schoenfeld, A. H. (ed.). Assessing Mathematical Proficiency . Mathematical Sciences Research Institute Publications (New ed.). Cambridge University Press. p. 122. ISBN  978-0-521-69766-8 . The correct [probability estimate...] is surprising to many; hence, the term paradox .   ^  Bar-Hillel, Maya (1980). ""The base-rate fallacy in probability judgments"". Acta Psychologica . 44 (3): 211–233. doi : 10.1016/0001-6918(80)90046-3 .   ^ a  b  c  Kahneman, Daniel; Amos Tversky (1973). ""On the psychology of prediction"". Psychological Review . 80 (4): 237–251. doi : 10.1037/h0034747 . S2CID  17786757 .   ^  Kahneman, Daniel; Amos Tversky (1985). ""Evidential impact of base rates"".  In Daniel Kahneman, Paul Slovic & Amos Tversky (ed.). Judgment under uncertainty: Heuristics and biases . pp. 153–160. PMID  17835457 .   ^  Kahneman, Daniel (2000). ""Evaluation by moments, past and future"".  In Daniel Kahneman and Amos Tversky (ed.). Choices, Values and Frames .   ^  Nisbett, Richard E.; E. Borgida; R. Crandall; H. Reed (1976). ""Popular induction: Information is not always informative"".  In J. S. Carroll & J. W. Payne (ed.). Cognition and social behavior . 2 . pp. 227–236.   ^  Koehler, J. J. (2010). ""The base rate fallacy reconsidered: Descriptive, normative, and methodological challenges"". Behavioral and Brain Sciences . 19 : 1–17. doi : 10.1017/S0140525X00041157 . S2CID  53343238 .   ^  Barbey, A. K.; Sloman, S. A. (2007). ""Base-rate respect: From ecological rationality to dual processes"". Behavioral and Brain Sciences . 30 (3): 241–254, discussion 255–297. doi : 10.1017/S0140525X07001653 . PMID  17963533 . S2CID  31741077 .   ^  Tversky, A.; Kahneman, D. (1974). ""Judgment under Uncertainty: Heuristics and Biases"". Science . 185 (4157): 1124–1131. Bibcode : 1974Sci...185.1124T . doi : 10.1126/science.185.4157.1124 . PMID  17835457 .   ^  Cosmides, Leda; John Tooby (1996). ""Are humans good intuitive statisticians after all? Rethinking some conclusions of the literature on judgment under uncertainty"". Cognition . 58 : 1–73. CiteSeerX  10.1.1.131.8290 . doi : 10.1016/0010-0277(95)00664-8 .   ^ a  b  Gigerenzer, G.; Hoffrage, U. (1995). ""How to improve Bayesian reasoning without instruction: Frequency formats"". Psychological Review . 102 (4): 684. CiteSeerX  10.1.1.128.3201 . doi : 10.1037/0033-295X.102.4.684 .   ^  Hoffrage, U.; Lindsey, S.; Hertwig, R.; Gigerenzer, G. (2000). ""Medicine: Communicating Statistical Information"". Science . 290 (5500): 2261–2262. doi : 10.1126/science.290.5500.2261 . PMID  11188724 . S2CID  33050943 .   ^  Akl, E. A.; Oxman, A. D.; Herrin, J.; Vist, G. E.; Terrenato, I.; Sperati, F.; Costiniuk, C.; Blank, D.; Schünemann, H. (2011).  Schünemann, Holger (ed.). ""Using alternative statistical formats for presenting risks and risk reductions"" . The Cochrane Library (3): CD006776. doi : 10.1002/14651858.CD006776.pub2 . PMC  6464912 . PMID  21412897 .   ^ a  b  Sedlmeier, P.; Gigerenzer, G. (2001). ""Teaching Bayesian reasoning in less than two hours"" . Journal of Experimental Psychology: General . 130 (3): 380. doi : 10.1037/0096-3445.130.3.380 . hdl : 11858/00-001M-0000-0025-9504-E .   ^  Brase, G. L. (2009). ""Pictorial representations in statistical reasoning"". Applied Cognitive Psychology . 23 (3): 369–381. doi : 10.1002/acp.1460 . S2CID  18817707 .   ^  Edwards, A.; Elwyn, G.; Mulley, A. (2002). ""Explaining risks: Turning numerical data into meaningful pictures"" . BMJ . 324 (7341): 827–830. doi : 10.1136/bmj.324.7341.827 . PMC  1122766 . PMID  11934777 .   ^  Girotto, V.; Gonzalez, M. (2001). ""Solving probabilistic and statistical problems: A matter of information structure and question form"". Cognition . 78 (3): 247–276. doi : 10.1016/S0010-0277(00)00133-5 . PMID  11124351 .   ^ a  b  Hoffrage, U.; Gigerenzer, G.; Krauss, S.; Martignon, L. (2002). ""Representation facilitates reasoning: What natural frequencies are and what they are not"". Cognition . 84 (3): 343–352. doi : 10.1016/S0010-0277(02)00050-1 . PMID  12044739 .   ^  Gigerenzer, G.; Hoffrage, U. (1999). ""Overcoming difficulties in Bayesian reasoning: A reply to Lewis and Keren (1999) and Mellers and McGraw (1999)"" . Psychological Review . 106 (2): 425. doi : 10.1037/0033-295X.106.2.425 . hdl : 11858/00-001M-0000-0025-9CB4-8 .   ^  Kleiter, G. D. (1994). ""Natural Sampling: Rationality without Base Rates"". Contributions to Mathematical Psychology, Psychometrics, and Methodology . Recent Research in Psychology. pp. 375–388. doi : 10.1007/978-1-4612-4308-3_27 . ISBN  978-0-387-94169-1 .    External links [ edit ]  The Base Rate Fallacy The Fallacy Files  v t e Biases Cognitive biases  Actor–observer  Acquiescence  Ambiguity  Anchoring  Attentional  Attribution  Authority  Automation  Belief  Blind spot  Choice-supportive  Confirmation  Congruence  Cultural  Distinction  Dunning–Kruger  Egocentric  Emotional  Extrinsic incentives  Fading affect  Framing  Correspondence  Halo effect  Hindsight  Horn effect  Hostile attribution  Impact  Implicit  In-group  Mere-exposure effect  Negativity  Normalcy  Omission  Optimism  Out-group homogeneity  Outcome  Overton window  Precision  Present  Pro-innovation  Response  Restraint  Self-serving  Social comparison  Status quo  Time-saving  Trait ascription  von Restorff effect  Zero-risk  In animals  Statistical biases  Estimator  Forecast  Healthy user  Information  Psychological  Lead time  Length time  Non-response  Observer  Omitted-variable  Participation  Recall  Sampling  Selection  Self-selection  Social desirability  Spectrum  Survivorship  Systematic error  Systemic  Verification  Wet  Other biases  Academic  Funding  FUTON  Inductive  Infrastructure  Inherent  In education  Media  False balance  Vietnam War  Norway  South Asia  Sweden  United States  Arab–Israeli conflict  Ukraine  Net  Political bias  Publication  Reporting  White hat  Bias reduction  Cognitive bias mitigation  Debiasing  Heuristics in judgment and decision-making  Lists: General  ·  Memory  v t e Fallacies ( list ) Formal In propositional logic  Affirming a disjunct  Affirming the consequent  Denying the antecedent  Argument from fallacy  In quantificational logic  Existential  Illicit conversion  Proof by example  Quantifier shift  Syllogistic fallacy  Affirmative conclusion from a negative premise  Exclusive premises  Existential  Necessity  Four terms  Illicit major  Illicit minor  Negative conclusion from affirmative premises  Undistributed middle   Masked man  Mathematical fallacy  Informal Equivocation  Equivocation  False equivalence  False attribution  Quoting out of context  Loki's Wager  No true Scotsman  Reification  Question-begging fallacies  Circular reasoning / Begging the question  Loaded language  Leading question  Compound question / Loaded question / Complex question  No true Scotsman  Correlative-based fallacies  False dilemma  Perfect solution  Denying the correlative  Suppressed correlative  Illicit transference  Composition  Division  Ecological  Secundum quid  Accident  Converse accident  Faulty generalization  Anecdotal evidence  Sampling bias  Cherry picking  McNamara  Base rate / Conjunction  Double counting  False analogy  Slothful induction  Overwhelming exception  Vagueness / Ambiguity  Accent  False precision  Moving the goalposts  Quoting out of context  Slippery slope  Sorites paradox  Syntactic ambiguity  Questionable cause  Animistic  Furtive  Correlation implies causation Cum hoc  Post hoc  Gambler's  Inverse  Regression  Single cause  Slippery slope  Texas sharpshooter  Fallacies of relevance Appeals to emotion  Fear  Flattery  Novelty  Pity  Ridicule  Think of the children  In-group favoritism  Invented here / Not invented here  Island mentality  Loyalty  Parade of horribles  Spite  Stirring symbols  Wisdom of repugnance  Genetic fallacies Ad hominem  Appeal to motive  Association  Reductio ad Hitlerum  Godwin's law  Reductio ad Stalinum  Bulverism  Poisoning the well  Tone  Tu quoque  Whataboutism   Authority  Accomplishment  Ipse dixit  Poverty / Wealth  Etymology  Nature  Tradition / Novelty  Chronological snobbery  Appeals to consequences  Argumentum ad baculum  Wishful thinking   Ad nauseam  Argument to moderation  Argumentum ad populum  Appeal to the stone / Proof by assertion  Ignoratio elenchi  Argument from silence  Invincible ignorance  Moralistic / Naturalistic  Motte-and-bailey fallacy  Rationalization  Red herring  Two wrongs make a right  Special pleading  Straw man  Cliché  I'm entitled to my opinion    Category       Retrieved from "" https://en.wikipedia.org/w/index.php?title=Base_rate_fallacy&oldid=963044273 ""  Categories : Relevance fallacies Cognitive biases Behavioral finance Probability fallacies Statistical paradoxes Hidden categories: Articles with short description         Navigation menu      Personal tools      Not logged in Talk Contributions Create account Log in        Namespaces      Article Talk        Variants              Views      Read Edit View history        More           Search                     Navigation      Main page Contents Current events Random article About Wikipedia Contact us Donate Wikipedia store       Contribute      Help Community portal Recent changes Upload file       Tools      What links here Related changes Upload file Special pages Permanent link Page information Wikidata item Cite this page       Print/export      Download as PDF Printable version       Languages      العربية Deutsch Español Français עברית Polski Русский ไทย Türkçe Українська 中文  Edit links        This page was last edited on 17 June 2020, at 13:34 (UTC) .  Text is available under the Creative Commons Attribution-ShareAlike License ;
additional terms may apply.  By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc. , a non-profit organization.    Privacy policy  About Wikipedia  Disclaimers  Contact Wikipedia  Developers  Statistics  Cookie statement  Mobile view          "
18,conjunction fallacy(fallacy OR bias OR heuristic) (coronavirus OR covid OR pandemic),2,https://www.greenbook.org/mr/market-research-trends/this-and-that-the-conjunction-fallacy-in-practice/,"Editor’s Intro: Behavioral science approaches to marketing and research have rapidly grown to take account of the kinds of cognitive phenomena Lindsay Cannon discusses in her article. As other authors have noted, there are a great many “biases” of the sort that Lindsay presents – maybe we shouldn’t be thinking of them as “biases” as much as what simply makes us human.

In 1983, two psychologists, Daniel Kahneman & Amos Tversky, outlined a cognitive error called the conjunction fallacy. Kahneman & Tversky described a phenomenon whereby individuals ignore the conjunction rule, which states that the probability of two joint events co-occurring cannot exceed the probability of the events happening separately. This means that the probability that two distinct things are true of one person or situation is always much lower than the probability that only one of those things is true.

Let’s try out an example: John is a 23-year-old graduate student. He is heading down the street walking a dog and carrying a bag containing a pint of mint chocolate chip ice cream. Which is more likely?

John loves dogs. John loves dogs and he loves mint chocolate chip ice cream.

Because you are given information that describes John as having both a dog and mint chocolate chip ice cream, it is likely that you picked option 2. And, if you picked option 2, you are in the majority, as 85% of people also chose the option that includes the conjunction, or the “and” statement. However, the probability that someone only loves dogs or only loves mint chocolate chip ice cream is much higher than the probability that they love both dogs AND mint chocolate chip ice cream.

Simply stated, the human brain tricks us into thinking that the statement that includes more information, or is more representative of what we are expecting, is more likely to be true than the simpler option. This thinking process relies on a key concept called the representativeness heuristic. This heuristic is a mental short-cut which we utilize in our day-to-day lives, which helps us to recognize familiar concepts and to make decisions about new situations based on our ability to frame new information in the context of what we have seen before. This explains our tendency to overestimate the likelihood of co-occurring events, as these events activate multiple internal stereotypes, causing us to believe that they are true in tandem.

When it comes to your business, how can you leverage the tendency of consumers to identify with scenarios that are information rich than those that are simpler? Is less really more?

Many times, businesses want consumers to believe that two concepts that are related, but not the same, both apply to the same product. By activating two different stereotypes, one may be able to persuade consumers that the product being marketed is the solution to multiple needs. But how can this be done subtly?

Take Neutrogena, for example. This skincare brand has built a platform that equates health with beauty by striving to show consumers that these concepts are one in the same and that their products can deliver both outcomes. However, health and beauty are not always synonymous, as beautiful skin can be unhealthy, and healthy skin can lack beauty. Yet, Neutrogena has built their brand on homogenizing these two concepts. How?

Imagine yourself, a teenager who has been struggling with frequent breakouts, searching for a solution to alleviate the acne that has been plaguing you for months. You long to have clear skin, and, when browsing through your favorite magazine, you stumble on a possible solution. A full-page Neutrogena ad shows the glowing face of your favorite celebrity, under the tagline “#1 Dermatologist Recommended.” Not only does the dermatologist recommendation make you believe that this product will help make your skin healthy, but you see the clear, beautiful skin of your favorite actress and assume that she must have healthy skin too. You get in your car and drive to the store, in search of this product to help you get the beautiful, healthy skin you so desperately desire.

With their simple ad, Neutrogena has successfully activated two stereotypes using the conjunction fallacy. By using a tagline that is intrinsically related to health, they have activated a representation of healthy skin. By pairing this tagline with the image of a beautiful celebrity, the consumer is additionally prompted to think of beauty, such that the two concepts become paired in the mind of the consumer. This marketing strategy has convinced the consumer that they will derive these dual benefits from Neutrogena’s products, merely by utilizing the pairing of a statement and image that invoke strong stereotypes representing the consumer’s expectations and needs.

It is possible to move your consumer from an “or” to an “and” mentality for your brand. It starts with understanding the associations your consumer holds today and crafting your message such that they perceive benefits in conjunction with each other.

Please share...

Related posts","          Base rate fallacy   From Wikipedia, the free encyclopedia    Jump to navigation  Jump to search  Statistical formal fallacy  The base rate fallacy , also called base rate neglect or base rate bias , is a fallacy . If presented with related base rate information (i.e. generic, general information) and specific information (information pertaining only to a certain case), the mind tends to ignore the former and focus on the latter. [1]  Base rate neglect is a specific form of the more general extension neglect .  Contents   1  False positive paradox  2  Examples   2.1  Example 1: Disease   2.1.1  High-incidence population  2.1.2  Low-incidence population    2.2  Example 2: Drunk drivers  2.3  Example 3: Terrorist identification    3  Findings in psychology  4  See also  5  References  6  External links    False positive paradox [ edit ]  An example of the base rate fallacy is how surprised people are by the false positive paradox ,    situations where there are more false positive test results than true positives. For example, it might be that of 1,000 people tested for AIDS, 50 of them test positive for having it, but that is due to 10 truly having it and 40  mistaken test results, because only 10 people of those tested actually have AIDS but the test sometimes gives false results.   The probability of a positive test result is determined not only by the accuracy of the test but by the characteristics of the sampled population. [2] When the prevalence, the proportion of those who have a given condition, is lower than the test's false positive rate, even tests that have a very low chance of giving a false positive in an individual case will give more false than true positives overall . [3] The paradox  surprises  most people. [4]  It is especially counter-intuitive when interpreting a positive result in a test on a low-prevalence population after having dealt with positive results drawn from a high-prevalence population. [3] If the false positive rate of the test is higher than the proportion of the new population with the condition, then a test administrator whose experience has been drawn from testing in a high-prevalence population may conclude from experience that a positive test result usually indicates a positive subject, when in fact a false positive is far more likely to have occurred.  Examples [ edit ]  Example 1: Disease [ edit ]  High-incidence population [ edit ]    Number of people  Infected  Uninfected  Total   Test positive  400 (true positive)  30 (false positive)  430   Test negative  0 (false negative)  570 (true negative)  570   Total  400  600  1000  Imagine running an infectious disease test on a population A of 1000 persons, in which 40% are infected. The test has a false positive rate of 5% (0.05) and no false negative rate. The expected outcome of the 1000 tests on population A would be:  Infected and test indicates disease ( true positive ) 1000 × 40 / 100 = 400 people would receive a true positive  Uninfected and test indicates disease (false positive) 1000 × 100 – 40 / 100 × 0.05 = 30 people would receive a false positive  The remaining 570 tests are correctly negative.  So, in population A , a person receiving a positive test could be over 93% confident ( 400 / 30 + 400 ) that it correctly indicates infection.  Low-incidence population [ edit ]    Number of people  Infected  Uninfected  Total   Test positive  20 (true positive)  49 (false positive)  69   Test negative  0 (false negative)  931 (true negative)  931   Total  20  980  1000  Now consider the same test applied to population B , in which only 2% is infected. The expected outcome of 1000 tests on population B would be:  Infected and test indicates disease ( true positive ) 1000 × 2 / 100 = 20 people would receive a true positive  Uninfected and test indicates disease (false positive) 1000 × 100 – 2 / 100 × 0.05 = 49 people would receive a false positive  The remaining 931 (= 1000 - (49 + 20)) tests are correctly negative.  In population B , only 20 of the 69 total people with a positive test result are actually infected. So, the probability of actually being infected after one is told that one is infected is only 29% ( 20 / 20 + 49 ) for a test that otherwise appears to be ""95% accurate"". A tester with experience of group A might find it a paradox that in group B , a result that had usually correctly indicated infection is now usually a false positive . The confusion of the posterior probability of infection with the prior probability of receiving a false positive is a natural error after receiving a health-threatening test result.  Example 2: Drunk drivers [ edit ]  A group of police officers have breathalyzers displaying false drunkenness in 5% of the cases in which the driver is sober. However, the breathalyzers never fail to detect a truly drunk person. One in a thousand drivers is driving drunk. Suppose the police officers then stop a driver at random to administer a breathalyzer test. It indicates that the driver is drunk. We assume you don't know anything else about them. How high is the probability they really are drunk?  Many would answer as high as 95%, but the correct probability is about 2%. An explanation for this is as follows: on average, for every 1,000 drivers tested,  1 driver is drunk, and it is 100% certain that for that driver there is a true positive test result, so there is 1 true positive test result  999 drivers are not drunk, and among those drivers there are 5% false positive test results, so there are 49.95 false positive test results  Therefore, the probability that one of the drivers among the 1 + 49.95 = 50.95 positive test results really is drunk is     1   /   50.95  ≈  0.019627    {\displaystyle 1/50.95\approx 0.019627}   . The validity of this result does, however, hinge on the validity of the initial assumption that the police officer stopped the driver truly at random, and not because of bad driving. If that or another non-arbitrary reason for stopping the driver was present, then the calculation also involves the probability of a drunk driver driving competently and a non-drunk driver driving (in-)competently. More formally, the same probability of roughly 0.02 can be established using Bayes's theorem . The goal is to find the probability that the driver is drunk given that the breathalyzer indicated they are drunk, which can be represented as      p  (   d  r  u  n  k   ∣  D  )    {\displaystyle p(\mathrm {drunk} \mid D)}    where D means that the breathalyzer indicates that the driver is drunk. Bayes's theorem tells us that      p  (   d  r  u  n  k   ∣  D  )  =     p  (  D  ∣   d  r  u  n  k   )   p  (   d  r  u  n  k   )    p  (  D  )     .    {\displaystyle p(\mathrm {drunk} \mid D)={\frac {p(D\mid \mathrm {drunk} )\,p(\mathrm {drunk} )}{p(D)}}.}    We were told the following in the first paragraph:      p  (   d  r  u  n  k   )  =  0.001  ,    {\displaystyle p(\mathrm {drunk} )=0.001,}        p  (   s  o  b  e  r   )  =  0.999  ,    {\displaystyle p(\mathrm {sober} )=0.999,}        p  (  D  ∣   d  r  u  n  k   )  =  1.00  ,    {\displaystyle p(D\mid \mathrm {drunk} )=1.00,}   and      p  (  D  ∣   s  o  b  e  r   )  =  0.05.    {\displaystyle p(D\mid \mathrm {sober} )=0.05.}    As you can see from the formula, one needs p ( D ) for Bayes' theorem, which one can compute from the preceding values using the law of total probability :      p  (  D  )  =  p  (  D  ∣   d  r  u  n  k   )   p  (   d  r  u  n  k   )  +  p  (  D  ∣   s  o  b  e  r   )   p  (   s  o  b  e  r   )    {\displaystyle p(D)=p(D\mid \mathrm {drunk} )\,p(\mathrm {drunk} )+p(D\mid \mathrm {sober} )\,p(\mathrm {sober} )}    which gives      p  (  D  )  =  (  1.00  ×  0.001  )  +  (  0.05  ×  0.999  )  =  0.05095.    {\displaystyle p(D)=(1.00\times 0.001)+(0.05\times 0.999)=0.05095.}    Plugging these numbers into Bayes' theorem, one finds that      p  (   d  r  u  n  k   ∣  D  )  =     1.00  ×  0.001   0.05095    =  0.019627.    {\displaystyle p(\mathrm {drunk} \mid D)={\frac {1.00\times 0.001}{0.05095}}=0.019627.}    Example 3: Terrorist identification [ edit ]  In a city of 1 million inhabitants let there be 100 terrorists and 999,900 non-terrorists. To simplify the example, it is assumed that all people present in the city are inhabitants. Thus, the base rate probability of a randomly selected inhabitant of the city being a terrorist is 0.0001, and the base rate probability of that same inhabitant being a non-terrorist is 0.9999. In an attempt to catch the terrorists, the city installs an alarm system with a surveillance camera and automatic facial recognition software . The software has two failure rates of 1%:  The false negative rate: If the camera scans a terrorist, a bell will ring 99% of the time, and it will fail to ring 1% of the time.  The false positive rate: If the camera scans a non-terrorist, a bell will not ring 99% of the time, but it will ring 1% of the time.  Suppose now that an inhabitant triggers the alarm. What is the chance that the person is a terrorist? In other words, what is P(T | B), the probability that a terrorist has been detected given the ringing of the bell? Someone making the 'base rate fallacy' would infer that there is a 99% chance that the detected person is a terrorist. Although the inference seems to make sense, it is actually bad reasoning, and a calculation below will show that the chances they are a terrorist are actually near 1%, not near 99%. The fallacy arises from confusing the natures of two different failure rates. The 'number of non-bells per 100 terrorists' and the 'number of non-terrorists per 100 bells' are unrelated quantities. One does not necessarily equal the other, and they don't even have to be almost equal. To show this, consider what happens if an identical alarm system were set up in a second city with no terrorists at all. As in the first city, the alarm sounds for 1 out of every 100 non-terrorist inhabitants detected, but unlike in the first city, the alarm never sounds for a terrorist. Therefore, 100% of all occasions of the alarm sounding are for non-terrorists, but a false negative rate cannot even be calculated. The 'number of non-terrorists per 100 bells' in that city is 100, yet P(T | B) = 0%. There is zero chance that a terrorist has been detected given the ringing of the bell. Imagine that the first city's entire population of one million people pass in front of the camera. About 99 of the 100 terrorists will trigger the alarm—and so will about 9,999 of the 999,900 non-terrorists. Therefore, about 10,098 people will trigger the alarm, among which about 99 will be terrorists. So, the probability that a person triggering the alarm actually is a terrorist, is only about 99 in 10,098, which is less than 1%, and very, very far below our initial guess of 99%. The base rate fallacy is so misleading in this example because there are many more non-terrorists than terrorists, and the number of false positives (non-terrorists scanned as terrorists) is so much larger than the true positives (the real number of terrorists).  Findings in psychology [ edit ]  In experiments, people have been found to prefer individuating information over general information when the former is available. [5] [6] [7]  In some experiments, students were asked to estimate the grade point averages (GPAs) of hypothetical students. When given relevant statistics about GPA distribution, students tended to ignore them if given descriptive information about the particular student even if the new descriptive information was obviously of little or no relevance to school performance. [6] This finding has been used to argue that interviews are an unnecessary part of the college admissions process because interviewers are unable to pick successful candidates better than basic statistics. Psychologists  Daniel Kahneman and Amos Tversky attempted to explain this finding in terms of a simple rule or ""heuristic"" called representativeness . They argued that many judgments relating to likelihood, or to cause and effect, are based on how representative one thing is of another, or of a category. [6] Kahneman considers base rate neglect to be a specific form of extension neglect . [8]  Richard Nisbett has argued that some attributional biases like the fundamental attribution error are instances of the base rate fallacy: people do not use the ""consensus information"" (the ""base rate"") about how others behaved in similar situations and instead prefer simpler dispositional attributions . [9]  There is considerable debate in psychology on the conditions under which people do or do not appreciate base rate information. [10] [11] Researchers in the heuristics-and-biases program have stressed empirical findings showing that people tend to ignore base rates and make inferences that violate certain norms of probabilistic reasoning, such as Bayes' theorem . The conclusion drawn from this line of research was that human probabilistic thinking is fundamentally flawed and error-prone. [12] Other researchers have emphasized the link between cognitive processes and information formats, arguing that such conclusions are not generally warranted. [13] [14]  Consider again Example 2 from above. The required inference is to estimate the (posterior) probability that a (randomly picked) driver is drunk, given that the breathalyzer test is positive. Formally, this probability can be calculated using Bayes' theorem , as shown above. However, there are different ways of presenting the relevant information. Consider the following, formally equivalent variant of the problem:  1 out of 1000 drivers are driving drunk. The breathalyzers never fail to detect a truly drunk person. For 50 out of the 999 drivers who are not drunk the breathalyzer falsely displays drunkenness. Suppose the policemen then stop a driver at random, and force them to take a breathalyzer test. It indicates that they are drunk. We assume you don't know anything else about them. How high is the probability they really are drunk?  In this case, the relevant numerical information— p (drunk), p ( D | drunk), p ( D | sober)—is presented in terms of natural frequencies with respect to a certain reference class (see reference class problem ). Empirical studies show that people's inferences correspond more closely to Bayes' rule when information is presented this way, helping to overcome base-rate neglect in laypeople [14] and experts. [15] As a consequence, organizations like the Cochrane Collaboration recommend using this kind of format for communicating health statistics. [16] Teaching people to translate these kinds of Bayesian reasoning problems into natural frequency formats is more effective than merely teaching them to plug probabilities (or percentages) into Bayes' theorem. [17] It has also been shown that graphical representations of natural frequencies (e.g., icon arrays) help people to make better inferences. [17] [18] [19]  Why are natural frequency formats helpful? One important reason is that this information format facilitates the required inference because it simplifies the necessary calculations. This can be seen when using an alternative way of computing the required probability p (drunk| D ):      p  (   d  r  u  n  k   ∣  D  )  =     N  (   d  r  u  n  k   ∩  D  )    N  (  D  )     =    1  51    =  0.0196    {\displaystyle p(\mathrm {drunk} \mid D)={\frac {N(\mathrm {drunk} \cap D)}{N(D)}}={\frac {1}{51}}=0.0196}    where N (drunk ∩ D ) denotes the number of drivers that are drunk and get a positive breathalyzer result, and N ( D ) denotes the total number of cases with a positive breathalyzer result. The equivalence of this equation to the above one follows from the axioms of probability theory, according to which N (drunk ∩ D ) = N × p ( D | drunk) × p (drunk). Importantly, although this equation is formally equivalent to Bayes' rule, it is not psychologically equivalent. Using natural frequencies simplifies the inference because the required mathematical operation can be performed on natural numbers, instead of normalized fractions (i.e., probabilities), because it makes the high number of false positives more transparent, and because natural frequencies exhibit a ""nested-set structure"". [20] [21]  Not every frequency format facilitates Bayesian reasoning. [21] [22] Natural frequencies refer to frequency information that results from natural sampling , [23] which preserves base rate information (e.g., number of drunken drivers when taking a random sample of drivers). This is different from systematic sampling , in which base rates are fixed a priori (e.g., in scientific experiments). In the latter case it is not possible to infer the posterior probability p (drunk | positive test) from comparing the number of drivers who are drunk and test positive compared to the total number of people who get a positive breathalyzer result, because base rate information is not preserved and must be explicitly re-introduced using Bayes' theorem.  See also [ edit ]  Bayesian probability  Bayes' theorem  Data dredging  Inductive argument  List of cognitive biases  List of paradoxes  Misleading vividness  Prevention paradox  Prosecutor's fallacy , a mistake in reasoning that involves ignoring a low prior probability  Simpson's paradox , another error in statistical reasoning dealing with comparing groups  Stereotype  References [ edit ]    ^  ""Logical Fallacy: The Base Rate Fallacy"" . Fallacyfiles.org . Retrieved 2013-06-15 .   ^  Rheinfurth, M. H.; Howell, L. W. (March 1998). Probability and Statistics in Aerospace Engineering  (PDF) . NASA . p. 16. MESSAGE: False positive tests are more probable than true positive tests when the overall population has a low prevalence of the disease. This is called the false-positive paradox.   ^ a  b  Vacher, H. L. (May 2003). ""Quantitative literacy - drug testing, cancer screening, and the identification of igneous rocks"" . Journal of Geoscience Education : 2. At first glance, this seems perverse: the less the students as a whole use steroids , the more likely a student identified as a user will be a non-user. This has been called the False Positive Paradox - Citing: Gonick, L.; Smith, W. (1993). The cartoon guide to statistics . New York: Harper Collins. p. 49.   ^  Madison, B. L. (August 2007). ""Mathematical Proficiency for Citizenship"" .  In Schoenfeld, A. H. (ed.). Assessing Mathematical Proficiency . Mathematical Sciences Research Institute Publications (New ed.). Cambridge University Press. p. 122. ISBN  978-0-521-69766-8 . The correct [probability estimate...] is surprising to many; hence, the term paradox .   ^  Bar-Hillel, Maya (1980). ""The base-rate fallacy in probability judgments"". Acta Psychologica . 44 (3): 211–233. doi : 10.1016/0001-6918(80)90046-3 .   ^ a  b  c  Kahneman, Daniel; Amos Tversky (1973). ""On the psychology of prediction"". Psychological Review . 80 (4): 237–251. doi : 10.1037/h0034747 . S2CID  17786757 .   ^  Kahneman, Daniel; Amos Tversky (1985). ""Evidential impact of base rates"".  In Daniel Kahneman, Paul Slovic & Amos Tversky (ed.). Judgment under uncertainty: Heuristics and biases . pp. 153–160. PMID  17835457 .   ^  Kahneman, Daniel (2000). ""Evaluation by moments, past and future"".  In Daniel Kahneman and Amos Tversky (ed.). Choices, Values and Frames .   ^  Nisbett, Richard E.; E. Borgida; R. Crandall; H. Reed (1976). ""Popular induction: Information is not always informative"".  In J. S. Carroll & J. W. Payne (ed.). Cognition and social behavior . 2 . pp. 227–236.   ^  Koehler, J. J. (2010). ""The base rate fallacy reconsidered: Descriptive, normative, and methodological challenges"". Behavioral and Brain Sciences . 19 : 1–17. doi : 10.1017/S0140525X00041157 . S2CID  53343238 .   ^  Barbey, A. K.; Sloman, S. A. (2007). ""Base-rate respect: From ecological rationality to dual processes"". Behavioral and Brain Sciences . 30 (3): 241–254, discussion 255–297. doi : 10.1017/S0140525X07001653 . PMID  17963533 . S2CID  31741077 .   ^  Tversky, A.; Kahneman, D. (1974). ""Judgment under Uncertainty: Heuristics and Biases"". Science . 185 (4157): 1124–1131. Bibcode : 1974Sci...185.1124T . doi : 10.1126/science.185.4157.1124 . PMID  17835457 .   ^  Cosmides, Leda; John Tooby (1996). ""Are humans good intuitive statisticians after all? Rethinking some conclusions of the literature on judgment under uncertainty"". Cognition . 58 : 1–73. CiteSeerX  10.1.1.131.8290 . doi : 10.1016/0010-0277(95)00664-8 .   ^ a  b  Gigerenzer, G.; Hoffrage, U. (1995). ""How to improve Bayesian reasoning without instruction: Frequency formats"". Psychological Review . 102 (4): 684. CiteSeerX  10.1.1.128.3201 . doi : 10.1037/0033-295X.102.4.684 .   ^  Hoffrage, U.; Lindsey, S.; Hertwig, R.; Gigerenzer, G. (2000). ""Medicine: Communicating Statistical Information"". Science . 290 (5500): 2261–2262. doi : 10.1126/science.290.5500.2261 . PMID  11188724 . S2CID  33050943 .   ^  Akl, E. A.; Oxman, A. D.; Herrin, J.; Vist, G. E.; Terrenato, I.; Sperati, F.; Costiniuk, C.; Blank, D.; Schünemann, H. (2011).  Schünemann, Holger (ed.). ""Using alternative statistical formats for presenting risks and risk reductions"" . The Cochrane Library (3): CD006776. doi : 10.1002/14651858.CD006776.pub2 . PMC  6464912 . PMID  21412897 .   ^ a  b  Sedlmeier, P.; Gigerenzer, G. (2001). ""Teaching Bayesian reasoning in less than two hours"" . Journal of Experimental Psychology: General . 130 (3): 380. doi : 10.1037/0096-3445.130.3.380 . hdl : 11858/00-001M-0000-0025-9504-E .   ^  Brase, G. L. (2009). ""Pictorial representations in statistical reasoning"". Applied Cognitive Psychology . 23 (3): 369–381. doi : 10.1002/acp.1460 . S2CID  18817707 .   ^  Edwards, A.; Elwyn, G.; Mulley, A. (2002). ""Explaining risks: Turning numerical data into meaningful pictures"" . BMJ . 324 (7341): 827–830. doi : 10.1136/bmj.324.7341.827 . PMC  1122766 . PMID  11934777 .   ^  Girotto, V.; Gonzalez, M. (2001). ""Solving probabilistic and statistical problems: A matter of information structure and question form"". Cognition . 78 (3): 247–276. doi : 10.1016/S0010-0277(00)00133-5 . PMID  11124351 .   ^ a  b  Hoffrage, U.; Gigerenzer, G.; Krauss, S.; Martignon, L. (2002). ""Representation facilitates reasoning: What natural frequencies are and what they are not"". Cognition . 84 (3): 343–352. doi : 10.1016/S0010-0277(02)00050-1 . PMID  12044739 .   ^  Gigerenzer, G.; Hoffrage, U. (1999). ""Overcoming difficulties in Bayesian reasoning: A reply to Lewis and Keren (1999) and Mellers and McGraw (1999)"" . Psychological Review . 106 (2): 425. doi : 10.1037/0033-295X.106.2.425 . hdl : 11858/00-001M-0000-0025-9CB4-8 .   ^  Kleiter, G. D. (1994). ""Natural Sampling: Rationality without Base Rates"". Contributions to Mathematical Psychology, Psychometrics, and Methodology . Recent Research in Psychology. pp. 375–388. doi : 10.1007/978-1-4612-4308-3_27 . ISBN  978-0-387-94169-1 .    External links [ edit ]  The Base Rate Fallacy The Fallacy Files  v t e Biases Cognitive biases  Actor–observer  Acquiescence  Ambiguity  Anchoring  Attentional  Attribution  Authority  Automation  Belief  Blind spot  Choice-supportive  Confirmation  Congruence  Cultural  Distinction  Dunning–Kruger  Egocentric  Emotional  Extrinsic incentives  Fading affect  Framing  Correspondence  Halo effect  Hindsight  Horn effect  Hostile attribution  Impact  Implicit  In-group  Mere-exposure effect  Negativity  Normalcy  Omission  Optimism  Out-group homogeneity  Outcome  Overton window  Precision  Present  Pro-innovation  Response  Restraint  Self-serving  Social comparison  Status quo  Time-saving  Trait ascription  von Restorff effect  Zero-risk  In animals  Statistical biases  Estimator  Forecast  Healthy user  Information  Psychological  Lead time  Length time  Non-response  Observer  Omitted-variable  Participation  Recall  Sampling  Selection  Self-selection  Social desirability  Spectrum  Survivorship  Systematic error  Systemic  Verification  Wet  Other biases  Academic  Funding  FUTON  Inductive  Infrastructure  Inherent  In education  Media  False balance  Vietnam War  Norway  South Asia  Sweden  United States  Arab–Israeli conflict  Ukraine  Net  Political bias  Publication  Reporting  White hat  Bias reduction  Cognitive bias mitigation  Debiasing  Heuristics in judgment and decision-making  Lists: General  ·  Memory  v t e Fallacies ( list ) Formal In propositional logic  Affirming a disjunct  Affirming the consequent  Denying the antecedent  Argument from fallacy  In quantificational logic  Existential  Illicit conversion  Proof by example  Quantifier shift  Syllogistic fallacy  Affirmative conclusion from a negative premise  Exclusive premises  Existential  Necessity  Four terms  Illicit major  Illicit minor  Negative conclusion from affirmative premises  Undistributed middle   Masked man  Mathematical fallacy  Informal Equivocation  Equivocation  False equivalence  False attribution  Quoting out of context  Loki's Wager  No true Scotsman  Reification  Question-begging fallacies  Circular reasoning / Begging the question  Loaded language  Leading question  Compound question / Loaded question / Complex question  No true Scotsman  Correlative-based fallacies  False dilemma  Perfect solution  Denying the correlative  Suppressed correlative  Illicit transference  Composition  Division  Ecological  Secundum quid  Accident  Converse accident  Faulty generalization  Anecdotal evidence  Sampling bias  Cherry picking  McNamara  Base rate / Conjunction  Double counting  False analogy  Slothful induction  Overwhelming exception  Vagueness / Ambiguity  Accent  False precision  Moving the goalposts  Quoting out of context  Slippery slope  Sorites paradox  Syntactic ambiguity  Questionable cause  Animistic  Furtive  Correlation implies causation Cum hoc  Post hoc  Gambler's  Inverse  Regression  Single cause  Slippery slope  Texas sharpshooter  Fallacies of relevance Appeals to emotion  Fear  Flattery  Novelty  Pity  Ridicule  Think of the children  In-group favoritism  Invented here / Not invented here  Island mentality  Loyalty  Parade of horribles  Spite  Stirring symbols  Wisdom of repugnance  Genetic fallacies Ad hominem  Appeal to motive  Association  Reductio ad Hitlerum  Godwin's law  Reductio ad Stalinum  Bulverism  Poisoning the well  Tone  Tu quoque  Whataboutism   Authority  Accomplishment  Ipse dixit  Poverty / Wealth  Etymology  Nature  Tradition / Novelty  Chronological snobbery  Appeals to consequences  Argumentum ad baculum  Wishful thinking   Ad nauseam  Argument to moderation  Argumentum ad populum  Appeal to the stone / Proof by assertion  Ignoratio elenchi  Argument from silence  Invincible ignorance  Moralistic / Naturalistic  Motte-and-bailey fallacy  Rationalization  Red herring  Two wrongs make a right  Special pleading  Straw man  Cliché  I'm entitled to my opinion    Category       Retrieved from "" https://en.wikipedia.org/w/index.php?title=Base_rate_fallacy&oldid=963044273 ""  Categories : Relevance fallacies Cognitive biases Behavioral finance Probability fallacies Statistical paradoxes Hidden categories: Articles with short description         Navigation menu      Personal tools      Not logged in Talk Contributions Create account Log in        Namespaces      Article Talk        Variants              Views      Read Edit View history        More           Search                     Navigation      Main page Contents Current events Random article About Wikipedia Contact us Donate Wikipedia store       Contribute      Help Community portal Recent changes Upload file       Tools      What links here Related changes Upload file Special pages Permanent link Page information Wikidata item Cite this page       Print/export      Download as PDF Printable version       Languages      العربية Deutsch Español Français עברית Polski Русский ไทย Türkçe Українська 中文  Edit links        This page was last edited on 17 June 2020, at 13:34 (UTC) .  Text is available under the Creative Commons Attribution-ShareAlike License ;
additional terms may apply.  By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc. , a non-profit organization.    Privacy policy  About Wikipedia  Disclaimers  Contact Wikipedia  Developers  Statistics  Cookie statement  Mobile view          "
19,conservatism(fallacy OR bias OR heuristic) (coronavirus OR covid OR pandemic),1,https://www.nytimes.com/2020/03/31/opinion/covid-conservatism.html,"Where does this leave the theories of conservative and liberal minds? It’s too much to say that they don’t describe anything real. A certain kind of conservative personality (a kind that includes more than a few of my own friends) really did seem particularly well attuned to this crisis and ended up out ahead of the conventional wisdom in exactly the way that you would expect a mind-set attuned to risk and danger, shot through with pessimism and inclined to in-group loyalty to be.

At the same time, the behavior of what you might call “normie” Republicans — not Very Online right-wingers or MAGA populists but longtime Fox News and talk-radio consumers — suggests that any such conservative mind-set is easily confounded by other factors, partisanship chief among them. The fact that the virus seemed poised to help Democrats and hurt the Trump administration, the fact that it was being hyped by CNN and played down by Hannity, the fact that Trump himself declined to take it seriously — all of this mattered more to many Republicans than the fear of foreign contamination that the virus theoretically should have activated or the ways in which its progress seemed to confirm certain right-wing priors.

So one might say that the pandemic illustrates the power of partisan mood affiliation over any kind of deeper ideological mind-set. Or relatedly, it illustrates the ways in which under the right circumstances, people can easily swing between different moral intuitions. (This holds for liberals as well as conservatives: A good liberal will be as deferential to authority as any conservative when the authority has the right academic degrees, and as zealous about purity and contamination when it’s their own neighborhood that’s threatened.)

But the right’s varying responses to the pandemic also illustrate two further points. The first point is that what we call “American conservatism” is probably more ideologically and psychologically heterogeneous than the conservative mind-set that social scientists aspire to measure and pin down. In particular, it includes an incredibly powerful streak of what you might call folk libertarianism — which comes in both highbrow and middlebrow forms, encompassing both famous legal scholars predicting minimal fatalities from their armchairs and “you can’t stop the American economy … for anything” tough guys attacking social distancing on Twitter.

This mentality, with its reflexive Ayn Randism and its Panglossian hyper-individualism, is definitely essential to understanding part of the American right. But it’s very much an American thing unto itself, and I’m doubtful that it corresponds to any universal set of psychological tendencies that we could reasonably call conservative.

The second point is that on the fringes of the right, among QAnon devotees and believers in the satanic depravity of liberalism, the only psychology that matters is paranoia, not conservatism. And their minimizing response to the coronavirus illustrates the unwillingness of the conspiratorial mind to ever take yes for an answer — meaning that even true events that seem to vindicate a somewhat paranoid worldview will be dismissed as not true enough, not the deepest truth, not the Grandest of All Grand Conspiracies that will someday (someday) be unraveled.

[Listen to “The Argument” podcast every Thursday morning, with Ross Douthat, Michelle Goldberg and David Leonhardt.]","   Sections SEARCH Skip to content Skip to site index Today’s Paper Opinion | The Coronavirus and the Conservative Mind https://nyti.ms/2UM3o3W Advertisement Continue reading the main story Opinion Supported by Continue reading the main story The Coronavirus and the Conservative Mind The pandemic has put psychological theories of politics to a very interesting test. By  Ross Douthat Opinion Columnist March 31, 2020 A recent  coronavirus briefing by President Trump was broadcast by  Fox News. Credit... Doug Mills/The New York Times Over the past two decades, as conservatives and liberals have drifted ever farther from each other, an influential body of literature has attempted to psychologize the partisan divide — to identify conservative and liberal personality types, right-wing or left-wing minds or brains, and to vindicate the claim of the noted political scientists Gilbert and Sullivan, That every boy and every gal / That’s born into the world alive. / Is either a little Liberal / Or else a little Conservative. In its crudest form this literature just amounts to liberal self-congratulation , with survey questions and regression analyses deployed to “prove” with “science” that liberals are broad-minded freethinkers and conservatives are cramped authoritarians. But there have been more sophisticated and sympathetic efforts, too, like the influential work of New York University’s Jonathan Haidt on the “moral foundations” of politics: Haidt argues that conservatives actually have more diverse moral intuitions than liberals, encompassing categories like purity and loyalty as well as care and fairness, and that the right-wing mind therefore sometimes understands the left-wing mind better than vice versa. Both the crude and sophisticated efforts tended to agree, though, that the supposed conservative mind is more attuned to external threat and internal contamination, more inclined to support authority and hierarchy, and fear subversion and dissent. And so the political responses to the pandemic have put these psychological theories to a very interesting test. In the coronavirus, America confronts a contaminating force (a deadly disease) that originated in our leading geopolitical rival (an external threat) and that plainly requires a strong, even authoritarian government response. If there was ever a crisis tailored to the conservative mind-set, surely it would be this one, with the main peril being that conservatives would wildly overreact to such a trigger. So what has happened? Well, several different things. From the Wuhan outbreak through somewhere in mid-February, the responses to the coronavirus did seem to correspond — very roughly — to theories of conservative and liberal psychology. Along with infectious-disease specialists, the people who seemed most alarmed by the virus included the inhabitants of Weird Right-Wing Twitter (a collection of mordant, mostly anonymous  accounts interested in civilizational decline), various Silicon Valley eccentrics, plus original-MAGA figures like Mike Cernovich and Steve Bannon. (The radio host Michael Savage, often considered the most extreme of the right’s talkers, was also an early alarmist.) Meanwhile, liberal officialdom and its media appendages were more likely to play down the threat, out of fear of giving aid and comfort to sinophobia or populism. This period was the high-water mark of “it’s just the flu” reassurances in liberal outlets, of pious critiques of Donald Trump’s travel restrictions, of deceptive public-health propaganda about how masks don’t work, of lectures from the head of the World Health Organization about how “the greatest enemy we face is not the virus itself; it’s the stigma that turns us against each other.” But then, somewhere in February, the dynamic shifted. As the disease spread and the debate went mainstream, liberal opinion mostly abandoned its anti-quarantine posture and swung toward a reasonable panic, while conservative opinion divided, with a large portion of the right following the lead of Trump himself, who spent crucial weeks trying to wish the crisis away. Where figures like Bannon and Cernovich manifested a conservatism attuned to external perils, figures like Rush Limbaugh and Sean Hannity manifested a conservatism of tribal denial, owning the libs by minimizing the coronavirus threat. Now we are in a third phase, where Trump is (more or less, depending on the day) on board with a robust response and most conservatives have joined most liberals in alarm. Polls show a minimal partisan divide in support for social distancing and lockdowns, and some of that minimal divide is explained by the fact that rural areas are thus far less likely to face outbreaks. (You don’t need a complicated theory of the ideological mind to explain why New Yorkers are more freaked out than Nebraskans.) But even now, there remains a current of conservative opinion that wants to believe that all of this is overblown, that the experts are wrong about the likely death toll, that Trump should reopen everything as soon as possible, that the liberal media just wants to crash the American economy to take his presidency down. Where does this leave the theories of conservative and liberal minds? It’s too much to say that they don’t describe anything real. A certain kind of conservative personality (a kind that includes more than a few of my own friends) really did seem particularly well attuned to this crisis and ended up out ahead of the conventional wisdom in exactly the way that you would expect a mind-set attuned to risk and danger, shot through with pessimism and inclined to in-group loyalty to be. At the same time, the behavior of what you might call “normie” Republicans — not Very Online right-wingers or MAGA populists but longtime Fox News and talk-radio consumers — suggests that any such conservative mind-set is easily confounded by other factors, partisanship chief among them. The fact that the virus seemed poised to help Democrats and hurt the Trump administration, the fact that it was being hyped by CNN and played down by Hannity, the fact that Trump himself declined to take it seriously — all of this mattered more to many Republicans than the fear of foreign contamination that the virus theoretically should have activated or the ways in which its progress seemed to confirm certain right-wing priors. So one might say that the pandemic illustrates the power of partisan mood affiliation over any kind of deeper ideological mind-set. Or relatedly, it illustrates the ways in which under the right circumstances, people can easily swing between different moral intuitions. (This holds for liberals as well as conservatives: A good liberal will be as deferential to authority as any conservative when the authority has the right academic degrees, and as zealous about purity and contamination when it’s their own neighborhood that’s threatened.) But the right’s varying responses to the pandemic also illustrate two further points. The first point is that what we call “American conservatism” is probably more ideologically and psychologically heterogeneous than the conservative mind-set that social scientists aspire to measure and pin down. In particular, it includes an incredibly powerful streak of what you might call folk libertarianism — which comes in both highbrow and middlebrow forms, encompassing both famous legal scholars predicting minimal fatalities from their armchairs and “you can’t stop the American economy … for anything ” tough guys attacking social distancing on Twitter. This mentality, with its reflexive Ayn Randism and its Panglossian hyper-individualism, is definitely essential to understanding part of the American right. But it’s very much an American thing unto itself, and I’m doubtful that it corresponds to any universal set of psychological tendencies that we could reasonably call conservative. The second point is that on the fringes of the right, among QAnon devotees and believers in the satanic depravity of liberalism, the only psychology that matters is paranoia, not conservatism. And their minimizing response to the coronavirus illustrates the unwillingness of the conspiratorial mind to ever take yes for an answer — meaning that even true events that seem to vindicate a somewhat paranoid worldview will be dismissed as not true enough, not the deepest truth, not the Grandest of All Grand Conspiracies that will someday (someday) be unraveled. [ Listen to “The Argument” podcast every Thursday morning, with Ross Douthat, Michelle Goldberg and David Leonhardt. ] In his novel “Foucault’s Pendulum,” a sendup of crackpot esotericism that anticipated “The Da Vinci Code” years before its publication, Umberto Eco captured this spirit by describing the way that self-conscious seekers after hermetic wisdom and gnostic mysteries approached the rise of Christianity: … someone had just arrived and declared himself the Son of God, the Son of God made flesh, to redeem the sins of the world. Was that a run-of-the-mill mystery? And he promised salvation to all: you only had to love your neighbor. Was that a trivial secret? And he bequeathed the idea that whoever uttered the right words at the right time could turn a chunk of bread and a half-glass of wine into the body and blood of the Son of God, and be nourished by it. Was that a paltry riddle? … And yet they, who now had salvation within their grasp — do-it-yourself salvation — turned deaf ears. Is that all there is to it? How trite. And they kept on scouring the Mediterranean in their boats, looking for a lost knowledge of which those thirty-denarii dogmas were but the superficial veil, the parable for the poor in spirit, the allusive hieroglyph, the wink of the eye at the pneumatics. The mystery of the Trinity? Too simple: there had to be more to it. This is where the pandemic-minimizing sort of conservative has ended up. They are confronted with a world crisis tailor-made for an anti-globalization, anti-deep-state worldview — a crisis in which China lit the fuse, the World Health Organization ran interference for Beijing, the American public health bureaucracy botched its one essential job, pious anti-racism inhibited an early public-health response, and outsourcing and offshoring left our economy exposed. And their response? Too simple: Just a feint, a false flag, another deep state plot or power grab, another hoax to take down Trump. It can’t be real unless Hillary Clinton is somehow at the bottom of it. The Times is committed to publishing a diversity of letters to the editor. We’d like to hear what you think about this or any of our articles. Here are some tips . And here’s our email: letters@nytimes.com . Follow The New York Times Opinion section on Facebook , Twitter (@NYTOpinion) and Instagram , join the Facebook political discussion group, Voting While Female . Advertisement Continue reading the main story Site Index Site Information Navigation © 2020  The New York Times Company NYTCo Contact Us Work with us Advertise T Brand Studio Your Ad Choices Privacy Terms of Service Terms of Sale Site Map Help Subscriptions                 "
19,conservatism(fallacy OR bias OR heuristic) (coronavirus OR covid OR pandemic),2,https://www.bbc.com/future/article/20200505-why-its-so-hard-to-be-rational-about-covid-19,"The world’s best scientists are currently deployed in a war-like effort to counter the coronavirus pandemic, devising vaccines, treatments, modelling outcomes and advising the rest of us. This is a fast-moving contagion, borne of our 21st-Century globalised society, and it calls for the very latest evidence-based science. On this, we all agree, because we’re rational 21st-Century people, right?

Only up to a point. Surveys of the American public reveal that attitudes towards the same fatal virus in the same nation are strongly influenced by partisan voting patterns. Republican voters are generally less concerned than Democrats about Covid-19, and less likely to support public lockdown measures to prevent spread of the coronavirus, which at the time of publishing had infected over a million Americans and killed more than 69,000 people there.

Since Covid-19 is an infectious disease, it depends entirely on human hosts to carry and spread it – the more people act as regular socialising humans, the more chances the virus has to replicate and spread, and the worse the epidemic. That’s the science. Only by recognising the threat of the disease, will people be mobilised to change their innate social behaviours, to actively slow its spread. However, while scientific and medical experts in the US and around the world alert the public to the risks, and reiterate the importance of social distancing, several global leaders, with no scientific training, have spent months playing down the risks.

Protective movement restrictions have nevertheless been ordered by most state governments, with resulting business shutdowns and record unemployment. In response, more than a dozen states have seen anti-lockdown protests, as thousands of conservative and far-right Americans demand the restrictions are lifted, in spite of the health costs. In Michigan and Washington, pro-Trump, gun-toting protestors called for “liberty” from the “tyranny” of state governors. In their support, President Donald Trump tweeted “LIBERATE MICHIGAN!” and “LIBERATE VIRGINIA!”, and described the protesters as people who “love our country”.

You might also like:

Last week, hundreds of protestors stormed the Michigan Capitol and threatened the Governor, who had extended the state’s stay-at-home order by two weeks up to 15 May. Michigan has been one of the state’s hardest hit by the virus and is still experiencing more than 100 daily deaths. On 1 May, the day after this Capitol protest, during which demonstrators called the Michigan Governor a tyrant and compared her to Hitler, Trump described those same protestors as “very good people”. Meanwhile, all over the US, anti-lockdown protests continue.","      Homepage Accessibility links Skip to content Accessibility Help  BBC Account Notifications  Home News Sport Weather iPlayer Sounds CBBC CBeebies Food Bitesize Arts Taster Local Three Menu Search Search the BBC Search the BBC    Menu  Loading  Comment & Analysis  Why it’s so hard to be rational about Covid-19  Share using Email Share on Twitter Share on Facebook Share on Linkedin Bookmark this article By Gaia Vince  6th May 2020   Attitudes towards lockdown are proving divisive in countries like the US – and those divisions are falling down familiar party lines. But why does partisanship shape our compliance with public health campaigns?  T The world’s best scientists are currently deployed in a war-like effort to counter the coronavirus pandemic, devising vaccines, treatments, modelling outcomes and advising the rest of us. This is a fast-moving contagion, borne of our 21st-Century globalised society, and it calls for the very latest evidence-based science. On this, we all agree, because we’re rational 21st-Century people, right?  Only up to a point. Surveys of the American public reveal that attitudes towards the same fatal virus in the same nation are strongly influenced by partisan voting patterns . Republican voters are generally less concerned than Democrats about Covid-19, and less likely to support public lockdown measures to prevent spread of the coronavirus, which at the time of publishing had infected over a million Americans and killed more than 69,000 people there.  Since Covid-19 is an infectious disease, it depends entirely on human hosts to carry and spread it – the more people act as regular socialising humans, the more chances the virus has to replicate and spread, and the worse the epidemic. That’s the science. Only by recognising the threat of the disease, will people be mobilised to change their innate social behaviours, to actively slow its spread. However, while scientific and medical experts in the US and around the world alert the public to the risks, and reiterate the importance of social distancing, several global leaders, with no scientific training, have spent months playing down the risks.  Protective movement restrictions have nevertheless been ordered by most state governments, with resulting business shutdowns and record unemployment. In response, more than a dozen states have seen anti-lockdown protests, as thousands of conservative and far-right Americans demand the restrictions are lifted, in spite of the health costs . In Michigan and Washington, pro-Trump, gun-toting protestors called for “liberty” from the “tyranny” of state governors. In their support, President Donald Trump tweeted “LIBERATE MICHIGAN!” and “LIBERATE VIRGINIA!”, and described the protesters as people who “love our country”.  You might also like:   The '3.5% rule': How a small minority can change the world  How the views of a few can determine a country’s fate  How robots are coming for your vote   Last week, hundreds of protestors stormed the Michigan Capitol and threatened the Governor, who had extended the state’s stay-at-home order by two weeks up to 15 May. Michigan has been one of the state’s hardest hit by the virus and is still experiencing more than 100 daily deaths. On 1 May, the day after this Capitol protest, during which demonstrators called the Michigan Governor a tyrant and compared her to Hitler, Trump described those same protestors as “very good people”. Meanwhile, all over the US, anti-lockdown protests continue . Political beliefs and levels of concern about the outbreak are closely tied together (Credit: Getty Images) These protests, which contradict health advice to reduce transmission rates of the virus, come at a time of widely circulated conspiracy theories about the virus , including that it is a hoax ( believed by 13% of Americans polled ), or that the virus was deliberately created in a Chinese weapons lab (a claim believed by as much as half the population), and that 5G wireless technology somehow spreads the virus. Such theories have been boosted and spread by a handful of prominent conservative politicians and far-right activists, including Republican Senator Tom Cotton. And research shows that even smart people can believe such conspiracy theories , if they’re couched in the right language .  “If you have the kind of hyper-partisanship we have in the United States, it’s like a dry forest and all it’ll take is one match to light it and cause a problem,” says Jay van Bavel, associate professor of psychology at New York University. “That’s what we’ve seen in the last few months in the US, where Trump didn’t take the virus seriously at first, and the right-wing media sphere – Fox News and talk radio – downplayed the threat of the pandemic for a long time to protect his electoral chances. So then you have the recipe for differences in beliefs.”  As polls have shown, as far back as February, Americans’ attitudes to Covid-19 risk are closely tied to voting behaviour , with Republicans showing much less concern about the outbreak.  Tribal culture affects how people see the world more than facts do. Take human-caused climate change, for which there is near-unanimous global scientific consensus. This, too, divides Americans, but in an unlikely way: the more education that Democrats and Republicans have, the more their beliefs in climate change diverge. Of Republicans with only a high school education, 23% report being very worried about climate change. But among college-educated Republicans, that figure was just 8% .  This may seem counter-intuitive, because better-educated Republicans are more likely to be aware of the scientific consensus. But in the realm of public opinion, climate change isn’t a scientific issue, it’s a political one. Climate change science is relatively new and technically complicated, and many Americans adopt the opinions of their tribal leaders: the political elites. Even though better-educated Republicans may have more exposure to information about the science around climate change, they also have more exposure to partisan messages about it, and this matters more. Most protestors are concerned about the economic effects of an extended period of lockdown (Credit: Getty Images) “We’ve had three years of Americans arguing about different perceptions around facts: say, the size of crowds at Trump’s inauguration versus Obama’s inauguration. It’s easy to laugh that off, because it doesn’t have any consequences. But now we have a virus that imposes enormous risks to people’s health,” van Bavel says. “And the risks have non-partisan consequences because most people have a family member or work with someone who’s from a different political party. If they get exposed to the virus and contract the disease, they put you at risk. So there is a very strong reason to try to figure out a way to solve this.”  Since we have culturally evolved to acquire our knowledge and beliefs primarily through high-fidelity copying of others rather than by invention (by looking at the evidence and deciding for ourselves), we are vulnerable to this problem of copying unreliable models. Worse still, because we have culturally learned to value rational explanations over subjective ones for scientific issues, we can be manipulated into believing the opinions we copy are rational, so it is harder to change them.  Despite our culturally evolved norms for rationality and evidence-based decision making, our biological evolution has not caught up and our cognition continues to be emotionally led. The problem is not necessarily that we use the emotive part of our brain more than the rational in decision making, but that we are self-delusional. Even experts are prone to biases and these mean costly mistakes are made, and irrational prejudices are systemic in organisations where people believe themselves to be non-racist, non-sexist and to hold the positions they do through skill rather than luck .  Often, the main role of reasoning in decision making is actually not to arrive at the decision but to be able to present the decision as something that’s rational. Some psychologists believe we only use reason to retrospectively justify our decisions, and largely rely on unquestioned instincts to make choices. It may be that our unconscious instincts – despite our cognitive biases and prejudices – are more capable of rationality than our logical thought-processing minds. Few of us are able to fully separate our subjective and objective reasoning during decision making – this is one of the promises of artificial intelligence. Nurses in PPE have started standing in front of crowds during recent rallies in the US as a way of challenging their protests (Credit: Getty Images) Our decision making is influenced by our biology and our social environment. Take the psychological and physiological influence of fear: it’s been shown that people who vote more conservatively tend to have a bigger amygdala , the brain’s fear centre. In one study, the more fear a three- or four-year-old showed during a lab study , the more conservative their political attitude was found to be 20 years later .  The impact of fear is instant: when people with liberal attitudes experienced physical threat, during a study, their political and social attitudes became more conservative , temporarily. Conservative politicians and electioneering exploit this, aiming to raise voters’ fears of immigration by comparing immigrants to germs, for example, which targets our deep, biologically evolved motivations to avoid contamination and disease . In one study, during an H1N1 flu epidemic, researchers reminded people of the dangers of the flu virus and then asked them their attitudes towards immigration, after which they were asked whether they had been vaccinated against flu yet. Those who hadn’t received their anti-flu shot were more likely to be anti-immigration than the ones who felt less threatened .  But in a follow-up study, the researchers offered people a squirt of hand sanitiser straight after the flu warning. The immigration bias went away. Making people feel safe changes their voting decision to more liberal . When researchers asked people to imagine themselves completely invulnerable to any harm, Republican voters became significantly more liberal in social attitudes to issues like abortion and immigration. Reason is suffused with emotion.  The social implications of most decisions are also important factors in decision making. In very partisan situations, people who disobey the social norm by voting against the group majority risk ostracism. In such cases, therefore, it may be more rational for the individual to go against the evidence because we are motivated more by social cohesion and maintaining support networks than being objectively right. The social situation in which we find ourselves has an important bearing on the decisions we make (Credit: Getty Images) Whatever your political persuasion, the Covid-19 virus will not discriminate as it seeks more lungs to infect. But, because contagion is inherently social, it may well turn out that those populations who continue to socialise undeterred might end up experiencing worse epidemics. In other words, your voting record may well influence your fate.  Needless to say, these overall trends linking political leanings and attitudes towards coronavirus are not the whole story. Rand Paul, junior Senator from Kentucky, for example, has been volunteering in a hospital to help patients during the crisis, including those with coronavirus, after he had the disease himself.  And there are signs things are changing. As Republicans get exposed to people they know affected by the virus, they are taking the threat more seriously – something known as the “reality constraint”. “People’s motivations for partisanship start to get outweighed by the value of being accurate and being healthy for themselves and their family,” says van Bavel.  The most recent poll shows that over 95% of Democrats support social distancing measures, and a large majority of of Republicans do too – over 80% – so the gap is narrowing. Perhaps, then, it is unsurprising that Trump’s approval rating has declined over the same time that approval ratings have risen for state governors who have shown leadership in responding to the virus.  Gaia Vince discusses these ideas in her book TRANSCENDENCE: How humans evolved through fire, language, beauty and time. She is @WanderingGaia on Twitter.  --  As an award-winning science site, BBC Future is committed to bringing you evidence-based analysis and myth-busting stories around the new coronavirus. You can read more of our Covid-19 coverage here .  --  Join one million Future fans by liking us on Facebook , or follow us on Twitter or Instagram .  If you liked this story, sign up for the weekly bbc.com features newsletter , called “The Essential List”. A handpicked selection of stories from BBC Future , Culture , Worklife , and Travel , delivered to your inbox every Friday. Share using Email Share on Twitter Share on Facebook Share on Linkedin Bookmark this article Share Around the BBC         Explore the BBC Home News Sport Weather iPlayer Sounds CBBC CBeebies Food Bitesize Arts Taster Local Three  Terms of Use About the BBC Privacy Policy Cookies Accessibility Help Parental Guidance Contact the BBC Get Personalised Newsletters Copyright © 2020 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking."
20,(endowment effect or status quo or sunk cost)(fallacy OR bias OR heuristic) (coronavirus OR covid OR pandemic),1,https://pubs.aeaweb.org/doi/pdfplus/10.1257/jep.5.1.193,,"   An Error Occurred Setting Your User Cookie  This site uses cookies to improve performance. If your browser does not accept cookies, you cannot view this site.  Setting Your Browser to Accept Cookies  There are many reasons why a cookie could not be set correctly. Below are the most common reasons:   You have cookies disabled in your browser. You need to reset your browser to accept cookies or to ask you if you want to accept cookies.  Your browser asks you whether you want to accept cookies and you declined.
            To accept cookies from this site, use the Back button and accept the cookie.  Your browser does not support cookies. Try a different browser if you suspect this.  The date on your computer is in the past. If your computer's clock shows a date before 1 Jan 1970,
            the browser will automatically forget the cookie. To fix this, set the correct time and date on your computer.  You have installed an application that monitors or blocks cookies from being set.
            You must disable the application while logging in or check with your system administrator.   Why Does this Site Require Cookies?  This site uses cookies to improve performance by remembering that you are logged in when you go from page to page. To provide access without cookies
        would require the site to create a new session for every page you visit, which slows the system down to an unacceptable level.  What Gets Stored in a Cookie?  This site stores nothing other than an automatically generated session ID in the cookie; no other information is captured.  In general, only the information that you provide, or the choices you make while visiting a web site, can be stored in a cookie. For example, the site
        cannot determine your email name unless you choose to type it. Allowing a website to create a cookie does not give that or any other site access to the
        rest of your computer, and only the site that created the cookie can read it.  "
20,(endowment effect or status quo or sunk cost)(fallacy OR bias OR heuristic) (coronavirus OR covid OR pandemic),2,https://www.aeaweb.org/articles?id=10.1257/jep.5.1.193,"Abstract A wine-loving economist we know purchased some nice Bordeaux wines years ago at low prices. The wines have greatly appreciated in value, so that a bottle that cost only $10 when purchased would now fetch $200 at auction. This economist now drinks some of this wine occasionally, but would neither be willing to sell the wine at the auction price nor buy an additional bottle at that price. Thaler (1980) called this pattern—the fact that people often demand much more to give up an object than they would be willing to pay to acquire it—the endowment effect. The example also illustrates what Samuelson and Zeckhauser (1988) call a status quo bias, a preference for the current state that biases the economist against both buying and selling his wine. These anomalies are a manifestation of an asymmetry of value that Kahneman and Tversky (1984) call loss aversion—the disutility of giving up an object is greater that the utility associated with acquiring it. This column documents the evidence supporting endowment effects and status quo biases, and discusses their relation to loss aversion.

Citation Kahneman, Daniel, Jack L. Knetsch, and Richard H. Thaler. 1991. ""Anomalies: The Endowment Effect, Loss Aversion, and Status Quo Bias."" Journal of Economic Perspectives , 5 (1): 193-206 . DOI: 10.1257/jep.5.1.193 Choose Format: BibTeX EndNote Refer/BibIX RIS Tab-Delimited","   The AEA is providing open access to all journal content on the AEA website through August 2020 to overcome any difficulties some may have accessing library subscriptions during these challenging times. The AEA is providing open access to all journal content on the AEA website through August 2020 to overcome any difficulties some may have accessing library subscriptions during these challenging times.                   Menu              Journals    Annual Meeting    Careers    Resources    EconLit    EconSpark          Membership    About AEA    Log In                  Home Journals Journal of Economic Perspectives Winter 1991 Anomalies: The Endowment Effect, Loss Aversion, and Status Quo Bias     Anomalies: The Endowment Effect, Loss Aversion, and Status Quo Bias   Daniel Kahneman  Jack L. Knetsch  Richard H. Thaler    Journal of Economic Perspectives    vol. 5,
					no. 1, 					Winter 1991    (pp. 193-206)    Download Full Text PDF (Complimentary)      Article Information    Comments ( 0 )      Abstract A wine-loving economist we know purchased some nice Bordeaux wines years ago at low prices. The wines have greatly appreciated in value, so that a bottle that cost only $10 when purchased would now fetch $200 at auction. This economist now drinks some of this wine occasionally, but would neither be willing to sell the wine at the auction price nor buy an additional bottle at that price. Thaler (1980) called this pattern—the fact that people often demand much more to give up an object than they would be willing to pay to acquire it—the endowment effect. The example also illustrates what Samuelson and Zeckhauser (1988) call a status quo bias, a preference for the current state that biases the economist against both buying and selling his wine. These anomalies are a manifestation of an asymmetry of value that Kahneman and Tversky (1984) call loss aversion—the disutility of giving up an object is greater that the utility associated with acquiring it. This column documents the evidence supporting endowment effects and status quo biases, and discusses their relation to loss aversion.   Citation  Kahneman, Daniel, Jack L. Knetsch, and Richard H. Thaler. 1991.  ""Anomalies: The Endowment Effect, Loss Aversion, and Status Quo Bias.""  Journal of Economic Perspectives , 5 (1):
					 193-206 .   DOI: 10.1257/jep.5.1.193        Choose Format:  BibTeX  EndNote  Refer/BibIX  RIS  Tab-Delimited           JEL Classification    D11 Consumer Economics: Theory  D81 Criteria for Decision-Making under Risk and Uncertainty      There are no comments for this article.  Login to Comment           Journals American Economic Review AER: Insights AEJ: Applied Economics AEJ: Economic Policy AEJ: Macroeconomics AEJ: Microeconomics Journal of Economic Literature Journal of Economic Perspectives About JEP Issues Proposals Reading Recommendations JEP in the Classroom Contact the JEP AEA Papers & Proceedings Back Issues & SGML Metadata Subscriptions Get Journal Alerts Policies, Copyright, and Permissions Advertise in AEA Journals JSTOR access for AEA members Athens Subscriber Login           Journals    Annual Meeting    Careers    Resources    EconLit    EconSpark    Membership    About AEA    Log In    Contact the AEA      Find us on Facebook and Twitter:    @AEAJournals    @AEAJournals    @ASSAMeeting    @EconSpark    @JOE_listings      Copyright 2020 American Economic Association. All
    rights reserved.  Terms of Use & Privacy Policy   "
21,(gambler’s fallacy or hot hand)(fallacy OR bias OR heuristic) (coronavirus OR covid OR pandemic),1,https://www.hartfordfunds.com/investor-insight/financial-fallacies-explained.html,"When watching sports, particularly basketball, it is common to hear announcers and commentators talk about players having a ""hot hand."" And sports fans often assume that a particular player having a streak of successful shots has an increased probability of making the next shot, irrespective of the player's historical shooting percentage. However, studies have shown that a basketball player's probability of making a shot, conditional on him having made the previous shot, is not generally greater than usual. A paper published in Cognitive Psychology, found that the outcomes of consecutive basketball shots from professional NBA players are independent.1 Further, they even found evidence that consecutive shots from these NBA players are slightly negatively related. When people unjustifiably form expectations about what is going to happen next based upon what has just happened, this is commonly called the hot hand fallacy. Economists refer to this fallacy as extrapolation bias.

Despite evidence against the existence of the hot hand, research has repeatedly shown that individuals can be influenced by extrapolation bias. One study found that casino gamblers bet more after winning than after losing.2 In other words, they bet more after winning because they believed that their chance of winning again was greater than before. Another study showed that basketball betting markets do indeed believe in the hot hand, and that the hot hand fallacy is also at play when people think about buying and selling in financial markets.3

Economists and financial analysts assess financial markets using what economists call ""base rate information"" — full historical stock market data. Everyday investors do not usually have access to such information, and instead base their judgments about risk and return on ""singular information"" — data which is more recent or more easily obtained. Making decisions based only upon recent information compared to all of the available data can often lead investors to think current trends are the best predictors of what will happen next.

For example, during a bull market, people will expect stocks to continue to earn high returns. In a bear market, they will think low returns will continue. This is extrapolation bias at work. You have probably experienced this countless times with clients a client calls during a bull market and wants to buy, or they call during a downturn and are anxious to sell. As an advisor, it is important to recognize that extrapolation bias may be driving these clients to request exactly the opposite of what you would advise them to do.

So what should you do as an advisor? With respect to the hot hand fallacy, it is important to get clients to focus on the bigger market picture and not just what has been happening in markets recently. One tip would be to provide clients with a deeper historical perspective, using the base rate data. A few weeks, months, or quarters of statistics and price information may not be sufficient. Providing a fuller picture of the historical value of an asset can be helpful. In addition, reminding clients to plan and schedule rebalancing of their portfolios can also eliminate the opportunity for them want to buy or sell rashly based upon very recent market trends.

Financial advisors are generally not as susceptible to the hot hand fallacy with regard to financial markets. You are trained to monitor markets in order to provide the best possible advice to clients. However, this familiarity with historical market data can lead you to be influenced by another type of bias called the gambler's fallacy. The gambler's fallacy is the tendency to overweight the probability of an event because it has not recently occurred. The gambler's fallacy can lead individuals familiar with base rate market information to view long streaks as being unlikely. You may feel that a recent run-up in the price of a stock is not consistent with the actual historical distribution. Thus, you may be overly prone to predict reversals in stock prices.

Using brain mapping experiments, researchers have found neurological-based evidence of the gambler's fallacy. One study found that individuals evaluate random events such that they overweight the probability of an event, if it has not recently occurred.4 In field experiments, Croson and Sundali observed that casino roulette wheel gamblers bet in accordance with the gambler's fallacy.2 For example, if black had not come up in a while on the roulette wheel, bettors would think that black was due to come up and would bet more on black. Making assessments based on a limited sample size like this leads to the gambler's fallacy. As an advisor, when formulating investment advice, you need to keep the gambler's fallacy in mind so that you do not predict a trend reversal too quickly.

Both the hot hand fallacy and the gambler's fallacy belong to a group of biases that economists classify as ""representative heuristics."" (Heuristic is a fancy term for rule of thumb.) These heuristics involve making assessments based upon a small number of data points. Assuming that a small sample reflects the full distribution of outcomes in the same way that a large sample does, can lead to mistakes. In general, to avoid making these types of forecasting mistakes, it is important to take into account the size of the sample when drawing conclusions.

Key Takeaways","              financial-advisors                                         Account Access     Financial Professionals    Mutual fund accounts    SMART529 accounts    CHET Advisor accounts    Individual Investors    Mutual fund accounts    SMART529 accounts    CHET Advisor accounts       Contact Us            Pre-Sales Support  Mutual Funds and ETFs - 800-456-7526 Monday-Thursday: 8:00 a.m. – 6:00 p.m. ET Friday: 8:00 a.m. – 5:00 p.m. ET  ETF Trading Support - 415-315-6600 Monday-Friday: 9:30 a.m. – 5:00 p.m. ET   Post-Sales and Website Support 888-843-7824 Monday-Friday: 9:00 a.m. - 6:00 p.m. ET    PHONE US  MAIL US          Advisor Log In                                Products         MUTUAL FUNDS    Performance    Prices & Yields    Domestic Equity    International / Global Equity    Taxable Bond    Tax-Advantaged Bond    Multi-Strategy              ETFS    Performance    Prices & Yields    Multifactor ETFs    Active Fixed Income ETFs      INTERVAL FUND    MODEL PORTFOLIOS      COLLEGE SAVINGS    Hartford SMART529    CHET Advisor                       Morningstar Ratings  View our fund line-up                    World Bond Fund Monthly Positioning and Outlook  A visual tour of month-to-month shifts of the Hartford World Bond Fund                 Market Perspectives         MARKET PERSPECTIVES    Equity    Fixed Income    Global Macro Analysis    Strategic Beta and ETFs    ESG / Sustainable Investing          FEATURED INSIGHTS    Coronavirus Updates    Global Investment Strategy from Nanette Abuhoff Jacobson    Charts That Got Us Thinking    The Factor Report                       Coronavirus Updates  Learn More >                    Charts That Got Us Thinking  2Q 2020 Outlook Explore Now >                 Investor Insight         INVESTOR INSIGHT    Navigating Longevity    Investor Behavior          RESEARCH & PLANNING    Applied Insights Team    MIT AgeLab    COVID-19 Research    8,000 Days    5 Ways Tech link to 5 Ways Tech Can Make Social Distancing Easier?    Investor Behavior Strategies    Client Seminars    InvestIQ                       HUMAN-CENTRIC INVESTING PODCAST  EPISODE 42: ABCs Of Asking For Referrals  Listen Now >                      Applied Insights Team  Expert insights and resources to meet clients’ rising expectations                 Practice Management         PRACTICE MANAGEMENT    Prospecting    Efficiency    Business Model    Servicing Clients          ENGAGING CLIENTS    Client Conversations    Volatility Resource Center    Investment Ideas      ADVISOR FOCUS    Practice Management Strategies    Informed Advisor    Defined Contribution Insights                       VOLATILITY RESOURCE CENTER  Helping advisors navigate through market volatility                     The Dos and Don'ts of Effective Virtual Communication  Read Now >                 Advisor Resources         ADVISOR SUPPORT    Forms & Literature    Tax Center    Subscriptions lock    Product Updates    DST Vision    Webinars & Podcasts          ROLE-BASED    Financial Advisors    Institutional Investors    Retirement Specialists    RIA / Private Banks                       Strategic Beta ETFs Explained                 About Us         OUR CULTURE     Leadership Team    Human-Centric Investing    Sub-Adviser: Wellington Management    Sub-Adviser: Schroders              OUR FIRM    Press Center    Careers    Contact Us                           Human-centric Investing  A different approach to client relationships and investment management.                                    Products            MUTUAL FUNDS        Performance      Prices & Yields        Domestic Equity        International / Global Equity        Taxable Bond        Tax-Advantaged Bond        Multi-Strategy         ETFS        Performance      Prices & Yields        Multifactor ETFs        Active Fixed Income ETFs         INTERVAL FUND        MODEL PORTFOLIOS         COLLEGE SAVINGS        Hartford SMART529      CHET Advisor          Market Perspectives       MARKET PERSPECTIVES     Equity        Fixed Income        Global Macro Analysis        Strategic Beta and ETFs        ESG / Sustainable Investing       FEATURED INSIGHTS     Coronavirus Updates        Global Investment Strategy from Nanette Abuhoff Jacobson        Charts That Got Us Thinking        The Factor Report          Investor Insight       INVESTOR INSIGHT     Navigating Longevity        Investor Behavior       RESEARCH & PLANNING     Applied Insights Team        MIT AgeLab        COVID-19 Research      8,000 Days      5 Ways Tech link to 5 Ways Tech Can Make Social Distancing Easier?        Investor Behavior Strategies        Client Seminars        InvestIQ          Practice Management       PRACTICE MANAGEMENT     Prospecting        Efficiency        Business Model        Servicing Clients       ENGAGING CLIENTS     Client Conversations        Volatility Resource Center        Investment Ideas       ADVISOR FOCUS     Practice Management Strategies        Informed Advisor        Defined Contribution Insights          Advisor Resources       ADVISOR SUPPORT     Forms & Literature        Tax Center        Subscriptions lock        Product Updates        DST Vision        Webinars & Podcasts       ROLE-BASED     Financial Advisors        Institutional Investors        Retirement Specialists        RIA / Private Banks          About Us       OUR CULTURE      Leadership Team        Human-Centric Investing        Sub-Adviser: Wellington Management        Sub-Adviser: Schroders       OUR FIRM     Press Center        Careers        Contact Us         Contact Us            Pre-Sales Support 800.456.7526 Monday-Thursday: 8:00 a.m. – 6:00 p.m. ET Friday: 8:00 a.m. – 5:00 p.m. ET   Post-Sales and Website Support 888.843.7824 Monday-Thursday: 8:00 a.m. - 7:00 p.m.ET Friday: 8:00 a.m. - 6:00 p.m. ET    EMAIL US  MAIL US           Account Access        Financial Professionals     Mutual fund accounts    SMART529 accounts    CHET Advisor accounts     Individual Investors     Mutual fund accounts    SMART529 accounts    CHET Advisor accounts           Advisor Log In           Session Expired  Your session has expired. Please login again.   OK                 Home  >  Investor Insight >  Financial Fallacies Explained: The Hot Hand Fallacy and the Gambler’s Fallacy                    February 2019   Investor Behavior  Vicki Bogan       Financial Fallacies Explained: The Hot Hand Fallacy and the Gambler’s Fallacy          Making decisions based only upon recent information compared to all of the available data can often lead investors to think current trends are the best predictors of what will happen next.                        Dr. Vicki Bogan   Professor and Director of the Institute for Behavioral and Household Finance (IBHF) at Cornell University The mission of the IBHF is research and education in the areas of behavioral finance and household finance with the goal of better understanding and modeling financial behavior.                                                                                      When watching sports, particularly basketball, it is common to hear announcers and commentators talk about players having a ""hot hand."" And sports fans often assume that a particular player having a streak of successful shots has an increased probability of making the next shot, irrespective of the player's historical shooting percentage. However, studies have shown that a basketball player's probability of making a shot, conditional on him having made the previous shot, is not generally greater than usual. A paper published in Cognitive Psychology , found that the outcomes of consecutive basketball shots from professional NBA players are independent. 1 Further, they even found evidence that consecutive shots from these NBA players are slightly negatively related. When people unjustifiably form expectations about what is going to happen next based upon what has just happened, this is commonly called the hot hand fallacy. Economists refer to this fallacy as extrapolation bias.  Despite evidence against the existence of the hot hand, research has repeatedly shown that individuals can be influenced by extrapolation bias. One study found that casino gamblers bet more after winning than after losing. 2 In other words, they bet more after winning because they believed that their chance of winning again was greater than before. Another study showed that basketball betting markets do indeed believe in the hot hand, and that the hot hand fallacy is also at play when people think about buying and selling in financial markets. 3  Economists and financial analysts assess financial markets using what economists call ""base rate information"" — full historical stock market data. Everyday investors do not usually have access to such information, and instead base their judgments about risk and return on ""singular information"" — data which is more recent or more easily obtained. Making decisions based only upon recent information compared to all of the available data can often lead investors to think current trends are the best predictors of what will happen next.  For example, during a bull market, people will expect stocks to continue to earn high returns. In a bear market, they will think low returns will continue. This is extrapolation bias at work. You have probably experienced this countless times with clients a client calls during a bull market and wants to buy, or they call during a downturn and are anxious to sell. As an advisor, it is important to recognize that extrapolation bias may be driving these clients to request exactly the opposite of what you would advise them to do.  So what should you do as an advisor? With respect to the hot hand fallacy, it is important to get clients to focus on the bigger market picture and not just what has been happening in markets recently. One tip would be to provide clients with a deeper historical perspective, using the base rate data. A few weeks, months, or quarters of statistics and price information may not be sufficient. Providing a fuller picture of the historical value of an asset can be helpful. In addition, reminding clients to plan and schedule rebalancing of their portfolios can also eliminate the opportunity for them want to buy or sell rashly based upon very recent market trends.  Financial advisors are generally not as susceptible to the hot hand fallacy with regard to financial markets. You are trained to monitor markets in order to provide the best possible advice to clients. However, this familiarity with historical market data can lead you to be influenced by another type of bias called the gambler's fallacy. The gambler's fallacy is the tendency to overweight the probability of an event because it has not recently occurred. The gambler's fallacy can lead individuals familiar with base rate market information to view long streaks as being unlikely. You may feel that a recent run-up in the price of a stock is not consistent with the actual historical distribution. Thus, you may be overly prone to predict reversals in stock prices.  Using brain mapping experiments, researchers have found neurological-based evidence of the gambler's fallacy. One study found that individuals evaluate random events such that they overweight the probability of an event, if it has not recently occurred. 4 In field experiments, Croson and Sundali observed that casino roulette wheel gamblers bet in accordance with the gambler's fallacy. 2 For example, if black had not come up in a while on the roulette wheel, bettors would think that black was due to come up and would bet more on black. Making assessments based on a limited sample size like this leads to the gambler's fallacy. As an advisor, when formulating investment advice, you need to keep the gambler's fallacy in mind so that you do not predict a trend reversal too quickly.  Both the hot hand fallacy and the gambler's fallacy belong to a group of biases that economists classify as ""representative heuristics."" (Heuristic is a fancy term for rule of thumb.) These heuristics involve making assessments based upon a small number of data points. Assuming that a small sample reflects the full distribution of outcomes in the same way that a large sample does, can lead to mistakes. In general, to avoid making these types of forecasting mistakes, it is important to take into account the size of the sample when drawing conclusions.    Key Takeaways   The hot hand fallacy or extrapolation bias is the unwarranted extrapolation of past trends in forming forecasts.  The gambler's fallacy is the tendency to overweight the probability of an event because it has not recently occurred.  While clients are often more influenced by the hot hand fallacy, advisors need to be more aware of the gambler's fallacy and its effects on their advice.  With respect to the hot hand fallacy, when advising clients it is important to get them to focus on more complete market data and not just recent market trends.  When developing investment advice for a client, keeping the gambler's fallacy in mind can decrease the tendency to overweight the probability of an event because it has not recently occurred.          Read More >                  Readers Also Enjoyed:                  Helping Clients Through The Honeymoon Phase of Retirement                         Helping Clients with Senior Housing Options       Why clients procrastinate this discussion and how you can help                   Hardwired to React: Why Investors Make Mistakes That Could Have Been Avoided       How to Help Prevent  Your Inner Alarm System From Hurting Your Investment Results                            1 Gilovich, Thomas, Vallone, Robert, & Tversky, Amos. (1985). ""The Hot Hand in Basketball: On the Misperception of Random Sequences."" Cognitive Psychology, 17, pp. 295-314.   2 Croson, Rachel, & Sundali, James. (2005). ""The Gambler's Fallacy and the Hot Hand: Empirical Data from Casinos"" The Journal of Risk and Uncertainty, 30 (3), pp. 195-209.   3 Camerer, Colin F. (1989). ""Does the Basketball Market Believe in the 'Hot Hand'?"" The American Economic Review, 79 (5), pp. 1257-1261.   4 Xue, Gui, Lu, Zhonglin, Levin, Irwin P., & Bechara, Antoine. (2011). ""An fMRI Study of Risk-Taking Following Wins and Losses: Implications for the Gambler's Fallacy?"" Human Brain Mapping, 32, pp. 271-281.    The views and opinions expressed herein are those of the author, who is not affiliated with Hartford Funds. The information contained herein should not be construed as investment advice or a recommendation of any product or service nor should it be relied upon to, replace the advice of an investor's own professional legal, tax and financial advisors. Hartford Funds Distributors, LLC.  Hartford Funds is not responsible for, and does not validate, any information, opinions, assertions, or statements expressed within these articles, or the identity or credentials of the individuals communicating through the site. Some of the articles may contain links to information created and maintained by other, unaffiliated organizations and individuals. Hartford Funds does not control, cannot guarantee, and is not responsible for the completeness, accuracy, timeliness, or the continued availability or existence of this outside information or the information presented herein. This material is intended for use by financial professionals or in conjunction with the advice of a financial professional.    210449                       Products   Market Perspectives   Investor Insight   Practice Management   Advisor Resources   About Us   Contact Us    Press Center    Careers                Contact Us    Press Center    Careers    Tax Center    Privacy Policy    Legal Notices    Index Provider Notices    Accessibility Statement    Business Continuity    The Hartford                      Products   Market Perspectives   Investor Insight   Practice Management   Advisor Resources   About Us      The material on this site is for informational and educational purposes only. The material should not be considered tax or legal advice and is not to be relied on as a forecast. The material is also not a recommendation or advice regarding any particular security, strategy or product. Hartford Funds does not represent that any products or strategies discussed are suitable for any particular investor so investors should seek their own professional advice before investing. Hartford Funds does not serve as a fiduciary. Content is current as of the publication date or date indicated, and may be superseded by subsequent market and economic conditions.   Investing involves risk, including the possible loss of principal. Investors should carefully consider a fund's investment objectives, risks, charges and expenses. This and other important information is contained in the mutual fund , ETF or closed-end interval fund prospectus or summary prospectus, which can be obtained from a financial professional and should be read carefully before investing.     Tax Center    Privacy Policy    Legal Notices    Index Provider Notices    Accessibility Statement    Business Continuity    The Hartford        Mutual funds and the closed-end interval fund are distributed by Hartford Funds Distributors, LLC (HFD), Member FINRA / SIPC . Exchange-traded products are distributed by ALPS Distributors, Inc. (ALPS). Advisory services may be provided by Hartford Funds Management Company, LLC (HFMC) or its wholly owned subsidiary, Lattice Strategies LLC (Lattice). Certain funds are sub-advised by Wellington Management Company LLP and/or Schroder Investment Management North America Inc. Schroder Investment Management North America Ltd. serves as a secondary sub-adviser to certain funds. Hartford Funds refers to Hartford Funds Management Group, Inc. and its subsidiaries, including HFD, HFMC, and Lattice, which are not affiliated with any sub-adviser or ALPS. The funds and other products referred to on this Site may be offered and sold only to persons in the United States and its territories.  © Copyright 2020 Hartford Funds Management Group, Inc. All Rights Reserved. Not FDIC Insured | No Bank Guarantee | May Lose Value                             Onload Advanced Overlay open registrationOverlay open loginoverlay Onload Overlay Reset your password Role Selection"
